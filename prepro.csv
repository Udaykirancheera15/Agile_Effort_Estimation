text,storypoint
report executor terminations to framework schedulers the scheduler interface has a callback for executorlost but currently it is never called,2
mesos slave should cache executors the slave should be smarter about how it handles pulling down executors in our environment executors rarely change but the slave will always pull it down from regardless hdfs this puts undue stress on our hdfs clusters and is not resilient to reduced hdfs availability,5
expose task_failed reason to frameworks we now have a message string inside taskstatus that provides human readable information about task_failed it would be good to add some structure to the failure reasons for framework schedulers to act on programmatically eg enum taskfailure  executor_oom executor_out_of_disk executor_terminated slave_lost etc,8
balloon framework fails to run due to bad flags i suspect this has to do with the latest flags refactor vinodsmfdbkq03sr4 build sudo glog_v1 binmesostestssh gtest_filterballoon verbose warning logging before initgooglelogging is written to stderr i0529 222813094351 31506 processcpp1426 libprocess is initialized on 103718410353425 for 24 cpus i0529 222813095010 31506 loggingcpp91 logging to stderr source directory homevinodmesos build directory homevinodmesosbuild  we cannot run any cgroups tests that require mounting hierarchies because you have the following hierarchies mounted cgroup well disable the cgroupsnohierarchytest test fixture for now  note google test filter  ballooncgroupsnohierarchytestroot_cgroups_nohierarchy_mountunmounthierarchy  running 1 test from 1 test case  global test environment setup  1 test from cgroupsisolatortest  run  cgroupsisolatortestroot_cgroups_balloonframework using temporary directory tmpcgroupsisolatortest_root_cgroups_balloonframework_pwwde1 launched master at 31574 failed to load unknown flag build_dir usage ltmesosmaster  supported options allocation_intervalvalue amount of time to wait between performing batch allocations eg 500ms 1sec etc default 1secs clustervalue human readable name for the cluster displayed in the webui framework_sortervalue policy to use for allocating resources between a given users frameworks options are the same as for user_allocator default drf nohelp prints this help message default false ipvalue ip address to listen on log_dirvalue location to put log files no default nothing is written to disk unless specified does not affect logging to stderr logbufsecsvalue how many seconds to buffer log messages for default 0 portvalue port to listen on default 5050 noquiet disable logging to stderr default false noroot_submissions can root submit frameworks default true slavesvalue initial slaves that should be considered part of this cluster or if using zookeeper a url default  user_sortervalue policy to use for allocating resources between users may be one of dominant_resource_fairness drf default drf webui_dirvalue location of the webui filesassets default usrlocalsharemesoswebui whitelistvalue path to a file with a list of slaves one per line to advertise offers for should be of the form filepathtofile default  zkvalue zookeeper url used for leader election amongst masters may be one of zkhost1port1host2port2path zkusernamepasswordhost1port1host2port2path filepathtofile where file contains one of the above default  redmaster crashed failing test homevinodmesossrctestsballoon_framework_testsh line 31 kill 31574  no such process srctestsscriptcpp76 failure failed balloon_framework_testsh exited with status 2  failed  cgroupsisolatortestroot_cgroups_balloonframework 2031 ms  1 test from cgroupsisolatortest 2031 ms total  global test environment teardown  1 test from 1 test case ran 2031 ms total  passed  0 tests  failed  1 test listed below  failed  cgroupsisolatortestroot_cgroups_balloonframework 1 failed test,1
also check git diff shortstat staged in postreviewspy we current check if you have any changes before we run postreviewspy but we dont check for staged changes which iiuc could get lost,1
static files missing lastmodified http headers static assets served by the mesos master dont return lastmodified http headers that means clients receive a 200 status code and redownload assets on every page request even if the assets havent changed because angular js does most of the work the downloading happens only when you navigate to mesos master in your browser or use the browsers refresh example header for mesoscss http11 200 ok date thu 26 sep 2013 171852 gmt contentlength 1670 contenttype textcss clients sometimes use the date header for the same effect as lastmodified but the date is always the time of the response from the server ie it changes on every request and makes the assets look new every time the lastmodified header should be added and should be the last modified time of the file on subsequent requests for the same files the master should return 304 responses with no content rather than 200 with the full files it could save clients a lot of download time since mesos assets are rather heavyweight,2
expose total number of resources allocated to the slave in its endpoint this could be useful information if there are bugs in masterslave that causes slaves to overcommit its resources,2
slaverecoverytest0reconciletasksmissingfromslave test is flaky  run  slaverecoverytest0reconciletasksmissingfromslave checkpointing executors forked pid 32281 to tmpslaverecoverytest_0_reconciletasksmissingfromslave_nt1btbmetaslaves2013101519131677734335153314910frameworks2013101519131677734335153314910000executors0514b52f3c174ee5ba16635198701ca2runs97c9e2ccceea40a8a915aed5fed1dcb3pidsforkedpid fetching resources into tmpslaverecoverytest_0_reconciletasksmissingfromslave_nt1btbslaves2013101519131677734335153314910frameworks2013101519131677734335153314910000executors0514b52f3c174ee5ba16635198701ca2runs97c9e2ccceea40a8a915aed5fed1dcb3 registered executor on localhostlocaldomain starting task 0514b52f3c174ee5ba16635198701ca2 forked command at 32317 sh c sleep 10 testsslave_recovery_testscpp1927 failure mock function called more times than expected  returning directly function call statusupdate0x7fffae636eb0 0x7f1590027a00 64byte object f02f d0a1 157f 0000 0000 0000 0000 0000 40e9 0190 157f 0000 206b 0390 157f 0000 4891 c300 0000 0000 b03b 0190 157f 0000 0500 0000 0000 0000 1700 0000 0000 0000 expected to be called once actual called twice  oversaturated and active command exited with status 0 pid 32317,1
update semantics of when framework registeredreregistered get called current semantics 1 framework connects w master very first time  registered 2 framework reconnects w same master after a zk blip  reregistered 3 framework reconnects w failed over master  registered 4 failed over framework connects w same master  registered 5 failed over framework connects w failed over master  registered updated semantics everything same except 3 framework reconnects w failed over master  reregistered,3
examplestestjavaframework is flaky identify the cause of the following test failure  run  examplestestjavaframework using temporary directory tmpexamplestest_javaframework_wsc7u8 enabling authentication for the framework i1120 151339820032 1681264640 mastercpp285 master started on 1722513317152576 i1120 151339820180 1681264640 mastercpp299 master id 2013112015132877626796525763234 i1120 151339820194 1681264640 mastercpp302 master only allowing authenticated frameworks to register i1120 151339821197 1679654912 slavecpp112 slave started on 11722513317152576 i1120 151339821795 1679654912 slavecpp212 slave resources cpus4 mem7168 disk481998 ports3100032000 i1120 151339822855 1682337792 slavecpp112 slave started on 21722513317152576 i1120 151339823652 1682337792 slavecpp212 slave resources cpus4 mem7168 disk481998 ports3100032000 i1120 151339825330 1679118336 mastercpp744 the newly elected leader is master1722513317152576 i1120 151339825445 1679118336 mastercpp748 elected as the leading master i1120 151339825907 1681264640 statecpp33 recovering state from tmpexamplestest_javaframework_wsc7u80meta i1120 151339826127 1681264640 status_update_managercpp180 recovering status update manager i1120 151339826331 1681801216 process_isolatorcpp317 recovering isolator i1120 151339826738 1682874368 slavecpp2743 finished recovery i1120 151339827747 1682337792 statecpp33 recovering state from tmpexamplestest_javaframework_wsc7u81meta i1120 151339827945 1680191488 slavecpp112 slave started on 31722513317152576 i1120 151339828415 1682337792 status_update_managercpp180 recovering status update manager i1120 151339828608 1680728064 schedcpp260 authenticating with master master1722513317152576 i1120 151339828606 1680191488 slavecpp212 slave resources cpus4 mem7168 disk481998 ports3100032000 i1120 151339828680 1682874368 slavecpp497 new master detected at master1722513317152576 i1120 151339828765 1682337792 process_isolatorcpp317 recovering isolator i1120 151339829828 1680728064 schedcpp229 detecting new master i1120 151339830288 1679654912 authenticateehpp100 initializing client sasl i1120 151339831635 1680191488 statecpp33 recovering state from tmpexamplestest_javaframework_wsc7u82meta i1120 151339831991 1679118336 status_update_managercpp158 new master detected at master1722513317152576 i1120 151339832042 1682874368 slavecpp524 detecting new master i1120 151339832314 1682337792 slavecpp2743 finished recovery i1120 151339832309 1681264640 mastercpp1266 attempting to register slave on vkonelocal at slave11722513317152576 i1120 151339832929 1680728064 status_update_managercpp180 recovering status update manager i1120 151339833371 1681801216 slavecpp497 new master detected at master1722513317152576 i1120 151339833273 1681264640 mastercpp2513 adding slave 20131120151328776267965257632340 at vkonelocal with cpus4 mem7168 disk481998 ports3100032000 i1120 151339833595 1680728064 process_isolatorcpp317 recovering isolator i1120 151339833859 1681801216 slavecpp524 detecting new master i1120 151339833861 1682874368 status_update_managercpp158 new master detected at master1722513317152576 i1120 151339834092 1680191488 slavecpp542 registered with master master1722513317152576 given slave id 20131120151328776267965257632340 i1120 151339834486 1681264640 mastercpp1266 attempting to register slave on vkonelocal at slave21722513317152576 i1120 151339834549 1681264640 mastercpp2513 adding slave 20131120151328776267965257632341 at vkonelocal with cpus4 mem7168 disk481998 ports3100032000 i1120 151339834750 1680191488 slavecpp555 checkpointing slaveinfo to tmpexamplestest_javaframework_wsc7u80metaslaves20131120151328776267965257632340slaveinfo i1120 151339834875 1682874368 hierarchical_allocator_processhpp445 added slave 20131120151328776267965257632340 vkonelocal with cpus4 mem7168 disk481998 ports3100032000 and cpus4 mem7168 disk481998 ports3100032000 available i1120 151339835155 1680728064 slavecpp542 registered with master master1722513317152576 given slave id 20131120151328776267965257632341 i1120 151339835458 1679118336 slavecpp2743 finished recovery i1120 151339835739 1680728064 slavecpp555 checkpointing slaveinfo to tmpexamplestest_javaframework_wsc7u81metaslaves20131120151328776267965257632341slaveinfo i1120 151339835922 1682874368 hierarchical_allocator_processhpp445 added slave 20131120151328776267965257632341 vkonelocal with cpus4 mem7168 disk481998 ports3100032000 and cpus4 mem7168 disk481998 ports3100032000 available i1120 151339836120 1681264640 slavecpp497 new master detected at master1722513317152576 i1120 151339836340 1679118336 status_update_managercpp158 new master detected at master1722513317152576 i1120 151339836436 1681264640 slavecpp524 detecting new master i1120 151339836629 1682874368 mastercpp1266 attempting to register slave on vkonelocal at slave31722513317152576 i1120 151339836653 1682874368 mastercpp2513 adding slave 20131120151328776267965257632342 at vkonelocal with cpus4 mem7168 disk481998 ports3100032000 i1120 151339836804 1680728064 slavecpp542 registered with master master1722513317152576 given slave id 20131120151328776267965257632342 i1120 151339837190 1680728064 slavecpp555 checkpointing slaveinfo to tmpexamplestest_javaframework_wsc7u82metaslaves20131120151328776267965257632342slaveinfo i1120 151339837569 1682874368 hierarchical_allocator_processhpp445 added slave 20131120151328776267965257632342 vkonelocal with cpus4 mem7168 disk481998 ports3100032000 and cpus4 mem7168 disk481998 ports3100032000 available i1120 151339852011 1679654912 authenticateehpp124 creating new client sasl connection i1120 151339852219 1680191488 mastercpp1734 authenticating framework at scheduler11722513317152576 i1120 151339852577 1682337792 authenticatorhpp83 initializing server sasl i1120 151339856160 1682337792 authenticatorhpp140 creating new server sasl connection i1120 151339856334 1681264640 authenticateehpp212 received sasl authentication mechanisms crammd5 i1120 151339856360 1681264640 authenticateehpp238 attempting to authenticate with mechanism crammd5 i1120 151339856421 1681264640 authenticatorhpp243 received sasl authentication start i1120 151339856487 1681264640 authenticatorhpp325 authentication requires more steps i1120 151339856531 1681264640 authenticateehpp258 received sasl authentication step i1120 151339856576 1681264640 authenticatorhpp271 received sasl authentication step i1120 151339856643 1681264640 authenticatorhpp317 authentication success i1120 151339856724 1681264640 authenticateehpp298 authentication success i1120 151339856768 1681264640 mastercpp1774 successfully authenticated framework at scheduler11722513317152576 i1120 151339857028 1681264640 schedcpp334 successfully authenticated with master master1722513317152576 i1120 151339857139 1681264640 mastercpp798 received registration request from scheduler11722513317152576 i1120 151339857306 1681264640 mastercpp816 registering framework 20131120151328776267965257632340000 at scheduler11722513317152576 i1120 151339862296 1680191488 hierarchical_allocator_processhpp332 added framework 20131120151328776267965257632340000 i1120 151339863867 1680191488 mastercpp1700 sending 3 offers to framework 20131120151328776267965257632340000 registered id  20131120151328776267965257632340000 launching task 0 launching task 1 launching task 2 i1120 151339905390 1680191488 mastercpp2026 processing reply for offer 20131120151328776267965257632340 on slave 20131120151328776267965257632341 vkonelocal for framework 20131120151328776267965257632340000 i1120 151339905825 1680191488 masterhpp400 adding task 0 with resources cpus1 mem128 on slave 20131120151328776267965257632341 vkonelocal i1120 151339905886 1680191488 mastercpp2150 launching task 0 of framework 20131120151328776267965257632340000 with resources cpus1 mem128 on slave 20131120151328776267965257632341 vkonelocal i1120 151339906422 1680191488 mastercpp2026 processing reply for offer 20131120151328776267965257632341 on slave 20131120151328776267965257632342 vkonelocal for framework 20131120151328776267965257632340000 i1120 151339906664 1680191488 masterhpp400 adding task 1 with resources cpus1 mem128 on slave 20131120151328776267965257632342 vkonelocal i1120 151339906721 1680191488 mastercpp2150 launching task 1 of framework 20131120151328776267965257632340000 with resources cpus1 mem128 on slave 20131120151328776267965257632342 vkonelocal i1120 151339907171 1680191488 mastercpp2026 processing reply for offer 20131120151328776267965257632342 on slave 20131120151328776267965257632340 vkonelocal for framework 20131120151328776267965257632340000 i1120 151339907419 1680191488 masterhpp400 adding task 2 with resources cpus1 mem128 on slave 20131120151328776267965257632340 vkonelocal i1120 151339907480 1680191488 mastercpp2150 launching task 2 of framework 20131120151328776267965257632340000 with resources cpus1 mem128 on slave 20131120151328776267965257632340 vkonelocal i1120 151339907938 1680191488 slavecpp722 got assigned task 0 for framework 20131120151328776267965257632340000 i1120 151339908473 1680191488 slavecpp833 launching task 0 for framework 20131120151328776267965257632340000 i1120 151339914427 1682874368 slavecpp722 got assigned task 1 for framework 20131120151328776267965257632340000 i1120 151339914594 1680728064 slavecpp722 got assigned task 2 for framework 20131120151328776267965257632340000 i1120 151339914844 1681801216 hierarchical_allocator_processhpp590 framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632341 for 1secs i1120 151339915292 1682874368 slavecpp833 launching task 1 for framework 20131120151328776267965257632340000 i1120 151339915424 1681801216 hierarchical_allocator_processhpp590 framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632342 for 1secs i1120 151339915685 1681801216 hierarchical_allocator_processhpp590 framework 20131120151328776267965257632340000 filtered slave 20131120151328776267965257632340 for 1secs i1120 151339915828 1680728064 slavecpp833 launching task 2 for framework 20131120151328776267965257632340000 i1120 151339917840 1680191488 slavecpp943 queuing task 0 for executor default of framework 20131120151328776267965257632340000 i1120 151339917935 1679118336 process_isolatorcpp100 launching default usersvinodworkspaceapachemesosbuildsrcexamplesjavatestexecutor in tmpexamplestest_javaframework_wsc7u81slaves20131120151328776267965257632341frameworks20131120151328776267965257632340000executorsdefaultruns375b31a970934db1964de6b425b1e4b4 with resources  for framework 20131120151328776267965257632340000 i1120 151339922019 1679118336 process_isolatorcpp163 forked executor at 3268 i1120 151339922703 1679118336 slavecpp2073 monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3268 i1120 151339929134 1682874368 slavecpp943 queuing task 1 for executor default of framework 20131120151328776267965257632340000 i1120 151339929323 1682874368 process_isolatorcpp100 launching default usersvinodworkspaceapachemesosbuildsrcexamplesjavatestexecutor in tmpexamplestest_javaframework_wsc7u82slaves20131120151328776267965257632342frameworks20131120151328776267965257632340000executorsdefaultruns2bd0e75da2b94ae6be089782612309a5 with resources  for framework 20131120151328776267965257632340000 i1120 151339931243 1682874368 process_isolatorcpp163 forked executor at 3269 i1120 151339931612 1681801216 slavecpp2073 monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3269 e1120 151339931836 1681801216 slavecpp2099 failed to watch executor default of framework 20131120151328776267965257632340000 already watched i1120 151339936460 1680728064 slavecpp943 queuing task 2 for executor default of framework 20131120151328776267965257632340000 i1120 151339936619 1681801216 process_isolatorcpp100 launching default usersvinodworkspaceapachemesosbuildsrcexamplesjavatestexecutor in tmpexamplestest_javaframework_wsc7u80slaves20131120151328776267965257632340frameworks20131120151328776267965257632340000executorsdefaultruns16d600dada86461491cb58a7b27ab534 with resources  for framework 20131120151328776267965257632340000 i1120 151339941299 1681801216 process_isolatorcpp163 forked executor at 3270 i1120 151339942179 1681801216 slavecpp2073 monitoring executor default of framework 20131120151328776267965257632340000 forked at pid 3270 e1120 151339942395 1681801216 slavecpp2099 failed to watch executor default of framework 20131120151328776267965257632340000 already watched fetching resources into tmpexamplestest_javaframework_wsc7u82slaves20131120151328776267965257632342frameworks20131120151328776267965257632340000executorsdefaultruns2bd0e75da2b94ae6be089782612309a5 fetching resources into tmpexamplestest_javaframework_wsc7u81slaves20131120151328776267965257632341frameworks20131120151328776267965257632340000executorsdefaultruns375b31a970934db1964de6b425b1e4b4 fetching resources into tmpexamplestest_javaframework_wsc7u80slaves20131120151328776267965257632340frameworks20131120151328776267965257632340000executorsdefaultruns16d600dada86461491cb58a7b27ab534 i1120 151340372573 1681801216 slavecpp1406 got registration for executor default of framework 20131120151328776267965257632340000 i1120 151340373258 1681801216 slavecpp1527 flushing queued task 1 for executor default of framework 20131120151328776267965257632340000 i1120 151340388317 1681801216 slavecpp1406 got registration for executor default of framework 20131120151328776267965257632340000 i1120 151340388983 1681801216 slavecpp1527 flushing queued task 0 for executor default of framework 20131120151328776267965257632340000 i1120 151340398084 1679654912 slavecpp1406 got registration for executor default of framework 20131120151328776267965257632340000 i1120 151340399344 1679654912 slavecpp1527 flushing queued task 2 for executor default of framework 20131120151328776267965257632340000 registered executor on vkonelocal i1120 151340491843 1679654912 slavecpp1740 handling status update task_running uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 from executor11722513317152577 i1120 151340492202 1679654912 status_update_managercpp305 received status update task_running uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 i1120 151340492424 1679654912 status_update_managercpp356 forwarding status update task_running uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 to master1722513317152576 registered executor on vkonelocal i1120 151340492671 1682337792 mastercpp1452 status update task_running uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 from slave31722513317152576 i1120 151340492735 1682337792 slavecpp1865 sending acknowledgement for status update task_running uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 to executor11722513317152577 status update task 1 is in state task_running i1120 151340502235 1679654912 status_update_managercpp380 received status update acknowledgement uuid f04b18523669444a906f3675f784c14f for task 1 of framework 20131120151328776267965257632340000 registered executor on vkonelocal i1120 151340531292 1679654912 slavecpp1740 handling status update task_running uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 from executor11722513317152579 i1120 151340532091 1680728064 status_update_managercpp305 received status update task_running uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 i1120 151340532305 1680728064 status_update_managercpp356 forwarding status update task_running uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 to master1722513317152576 i1120 151340532776 1682874368 slavecpp1865 sending acknowledgement for status update task_running uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 to executor11722513317152579 i1120 151340532951 1681801216 mastercpp1452 status update task_running uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 from slave11722513317152576 status update task 2 is in state task_running i1120 151340538895 1682874368 status_update_managercpp380 received status update acknowledgement uuid c19b6a5a19ce46138a5a08fe807ff27c for task 2 of framework 20131120151328776267965257632340000 i1120 151340541267 1682874368 slavecpp1740 handling status update task_running uuid c218b0c3d77c49018570391c330ba117 for task 0 of framework 20131120151328776267965257632340000 from executor11722513317152578 i1120 151340541555 1682874368 status_update_managercpp305 received status update task_running uuid c218b0c3d77c49018570391c330ba117 for task 0 of framework 20131120151328776267965257632340000 i1120 151340541725 1682874368 status_update_managercpp356 forwarding status update task_running uuid c218b0c3d77c49018570391c330ba117 for task 0 of framework 20131120151328776267965257632340000 to master1722513317152576 i1120 151340542196 1682874368 mastercpp1452 status update task_running uuid c218b0c3d77c49018570391c330ba117 for task 0 of framework 20131120151328776267965257632340000 from slave21722513317152576 i1120 151340542251 1682874368 slavecpp1865 sending acknowledgement for status update task_running uuid c218b0c3d77c49018570391c330ba117 for task 0 of framework 20131120151328776267965257632340000 to executor11722513317152578 status update task 0 is in state task_running i1120 151340545537 1682874368 status_update_managercpp380 received status update acknowledgement uuid c218b0c3d77c49018570391c330ba117 for task 0 of,8
set glog_drop_log_memoryfalse in environment prior to logging initialization weve observed issues where the masters are slow to respond two perf traces collected while the masters were slow to respond noformat 2584 kernel k default_send_ipi_mask_sequence_phys 2044 kernel k native_write_msr_safe 454 kernel k _raw_spin_lock 295 libc25so  _int_malloc 182 libc25so  malloc 155 kernel k apic_timer_interrupt 136 libc25so  _int_free noformat noformat 2903 kernel k default_send_ipi_mask_sequence_phys 964 kernel k _raw_spin_lock 738 kernel k native_write_msr_safe 243 libc25so  _int_malloc 205 libc25so  _int_free 167 kernel k apic_timer_interrupt 158 libc25so  malloc noformat these have been found to be attributed to the posix_fadvise calls made by glog we can disable these via the environment noformat glog_define_booldrop_log_memory true drop inmemory buffers of log contents  logs can grow very quickly and they are rarely read before they  need to be evicted from memory instead drop them from memory  as soon as they are flushed to disk noformat code if flags_drop_log_memory  if file_length_  loggingkpagesize   dont evict the most recent page uint32 len  file_length_  loggingkpagesize  1 posix_fadvisefilenofile_ 0 len posix_fadv_dontneed   code we should set glog_drop_log_memoryfalse prior to making our call to googleinitgooglelogging to avoid others running into this issue,2
logging and debugging document is outofdate the following is no longer correct httpmesosapacheorgdocumentationlatestlogginganddebugging we should either delete this document or rewrite it entirely,1
slaverecoverytest1schedulerfailover is flaky  running 1 test from 1 test case  global test environment setup  1 test from slaverecoverytest1 where typeparam  mesosinternalslavecgroupsisolator  run  slaverecoverytest1schedulerfailover i0206 201831525116 56447 mastercpp239 master id 2014020620183117401213545556656447 hostname smfdbkq03sr4develtwittercom i0206 201831525295 56481 mastercpp321 master started on 103718410355566 i0206 201831525315 56481 mastercpp324 master only allowing authenticated frameworks to register i0206 201831527093 56481 mastercpp756 the newly elected leader is master103718410355566 i0206 201831527122 56481 mastercpp764 elected as the leading master i0206 201831530642 56473 slavecpp112 slave started on 9103718410355566 i0206 201831530802 56473 slavecpp212 slave resources cpus2 mem1024 disk1024 ports3100032000 i0206 201831531203 56473 slavecpp240 slave hostname smfdbkq03sr4develtwittercom i0206 201831531221 56473 slavecpp241 slave checkpoint true i0206 201831531991 56482 cgroups_isolatorcpp225 using tmpmesos_test_cgroup as cgroups hierarchy root i0206 201831532470 56478 statecpp33 recovering state from tmpslaverecoverytest_1_schedulerfailover_7dc2n1meta i0206 201831532698 56469 status_update_managercpp188 recovering status update manager i0206 201831533962 56472 schedcpp265 authenticating with master master103718410355566 i0206 201831534102 56472 schedcpp234 detecting new master i0206 201831534124 56484 authenticateehpp124 creating new client sasl connection i0206 201831534299 56473 mastercpp2317 authenticating framework at scheduler9103718410355566 i0206 201831534459 56461 authenticatorhpp140 creating new server sasl connection i0206 201831534572 56466 authenticateehpp212 received sasl authentication mechanisms crammd5 i0206 201831534595 56466 authenticateehpp238 attempting to authenticate with mechanism crammd5 i0206 201831534667 56474 authenticatorhpp243 received sasl authentication start i0206 201831534732 56474 authenticatorhpp325 authentication requires more steps i0206 201831534814 56468 authenticateehpp258 received sasl authentication step i0206 201831534946 56466 authenticatorhpp271 received sasl authentication step i0206 201831535007 56466 authenticatorhpp317 authentication success i0206 201831535084 56471 authenticateehpp298 authentication success i0206 201831535107 56461 mastercpp2357 successfully authenticated framework at scheduler9103718410355566 i0206 201831535392 56476 schedcpp339 successfully authenticated with master master103718410355566 i0206 201831535512 56465 mastercpp812 received registration request from scheduler9103718410355566 i0206 201831535570 56465 mastercpp830 registering framework 20140206201831174012135455566564470000 at scheduler9103718410355566 i0206 201831535856 56465 hierarchical_allocator_processhpp332 added framework 20140206201831174012135455566564470000 i0206 201831537802 56482 cgroups_isolatorcpp840 recovering isolator i0206 201831538462 56472 slavecpp2760 finished recovery i0206 201831538910 56472 slavecpp508 new master detected at master103718410355566 i0206 201831539036 56478 status_update_managercpp162 new master detected at master103718410355566 i0206 201831539223 56464 mastercpp1834 attempting to register slave on smfdbkq03sr4develtwittercom at slave9103718410355566 i0206 201831539271 56472 slavecpp533 detecting new master i0206 201831539330 56464 mastercpp2804 adding slave 20140206201831174012135455566564470 at smfdbkq03sr4develtwittercom with cpus2 mem1024 disk1024 ports3100032000 i0206 201831539454 56472 slavecpp551 registered with master master103718410355566 given slave id 20140206201831174012135455566564470 i0206 201831539620 56472 slavecpp564 checkpointing slaveinfo to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470slaveinfo i0206 201831539834 56475 hierarchical_allocator_processhpp445 added slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0206 201831540341 56472 mastercpp2272 sending 1 offers to framework 20140206201831174012135455566564470000 i0206 201831543433 56472 mastercpp1568 processing reply for offers  20140206201831174012135455566564470  on slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom for framework 20140206201831174012135455566564470000 i0206 201831543642 56472 masterhpp411 adding task d045a0bd2ed2410abd1f5bd9219896e3 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom i0206 201831543781 56472 mastercpp2441 launching task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom i0206 201831544002 56484 slavecpp736 got assigned task d045a0bd2ed2410abd1f5bd9219896e3 for framework 20140206201831174012135455566564470000 i0206 201831544097 56484 slavecpp2899 checkpointing frameworkinfo to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000frameworkinfo i0206 201831544272 56484 slavecpp2906 checkpointing framework pid scheduler9103718410355566 to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000frameworkpid i0206 201831544617 56484 slavecpp845 launching task d045a0bd2ed2410abd1f5bd9219896e3 for framework 20140206201831174012135455566564470000 i0206 201831546721 56484 slavecpp3169 checkpointing executorinfo to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3executorinfo i0206 201831547317 56484 slavecpp3257 checkpointing taskinfo to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3runs9adabe165d8445c9bc831a72a6d1c986tasksd045a0bd2ed2410abd1f5bd9219896e3taskinfo i0206 201831547514 56484 slavecpp955 queuing task d045a0bd2ed2410abd1f5bd9219896e3 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831547590 56481 cgroups_isolatorcpp517 launching d045a0bd2ed2410abd1f5bd9219896e3 homevinodmesosbuildsrcmesosexecutor in tmpslaverecoverytest_1_schedulerfailover_7dc2n1slaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3runs9adabe165d8445c9bc831a72a6d1c986 with resources cpus2 mem1024 disk1024 ports3100032000 for framework 20140206201831174012135455566564470000 in cgroup mesos_testframework_20140206201831174012135455566564470000_executor_d045a0bd2ed2410abd1f5bd9219896e3_tag_9adabe165d8445c9bc831a72a6d1c986 i0206 201831548408 56481 cgroups_isolatorcpp717 changing cgroup controls for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 with resources cpus2 mem1024 disk1024 ports3100032000 i0206 201831548833 56481 cgroups_isolatorcpp1007 updated cpushares to 2048 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831549294 56481 cgroups_isolatorcpp1117 updated memorysoft_limit_in_bytes to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831550107 56481 cgroups_isolatorcpp1147 updated memorylimit_in_bytes to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831550571 56481 cgroups_isolatorcpp1174 started listening for oom events for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831551553 56481 cgroups_isolatorcpp569 forked executor at  56671 checkpointing executors forked pid 56671 to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3runs9adabe165d8445c9bc831a72a6d1c986pidsforkedpid i0206 201831552222 56472 slavecpp2098 monitoring executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 forked at pid 56671 fetching resources into tmpslaverecoverytest_1_schedulerfailover_7dc2n1slaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3runs9adabe165d8445c9bc831a72a6d1c986 i0206 201831604012 56472 slavecpp1431 got registration for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831604167 56472 slavecpp1516 checkpointing executor pid executor1103718410346181 to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000executorsd045a0bd2ed2410abd1f5bd9219896e3runs9adabe165d8445c9bc831a72a6d1c986pidslibprocesspid i0206 201831605183 56472 slavecpp1552 flushing queued task d045a0bd2ed2410abd1f5bd9219896e3 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 registered executor on smfdbkq03sr4develtwittercom starting task d045a0bd2ed2410abd1f5bd9219896e3 sh c sleep 1000 forked command at 56712 i0206 201831613098 56481 slavecpp1765 handling status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 from executor1103718410346181 i0206 201831613628 56469 status_update_managercpp314 received status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831614006 56469 status_update_managerhpp342 checkpointing update for status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831795529 56469 status_update_managercpp367 forwarding status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 to master103718410355566 i0206 201831795992 56480 slavecpp1890 sending acknowledgement for status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 to executor1103718410346181 i0206 201831796131 56471 mastercpp2020 status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 from slave9103718410355566 i0206 201831797099 56483 status_update_managercpp392 received status update acknowledgement uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831797165 56483 status_update_managerhpp342 checkpointing ack for status update task_running uuid fc151a46751b4c4bb0481727752f34e3 for task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831882767 56481 slavecpp394 slave terminating i0206 201831883112 56481 mastercpp641 slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom disconnected i0206 201831883200 56476 hierarchical_allocator_processhpp484 slave 20140206201831174012135455566564470 disconnected i0206 201831888206 56473 schedcpp265 authenticating with master master103718410355566 i0206 201831888473 56473 schedcpp234 detecting new master i0206 201831888556 56469 authenticateehpp124 creating new client sasl connection i0206 201831888978 56484 mastercpp2317 authenticating framework at scheduler10103718410355566 i0206 201831889348 56469 authenticatorhpp140 creating new server sasl connection i0206 201831889925 56469 authenticateehpp212 received sasl authentication mechanisms crammd5 i0206 201831889989 56469 authenticateehpp238 attempting to authenticate with mechanism crammd5 i0206 201831890059 56469 authenticatorhpp243 received sasl authentication start i0206 201831890233 56469 authenticatorhpp325 authentication requires more steps i0206 201831890399 56468 authenticateehpp258 received sasl authentication step i0206 201831890554 56484 authenticatorhpp271 received sasl authentication step i0206 201831890630 56484 authenticatorhpp317 authentication success i0206 201831890728 56470 authenticateehpp298 authentication success i0206 201831890748 56484 mastercpp2357 successfully authenticated framework at scheduler10103718410355566 i0206 201831892210 56469 schedcpp339 successfully authenticated with master master103718410355566 i0206 201831892410 56473 mastercpp900 reregistering framework 20140206201831174012135455566564470000 at scheduler10103718410355566 i0206 201831892460 56473 mastercpp926 framework 20140206201831174012135455566564470000 failed over w0206 201831892691 56465 mastercpp1048 ignoring deactivate framework message for framework 20140206201831174012135455566564470000 from scheduler9103718410355566 because it is not from the registered framework scheduler10103718410355566 i0206 201831897049 56466 slavecpp112 slave started on 10103718410355566 i0206 201831897207 56466 slavecpp212 slave resources cpus2 mem1024 disk1024 ports3100032000 i0206 201831897536 56466 slavecpp240 slave hostname smfdbkq03sr4develtwittercom i0206 201831897554 56466 slavecpp241 slave checkpoint true i0206 201831898388 56463 cgroups_isolatorcpp225 using tmpmesos_test_cgroup as cgroups hierarchy root i0206 201831898936 56472 statecpp33 recovering state from tmpslaverecoverytest_1_schedulerfailover_7dc2n1meta i0206 201831901702 56465 slavecpp2828 recovering framework 20140206201831174012135455566564470000 i0206 201831901759 56465 slavecpp3020 recovering executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201831902716 56464 status_update_managercpp188 recovering status update manager i0206 201831902884 56464 status_update_managercpp196 recovering executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834475915 56463 cgroups_isolatorcpp840 recovering isolator i0206 201834476066 56463 cgroups_isolatorcpp847 recovering executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834477478 56463 cgroups_isolatorcpp1174 started listening for oom events for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834478728 56463 slavecpp2700 sending reconnect request to executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 at executor1103718410346181 i0206 201834480114 56476 slavecpp1597 reregistering executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834480566 56476 cgroups_isolatorcpp717 changing cgroup controls for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 with resources cpus2 mem1024 disk1024 ports3100032000 i0206 201834481370 56476 cgroups_isolatorcpp1007 updated cpushares to 2048 for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834481827 56476 cgroups_isolatorcpp1117 updated memorysoft_limit_in_bytes to 1gb for executor d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 reregistered executor on smfdbkq03sr4develtwittercom i0206 201834489497 56471 slavecpp1713 cleaning up unreregistered executors i0206 201834489588 56471 slavecpp2760 finished recovery i0206 201834490048 56463 slavecpp508 new master detected at master103718410355566 i0206 201834490257 56475 status_update_managercpp162 new master detected at master103718410355566 i0206 201834490357 56463 slavecpp533 detecting new master w0206 201834490603 56480 mastercpp1878 slave at slave10103718410355566 smfdbkq03sr4develtwittercom is being allowed to reregister with an already in use id 20140206201831174012135455566564470 i0206 201834490927 56479 slavecpp601 reregistered with master master103718410355566 i0206 201834491322 56461 hierarchical_allocator_processhpp498 slave 20140206201831174012135455566564470 reconnected i0206 201834491421 56468 slavecpp1312 updating framework 20140206201831174012135455566564470000 pid to scheduler10103718410355566 i0206 201834491444 56480 mastercpp1673 asked to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834491488 56468 slavecpp1320 checkpointing framework pid scheduler10103718410355566 to tmpslaverecoverytest_1_schedulerfailover_7dc2n1metaslaves20140206201831174012135455566564470frameworks20140206201831174012135455566564470000frameworkpid i0206 201834491497 56480 mastercpp1707 telling slave 20140206201831174012135455566564470 smfdbkq03sr4develtwittercom to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 i0206 201834491657 56468 slavecpp1013 asked to kill task d045a0bd2ed2410abd1f5bd9219896e3 of framework 20140206201831174012135455566564470000 shutting down killing process tree at pid 56712,1
examplestestpythonframework is flaky looks like a segfault during shutdown noformat  run  examplestestpythonframework using temporary directory tmpexamplestest_pythonframework_rz4yaf warning logging before initgooglelogging is written to stderr i0211 211447861803 21045 processcpp1591 libprocess is initialized on 67195138953443 for 8 cpus i0211 211447861884 21045 loggingcpp140 logging to stderr i0211 211447862761 21045 mastercpp240 master id 201402112114471600888995344321045 hostname vestaapacheorg i0211 211447862897 21054 mastercpp322 master started on 67195138953443 i0211 211447862908 21054 mastercpp325 master only allowing authenticated frameworks to register i0211 211447864362 21053 mastercpp86 no whitelist given advertising offers for all slaves i0211 211447864506 21055 slavecpp112 slave started on 167195138953443 i0211 211447864522 21059 slavecpp112 slave started on 267195138953443 i0211 211447864749 21055 slavecpp212 slave resources cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447864778 21059 slavecpp212 slave resources cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447864819 21055 slavecpp240 slave hostname vestaapacheorg i0211 211447864827 21055 slavecpp241 slave checkpoint true i0211 211447864850 21059 slavecpp240 slave hostname vestaapacheorg i0211 211447864858 21059 slavecpp241 slave checkpoint true i0211 211447865329 21055 mastercpp760 the newly elected leader is master67195138953443 with id 201402112114471600888995344321045 i0211 211447865350 21055 mastercpp770 elected as the leading master i0211 211447865399 21055 statecpp33 recovering state from tmpmesosz8v6cu1meta i0211 211447865407 21059 statecpp33 recovering state from tmpmesosz8v6cu0meta i0211 211447865502 21052 hierarchical_allocator_processhpp302 initializing hierarchical allocator process with master  master67195138953443 i0211 211447865540 21054 status_update_managercpp188 recovering status update manager i0211 211447865619 21053 process_isolatorcpp319 recovering isolator i0211 211447865674 21057 status_update_managercpp188 recovering status update manager i0211 211447865699 21059 slavecpp2760 finished recovery i0211 211447865733 21053 process_isolatorcpp319 recovering isolator i0211 211447865789 21053 slavecpp2760 finished recovery i0211 211447865921 21059 slavecpp508 new master detected at master67195138953443 i0211 211447865958 21053 status_update_managercpp162 new master detected at master67195138953443 i0211 211447865978 21059 slavecpp533 detecting new master i0211 211447866019 21053 slavecpp508 new master detected at master67195138953443 i0211 211447866063 21053 slavecpp533 detecting new master i0211 211447866070 21055 status_update_managercpp162 new master detected at master67195138953443 i0211 211447866077 21059 mastercpp1840 attempting to register slave on vestaapacheorg at slave267195138953443 i0211 211447866092 21059 mastercpp2810 adding slave 2014021121144716008889953443210450 at vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447866216 21059 mastercpp1840 attempting to register slave on vestaapacheorg at slave167195138953443 i0211 211447866225 21053 slavecpp551 registered with master master67195138953443 given slave id 2014021121144716008889953443210450 i0211 211447866228 21059 mastercpp2810 adding slave 2014021121144716008889953443210451 at vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447866278 21055 hierarchical_allocator_processhpp445 added slave 2014021121144716008889953443210450 vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 and cpus8 mem6961 disk138501e06 ports3100032000 available i0211 211447866297 21059 slavecpp551 registered with master master67195138953443 given slave id 2014021121144716008889953443210451 i0211 211447866327 21055 hierarchical_allocator_processhpp708 performed allocation for slave 2014021121144716008889953443210450 in 11us i0211 211447866330 21053 slavecpp564 checkpointing slaveinfo to tmpmesosz8v6cu1metaslaves2014021121144716008889953443210450slaveinfo i0211 211447866400 21059 slavecpp564 checkpointing slaveinfo to tmpmesosz8v6cu0metaslaves2014021121144716008889953443210451slaveinfo i0211 211447866399 21055 hierarchical_allocator_processhpp445 added slave 2014021121144716008889953443210451 vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 and cpus8 mem6961 disk138501e06 ports3100032000 available i0211 211447866423 21055 hierarchical_allocator_processhpp708 performed allocation for slave 2014021121144716008889953443210451 in 2505ns i0211 211447866636 21059 slavecpp112 slave started on 367195138953443 i0211 211447866727 21059 slavecpp212 slave resources cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447866766 21059 slavecpp240 slave hostname vestaapacheorg i0211 211447866772 21059 slavecpp241 slave checkpoint true i0211 211447867300 21052 statecpp33 recovering state from tmpmesosz8v6cu2meta i0211 211447867368 21052 status_update_managercpp188 recovering status update manager i0211 211447867419 21055 process_isolatorcpp319 recovering isolator i0211 211447867544 21052 slavecpp2760 finished recovery i0211 211447867729 21052 slavecpp508 new master detected at master67195138953443 i0211 211447867770 21054 status_update_managercpp162 new master detected at master67195138953443 i0211 211447867777 21052 slavecpp533 detecting new master i0211 211447867815 21055 mastercpp1840 attempting to register slave on vestaapacheorg at slave367195138953443 i0211 211447867827 21055 mastercpp2810 adding slave 2014021121144716008889953443210452 at vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 i0211 211447867885 21052 slavecpp551 registered with master master67195138953443 given slave id 2014021121144716008889953443210452 i0211 211447867961 21055 hierarchical_allocator_processhpp445 added slave 2014021121144716008889953443210452 vestaapacheorg with cpus8 mem6961 disk138501e06 ports3100032000 and cpus8 mem6961 disk138501e06 ports3100032000 available i0211 211447867985 21052 slavecpp564 checkpointing slaveinfo to tmpmesosz8v6cu2metaslaves2014021121144716008889953443210452slaveinfo i0211 211447867987 21055 hierarchical_allocator_processhpp708 performed allocation for slave 2014021121144716008889953443210452 in 3308ns i0211 211447868468 21045 schedcpp121 version 0180 i0211 211447868633 21055 schedcpp217 new master detected at master67195138953443 i0211 211447868651 21055 schedcpp268 authenticating with master master67195138953443 i0211 211447868696 21055 schedcpp237 detecting new master i0211 211447868708 21054 authenticateehpp100 initializing client sasl i0211 211447869549 21054 authenticateehpp124 creating new client sasl connection i0211 211447869633 21055 mastercpp2323 authenticating framework at scheduler167195138953443 i0211 211447869818 21059 authenticatorhpp83 initializing server sasl i0211 211447870029 21059 auxpropcpp45 initialized inmemory auxiliary property plugin i0211 211447870040 21059 authenticatorhpp140 creating new server sasl connection i0211 211447870144 21057 authenticateehpp212 received sasl authentication mechanisms crammd5 i0211 211447870174 21057 authenticateehpp238 attempting to authenticate with mechanism crammd5 i0211 211447870203 21057 authenticatorhpp243 received sasl authentication start i0211 211447870256 21057 authenticatorhpp325 authentication requires more steps i0211 211447870282 21057 authenticateehpp258 received sasl authentication step i0211 211447870348 21057 authenticatorhpp271 received sasl authentication step i0211 211447870376 21057 auxpropcpp81 request to lookup properties for user testprincipal realm vestaapacheorg server fqdn vestaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0211 211447870384 21057 auxpropcpp153 looking up auxiliary property userpassword i0211 211447870396 21057 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0211 211447870405 21057 auxpropcpp81 request to lookup properties for user testprincipal realm vestaapacheorg server fqdn vestaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0211 211447870411 21057 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0211 211447870415 21057 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0211 211447870425 21057 authenticatorhpp317 authentication success i0211 211447870445 21057 mastercpp2363 successfully authenticated framework at scheduler167195138953443 i0211 211447870448 21055 authenticateehpp298 authentication success i0211 211447870492 21055 schedcpp342 successfully authenticated with master master67195138953443 i0211 211447870538 21057 mastercpp818 received registration request from scheduler167195138953443 i0211 211447870590 21057 mastercpp836 registering framework 2014021121144716008889953443210450000 at scheduler167195138953443 i0211 211447870661 21055 schedcpp391 framework registered with 2014021121144716008889953443210450000 i0211 211447870661 21057 hierarchical_allocator_processhpp332 added framework 2014021121144716008889953443210450000 i0211 211447870707 21057 hierarchical_allocator_processhpp752 offering cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210450 to framework 2014021121144716008889953443210450000 i0211 211447870798 21057 hierarchical_allocator_processhpp752 offering cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210451 to framework 2014021121144716008889953443210450000 i0211 211447870869 21057 hierarchical_allocator_processhpp752 offering cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210452 to framework 2014021121144716008889953443210450000 i0211 211447870894 21055 schedcpp405 schedulerregistered took 222149ns i0211 211447871038 21057 hierarchical_allocator_processhpp688 performed allocation for 3 slaves in 351098ns i0211 211447871106 21058 masterhpp439 adding offer 2014021121144716008889953443210450 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210452 vestaapacheorg i0211 211447871215 21058 masterhpp439 adding offer 2014021121144716008889953443210451 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210451 vestaapacheorg i0211 211447871296 21058 masterhpp439 adding offer 2014021121144716008889953443210452 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210450 vestaapacheorg i0211 211447871333 21058 mastercpp2278 sending 3 offers to framework 2014021121144716008889953443210450000 i0211 211447873667 21055 schedcpp525 schedulerresourceoffers took 2150843ms i0211 211447873884 21053 masterhpp449 removing offer 2014021121144716008889953443210450 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210452 vestaapacheorg i0211 211447873934 21053 mastercpp1574 processing reply for offers  2014021121144716008889953443210450  on slave 2014021121144716008889953443210452 vestaapacheorg for framework 2014021121144716008889953443210450000 i0211 211447874035 21053 masterhpp411 adding task 0 with resources cpus1 mem32 on slave 2014021121144716008889953443210452 vestaapacheorg i0211 211447874059 21053 mastercpp2447 launching task 0 of framework 2014021121144716008889953443210450000 with resources cpus1 mem32 on slave 2014021121144716008889953443210452 vestaapacheorg i0211 211447874150 21059 slavecpp736 got assigned task 0 for framework 2014021121144716008889953443210450000 i0211 211447874200 21058 hierarchical_allocator_processhpp547 framework 2014021121144716008889953443210450000 left cpus7 mem6929 disk138501e06 ports3100032000 unused on slave 2014021121144716008889953443210452 i0211 211447874250 21053 masterhpp449 removing offer 2014021121144716008889953443210451 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210451 vestaapacheorg i0211 211447874307 21053 mastercpp1574 processing reply for offers  2014021121144716008889953443210451  on slave 2014021121144716008889953443210451 vestaapacheorg for framework 2014021121144716008889953443210450000 i0211 211447874322 21058 hierarchical_allocator_processhpp590 framework 2014021121144716008889953443210450000 filtered slave 2014021121144716008889953443210452 for 5secs i0211 211447874354 21059 slavecpp845 launching task 0 for framework 2014021121144716008889953443210450000 i0211 211447874404 21053 masterhpp411 adding task 1 with resources cpus1 mem32 on slave 2014021121144716008889953443210451 vestaapacheorg i0211 211447874428 21053 mastercpp2447 launching task 1 of framework 2014021121144716008889953443210450000 with resources cpus1 mem32 on slave 2014021121144716008889953443210451 vestaapacheorg i0211 211447874479 21058 slavecpp736 got assigned task 1 for framework 2014021121144716008889953443210450000 i0211 211447874586 21053 masterhpp449 removing offer 2014021121144716008889953443210452 with resources cpus8 mem6961 disk138501e06 ports3100032000 on slave 2014021121144716008889953443210450 vestaapacheorg i0211 211447874646 21053 mastercpp1574 processing reply for offers  2014021121144716008889953443210452  on slave 2014021121144716008889953443210450 vestaapacheorg for framework 2014021121144716008889953443210450000 i0211 211447874690 21058 slavecpp845 launching task 1 for framework 2014021121144716008889953443210450000 i0211 211447874694 21053 masterhpp411 adding task 2 with resources cpus1 mem32 on slave 2014021121144716008889953443210450 vestaapacheorg i0211 211447874716 21053 mastercpp2447 launching task 2 of framework 2014021121144716008889953443210450000 with resources cpus1 mem32 on slave 2014021121144716008889953443210450 vestaapacheorg i0211 211447874820 21053 hierarchical_allocator_processhpp547 framework 2014021121144716008889953443210450000 left cpus7 mem6929 disk138501e06 ports3100032000 unused on slave 2014021121144716008889953443210451 i0211 211447874892 21053 hierarchical_allocator_processhpp590 framework 2014021121144716008889953443210450000 filtered slave 2014021121144716008889953443210451 for 5secs i0211 211447874922 21053 hierarchical_allocator_processhpp547 framework 2014021121144716008889953443210450000 left cpus7 mem6929 disk138501e06 ports3100032000 unused on slave 2014021121144716008889953443210450 i0211 211447874980 21053 hierarchical_allocator_processhpp590 framework 2014021121144716008889953443210450000 filtered slave 2014021121144716008889953443210450 for 5secs i0211 211447875012 21053 slavecpp736 got assigned task 2 for framework 2014021121144716008889953443210450000 i0211 211447875151 21053 slavecpp845 launching task 2 for framework 2014021121144716008889953443210450000 i0211 211447875527 21059 slavecpp955 queuing task 0 for executor default of framework 2014021121144716008889953443210450000 i0211 211447875608 21059 process_isolatorcpp102 launching default homehudsonjenkinsslaveworkspacemesostrunkubuntubuildinsrcsetjava_homesrcexamplespythontestexecutor in tmpmesosz8v6cu2slaves2014021121144716008889953443210452frameworks2014021121144716008889953443210450000executorsdefaultruns02cdf8bd07574a408e77af60bb202d71 with resources cpus1 mem32 for framework 2014021121144716008889953443210450000 i0211 211447876787 21054 slavecpp469 successfully attached file tmpmesosz8v6cu2slaves2014021121144716008889953443210452frameworks2014021121144716008889953443210450000executorsdefaultruns02cdf8bd07574a408e77af60bb202d71 i0211 211447876852 21059 process_isolatorcpp165 forked executor at 21061 i0211 211447876940 21058 slavecpp955 queuing task 1 for executor default of framework 2014021121144716008889953443210450000 i0211 211447877095 21057 process_isolatorcpp102 launching default homehudsonjenkinsslaveworkspacemesostrunkubuntubuildinsrcsetjava_homesrcexamplespythontestexecutor in tmpmesosz8v6cu0slaves2014021121144716008889953443210451frameworks2014021121144716008889953443210450000executorsdefaultruns568b657d839d483faff14872fbfc27dc with resources cpus1 mem32 for framework 2014021121144716008889953443210450000 i0211 211447877102 21052 slavecpp469 successfully attached file tmpmesosz8v6cu0slaves2014021121144716008889953443210451frameworks2014021121144716008889953443210450000executorsdefaultruns568b657d839d483faff14872fbfc27dc i0211 211447878783 21057 process_isolatorcpp165 forked executor at 21062 i0211 211447879032 21053 slavecpp955 queuing task 2 for executor default of framework 2014021121144716008889953443210450000 i0211 211447879192 21054 slavecpp2098 monitoring executor default of framework 2014021121144716008889953443210450000 forked at pid 21062 i0211 211447879192 21058 slavecpp469 successfully attached file tmpmesosz8v6cu1slaves2014021121144716008889953443210450frameworks2014021121144716008889953443210450000executorsdefaultrunsa7c4170af40b449381b30ea8c70e3977 i0211 211447879166 21052 process_isolatorcpp102 launching default homehudsonjenkinsslaveworkspacemesostrunkubuntubuildinsrcsetjava_homesrcexamplesp,3
slave should wait until containerizerupdate completes successfully container resources are updated in several places in the slave and we dont check the update was successful or even wait until it completes,5
slave should wait and start executor registration timeout after launch the current code will start launch a container and wait on it before the launch is complete we should do this only after the container has successfully launched likewise for the executor registration timeout,3
python extension build is broken if gflagsdev is installed in my environment mesos build from master results in broken python api module _mesosso noformat nekto0nyadarkstar workspacemesossrcpython  pythonpathbuildliblinuxx86_6427 python c import _mesos traceback most recent call last file string line 1 in module importerror homenekto0nworkspacemesossrcpythonbuildliblinuxx86_6427_mesosso undefined symbol _zn6google14flagregistererc1epkcs2_s2_s2_pvs3_ noformat unmangled version of symbol looks like this noformat googleflagregistererflagregistererchar const char const char const char const void void noformat during configure step glog finds gflags development files and starts using them thus implicitly adding dependency on libgflagsso this breaks python extensions module and perhaps can break other mesos subsystems when moved to hosts without gflags installed this task is done when the examplestestpythonframework test will pass on a system with gflags installed,3
examplestestjavalog is flaky the examplestestjavalog test framework is flaky possibly related to a race condition between mutexes noformat  run  examplestestjavalog using temporary directory tmpexamplestest_javalog_wbweb9 feb 18 2014 121057 pm testlog main info starting a local zookeeper server  f0218 121058575036 17450 coordinatorcpp394 check failed missing not expecting local replica to be missing position 3 after the writing is done  check failure stack trace  testsscriptcpp81 failure failed java_log_testsh terminated with signal aborted  failed  examplestestjavalog 2166 ms noformat full logs attached,2
master should not deactivate authenticated frameworkslave on new authenticatemessage unless new authentication succeeds master should not deactivate an authenticated frameworkslave upon receiving a new authenticatemessage unless new authentication succeeds as it stands now a malicious user could spoof the pid of an authenticated frameworkslave and send an authenticatemessage to knock a valid frameworkslave off the authenticated list forcing the valid frameworkslave to reauthenticate and reregister this could be used in a dos attack but how should we handle the scenario when the actual authenticated frameworkslave sends an authenticatemessage that fails authentication,1
authorize taskexecutor launches,8
allocator should make an allocation decision per slave instead of per frameworkrole currently the allocatorallocate code loops through roles and frameworks based on drf sort and allocates all slaves resources to the first framework this logic should be a bit inversed instead the slave should go through each slave allocate it a roleframework and update the drf shares,2
http auth for cli integrate http auth into the cli programs,3
implement the protobufs for the scheduler api the default schedulerexecutor interface and implementation in mesos have a few drawbacks 1 the interface is fairly highlevel which makes it hard to do certain things for example handle events callbacks in batch this can have a big impact on the performance of schedulers for example writing task updates that need to be persisted 2 the implementation requires writing a lot of boilerplate jni and native python wrappers when adding additional api components the plan is to provide a lowerlevel api that can easily be used to implement the higherlevel api that is currently provided this will also open the door to more easily building nativelanguage mesos libraries ie not needing the c shim layer and building new higherlevel abstractions on top of the lowerlevel api,8
add a task_error task status during task validation we drop tasks that have errors and send task_lost status updates in most circumstances a framework will want to relaunch a task that has gone lost and in the event the task is actually malformed thus invalid this will result in an infinite loop of sending a task and having it go lost,2
add support for rate limiting slave removal to safeguard against unforeseen bugs leading to widespread slave removal it would be nice to allow for rate limiting of the decision to remove slaves andor send task_lost messages for tasks on those slaves ideally this would allow an operator to be notified soon enough to intervene before causing cluster impact,3
systemdslice  cgroup enablement fails in multiple ways when attempting to configure mesos to use systemd slices on a rawhidef21 machine it fails creating the isolator i0407 123928035354 14916 containerizercpp180 using isolation cgroupscpucgroupsmem failed to create a containerizer could not create isolator cgroupscpu failed to create isolator the cpu subsystem is comounted at sysfscgroupcpu with other subsytems  details  sysfscgroup total 0 drwxrxrx 12 root root 280 mar 18 0847  drwxrxrx 6 root root 0 mar 18 0847  drwxrxrx 2 root root 0 mar 18 0847 blkio lrwxrwxrwx 1 root root 11 mar 18 0847 cpu  cpucpuacct lrwxrwxrwx 1 root root 11 mar 18 0847 cpuacct  cpucpuacct drwxrxrx 2 root root 0 mar 18 0847 cpucpuacct drwxrxrx 2 root root 0 mar 18 0847 cpuset drwxrxrx 2 root root 0 mar 18 0847 devices drwxrxrx 2 root root 0 mar 18 0847 freezer drwxrxrx 2 root root 0 mar 18 0847 hugetlb drwxrxrx 3 root root 0 apr 3 1126 memory drwxrxrx 2 root root 0 mar 18 0847 net_cls drwxrxrx 2 root root 0 mar 18 0847 perf_event drwxrxrx 4 root root 0 mar 18 0847 systemd,3
subprocess is slow  gated by processreap poll interval subprocess uses processreap to wait on the subprocess pid and set the exit status however processreap polls with a one second interval resulting in a delay up to the interval duration before the status future is set this means if you need to wait for the subprocess to complete you get hit with edelay  05 seconds independent of the execution time for example the mesoscontainerizer uses mesosfetcher in a subprocess to fetch the executor during launch at twitter we fetch a local file ie a very fast operation but the launch is blocked until the mesosfetcher pid is reaped  adding 0 to 1 seconds for every launch the problem is even worse with a chain of short subprocesses because after the first subprocess completes youll be synchronized with the reap interval and youll see nearly the full interval before notification ie 10 subprocesses each of  1 second duration with take 10 seconds this has become particularly apparent in some new tests im working on where test durations are now greatly extended with each taking several seconds,1
master should disallow frameworks that reconnect after failover timeout when a scheduler reconnects after the failover timeout has exceeded the framework id is usually reused because the scheduler doesnt know that the timeout exceeded and it is actually handled as a new framework the frameworkframework_id route of the web ui doesnt handle those cases very well because its key is reused it only shows the terminated one would it make sense to ignore the provided framework id when a scheduler reconnects to a terminated framework and generate a new id to make sure its unique,2
stouts os module uses a mix of trynothing and bool returns stouts os module should use trynothing for return values throughout,2
stouts osls should return a try stouts osls returns a list that can be empty  instead it should return a trylist to be consistent,2
examplestesttestframework noexecutorframework flaky im having trouble reproducing this but i did observe it once on my osx system noformat  running 2 tests from 1 test case  global test environment setup  2 tests from examplestest  run  examplestesttestframework srctestsscriptcpp81 failure failed test_framework_testsh terminated with signal abort trap 6  failed  examplestesttestframework 953 ms  run  examplestestnoexecutorframework  ok  examplestestnoexecutorframework 10162 ms  2 tests from examplestest 11115 ms total  global test environment teardown  2 tests from 1 test case ran 11121 ms total  passed  1 test  failed  1 test listed below  failed  examplestesttestframework noformat when investigating a failed make check for httpsreviewsapacheorgr20971 noformat  6 tests from examplestest  run  examplestesttestframework  ok  examplestesttestframework 8643 ms  run  examplestestnoexecutorframework testsscriptcpp81 failure failed no_executor_framework_testsh terminated with signal aborted  failed  examplestestnoexecutorframework 7220 ms  run  examplestestjavaframework  ok  examplestestjavaframework 11181 ms  run  examplestestjavaexception  ok  examplestestjavaexception 5624 ms  run  examplestestjavalog  ok  examplestestjavalog 6472 ms  run  examplestestpythonframework  ok  examplestestpythonframework 14467 ms  6 tests from examplestest 53607 ms total noformat,1
authorize offer allocations when frameworks register or reregister they should authorize their roles split register framework  reregister framework,8
implement decent unit test coverage for the mesosfetcher tool there are current no tests that cover the mesosfetcher tool itself and hence bugs like mesos1313 have accidentally slipped though,2
improve master and slave metric names as we move the metrics to a new endpoint we should consider revisiting the names of some of the current metrics to make them clearer it may also be worth considering changing some existing counterstyle metrics to gauges,3
add perframeworkprincipal counters for all messages from a scheduler on master frameworkprincipal is used identify one or more frameworks if multiple frameworks use the same principal theyll have one counter showing their combined message count,3
add flags support for json,2
garbagecollectorintegrationtestdiskusage is flaky from jenkins httpsbuildsapacheorgjobmesosubuntudistcheck79consolefull noformat  run  garbagecollectorintegrationtestdiskusage using temporary directory tmpgarbagecollectorintegrationtest_diskusage_pu3ym7 i0507 032738775058 5758 leveldbcpp174 opened db in 44343989ms i0507 032738787498 5758 leveldbcpp181 compacted db in 12411065ms i0507 032738787533 5758 leveldbcpp196 created db iterator in 4008ns i0507 032738787545 5758 leveldbcpp202 seeked to beginning of db in 598ns i0507 032738787552 5758 leveldbcpp271 iterated through 0 keys in the db in 173ns i0507 032738787564 5758 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0507 032738787858 5777 recovercpp425 starting replica recovery i0507 032738788352 5793 mastercpp267 master 20140507032738453759884584625758 hemeraapacheorg started on 140211112758462 i0507 032738788377 5793 mastercpp304 master only allowing authenticated frameworks to register i0507 032738788383 5793 mastercpp309 master only allowing authenticated slaves to register i0507 032738788389 5793 credentialshpp35 loading credentials for authentication i0507 032738789064 5779 recovercpp451 replica is in empty status w0507 032738789115 5793 credentialshpp48 failed to stat credentials file filetmpgarbagecollectorintegrationtest_diskusage_pu3ym7credentials no such file or directory i0507 032738789489 5779 mastercpp104 no whitelist given advertising offers for all slaves i0507 032738789531 5778 hierarchical_allocator_processhpp301 initializing hierarchical allocator process with master  master140211112758462 i0507 032738791007 5788 replicacpp638 replica in empty status received a broadcasted recover request i0507 032738791177 5780 mastercpp921 the newly elected leader is master140211112758462 with id 20140507032738453759884584625758 i0507 032738791198 5780 mastercpp931 elected as the leading master i0507 032738791205 5780 mastercpp752 recovering from registrar i0507 032738791251 5796 recovercpp188 received a recover response from a replica in empty status i0507 032738791323 5797 registrarcpp313 recovering registrar i0507 032738792137 5795 recovercpp542 updating replica status to starting i0507 032738807531 5781 leveldbcpp304 persisting metadata 8 bytes to leveldb took 15124092ms i0507 032738807559 5781 replicacpp320 persisted replica status to starting i0507 032738807621 5781 recovercpp451 replica is in starting status i0507 032738809319 5799 replicacpp638 replica in starting status received a broadcasted recover request i0507 032738809983 5795 recovercpp188 received a recover response from a replica in starting status i0507 032738811204 5778 recovercpp542 updating replica status to voting i0507 032738827595 5795 leveldbcpp304 persisting metadata 8 bytes to leveldb took 16011355ms i0507 032738827627 5795 replicacpp320 persisted replica status to voting i0507 032738827683 5795 recovercpp556 successfully joined the paxos group i0507 032738827775 5795 recovercpp440 recover process terminated i0507 032738828966 5780 logcpp656 attempting to start the writer i0507 032738831114 5782 replicacpp474 replica received implicit promise request with proposal 1 i0507 032738847708 5782 leveldbcpp304 persisting metadata 8 bytes to leveldb took 16573137ms i0507 032738847739 5782 replicacpp342 persisted promised to 1 i0507 032738848141 5797 coordinatorcpp230 coordinator attemping to fill missing position i0507 032738849684 5790 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0507 032738863777 5790 leveldbcpp341 persisting action 8 bytes to leveldb took 14076775ms i0507 032738863801 5790 replicacpp676 persisted action at 0 i0507 032738864915 5798 replicacpp508 replica received write request for position 0 i0507 032738864949 5798 leveldbcpp436 reading position from leveldb took 11807ns i0507 032738879945 5798 leveldbcpp341 persisting action 14 bytes to leveldb took 14978446ms i0507 032738879976 5798 replicacpp676 persisted action at 0 i0507 032738880491 5797 replicacpp655 replica received learned notice for position 0 i0507 032738895969 5797 leveldbcpp341 persisting action 16 bytes to leveldb took 15459949ms i0507 032738895992 5797 replicacpp676 persisted action at 0 i0507 032738896003 5797 replicacpp661 replica learned nop action at position 0 i0507 032738896411 5783 logcpp672 writer started with ending position 0 i0507 032738898058 5798 leveldbcpp436 reading position from leveldb took 11910ns i0507 032738899749 5777 registrarcpp346 successfully fetched the registry 0b i0507 032738899766 5777 registrarcpp422 attempting to update the registry i0507 032738901458 5791 logcpp680 attempting to append 137 bytes to the log i0507 032738901666 5780 coordinatorcpp340 coordinator attempting to write append action at position 1 i0507 032738902773 5783 replicacpp508 replica received write request for position 1 i0507 032738916127 5783 leveldbcpp341 persisting action 156 bytes to leveldb took 13225715ms i0507 032738916152 5783 replicacpp676 persisted action at 1 i0507 032738916534 5790 replicacpp655 replica received learned notice for position 1 i0507 032738928203 5790 leveldbcpp341 persisting action 158 bytes to leveldb took 11652434ms i0507 032738928225 5790 replicacpp676 persisted action at 1 i0507 032738928236 5790 replicacpp661 replica learned append action at position 1 i0507 032738928546 5790 registrarcpp479 successfully updated registry i0507 032738928642 5790 registrarcpp372 successfully recovered registrar i0507 032738929044 5783 mastercpp779 recovered 0 slaves from the registry 99b  allowing 10mins for slaves to reregister i0507 032738929502 5799 logcpp699 attempting to truncate the log to 1 i0507 032738929888 5797 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0507 032738930161 5781 replicacpp508 replica received write request for position 2 i0507 032738932977 5789 slavecpp140 slave started on 56140211112758462 i0507 032738932991 5789 credentialshpp35 loading credentials for authentication w0507 032738933567 5789 credentialshpp48 failed to stat credentials file filetmpgarbagecollectorintegrationtest_diskusage_a9pxkscredential no such file or directory i0507 032738933585 5789 slavecpp230 slave using credential for testprincipal i0507 032738933765 5789 slavecpp243 slave resources cpus2 mem1024 disk1024 ports3100032000 i0507 032738933854 5789 slavecpp271 slave hostname hemeraapacheorg i0507 032738933863 5789 slavecpp272 slave checkpoint false i0507 032738934239 5778 statecpp33 recovering state from tmpgarbagecollectorintegrationtest_diskusage_a9pxksmeta i0507 032738934960 5792 status_update_managercpp193 recovering status update manager i0507 032738935123 5779 slavecpp2945 finished recovery i0507 032738936998 5779 slavecpp526 new master detected at master140211112758462 i0507 032738937021 5779 slavecpp586 authenticating with master master140211112758462 i0507 032738937077 5798 status_update_managercpp167 new master detected at master140211112758462 i0507 032738937306 5779 slavecpp559 detecting new master i0507 032738937335 5800 authenticateehpp128 creating new client sasl connection i0507 032738938030 5778 mastercpp2798 authenticating slave56140211112758462 i0507 032738938742 5783 authenticatorhpp148 creating new server sasl connection i0507 032738939312 5786 authenticateehpp219 received sasl authentication mechanisms crammd5 i0507 032738939340 5786 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0507 032738939390 5786 authenticatorhpp254 received sasl authentication start i0507 032738939553 5786 authenticatorhpp342 authentication requires more steps i0507 032738939592 5786 authenticateehpp265 received sasl authentication step i0507 032738939715 5786 authenticatorhpp282 received sasl authentication step i0507 032738939803 5786 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0507 032738939821 5786 auxpropcpp153 looking up auxiliary property userpassword i0507 032738939831 5786 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0507 032738939841 5786 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0507 032738939851 5786 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0507 032738939857 5786 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0507 032738939870 5786 authenticatorhpp334 authentication success i0507 032738939937 5786 authenticateehpp305 authentication success i0507 032738940016 5778 mastercpp2838 successfully authenticated slave56140211112758462 i0507 032738940449 5799 slavecpp643 successfully authenticated with master master140211112758462 i0507 032738940513 5799 slavecpp872 will retry registration in 5176207635secs if necessary i0507 032738940625 5794 mastercpp2134 registering slave at slave56140211112758462 hemeraapacheorg with id 201405070327384537598845846257580 i0507 032738940800 5796 registrarcpp422 attempting to update the registry i0507 032738940850 5781 leveldbcpp341 persisting action 16 bytes to leveldb took 10659152ms i0507 032738940871 5781 replicacpp676 persisted action at 2 i0507 032738941843 5788 replicacpp655 replica received learned notice for position 2 i0507 032738953193 5788 leveldbcpp341 persisting action 18 bytes to leveldb took 11291343ms i0507 032738953258 5788 leveldbcpp399 deleting 1 keys from leveldb took 33725ns i0507 032738953274 5788 replicacpp676 persisted action at 2 i0507 032738953282 5788 replicacpp661 replica learned truncate action at position 2 i0507 032738953541 5797 logcpp680 attempting to append 330 bytes to the log i0507 032738953614 5797 coordinatorcpp340 coordinator attempting to write append action at position 3 i0507 032738954731 5789 replicacpp508 replica received write request for position 3 i0507 032738965240 5789 leveldbcpp341 persisting action 349 bytes to leveldb took 10489719ms i0507 032738965261 5789 replicacpp676 persisted action at 3 i0507 032738966253 5780 replicacpp655 replica received learned notice for position 3 i0507 032738977375 5780 leveldbcpp341 persisting action 351 bytes to leveldb took 11098798ms i0507 032738977408 5780 replicacpp676 persisted action at 3 i0507 032738977421 5780 replicacpp661 replica learned append action at position 3 i0507 032738977859 5792 registrarcpp479 successfully updated registry i0507 032738977926 5780 logcpp699 attempting to truncate the log to 3 i0507 032738978060 5792 mastercpp2174 registered slave 201405070327384537598845846257580 at slave56140211112758462 hemeraapacheorg i0507 032738978112 5792 mastercpp3283 adding slave 201405070327384537598845846257580 at slave56140211112758462 hemeraapacheorg with cpus2 mem1024 disk1024 ports3100032000 i0507 032738978134 5784 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0507 032738978508 5785 slavecpp676 registered with master master140211112758462 given slave id 201405070327384537598845846257580 i0507 032738978631 5786 hierarchical_allocator_processhpp444 added slave 201405070327384537598845846257580 hemeraapacheorg with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0507 032738978677 5786 hierarchical_allocator_processhpp707 performed allocation for slave 201405070327384537598845846257580 in 5421ns i0507 032738979872 5796 replicacpp508 replica received write request for position 4 i0507 032738982084 5758 schedcpp121 version 0190 i0507 032738982213 5789 schedcpp217 new master detected at master140211112758462 i0507 032738982228 5789 schedcpp268 authenticating with master master140211112758462 i0507 032738982347 5788 authenticateehpp128 creating new client sasl connection i0507 032738982676 5788 mastercpp2798 authenticating scheduler59140211112758462 i0507 032738983100 5788 authenticatorhpp148 creating new server sasl connection i0507 032738983294 5788 authenticateehpp219 received sasl authentication mechanisms crammd5 i0507 032738983312 5788 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0507 032738983360 5788 authenticatorhpp254 received sasl authentication start i0507 032738983505 5788 authenticatorhpp342 authentication requires more steps i0507 032738984220 5782 authenticateehpp265 received sasl authentication step i0507 032738984275 5782 authenticatorhpp282 received sasl authentication step i0507 032738984315 5782 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0507 032738984347 5782 auxpropcpp153 looking up auxiliary property userpassword i0507 032738984359 5782 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0507 032738984370 5782 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0507 032738984377 5782 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0507 032738984383 5782 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0507 032738984397 5782 authenticatorhpp334 authentication success i0507 032738984429 5782 authenticateehpp305 authentication success i0507 032738984469 5795 mastercpp2838 successfully authenticated scheduler59140211112758462 i0507 032738985110 5782 schedcpp342 successfully authenticated with master master140211112758462 i0507 032738985133 5782 schedcpp461 sending registration request to master140211112758462 i0507 032738985326 5795 mastercpp980 received registration request from scheduler59140211112758462 i0507 032738985357 5795 mastercpp998 registering framework 201405070327384537598845846257580000 at scheduler59140211112758462 i0507 032738985424 5795 schedcpp392 framework registered with 201405070327384537598845846257580000 i0507 032738985471 5792 hierarchical_allocator_processhpp331 added framework 201405070327384537598845846257580000 i0507 032738985610 5795 schedcpp406 schedulerregistered took 36702ns i0507 032738985646 5792 hierarchical_allocator_processhpp751 offering cpus2 mem1024 disk1024 ports3100032000 on slave 201405070327384537598845846257580 to framework 201405070327384537598845846257580000 i0507 032738985954 5792 hierarchical_allocator_processhpp687 performed allocation for 1 slaves in 330895ns i0507 032738986001 5789 masterhpp612 adding offer 201405070327384537598845846257580 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201405070327384537598845846257580 hemeraapacheorg i0507 032738986090 5789 mastercpp2747 sending 1 offers to framework 201405070327384537598845846257580000 i0507 032738986548 5792 schedcpp529 schedulerresourceoffers took 162873ns i0507 032738986721 5792 masterhpp622 removing offer 201405070327384537598845846257580 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201405070327384537598845846257580 hemeraapacheorg i0507 032738986781 5792 mastercpp1812 processing reply for offers  201405070327384537598845846257580  on slave 201405070327384537598845846257580 at slave56140211112758462 hemeraapacheorg for framework 201405070327384537598845846257580000 i0507 032738986843 5792 masterhpp584 adding task 0 with resources cpus2 mem1024 on slave 201405070327384537598845846257580 hemeraapacheorg i0507 032738986876 5792 mastercpp2922 launching task 0 of framework 201405070327384537598845846257580000 with resources cpus2 mem1024 on slave 201405070327384537598845846257580 at slave56140211112758462 hemeraapacheorg i0507 032738986981 5795 slavecpp906 got assigned task 0 for framework 201405070327384537598845846257580000 i0507 032738987180 5795 slavecpp1016 launching task 0 for framework 201405070327384537598845846257580000 i0507 032738987203 5787 hierarchical_allocator_processhpp546 framework 201405070327384537598845846257580000 left disk1024 ports3100032000 unused on slave 201405070327384537598845846257580 i0507 032738987287 5787 hierarchical_allocator_processhpp589 framework 201405070327384537598845846257580000 filtered slave 201405070327384537598845846257580 for 5secs i0507 032738991395 5795 execcpp131 version 0190 i0507 032738991497 5779 execcpp181 executor started at executor27140211112758462 with pid 5758 i0507 032738991510 5795 slavecpp1126 queuing task 0 for executor default of framework 201405070327384537598845846257580000 i0507 032738991566 5795 slavecpp487 successfully attached file tmpgarbagecollectorintegrationtest_diskusage_a9pxksslaves201405070327384537598845846257580frameworks201405070327384537598845846257580000executorsdefaultrunsde776bec28224bbcbefceec40eb5f674 i0507 032738991595 5795 slavecpp2283 monitoring executor default of framework 201405070327384537598845846257580000 in container de776bec28224bbcbefceec40eb5f674 i0507 032738991778 5795 slavecpp1599 got registration for executor default of framework 201405070327384537598845846257580000 i0507 032738991874 5795 slavecpp1718 flushing queued task 0 for executor default of framework 201405070327384537598845846257580000 i0507 032738991935 5780 execcpp205 executor registered on slave 201405070327384537598845846257580 i0507 032738993419 5796 leveldbcpp341 persisting action 16 bytes to leveldb took 13489998ms i0507 032738993449 5796 replicacpp676 persisted action at 4 i0507 032738994510 5777 replicacpp655 replica received learned notice for position 4 i0507 0327,2
show when the leading master was elected in the webui this would be nice to have during debugging,1
slaverecoverytest0multipleframeworks is flaky gtest_repeat1 gtest_shuffle gtest_break_on_failure noformat  run  slaverecoverytest0multipleframeworks warning logging before initgooglelogging is written to stderr i0513 154205931761 4320 execcpp131 version 0190 i0513 154205936698 4340 execcpp205 executor registered on slave 201405131542041684287951872130620 registered executor on artoo starting task 51991f97f5fd4905ad0f02668083af7c forked command at 4367 sh c sleep 1000 warning logging before initgooglelogging is written to stderr i0513 154206915061 4408 execcpp131 version 0190 i0513 154206931149 4435 execcpp205 executor registered on slave 201405131542041684287951872130620 registered executor on artoo starting task eaf5d8d63a6c4ee184c1fae20fb1df83 sh c sleep 1000 forked command at 4439 i0513 154206998332 4340 execcpp251 received reconnect request from slave 201405131542041684287951872130620 i0513 154206998414 4436 execcpp251 received reconnect request from slave 201405131542041684287951872130620 i0513 154207006350 4437 execcpp228 executor reregistered on slave 201405131542041684287951872130620 reregistered executor on artoo i0513 154207027039 4337 execcpp378 executor asked to shutdown shutting down sending sigterm to process tree at pid 4367 killing the following process trees   4367 sh c sleep 1000  4368 sleep 1000  srctestsslave_recovery_testscpp2807 failure value of status1getstate actual task_failed expected task_killed program received signal sigsegv segmentation fault testingunittestaddtestpartresult this0x154dac0 testingunittestgetinstanceinstance result_typetestingtestpartresultkfatalfailure file_name0xeb6b6c srctestsslave_recovery_testscpp line_number2807 message os_stack_trace at gmock160gtestsrcgtestcc3795 3795 static_castvolatile intnull  1 gdb bt 0 testingunittestaddtestpartresult this0x154dac0 testingunittestgetinstanceinstance result_typetestingtestpartresultkfatalfailure file_name0xeb6b6c srctestsslave_recovery_testscpp line_number2807 message os_stack_trace at gmock160gtestsrcgtestcc3795 1 0x0000000000df98b9 in testinginternalasserthelperoperator this0x7fffffffb860 message at gmock160gtestsrcgtestcc356 2 0x0000000000cdfa57 in slaverecoverytest_multipleframeworks_testmesosinternalslavemesoscontainerizertestbody this0x1954db0 at srctestsslave_recovery_testscpp2807 3 0x0000000000e22583 in testinginternalhandlesehexceptionsinmethodifsupportedtestingtest void object0x1954db0 methodvirtual testingtesttestbody location0xed0af0 the test body at gmock160gtestsrcgtestcc2090 4 0x0000000000e12467 in testinginternalhandleexceptionsinmethodifsupportedtestingtest void object0x1954db0 methodvirtual testingtesttestbody location0xed0af0 the test body at gmock160gtestsrcgtestcc2126 5 0x0000000000e010d5 in testingtestrun this0x1954db0 at gmock160gtestsrcgtestcc2161 6 0x0000000000e01ceb in testingtestinforun this0x158cf80 at gmock160gtestsrcgtestcc2338 7 0x0000000000e02387 in testingtestcaserun this0x158a880 at gmock160gtestsrcgtestcc2445 8 0x0000000000e079ed in testinginternalunittestimplrunalltests this0x1558b40 at gmock160gtestsrcgtestcc4237 9 0x0000000000e1ec83 in testinginternalhandlesehexceptionsinmethodifsupportedtestinginternalunittestimpl bool object0x1558b40 methodbool testinginternalunittestimpltestinginternalunittestimpl  const 0xe07700 testinginternalunittestimplrunalltests location0xed1219 auxiliary test code environments or event listeners at gmock160gtestsrcgtestcc2090 10 0x0000000000e14217 in testinginternalhandleexceptionsinmethodifsupportedtestinginternalunittestimpl bool object0x1558b40 methodbool testinginternalunittestimpltestinginternalunittestimpl  const 0xe07700 testinginternalunittestimplrunalltests location0xed1219 auxiliary test code environments or event listeners at gmock160gtestsrcgtestcc2126 11 0x0000000000e076d7 in testingunittestrun this0x154dac0 testingunittestgetinstanceinstance at gmock160gtestsrcgtestcc3872 12 0x0000000000b99887 in main argc1 argv0x7fffffffd9f8 at srctestsmaincpp107 gdb frame 2 2 0x0000000000cdfa57 in slaverecoverytest_multipleframeworks_testmesosinternalslavemesoscontainerizertestbody this0x1954db0 at srctestsslave_recovery_testscpp2807 2807 assert_eqtask_killed status1getstate gdb p status1 1  data  std__shared_ptrprocessfuturemesostaskstatusdata 2  _m_ptr  0x1963140 _m_refcount  _m_pi  0x198a620 no data fields gdb p status1get 2  const mesostaskstatus  0x7fffdc5bf5f0 googleprotobufmessage  googleprotobufmessagelite  _vptrmessagelite  0x7ffff74bc940 vtable for mesostaskstatus16 no data fields static ktaskidfieldnumber  1 static kstatefieldnumber  2 static kmessagefieldnumber  4 static kdatafieldnumber  3 static kslaveidfieldnumber  5 static ktimestampfieldnumber  6 _unknown_fields_  fields_  0x0 task_id_  0x7fffdc5ce9a0 message_  0x7fffdc5f5880 data_  0x154b4b0 googleprotobufinternalkemptystring slave_id_  0x7fffdc59c4f0 timestamp_  1429688582046252 state_  3 _cached_size_  0 _has_bits_  55 static default_instance_  0x0 gdb p status1getstate 3  mesostask_failed gdb list 2802  kill task 1 2803 driver1killtasktask1task_id 2804 2805  wait for task_killed update 2806 await_readystatus1 2807 assert_eqtask_killed status1getstate 2808 2809  kill task 2 2810 driver2killtasktask2task_id 2811 noformat,1
expose libprocess queue length from scheduler driver to metrics endpoint we expose the masters event queue length and we should do the same for the scheduler driver,1
keep track of the principals for authenticated pids in master need to add a principal field to frameworkinfo and verify if the framework has the claimed principal during registration,3
verify static libprocess scheduler port works with mesos master,5
failure when znode is removed before we can read its contents looks like the following can occur when a znode goes away right before we can read its contents noformat titleslave exit i0520 163345721727 29155 groupcpp382 trying to create path homemesostestmaster in zookeeper i0520 163348600837 29155 detectorcpp134 detected a new leader id2617 i0520 163348601428 29147 groupcpp655 trying to get homemesostestmasterinfo_0000002617 in zookeeper failed to detect a master failed to get data for ephemeral node homemesostestmasterinfo_0000002617 in zookeeper no node slave exit status 1 noformat,3
write parser for perf output 1 should support output from pid and cgroup targets 2 should support output for the same events from  1 cgroup 3 should return as perfstatistics protobuf,3
test different versions of perf test across different kernel versions at least 26xx and 3x and across different distributions test input flags and parsing output,3
test perf isolator for slave roll forwardroll back test that changes to addremove perf isolator will be handled through slave recovery eg containers started without the perf isolator continue to report resource statistics and containers started with the perf isolator will include perf statistics,2
introduce a perfstatistics protobuf field names from perf list normalized to convert hyphens to underscores and downcased start with just the hardware and software events not raw hardware breakpoints or tracepoints all fields should be optional include as an optional field to resourcestatistics,2
rename resourcestatistics for containers rename containerstatistics which includes optional resourcestatistics and optional perfstatistics,8
document perf isolator flags document interval duration and the event flags document event name normalization for the protobuf,1
keep terminal unacknowledged tasks in the masters state once we are sending acknowledgments through the master as per mesos1409 we need to keep terminal tasks that are unacknowledged in the masters memory this will allow us to identify these tasks to frameworks when we havent yet forwarded them an update without this were susceptible to mesos1389,5
mesos tests should not rely on echo triggered by mesos1413 i would like to propose changing our tests to not rely on echo but to use printf instead this seems to be useful as echo is introducing an extra linefeed after the supplied string whereas printf does not the n switch preventing that extra linefeed is unfortunately not portable  it is not supported by the builtin echo of the bsd  osx binsh,1
logzookeepertestwriteread test is flaky code  run  logzookeepertestwriteread i0527 232348286031 1352 zookeeper_test_servercpp158 started zookeepertestserver on port 39446 i0527 232348293916 1352 log_testscpp1945 using temporary directory tmplogzookeepertest_writeread_vyty8g i0527 232348296430 1352 leveldbcpp176 opened db in 2459713ms i0527 232348296740 1352 leveldbcpp183 compacted db in 286843ns i0527 232348296761 1352 leveldbcpp198 created db iterator in 3083ns i0527 232348296772 1352 leveldbcpp204 seeked to beginning of db in 4541ns i0527 232348296777 1352 leveldbcpp273 iterated through 0 keys in the db in 87ns i0527 232348296788 1352 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0527 232348297499 1383 leveldbcpp306 persisting metadata 8 bytes to leveldb took 505340ns i0527 232348297513 1383 replicacpp320 persisted replica status to voting i0527 232348299492 1352 leveldbcpp176 opened db in 173582ms i0527 232348299773 1352 leveldbcpp183 compacted db in 263937ns i0527 232348299793 1352 leveldbcpp198 created db iterator in 7494ns i0527 232348299806 1352 leveldbcpp204 seeked to beginning of db in 235ns i0527 232348299813 1352 leveldbcpp273 iterated through 0 keys in the db in 93ns i0527 232348299821 1352 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0527 232348300503 1380 leveldbcpp306 persisting metadata 8 bytes to leveldb took 492309ns i0527 232348300516 1380 replicacpp320 persisted replica status to voting i0527 232348302500 1352 leveldbcpp176 opened db in 1793829ms i0527 232348303642 1352 leveldbcpp183 compacted db in 1123929ms i0527 232348303669 1352 leveldbcpp198 created db iterator in 5865ns i0527 232348303689 1352 leveldbcpp204 seeked to beginning of db in 8811ns i0527 232348303705 1352 leveldbcpp273 iterated through 1 keys in the db in 9545ns i0527 232348303715 1352 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 20140527 23234830313520x2b1173a29700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140527 23234830313520x2b1173a29700zoo_infolog_env716 client environmenthostnameminerva 20140527 23234830313520x2b1173a29700zoo_infolog_env723 client environmentosnamelinux 20140527 23234830313520x2b1173a29700zoo_infolog_env724 client environmentosarch32057generic 20140527 23234830313520x2b1173a29700zoo_infolog_env725 client environmentosversion87ubuntu smp tue nov 12 213510 utc 2013 20140527 23234830313520x2b1173e2b700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140527 23234830413520x2b1173e2b700zoo_infolog_env716 client environmenthostnameminerva 20140527 23234830413520x2b1173e2b700zoo_infolog_env723 client environmentosnamelinux 20140527 23234830413520x2b1173e2b700zoo_infolog_env724 client environmentosarch32057generic 20140527 23234830413520x2b1173e2b700zoo_infolog_env725 client environmentosversion87ubuntu smp tue nov 12 213510 utc 2013 20140527 23234830413520x2b1173a29700zoo_infolog_env733 client environmentusernamenull i0527 232348303988 1380 logcpp238 attempting to join replica to zookeeper group 20140527 23234830413520x2b1173e2b700zoo_infolog_env733 client environmentusernamenull 20140527 23234830413520x2b1173a29700zoo_infolog_env741 client environmentuserhomehomejenkins i0527 232348304198 1385 recovercpp425 starting replica recovery 20140527 23234830413520x2b1173e2b700zoo_infolog_env741 client environmentuserhomehomejenkins 20140527 23234830413520x2b1173a29700zoo_infolog_env753 client environmentuserdirtmplogzookeepertest_writeread_vyty8g 20140527 23234830413520x2b1173a29700zoo_infozookeeper_init786 initiating client connection host12700139446 sessiontimeout5000 watcher0x2b11708e98d0 sessionid0 sessionpasswdnull context0x2b118002f4e0 flags0 20140527 23234830413520x2b1173e2b700zoo_infolog_env753 client environmentuserdirtmplogzookeepertest_writeread_vyty8g i0527 232348304352 1385 recovercpp451 replica is in voting status 20140527 23234830413520x2b1173e2b700zoo_infozookeeper_init786 initiating client connection host12700139446 sessiontimeout5000 watcher0x2b11708e98d0 sessionid0 sessionpasswdnull context0x2b1198015ca0 flags0 i0527 232348304417 1385 recovercpp440 recover process terminated 20140527 23234830413520x2b12897b8700zoo_infocheck_events1703 initiated connection to server 12700139446 20140527 23234830413520x2b12891b5700zoo_infocheck_events1703 initiated connection to server 12700139446 i0527 232348311262 1352 leveldbcpp176 opened db in 7261703ms 20140527 23234831113520x2b12897b8700zoo_infocheck_events1750 session establishment complete on server 12700139446 sessionid0x1463fff34bd0000 negotiated timeout6000 i0527 232348312379 1381 groupcpp310 group process 61467195138835151 connected to zookeeper i0527 232348312407 1381 groupcpp784 syncing group operations queue size joins cancels datas  0 0 0 i0527 232348312417 1381 groupcpp382 trying to create path log in zookeeper i0527 232348312422 1352 leveldbcpp183 compacted db in 1119843ms i0527 232348312505 1352 leveldbcpp198 created db iterator in 3901ns i0527 232348312526 1352 leveldbcpp204 seeked to beginning of db in 7398ns i0527 232348312541 1352 leveldbcpp273 iterated through 1 keys in the db in 6345ns i0527 232348312553 1352 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 20140527 23234831213520x2b1173627700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140527 23234831213520x2b1173627700zoo_infolog_env716 client environmenthostnameminerva 20140527 23234831213520x2b1173627700zoo_infolog_env723 client environmentosnamelinux 20140527 23234831213520x2b1173627700zoo_infolog_env724 client environmentosarch32057generic 20140527 23234831213520x2b1173627700zoo_infolog_env725 client environmentosversion87ubuntu smp tue nov 12 213510 utc 2013 20140527 23234831213520x2b1173627700zoo_infolog_env733 client environmentusernamenull 20140527 23234831213520x2b12891b5700zoo_infocheck_events1750 session establishment complete on server 12700139446 sessionid0x1463fff34bd0001 negotiated timeout6000 20140527 23234831313520x2b1173627700zoo_infolog_env741 client environmentuserhomehomejenkins 20140527 23234831313520x2b1173627700zoo_infolog_env753 client environmentuserdirtmplogzookeepertest_writeread_vyty8g 20140527 23234831313520x2b1173627700zoo_infozookeeper_init786 initiating client connection host12700139446 sessiontimeout5000 watcher0x2b11708e98d0 sessionid0 sessionpasswdnull context0x2b119001fd20 flags0 i0527 232348313247 1380 groupcpp310 group process 61667195138835151 connected to zookeeper i0527 232348313266 1380 groupcpp784 syncing group operations queue size joins cancels datas  1 0 0 i0527 232348313273 1380 groupcpp382 trying to create path log in zookeeper 20140527 23234831313520x2b12889b0700zoo_infocheck_events1703 initiated connection to server 12700139446 20140527 23234831313520x2b1173828700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140527 23234831313520x2b1173828700zoo_infolog_env716 client environmenthostnameminerva 20140527 23234831313520x2b1173828700zoo_infolog_env723 client environmentosnamelinux 20140527 23234831313520x2b1173828700zoo_infolog_env724 client environmentosarch32057generic 20140527 23234831313520x2b1173828700zoo_infolog_env725 client environmentosversion87ubuntu smp tue nov 12 213510 utc 2013 i0527 232348313436 1387 logcpp238 attempting to join replica to zookeeper group 20140527 23234831313520x2b1173828700zoo_infolog_env733 client environmentusernamenull 20140527 23234831313520x2b1173828700zoo_infolog_env741 client environmentuserhomehomejenkins 20140527 23234831313520x2b1173828700zoo_infolog_env753 client environmentuserdirtmplogzookeepertest_writeread_vyty8g 20140527 23234831313520x2b1173828700zoo_infozookeeper_init786 initiating client connection host12700139446 sessiontimeout5000 watcher0x2b11708e98d0 sessionid0 sessionpasswdnull context0x2b1190011ea0 flags0 i0527 232348313601 1387 recovercpp425 starting replica recovery i0527 232348313721 1382 recovercpp451 replica is in voting status i0527 232348313794 1382 recovercpp440 recover process terminated 20140527 23234831313520x2b1288bb1700zoo_infocheck_events1703 initiated connection to server 12700139446 i0527 232348313973 1383 logcpp656 attempting to start the writer 20140527 23234831513520x2b12889b0700zoo_infocheck_events1750 session establishment complete on server 12700139446 sessionid0x1463fff34bd0002 negotiated timeout6000 i0527 232348315682 1387 groupcpp310 group process 61967195138835151 connected to zookeeper 20140527 23234831513520x2b1288bb1700zoo_infocheck_events1750 session establishment complete on server 12700139446 sessionid0x1463fff34bd0003 negotiated timeout6000 i0527 232348315709 1387 groupcpp784 syncing group operations queue size joins cancels datas  0 0 0 i0527 232348315738 1387 groupcpp382 trying to create path log in zookeeper i0527 232348315964 1386 groupcpp310 group process 62167195138835151 connected to zookeeper i0527 232348315981 1386 groupcpp784 syncing group operations queue size joins cancels datas  1 0 0 i0527 232348315989 1386 groupcpp382 trying to create path log in zookeeper i0527 232348317881 1385 networkhpp423 zookeeper group memberships changed i0527 232348317937 1381 groupcpp655 trying to get log0000000000 in zookeeper i0527 232348318205 1382 networkhpp423 zookeeper group memberships changed i0527 232348318317 1383 groupcpp655 trying to get log0000000000 in zookeeper i0527 232348319154 1382 networkhpp461 zookeeper group pids  logreplica2267195138835151  i0527 232348319541 1386 networkhpp461 zookeeper group pids  logreplica2267195138835151  i0527 232348319851 1381 replicacpp474 replica received implicit promise request with proposal 1 i0527 232348319905 1387 replicacpp474 replica received implicit promise request with proposal 1 i0527 232348319907 1384 networkhpp423 zookeeper group memberships changed i0527 232348320091 1385 groupcpp655 trying to get log0000000000 in zookeeper i0527 232348320384 1383 networkhpp423 zookeeper group memberships changed i0527 232348320441 1381 leveldbcpp306 persisting metadata 8 bytes to leveldb took 568396ns i0527 232348320456 1384 groupcpp655 trying to get log0000000000 in zookeeper i0527 232348320461 1381 replicacpp342 persisted promised to 1 i0527 232348320446 1387 leveldbcpp306 persisting metadata 8 bytes to leveldb took 516015ns i0527 232348320497 1387 replicacpp342 persisted promised to 1 i0527 232348320814 1383 coordinatorcpp230 coordinator attemping to fill missing position i0527 232348321050 1384 groupcpp655 trying to get log0000000001 in zookeeper i0527 232348321063 1385 groupcpp655 trying to get log0000000001 in zookeeper i0527 232348321341 1387 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0527 232348321375 1381 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0527 232348321506 1387 leveldbcpp343 persisting action 8 bytes to leveldb took 89us i0527 232348321530 1387 replicacpp676 persisted action at 0 i0527 232348321584 1381 leveldbcpp343 persisting action 8 bytes to leveldb took 122910ns i0527 232348321602 1381 replicacpp676 persisted action at 0 i0527 232348321775 1383 networkhpp461 zookeeper group pids  logreplica2267195138835151 logreplica2367195138835151  i0527 232348321961 1381 replicacpp508 replica received write request for position 0 i0527 232348321984 1381 leveldbcpp438 reading position from leveldb took 7813ns i0527 232348322064 1380 networkhpp461 zookeeper group pids  logreplica2267195138835151 logreplica2367195138835151  i0527 232348322073 1381 leveldbcpp343 persisting action 14 bytes to leveldb took 78683ns i0527 232348322077 1383 replicacpp508 replica received write request for position 0 i0527 232348322084 1381 replicacpp676 persisted action at 0 i0527 232348322111 1383 leveldbcpp438 reading position from leveldb took 17416ns i0527 232348322330 1383 leveldbcpp343 persisting action 14 bytes to leveldb took 157199ns i0527 232348322345 1383 replicacpp676 persisted action at 0 i0527 232348322522 1386 replicacpp655 replica received learned notice for position 0 i0527 232348322523 1382 replicacpp655 replica received learned notice for position 0 i0527 232348322638 1386 leveldbcpp343 persisting action 16 bytes to leveldb took 86907ns i0527 232348322661 1386 replicacpp676 persisted action at 0 i0527 232348322670 1386 replicacpp661 replica learned nop action at position 0 i0527 232348322682 1382 leveldbcpp343 persisting action 16 bytes to leveldb took 85031ns i0527 232348322693 1382 replicacpp676 persisted action at 0 i0527 232348322700 1382 replicacpp661 replica learned nop action at position 0 i0527 232348322790 1380 logcpp672 writer started with ending position 0 i0527 232348322898 1380 logcpp680 attempting to append 11 bytes to the log i0527 232348322978 1383 coordinatorcpp340 coordinator attempting to write append action at position 1 i0527 232348323122 1380 replicacpp508 replica received write request for position 1 i0527 232348323158 1381 replicacpp508 replica received write request for position 1 i0527 232348323202 1380 leveldbcpp343 persisting action 27 bytes to leveldb took 66527ns i0527 232348323215 1380 replicacpp676 persisted action at 1 i0527 232348323238 1381 leveldbcpp343 persisting action 27 bytes to leveldb took 67074ns i0527 232348323252 1381 replicacpp676 persisted action at 1 i0527 232348323354 1380 replicacpp655 replica received learned notice for position 1 i0527 232348323362 1382 replicacpp655 replica received learned notice for position 1 i0527 232348323443 1380 leveldbcpp343 persisting action 29 bytes to leveldb took 77398ns i0527 232348323461 1380 replicacpp676 persisted action at 1 i0527 232348323463 1382 leveldbcpp343 persisting action 29 bytes to leveldb took 90567ns i0527 232348323467 1380 replicacpp661 replica learned append action at position 1 i0527 232348323477 1382 replicacpp676 persisted action at 1 i0527 232348323484 1382 replicacpp661 replica learned append action at position 1 i0527 232348323729 1380 leveldbcpp438 reading position from leveldb took 7224ns 20140527 23234832413520x2b1173c2a700zoo_infozookeeper_close2505 closing zookeeper sessionid0x1463fff34bd0003 to 12700139446 20140527 23234832413520x2b117301ff80zoo_infozookeeper_close2505 closing zookeeper sessionid0x1463fff34bd0002 to 12700139446 i0527 232348326591 1386 networkhpp423 zookeeper group memberships changed i0527 232348326690 1382 groupcpp655 trying to get log0000000000 in zookeeper i0527 232348327450 1384 networkhpp461 zookeeper group pids  logreplica2267195138835151  20140527 23234844613520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23235178213520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23235511813520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client i0527 232357002908 1381 networkhpp423 zookeeper group memberships changed i0527 232357003042 1381 networkhpp461 zookeeper group pids   20140527 23235845513520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23240179113520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23240512713520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23240846413520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23241180013520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23241513613520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23241847313520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23242180913520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23242514613520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23242848213520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23243181813520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23243515513520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23243849113520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retcode4 errno111connection refused server refused to accept the client 20140527 23244182713520x2b12bc401700zoo_errorhandle_socket_error_msg1697 socket 12700151020 zk retc,1
create a protobuf for framework rate limit configuration and load it as json through master flags,2
integrate rate limiter into the master,5
add new tests for framework rate limiting,3
build failure ubuntu 1310clang due to missing virtual destructor in file included from launchermaincpp19 in file included from launcherlauncherhpp24 in file included from 3rdpartylibprocessincludeprocessfuturehpp23 3rdpartylibprocessincludeprocessownedhpp1885 error delete called on mesosinternallauncheroperation that is abstract but has nonvirtual destructor werrorwdeletenonvirtualdtor delete t  usrbinlibgccx86_64linuxgnu48includec48bitsshared_ptr_baseh4568 note in instantiation of member function processownedmesosinternallauncheroperationdatadata requested here delete __p  usrbinlibgccx86_64linuxgnu48includec48bitsshared_ptr_baseh76824 note in instantiation of function template specialization std__shared_count2__shared_countprocessownedmesosinternallauncheroperationdata  requested here  _m_ptr__p _m_refcount__p  usrbinlibgccx86_64linuxgnu48includec48bitsshared_ptr_baseh9194 note in instantiation of function template specialization std__shared_ptrprocessownedmesosinternallauncheroperationdata 2__shared_ptrprocessownedmesosinternallauncheroperationdata requested here __shared_ptr__pswapthis  3rdpartylibprocessincludeprocessownedhpp6810 note in instantiation of function template specialization std__shared_ptrprocessownedmesosinternallauncheroperationdata 2resetprocessownedmesosinternallauncheroperationdata requested here dataresetnew datat  launcherlauncherhpp1017 note in instantiation of member function processownedmesosinternallauncheroperationowned requested here addprocessownedoperationnew t  launchermaincpp263 note in instantiation of function template specialization mesosinternallauncheraddmesosinternallaunchershelloperation requested here launcheraddlaunchershelloperation  1 error generated,1
race between executor exited event and launch task can cause overcommit of resources the following sequence of events can cause an overcommit  launch task is called for a task whose executor is already running  executors resources are not accounted for on the master  executor exits and the event is enqueued behind launch tasks on the master  master sends the task to the slave which needs to commit for resources for task and the new executor  master processes the executor exited event and reoffers the executors resources causing an overcommit of resources,8
no output from review bot on timeout when the mesos review build times out likely due to a longrunning failing test we have no output to debug we should find a way to stream the output from the build instead of waiting for the build to finish,1
document replicated log designinternals the replicated log could benefit from some documentation in particular how does it work what do operators need to know possibly there is some overlap with our future maintenance documentation in mesos1470 i believe jieyu has some unpublished work that could be leveraged here,5
improve child exit if slave dies during executor launch in mc when restarting many slaves theres a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the mesoscontainerizer the forked child correctly detects this however rather than abort it should safely log and then exit nonzero cleanly,1
update rate limiting design doc to reflect the latest changes  usage  design  implementation notes,2
choose containerizer at runtime currently you have to choose the containerizer at mesosslave start time via the isolation option id like to be able to specify the containerizer in the request to launch the job this could be specified by a new provider field in the containerinfo proto buf,3
handle a network partition between master and slave if a network partition occurs between a master and slave the master will remove the slave as it fails health check and mark the tasks being run there as lost however the slave is not aware that it has been removed so the tasks will continue to run to clarify a little bit neither the master nor the slave receives exited event indicating that the connection between the master and slave is not closed there are at least two possible approaches to solving this issue 1 introduce a health check from slave to master so they have a consistent view of a network partition we may still see this issue should a oneway connection error occur 2 be less aggressive about marking tasks and slaves as lost wait until the slave reappears and reconcile then wed still need to mark slaves and tasks as potentially lost zombie state but maybe the scheduler can make a more intelligent decision,5
slaverecoverytest0multipleframeworks is flaky code  run  slaverecoverytest0multipleframeworks using temporary directory tmpslaverecoverytest_0_multipleframeworks_6djqxr i0626 000439557339 5450 leveldbcpp176 opened db in 179857593ms i0626 000439565433 5450 leveldbcpp183 compacted db in 8071041ms i0626 000439565457 5450 leveldbcpp198 created db iterator in 4065ns i0626 000439565466 5450 leveldbcpp204 seeked to beginning of db in 596ns i0626 000439565474 5450 leveldbcpp273 iterated through 0 keys in the db in 396ns i0626 000439565490 5450 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0626 000439565827 5476 recovercpp425 starting replica recovery i0626 000439566033 5474 recovercpp451 replica is in empty status i0626 000439566504 5474 replicacpp638 replica in empty status received a broadcasted recover request i0626 000439566686 5477 recovercpp188 received a recover response from a replica in empty status i0626 000439566905 5472 recovercpp542 updating replica status to starting i0626 000439568307 5471 mastercpp288 master 201406260004391032504131554235450 junoapacheorg started on 671951386155423 i0626 000439568332 5471 mastercpp325 master only allowing authenticated frameworks to register i0626 000439568339 5471 mastercpp330 master only allowing authenticated slaves to register i0626 000439568348 5471 credentialshpp35 loading credentials for authentication from tmpslaverecoverytest_0_multipleframeworks_6djqxrcredentials i0626 000439568461 5471 mastercpp356 authorization enabled i0626 000439568739 5478 mastercpp122 no whitelist given advertising offers for all slaves i0626 000439568814 5475 hierarchical_allocator_processhpp301 initializing hierarchical allocator process with master  master671951386155423 i0626 000439569206 5478 mastercpp1122 the newly elected leader is master671951386155423 with id 201406260004391032504131554235450 i0626 000439569223 5478 mastercpp1135 elected as the leading master i0626 000439569231 5478 mastercpp953 recovering from registrar i0626 000439569286 5475 registrarcpp313 recovering registrar i0626 000439600639 5477 leveldbcpp306 persisting metadata 8 bytes to leveldb took 33682136ms i0626 000439600661 5477 replicacpp320 persisted replica status to starting i0626 000439600790 5476 recovercpp451 replica is in starting status i0626 000439601184 5474 replicacpp638 replica in starting status received a broadcasted recover request i0626 000439601274 5477 recovercpp188 received a recover response from a replica in starting status i0626 000439601465 5471 recovercpp542 updating replica status to voting i0626 000439610605 5471 leveldbcpp306 persisting metadata 8 bytes to leveldb took 9076262ms i0626 000439610638 5471 replicacpp320 persisted replica status to voting i0626 000439610683 5471 recovercpp556 successfully joined the paxos group i0626 000439610780 5471 recovercpp440 recover process terminated i0626 000439610946 5474 logcpp656 attempting to start the writer i0626 000439611486 5475 replicacpp474 replica received implicit promise request with proposal 1 i0626 000439618924 5475 leveldbcpp306 persisting metadata 8 bytes to leveldb took 7418789ms i0626 000439618942 5475 replicacpp342 persisted promised to 1 i0626 000439619220 5476 coordinatorcpp230 coordinator attemping to fill missing position i0626 000439619763 5476 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0626 000439627267 5476 leveldbcpp343 persisting action 8 bytes to leveldb took 7485492ms i0626 000439627295 5476 replicacpp676 persisted action at 0 i0626 000439627822 5473 replicacpp508 replica received write request for position 0 i0626 000439627861 5473 leveldbcpp438 reading position from leveldb took 17132ns i0626 000439635592 5473 leveldbcpp343 persisting action 14 bytes to leveldb took 7714322ms i0626 000439635612 5473 replicacpp676 persisted action at 0 i0626 000439635797 5473 replicacpp655 replica received learned notice for position 0 i0626 000439643941 5473 leveldbcpp343 persisting action 16 bytes to leveldb took 8129347ms i0626 000439643960 5473 replicacpp676 persisted action at 0 i0626 000439643970 5473 replicacpp661 replica learned nop action at position 0 i0626 000439644207 5473 logcpp672 writer started with ending position 0 i0626 000439644625 5471 leveldbcpp438 reading position from leveldb took 9128ns i0626 000439646010 5476 registrarcpp346 successfully fetched the registry 0b i0626 000439646044 5476 registrarcpp422 attempting to update the registry i0626 000439647274 5471 logcpp680 attempting to append 136 bytes to the log i0626 000439647337 5471 coordinatorcpp340 coordinator attempting to write append action at position 1 i0626 000439647687 5476 replicacpp508 replica received write request for position 1 i0626 000439655206 5476 leveldbcpp343 persisting action 155 bytes to leveldb took 7499736ms i0626 000439655225 5476 replicacpp676 persisted action at 1 i0626 000439655467 5476 replicacpp655 replica received learned notice for position 1 i0626 000439663534 5476 leveldbcpp343 persisting action 157 bytes to leveldb took 8054929ms i0626 000439663554 5476 replicacpp676 persisted action at 1 i0626 000439663563 5476 replicacpp661 replica learned append action at position 1 i0626 000439663890 5478 registrarcpp479 successfully updated registry i0626 000439663947 5478 registrarcpp372 successfully recovered registrar i0626 000439663969 5476 logcpp699 attempting to truncate the log to 1 i0626 000439664044 5478 mastercpp980 recovered 0 slaves from the registry 98b  allowing 10mins for slaves to reregister i0626 000439664057 5476 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0626 000439664341 5476 replicacpp508 replica received write request for position 2 i0626 000439664681 5450 containerizercpp124 using isolation posixcpuposixmem i0626 000439666721 5471 slavecpp168 slave started on 173671951386155423 i0626 000439666741 5471 credentialshpp35 loading credentials for authentication from tmpslaverecoverytest_0_multipleframeworks_g6obtkcredential i0626 000439666806 5471 slavecpp268 slave using credential for testprincipal i0626 000439666936 5471 slavecpp281 slave resources cpus2 mem1024 disk1024 ports3100032000 i0626 000439667000 5471 slavecpp326 slave hostname junoapacheorg i0626 000439667009 5471 slavecpp327 slave checkpoint true i0626 000439667572 5478 statecpp33 recovering state from tmpslaverecoverytest_0_multipleframeworks_g6obtkmeta i0626 000439667703 5475 status_update_managercpp193 recovering status update manager i0626 000439667840 5475 containerizercpp287 recovering containerizer i0626 000439668478 5471 slavecpp3128 finished recovery i0626 000439668712 5471 slavecpp601 new master detected at master671951386155423 i0626 000439668738 5471 slavecpp677 authenticating with master master671951386155423 i0626 000439668802 5471 slavecpp650 detecting new master i0626 000439668861 5471 status_update_managercpp167 new master detected at master671951386155423 i0626 000439668916 5471 authenticateehpp128 creating new client sasl connection i0626 000439669087 5471 mastercpp3499 authenticating slave173671951386155423 i0626 000439669203 5471 authenticatorhpp156 creating new server sasl connection i0626 000439669340 5471 authenticateehpp219 received sasl authentication mechanisms crammd5 i0626 000439669359 5471 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0626 000439669386 5471 authenticatorhpp262 received sasl authentication start i0626 000439669414 5471 authenticatorhpp384 authentication requires more steps i0626 000439669457 5471 authenticateehpp265 received sasl authentication step i0626 000439669514 5471 authenticatorhpp290 received sasl authentication step i0626 000439669534 5471 auxpropcpp81 request to lookup properties for user testprincipal realm junoapacheorg server fqdn junoapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0626 000439669543 5471 auxpropcpp153 looking up auxiliary property userpassword i0626 000439669567 5471 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0626 000439669580 5471 auxpropcpp81 request to lookup properties for user testprincipal realm junoapacheorg server fqdn junoapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0626 000439669589 5471 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0626 000439669594 5471 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0626 000439669606 5471 authenticatorhpp376 authentication success i0626 000439669641 5471 authenticateehpp305 authentication success i0626 000439669669 5471 mastercpp3539 successfully authenticated principal testprincipal at slave173671951386155423 i0626 000439669761 5450 schedcpp139 version 0200 i0626 000439669764 5478 slavecpp734 successfully authenticated with master master671951386155423 i0626 000439669826 5478 slavecpp972 will retry registration in 3190666ms if necessary i0626 000439669950 5471 mastercpp2781 registering slave at slave173671951386155423 junoapacheorg with id 2014062600043910325041315542354500 i0626 000439669960 5475 schedcpp235 new master detected at master671951386155423 i0626 000439669977 5475 schedcpp285 authenticating with master master671951386155423 i0626 000439670073 5471 registrarcpp422 attempting to update the registry i0626 000439670114 5475 authenticateehpp128 creating new client sasl connection i0626 000439670263 5475 mastercpp3499 authenticating schedulere66c50d227904d20bc77a57af0e1780b671951386155423 i0626 000439670361 5474 authenticatorhpp156 creating new server sasl connection i0626 000439670506 5475 authenticateehpp219 received sasl authentication mechanisms crammd5 i0626 000439670526 5475 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0626 000439670559 5475 authenticatorhpp262 received sasl authentication start i0626 000439670590 5475 authenticatorhpp384 authentication requires more steps i0626 000439670619 5475 authenticateehpp265 received sasl authentication step i0626 000439670650 5475 authenticatorhpp290 received sasl authentication step i0626 000439670670 5475 auxpropcpp81 request to lookup properties for user testprincipal realm junoapacheorg server fqdn junoapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0626 000439670677 5475 auxpropcpp153 looking up auxiliary property userpassword i0626 000439670687 5475 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0626 000439670697 5475 auxpropcpp81 request to lookup properties for user testprincipal realm junoapacheorg server fqdn junoapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0626 000439670706 5475 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0626 000439670712 5475 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0626 000439670723 5475 authenticatorhpp376 authentication success i0626 000439670749 5475 authenticateehpp305 authentication success i0626 000439670773 5475 mastercpp3539 successfully authenticated principal testprincipal at schedulere66c50d227904d20bc77a57af0e1780b671951386155423 i0626 000439670845 5475 schedcpp359 successfully authenticated with master master671951386155423 i0626 000439670858 5475 schedcpp478 sending registration request to master671951386155423 i0626 000439670899 5475 mastercpp1241 received registration request from schedulere66c50d227904d20bc77a57af0e1780b671951386155423 i0626 000439670922 5475 mastercpp1201 authorizing framework principal testprincipal to receive offers for role  i0626 000439671052 5475 mastercpp1300 registering framework 2014062600043910325041315542354500000 at schedulere66c50d227904d20bc77a57af0e1780b671951386155423 i0626 000439671159 5474 schedcpp409 framework registered with 2014062600043910325041315542354500000 i0626 000439671185 5474 schedcpp423 schedulerregistered took 10223ns i0626 000439671226 5474 hierarchical_allocator_processhpp331 added framework 2014062600043910325041315542354500000 i0626 000439671241 5474 hierarchical_allocator_processhpp724 no resources available to allocate i0626 000439671247 5474 hierarchical_allocator_processhpp686 performed allocation for 0 slaves in 8574ns i0626 000439671879 5476 leveldbcpp343 persisting action 16 bytes to leveldb took 748781ms i0626 000439671900 5476 replicacpp676 persisted action at 2 i0626 000439672164 5471 replicacpp655 replica received learned notice for position 2 i0626 000439674092 5472 slavecpp972 will retry registration in 25467893ms if necessary i0626 000439674108 5476 mastercpp2769 ignoring register slave message from slave173671951386155423 junoapacheorg as admission is already in progress i0626 000439680193 5471 leveldbcpp343 persisting action 18 bytes to leveldb took 801285ms i0626 000439680223 5471 leveldbcpp401 deleting 1 keys from leveldb took 11393ns i0626 000439680234 5471 replicacpp676 persisted action at 2 i0626 000439680245 5471 replicacpp661 replica learned truncate action at position 2 i0626 000439680585 5472 logcpp680 attempting to append 326 bytes to the log i0626 000439680670 5477 coordinatorcpp340 coordinator attempting to write append action at position 3 i0626 000439680953 5474 replicacpp508 replica received write request for position 3 i0626 000439688521 5474 leveldbcpp343 persisting action 345 bytes to leveldb took 7548316ms i0626 000439688542 5474 replicacpp676 persisted action at 3 i0626 000439688750 5474 replicacpp655 replica received learned notice for position 3 i0626 000439696851 5474 leveldbcpp343 persisting action 347 bytes to leveldb took 8088289ms i0626 000439696869 5474 replicacpp676 persisted action at 3 i0626 000439696878 5474 replicacpp661 replica learned append action at position 3 i0626 000439697268 5474 registrarcpp479 successfully updated registry i0626 000439697350 5474 logcpp699 attempting to truncate the log to 3 i0626 000439697412 5474 mastercpp2821 registered slave 2014062600043910325041315542354500 at slave173671951386155423 junoapacheorg i0626 000439697423 5474 mastercpp3967 adding slave 2014062600043910325041315542354500 at slave173671951386155423 junoapacheorg with cpus2 mem1024 disk1024 ports3100032000 i0626 000439697535 5474 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0626 000439697618 5474 slavecpp768 registered with master master671951386155423 given slave id 2014062600043910325041315542354500 i0626 000439697754 5474 slavecpp781 checkpointing slaveinfo to tmpslaverecoverytest_0_multipleframeworks_g6obtkmetaslaves2014062600043910325041315542354500slaveinfo i0626 000439697762 5471 hierarchical_allocator_processhpp444 added slave 2014062600043910325041315542354500 junoapacheorg with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0626 000439697845 5471 hierarchical_allocator_processhpp750 offering cpus2 mem1024 disk1024 ports3100032000 on slave 2014062600043910325041315542354500 to framework 2014062600043910325041315542354500000 i0626 000439697854 5474 slavecpp2325 received ping from slaveobserver142671951386155423 i0626 000439698040 5471 hierarchical_allocator_processhpp706 performed allocation for slave 2014062600043910325041315542354500 in 231333ns i0626 000439698051 5474 replicacpp508 replica received write request for position 4 i0626 000439698118 5471 masterhpp794 adding offer 2014062600043910325041315542354500 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2014062600043910325041315542354500 junoapacheorg i0626 000439698170 5471 mastercpp3446 sending 1 offers to framework 2014062600043910325041315542354500000 i0626 000439698318 5471 schedcpp546 schedulerresourceoffers took 24371ns i0626 000439699718 5477 masterhpp804 removing offer 2014062600043910325041315542354500 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2014062600043910325041315542354500 junoapacheorg i0626 000439699787 5477 mastercpp2125 processing reply for offers  2014062600043910325041315542354500  on slave 2014062600043910325041315542354500 at slave173671951386155423 junoapacheorg for framework 2014062600043910325041315542354500000 i0626 000439699812 5477 mastercpp2211 authorizing framework principal testprincipal to launch task 897522cc4ec54904aed000b6b8c41028 as user jenkins i0626 000439700160 5477 masterhpp766 adding task 897522cc4ec54904aed000b6b8c41028 with resources cpus1 mem512 on slave 2014062600043910325041315542354500 junoapacheorg i0626 000439700188 5477 mastercpp2277 launching task 897522cc4ec54904aed000b6b8c41028 of framework 2014062600043910325041315542354500000 with resources cpus1 mem512 on slave 2014062600043910325041315542354500 at slave173671951386155423 junoapacheorg i0626 000439700392 5471 slavecpp1003 got assigned task 897522cc4ec54904aed000b6b8c41028 for framework 2014062600043910325041315542354500000 i0626 000439700479 5477 hierarchical_allocator_processhpp546 framework 2014062600043910325041315542354500000 left cpus1 mem512 disk1024 ports3100032000 unused on slave 2014062600043910325041315542354500 i0626 000439700505 5471 slavecpp3400 checkpointing frameworkinfo to tmpslaverecoverytest_0_multipleframeworks_g6obtkmetaslaves2014062600043910325041315542354500frameworks2014062600043910325041315542354500000frameworkinfo i0626 000439700597 5477 hierarchical_allocator_processhpp588 framework 2014062600043910325041315542354500000 filtered slave 2014062600043910325041315542354500 for 5secs i0626 000439700686 5471 slavecpp3407 checkpointing framework pid schedulere66c50d227904d20bc77a57af0e1780b671951386155423 to,1
allow jenkins build machine to dump stack traces of all threads when timeout many of the time when jenkins build times out we know that some test freezes at some place however most of the time its very hard to reproduce the deadlock on dev machines i would be cool if we can dump the stack traces of all threads when jenkins build times out some command like the following noformat echo thread apply all bt  tmp gdb attach pgrep ltmesostests  tmp noformat,5
add logging of the user uid when receiving sigterm we currently do not log the user id when receiving a sigterm this makes debugging a bit difficult its easy to get this information through sigaction,1
signal escalation timeout is not configurable even though the executor shutdown grace period is set to a larger interval the signal escalation timeout will still be 3 seconds it should either be configurable or dependent on executor_shutdown_grace_period thoughts,2
improve framework rate limiting by imposing the max number of outstanding messages per framework principal  rate limits config takes a configurable capacity for each principal  to ensure that master maintain the message order of a framework its important that master sends an frameworkerrormessage back to the scheduler to ask it to abort,5
isolate system directories eg percontainer tmp ideally tasks should not write outside their sandbox executor work directory but pragmatically they may need to write to tmp vartmp or some other directory 1 we should include any such files in disk usage and quota 2 we should make these shared directories private ie each container has their own 3 we should make the lifetime of any such files the same as the executor work directory,3
report disk usage from mesoscontainerizer we should report disk usage for the executor work directory from mesoscontainerizer and include in the resourcestatistics protobuf,5
allow loadgeneratorframework to read password from a file it currently just reads the flag as the value of the password,1
design inverse resource offer support an inverse resource offer means that mesos is requesting resources back from the framework possibly within some time interval this can be leveraged initially to provide more automated cluster maintenance by offering schedulers the opportunity to move tasks to compensate for planned maintenance operators can set a time limit on how long to wait for schedulers to relocate tasks before the tasks are forcibly terminated inverse resource offers have many other potential uses as it opens the opportunity for the allocator to attempt to move tasks in the cluster through the cooperation of the framework possibly providing better oversubscription fairness etc,5
slaverecoverytest0reconcilekilltask is flaky observed this on jenkins code  run  slaverecoverytest0reconcilekilltask using temporary directory tmpslaverecoverytest_0_reconcilekilltask_3zj6dg i0714 150843915114 27216 leveldbcpp176 opened db in 474695188ms i0714 150843933645 27216 leveldbcpp183 compacted db in 18068942ms i0714 150843934129 27216 leveldbcpp198 created db iterator in 7860ns i0714 150843934439 27216 leveldbcpp204 seeked to beginning of db in 2560ns i0714 150843934779 27216 leveldbcpp273 iterated through 0 keys in the db in 1400ns i0714 150843935098 27216 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0714 150843936027 27238 recovercpp425 starting replica recovery i0714 150843936225 27238 recovercpp451 replica is in empty status i0714 150843936867 27238 replicacpp638 replica in empty status received a broadcasted recover request i0714 150843937049 27238 recovercpp188 received a recover response from a replica in empty status i0714 150843937232 27238 recovercpp542 updating replica status to starting i0714 150843945600 27235 mastercpp288 master 20140714150843168428795585027216 quantal started on 12701155850 i0714 150843945643 27235 mastercpp325 master only allowing authenticated frameworks to register i0714 150843945651 27235 mastercpp330 master only allowing authenticated slaves to register i0714 150843945658 27235 credentialshpp36 loading credentials for authentication from tmpslaverecoverytest_0_reconcilekilltask_3zj6dgcredentials i0714 150843945808 27235 mastercpp359 authorization enabled i0714 150843946369 27235 hierarchical_allocator_processhpp301 initializing hierarchical allocator process with master  master12701155850 i0714 150843946419 27235 mastercpp122 no whitelist given advertising offers for all slaves i0714 150843946614 27235 mastercpp1128 the newly elected leader is master12701155850 with id 20140714150843168428795585027216 i0714 150843946630 27235 mastercpp1141 elected as the leading master i0714 150843946637 27235 mastercpp959 recovering from registrar i0714 150843946707 27235 registrarcpp313 recovering registrar i0714 150843957895 27238 leveldbcpp306 persisting metadata 8 bytes to leveldb took 20529301ms i0714 150843957978 27238 replicacpp320 persisted replica status to starting i0714 150843958142 27238 recovercpp451 replica is in starting status i0714 150843958664 27238 replicacpp638 replica in starting status received a broadcasted recover request i0714 150843958762 27238 recovercpp188 received a recover response from a replica in starting status i0714 150843958945 27238 recovercpp542 updating replica status to voting i0714 150843975685 27238 leveldbcpp306 persisting metadata 8 bytes to leveldb took 16646136ms i0714 150843976367 27238 replicacpp320 persisted replica status to voting i0714 150843976824 27241 recovercpp556 successfully joined the paxos group i0714 150843977072 27242 recovercpp440 recover process terminated i0714 150843980590 27236 logcpp656 attempting to start the writer i0714 150843981385 27236 replicacpp474 replica received implicit promise request with proposal 1 i0714 150843999141 27236 leveldbcpp306 persisting metadata 8 bytes to leveldb took 17705787ms i0714 150843999222 27236 replicacpp342 persisted promised to 1 i0714 150844004451 27240 coordinatorcpp230 coordinator attemping to fill missing position i0714 150844004914 27240 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0714 150844021456 27240 leveldbcpp343 persisting action 8 bytes to leveldb took 16499775ms i0714 150844021533 27240 replicacpp676 persisted action at 0 i0714 150844022006 27240 replicacpp508 replica received write request for position 0 i0714 150844022043 27240 leveldbcpp438 reading position from leveldb took 21376ns i0714 150844035969 27240 leveldbcpp343 persisting action 14 bytes to leveldb took 13885907ms i0714 150844036365 27240 replicacpp676 persisted action at 0 i0714 150844040156 27238 replicacpp655 replica received learned notice for position 0 i0714 150844058082 27238 leveldbcpp343 persisting action 16 bytes to leveldb took 17860707ms i0714 150844058161 27238 replicacpp676 persisted action at 0 i0714 150844058176 27238 replicacpp661 replica learned nop action at position 0 i0714 150844058526 27238 logcpp672 writer started with ending position 0 i0714 150844058872 27238 leveldbcpp438 reading position from leveldb took 25660ns i0714 150844060556 27238 registrarcpp346 successfully fetched the registry 0b i0714 150844060845 27238 registrarcpp422 attempting to update the registry i0714 150844062304 27238 logcpp680 attempting to append 120 bytes to the log i0714 150844062866 27236 coordinatorcpp340 coordinator attempting to write append action at position 1 i0714 150844063154 27236 replicacpp508 replica received write request for position 1 i0714 150844082813 27236 leveldbcpp343 persisting action 137 bytes to leveldb took 1961683ms i0714 150844082890 27236 replicacpp676 persisted action at 1 i0714 150844083256 27236 replicacpp655 replica received learned notice for position 1 i0714 150844097398 27236 leveldbcpp343 persisting action 139 bytes to leveldb took 14104796ms i0714 150844097475 27236 replicacpp676 persisted action at 1 i0714 150844097488 27236 replicacpp661 replica learned append action at position 1 i0714 150844098569 27236 registrarcpp479 successfully updated registry i0714 150844098906 27240 logcpp699 attempting to truncate the log to 1 i0714 150844099608 27240 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0714 150844100005 27240 replicacpp508 replica received write request for position 2 i0714 150844100566 27236 registrarcpp372 successfully recovered registrar i0714 150844101227 27239 mastercpp986 recovered 0 slaves from the registry 84b  allowing 10mins for slaves to reregister i0714 150844118376 27240 leveldbcpp343 persisting action 16 bytes to leveldb took 18329495ms i0714 150844118455 27240 replicacpp676 persisted action at 2 i0714 150844122258 27242 replicacpp655 replica received learned notice for position 2 i0714 150844137336 27242 leveldbcpp343 persisting action 18 bytes to leveldb took 15023553ms i0714 150844137460 27242 leveldbcpp401 deleting 1 keys from leveldb took 55049ns i0714 150844137480 27242 replicacpp676 persisted action at 2 i0714 150844137492 27242 replicacpp661 replica learned truncate action at position 2 i0714 150844143729 27216 containerizercpp124 using isolation posixcpuposixmem i0714 150844145934 27242 slavecpp168 slave started on 4312701155850 i0714 150844145953 27242 credentialshpp84 loading credential for authentication from tmpslaverecoverytest_0_reconcilekilltask_zl9dutcredential i0714 150844146040 27242 slavecpp266 slave using credential for testprincipal i0714 150844146136 27242 slavecpp279 slave resources cpus2 mem1024 disk1024 ports3100032000 i0714 150844146198 27242 slavecpp324 slave hostname quantal i0714 150844146209 27242 slavecpp325 slave checkpoint true i0714 150844146708 27242 statecpp33 recovering state from tmpslaverecoverytest_0_reconcilekilltask_zl9dutmeta i0714 150844146824 27242 status_update_managercpp193 recovering status update manager i0714 150844146901 27242 containerizercpp287 recovering containerizer i0714 150844147228 27242 slavecpp3126 finished recovery i0714 150844147531 27242 slavecpp599 new master detected at master12701155850 i0714 150844147562 27242 slavecpp675 authenticating with master master12701155850 i0714 150844147614 27242 slavecpp648 detecting new master i0714 150844147652 27242 status_update_managercpp167 new master detected at master12701155850 i0714 150844147691 27242 authenticateehpp128 creating new client sasl connection i0714 150844148533 27235 mastercpp3507 authenticating slave4312701155850 i0714 150844148666 27235 authenticatorhpp156 creating new server sasl connection i0714 150844149054 27242 authenticateehpp219 received sasl authentication mechanisms crammd5 i0714 150844149447 27242 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0714 150844149917 27236 authenticatorhpp262 received sasl authentication start i0714 150844149974 27236 authenticatorhpp384 authentication requires more steps i0714 150844150208 27242 authenticateehpp265 received sasl authentication step i0714 150844150720 27239 authenticatorhpp290 received sasl authentication step i0714 150844150749 27239 auxpropcpp81 request to lookup properties for user testprincipal realm quantal server fqdn quantal sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0714 150844150758 27239 auxpropcpp153 looking up auxiliary property userpassword i0714 150844150771 27239 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0714 150844150781 27239 auxpropcpp81 request to lookup properties for user testprincipal realm quantal server fqdn quantal sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0714 150844150787 27239 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0714 150844150792 27239 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0714 150844150804 27239 authenticatorhpp376 authentication success i0714 150844150848 27239 mastercpp3547 successfully authenticated principal testprincipal at slave4312701155850 i0714 150844157696 27242 authenticateehpp305 authentication success i0714 150844158855 27242 slavecpp732 successfully authenticated with master master12701155850 i0714 150844158936 27242 slavecpp970 will retry registration in 10352612ms if necessary i0714 150844161813 27216 schedcpp139 version 0200 i0714 150844162608 27236 schedcpp235 new master detected at master12701155850 i0714 150844162637 27236 schedcpp285 authenticating with master master12701155850 i0714 150844162747 27236 authenticateehpp128 creating new client sasl connection i0714 150844163506 27239 mastercpp2789 registering slave at slave4312701155850 quantal with id 201407141508431684287955850272160 i0714 150844164086 27238 registrarcpp422 attempting to update the registry i0714 150844165694 27238 logcpp680 attempting to append 295 bytes to the log i0714 150844166231 27240 coordinatorcpp340 coordinator attempting to write append action at position 3 i0714 150844166517 27240 replicacpp508 replica received write request for position 3 i0714 150844167199 27239 mastercpp3507 authenticating scheduler225679c4a9fd41199debc7712eba37e112701155850 i0714 150844167867 27241 authenticatorhpp156 creating new server sasl connection i0714 150844168058 27241 authenticateehpp219 received sasl authentication mechanisms crammd5 i0714 150844168081 27241 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0714 150844168107 27241 authenticatorhpp262 received sasl authentication start i0714 150844168149 27241 authenticatorhpp384 authentication requires more steps i0714 150844168176 27241 authenticateehpp265 received sasl authentication step i0714 150844168215 27241 authenticatorhpp290 received sasl authentication step i0714 150844168233 27241 auxpropcpp81 request to lookup properties for user testprincipal realm quantal server fqdn quantal sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0714 150844168793 27241 auxpropcpp153 looking up auxiliary property userpassword i0714 150844168820 27241 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0714 150844168834 27241 auxpropcpp81 request to lookup properties for user testprincipal realm quantal server fqdn quantal sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0714 150844168840 27241 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0714 150844168845 27241 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0714 150844168858 27241 authenticatorhpp376 authentication success i0714 150844168895 27241 authenticateehpp305 authentication success i0714 150844168970 27241 schedcpp359 successfully authenticated with master master12701155850 i0714 150844168987 27241 schedcpp478 sending registration request to master12701155850 i0714 150844169426 27239 mastercpp1239 queuing up registration request from scheduler225679c4a9fd41199debc7712eba37e112701155850 because authentication is still in progress i0714 150844169958 27239 mastercpp3547 successfully authenticated principal testprincipal at scheduler225679c4a9fd41199debc7712eba37e112701155850 i0714 150844170440 27241 slavecpp970 will retry registration in 876707ms if necessary i0714 150844175359 27239 mastercpp2777 ignoring register slave message from slave4312701155850 quantal as admission is already in progress i0714 150844175916 27239 mastercpp1247 received registration request from scheduler225679c4a9fd41199debc7712eba37e112701155850 i0714 150844176298 27239 mastercpp1207 authorizing framework principal testprincipal to receive offers for role  i0714 150844176858 27239 mastercpp1306 registering framework 201407141508431684287955850272160000 at scheduler225679c4a9fd41199debc7712eba37e112701155850 i0714 150844177408 27236 schedcpp409 framework registered with 201407141508431684287955850272160000 i0714 150844177443 27236 schedcpp423 schedulerregistered took 12527ns i0714 150844177727 27241 hierarchical_allocator_processhpp331 added framework 201407141508431684287955850272160000 i0714 150844177747 27241 hierarchical_allocator_processhpp724 no resources available to allocate i0714 150844177753 27241 hierarchical_allocator_processhpp686 performed allocation for 0 slaves in 8120ns i0714 150844179908 27241 slavecpp970 will retry registration in 66781028ms if necessary i0714 150844180007 27241 mastercpp2777 ignoring register slave message from slave4312701155850 quantal as admission is already in progress i0714 150844183082 27240 leveldbcpp343 persisting action 314 bytes to leveldb took 16533189ms i0714 150844183125 27240 replicacpp676 persisted action at 3 i0714 150844183465 27240 replicacpp655 replica received learned notice for position 3 i0714 150844203276 27240 leveldbcpp343 persisting action 316 bytes to leveldb took 19768951ms i0714 150844203376 27240 replicacpp676 persisted action at 3 i0714 150844203392 27240 replicacpp661 replica learned append action at position 3 i0714 150844204033 27240 registrarcpp479 successfully updated registry i0714 150844204138 27240 logcpp699 attempting to truncate the log to 3 i0714 150844204221 27240 mastercpp2829 registered slave 201407141508431684287955850272160 at slave4312701155850 quantal i0714 150844204241 27240 mastercpp3975 adding slave 201407141508431684287955850272160 at slave4312701155850 quantal with cpus2 mem1024 disk1024 ports3100032000 i0714 150844204387 27240 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0714 150844204489 27240 slavecpp766 registered with master master12701155850 given slave id 201407141508431684287955850272160 i0714 150844204745 27240 slavecpp779 checkpointing slaveinfo to tmpslaverecoverytest_0_reconcilekilltask_zl9dutmetaslaves201407141508431684287955850272160slaveinfo i0714 150844204954 27240 hierarchical_allocator_processhpp444 added slave 201407141508431684287955850272160 quantal with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0714 150844205023 27240 hierarchical_allocator_processhpp750 offering cpus2 mem1024 disk1024 ports3100032000 on slave 201407141508431684287955850272160 to framework 201407141508431684287955850272160000 i0714 150844205122 27240 hierarchical_allocator_processhpp706 performed allocation for slave 201407141508431684287955850272160 in 131192ns i0714 150844205189 27240 slavecpp2323 received ping from slaveobserver3212701155850 i0714 150844205258 27240 masterhpp801 adding offer 201407141508431684287955850272160 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201407141508431684287955850272160 quantal i0714 150844205303 27240 mastercpp3454 sending 1 offers to framework 201407141508431684287955850272160000 i0714 150844205469 27240 schedcpp546 schedulerresourceoffers took 23591ns i0714 150844206351 27241 replicacpp508 replica received write request for position 4 i0714 150844208353 27237 masterhpp811 removing offer 201407141508431684287955850272160 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201407141508431684287955850272160 quantal i0714 150844208436 27237 mastercpp2133 processing reply for offers  201407141508431684287955850272160  on slave 201407141508431684287955850272160 at slave4312701155850 quantal for framework 201407141508431684287955850272160000 i0714 150844208472 27237 mastercpp2219 authorizing framework principal testprincipal to launch task 4a6783aa8d0746e383992a5d047f0021 as user jenkins i0714 150844208909 27237 masterhpp773 adding task 4a6783aa8d0746e383992a5d047f0021 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201407141508431684287955850272160 quantal i0714 150844208947 27237 mastercpp2285 launching task 4a6783aa8d0746e383992a5d047f0021 of framework 201407141508431684287955850272160000 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201407141508431684287955850272160 at slave4312701155850 quantal i0714 150844209090 27237 slavecpp1001 got assigned task 4a6783aa8d0746e383992a5d047f0021 for framework 201407141508431684287955850272160000 i0714 150844209190 27237 slavecpp3398 checkpointing frameworkinfo to tmpslaverecoverytest_0_reconcilekilltask_zl9dutmetaslaves201407141508431684287955850272160frameworks201407141508431684287955850272160000frameworkinfo i0714 150844209413 27237 slavecpp3405 checkpointing framework pid scheduler225679c4a9fd41199debc7712eba37e112701155850 to tmpslaverecoverytest_0_reconcilekilltask_zl9dutmetaslaves201407141508431684287955850272160frameworks201407141508431684287955850272160000frameworkpid i0714 1508,1
cleanup stout build setup while investigating stout build setup for making it installable i came across some discrepancies stout tests are included in libprocesss makefile instead of stout makefile stouts 3rd party dependencies eg picojson live in libprocesss 3rdparty directory instead of living in stouts nonexistent 3rd party directory it would be nice to fix these issues before making stout installable,3
create design document for optimistic offers as a first step toward optimistic offers take the description from the epic and build an implementation design doc that can be shared for comments note the links to the working group notes and design doc are located in the jira epicmesos1607,8
reconciliation does not send back tasks pending validation  authorization per vinods feedback on httpsreviewsapacheorgr23542 we do not send back task_staging for those tasks that are pending in the master validation  authorization still in progress for both implicit and explicit task reconciliation the master could reply with task_staging for these tasks as this provides additional information to the framework,3
apache jenkins build fails due to lsnappy is set when building leveldb the failed build httpsbuildsapacheorgjobmesostrunkubuntubuildoutofsrcsetjava_home2261consolefull noformattitlethe log where lsnappy is used when compiling leveldb gzip d c 3rdpartyleveldbtargz  tar xf  test  e 3rdpartyleveldbpatch  patch d leveldb p1 3rdpartyleveldbpatch touch leveldbstamp cd leveldb   make ccgcc cxxg optg g2 o2 wnounusedlocaltypedefs stdc11 fpic make5 entering directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuildmesos0200_build3rdpartyleveldb g pthread lsnappy shared wlsoname wlhomejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuildmesos0200_build3rdpartyleveldblibleveldbso1 i iinclude fnobuiltinmemcmp pthread dos_linux dleveldb_platform_posix dsnappy g g2 o2 wnounusedlocaltypedefs stdc11 fpic fpic dbbuildercc dbccc dbdb_implcc dbdb_itercc dbdbformatcc dbfilenamecc dblog_readercc dblog_writercc dbmemtablecc dbrepaircc dbtable_cachecc dbversion_editcc dbversion_setcc dbwrite_batchcc tableblockcc tableblock_buildercc tablefilter_blockcc tableformatcc tableiteratorcc tablemergercc tabletablecc tabletable_buildercc tabletwo_level_iteratorcc utilarenacc utilbloomcc utilcachecc utilcodingcc utilcomparatorcc utilcrc32ccc utilenvcc utilenv_posixcc utilfilter_policycc utilhashcc utilhistogramcc utilloggingcc utiloptionscc utilstatuscc portport_posixcc o libleveldbso14 ln fs libleveldbso14 libleveldbso ln fs libleveldbso14 libleveldbso1 g i iinclude fnobuiltinmemcmp pthread dos_linux dleveldb_platform_posix dsnappy g g2 o2 wnounusedlocaltypedefs stdc11 fpic c dbbuildercc o dbbuildero noformat noformattitlethe error binbash libtool tagcxx modelink g pthread g g2 o2 wnounusedlocaltypedefs stdc11 o mesoslocal localmesos_localmaino libmesosla lsasl2 lcurl lz lrt libtool link g pthread g g2 o2 wnounusedlocaltypedefs stdc11 o libsmesoslocal localmesos_localmaino libslibmesosso lsasl2 usrlibx86_64linuxgnulibcurlso lz lrt pthread wlrpath wlhomejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuildmesos0200_instlib libslibmesosso undefined reference to snappyrawcompresschar const unsigned long char unsigned long libslibmesosso undefined reference to snappyrawuncompresschar const unsigned long char libslibmesosso undefined reference to snappygetuncompressedlengthchar const unsigned long unsigned long libslibmesosso undefined reference to snappymaxcompressedlengthunsigned long noformat,1
installed protobuf header files include wrong path to mesos header file playing with installed mesos headers realized that we expect users to include the path to mesos directory eg usrlocalincludemesos even though it is on the system path this is because schedulerpbh etc include mesospbh instead of mesosmesospbh,2
glog initialized twice if the framework scheduler also uses glog noformat could not create logging file no such file or directory could not create a loggingfile 2014072220522031450f0722 205220494424 31450 utilitiescc317 check failed isgooglelogginginitialized you called initgooglelogging twice  check failure stack trace   0x4399ce googlelogmessagefail  0x43991d googlelogmessagesendtolog  0x43932e googlelogmessageflush  0x43c0e5 googlelogmessagefatallogmessagefatal  0x44089f googleglog_internal_namespace_initgoogleloggingutilities  0x43c409 googleinitgooglelogging  0x7f0bdd43b55c mesosinternallogginginitialize  0x7f0bdcf9564d mesosschedulermesosprocessmesosprocess  0x7f0bdcf92de0 mesosschedulermesosmesos  0x421483 heronmesosschedulerscheduler  0x4305dc main  0x7f0bd97159c4 __libc_start_main  0x420869 unknown aborted noformat,2
0200 release i would like to volunteer to be the release manager for 0200 which will be releasing the following major features  docker support in mesos mesos1524  container level network monitoring for mesos containerizer mesos1228  authorization mesos1342  framework rate limiting mesos1306  enable building against installed thirdparty dependencies mesos1071 i would like to track blockers for the release on this ticket,5
network isolator should tolerate slave crashes while doing isolatecleanup a slave may crash while we are installingremoving filters the slave recovery for the network isolator should tolerate those partially installed filters also we want to avoid leaking a filter on host eth0 and host lo the current code cannot tolerate that thus may cause the following error noformat failed to perform recovery collect failed failed to recover container d409a1002afb497c864ffe3002cf65d9 with pid 50405 no ephemeral ports found to remedy this do as follows step 1 rm f varlibmesosmetaslaveslatest this ensures slave doesnt recover old live executors step 2 restart the slave noformat,3
inform framework when rate limiting is active when we ratelimit messages from a framework we should let them know so they can proactively backoff to avoid putting extra pressure on the master,3
set maximum executors per slave to avoid overcommit of ephemeral ports with network isolation we statically assign ephemeral port ranges as such there is a upper bound on the number of containers each slave can support we should avoid sending offers for slaves that have hit that limit as any tasks will fail to launch and will be lost,1
handle a temporary oneway master  slave socket closure in mesos1529 we realized that its possible for a slave to remain disconnected in the master if the following occurs  master and slave connected operating normally  temporary oneway network failure masterslave link breaks  master marks slave as disconnected  network restored and health checking continues normally slave is not removed as a result slave does not attempt to reregister since it is receiving pings once again  slave remains disconnected according to the master and the slave does not try to reregister bad we were originally thinking of using a failover timeout in the master to remove these slaves that dont reregister however it can be dangerous when zookeeper issues are preventing the slave from reregistering with the master we do not want to remove a ton of slaves in this situation rather when the slave is health checking correctly but does not reregister within a timeout we could send a registration request from the master to the slave telling the slave that it must reregister this message could also be used when receiving status updates or other messages from slaves that are disconnected in the master,2
expose executor metrics for slave expose the following metrics slaveexecutors_registering slaveexecutors_running slaveexecutors_terminating slaveexecutors_terminated,2
add filter to allocator resourcesrecovered method the allocator already allows filters to be added when resources are unused it is useful to also allow the same behaviour in resourcesrecovered,2
the value of master_ping_timeout is nondeterministic right now it is declared as follows noformat const duration master_ping_timeout  masterslave_ping_timeout  mastermax_slave_ping_timeouts noformat since static initialization order in c is undefined master_ping_timeouts value is nondeterministic weve already observed that in tests where master_ping_timeout  0,1
kill private_resources and treat ephemeral_ports as a resource as the first step to solve mesos1654 we need to kill private_resources in slaveinfo and add a ephemeral_ports resource for now the slave and the port mapping isolator will simply ignore the ephemeral_ports resource in executorinfo and taskinfo and make allocation by itself we will revisit this once the overcommit race mesos1466 is fixed,3
zookeepermastercontenderdetectortestmasterdetectortimedoutsession is flaky noformattitle  run  zookeepermastercontenderdetectortestmasterdetectortimedoutsession i0806 011837648684 17458 zookeeper_test_servercpp158 started zookeepertestserver on port 42069 20140806 011837650174580x2b4679ca5700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011837650174580x2b4679ca5700zoo_infolog_env716 client environmenthostnamelucid 20140806 011837650174580x2b4679ca5700zoo_infolog_env723 client environmentosnamelinux 20140806 011837650174580x2b4679ca5700zoo_infolog_env724 client environmentosarch263264generic 20140806 011837650174580x2b4679ca5700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011837651174580x2b4679ca5700zoo_infolog_env733 client environmentusernamenull 20140806 011837651174580x2b4679ca5700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011837651174580x2b4679ca5700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011837651174580x2b4679ca5700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x1682db0 flags0 20140806 011837656174580x2b468638b700zoo_infocheck_events1703 initiated connection to server 12700142069 20140806 011837669174580x2b468638b700zoo_infocheck_events1750 session establishment complete on server 12700142069 sessionid0x147aa6601cf0000 negotiated timeout6000 i0806 011837671725 17486 groupcpp313 group process group3712701155561 connected to zookeeper i0806 011837671758 17486 groupcpp787 syncing group operations queue size joins cancels datas  0 0 0 i0806 011837671771 17486 groupcpp385 trying to create path mesos in zookeeper 20140806 011839101174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client 20140806 011842441174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client i0806 011842656673 17481 contendercpp131 joining the zk group i0806 011842662484 17484 contendercpp247 new candidate id0 has entered the contest for leadership i0806 011842663754 17481 detectorcpp138 detected a new leader id0 i0806 011842663884 17481 groupcpp658 trying to get mesosinfo_0000000000 in zookeeper i0806 011842664788 17483 detectorcpp426 a new leading master upid128150152010000 is detected 20140806 011842666174580x2b4679ea6700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011842666174580x2b4679ea6700zoo_infolog_env716 client environmenthostnamelucid 20140806 011842666174580x2b4679ea6700zoo_infolog_env723 client environmentosnamelinux 20140806 011842666174580x2b4679ea6700zoo_infolog_env724 client environmentosarch263264generic 20140806 011842666174580x2b4679ea6700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011842666174580x2b4679ea6700zoo_infolog_env733 client environmentusernamenull 20140806 011842666174580x2b4679ea6700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011842666174580x2b4679ea6700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011842666174580x2b4679ea6700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x15c00f0 flags0 20140806 011842668174580x2b4686d91700zoo_infocheck_events1703 initiated connection to server 12700142069 20140806 011842672174580x2b4686d91700zoo_infocheck_events1750 session establishment complete on server 12700142069 sessionid0x147aa6601cf0001 negotiated timeout6000 i0806 011842673542 17485 groupcpp313 group process group3812701155561 connected to zookeeper i0806 011842673570 17485 groupcpp787 syncing group operations queue size joins cancels datas  0 0 0 i0806 011842673580 17485 groupcpp385 trying to create path mesos in zookeeper 20140806 011846796174580x2b468638b700zoo_warnzookeeper_interest1557 exceeded deadline by 2131ms 20140806 011846796174580x2b468638b700zoo_errorhandle_socket_error_msg1643 socket 12700142069 zk retcode7 errno110connection timed out connection to 12700142069 timed out exceeded timeout by 131ms 20140806 011846796174580x2b468638b700zoo_warnzookeeper_interest1557 exceeded deadline by 2131ms 20140806 011846796174580x2b4686d91700zoo_warnzookeeper_interest1557 exceeded deadline by 2115ms 20140806 011846796174580x2b4686d91700zoo_errorhandle_socket_error_msg1643 socket 12700142069 zk retcode7 errno110connection timed out connection to 12700142069 timed out exceeded timeout by 115ms 20140806 011846796174580x2b4686d91700zoo_warnzookeeper_interest1557 exceeded deadline by 2115ms 20140806 011846799174580x2b4687394700zoo_warnzookeeper_interest1557 exceeded deadline by 1025ms 20140806 011846800174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client i0806 011846806895 17486 groupcpp418 lost connection to zookeeper attempting to reconnect  i0806 011846807857 17479 groupcpp418 lost connection to zookeeper attempting to reconnect  i0806 011847669064 17482 contendercpp131 joining the zk group 20140806 011847669174580x2b4686d91700zoo_warnzookeeper_interest1557 exceeded deadline by 2989ms 20140806 011847669174580x2b4686d91700zoo_infocheck_events1703 initiated connection to server 12700142069 20140806 011847671174580x2b4686d91700zoo_infocheck_events1750 session establishment complete on server 12700142069 sessionid0x147aa6601cf0001 negotiated timeout6000 i0806 011847682868 17485 contendercpp247 new candidate id1 has entered the contest for leadership i0806 011847683404 17482 groupcpp313 group process group3812701155561 reconnected to zookeeper i0806 011847683445 17482 groupcpp787 syncing group operations queue size joins cancels datas  0 0 0 i0806 011847685998 17482 detectorcpp138 detected a new leader id0 i0806 011847686142 17482 groupcpp658 trying to get mesosinfo_0000000000 in zookeeper i0806 011847687289 17479 detectorcpp426 a new leading master upid128150152010000 is detected 20140806 011847687174580x2b467a2a8700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011847687174580x2b467a2a8700zoo_infolog_env716 client environmenthostnamelucid 20140806 011847687174580x2b467a2a8700zoo_infolog_env723 client environmentosnamelinux 20140806 011847687174580x2b467a2a8700zoo_infolog_env724 client environmentosarch263264generic 20140806 011847687174580x2b467a2a8700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011847687174580x2b467a2a8700zoo_infolog_env733 client environmentusernamenull 20140806 011847687174580x2b467a2a8700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011847687174580x2b467a2a8700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011847687174580x2b467a2a8700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x2b467c0421c0 flags0 20140806 011847699174580x2b4687de6700zoo_infocheck_events1703 initiated connection to server 12700142069 20140806 011847712174580x2b4687de6700zoo_infocheck_events1750 session establishment complete on server 12700142069 sessionid0x147aa6601cf0002 negotiated timeout6000 i0806 011847712846 17479 groupcpp313 group process group3912701155561 connected to zookeeper i0806 011847712873 17479 groupcpp787 syncing group operations queue size joins cancels datas  0 0 0 i0806 011847712882 17479 groupcpp385 trying to create path mesos in zookeeper i0806 011847714648 17479 detectorcpp138 detected a new leader id0 i0806 011847714759 17479 groupcpp658 trying to get mesosinfo_0000000000 in zookeeper i0806 011847716130 17479 detectorcpp426 a new leading master upid128150152010000 is detected 20140806 011847718174580x2b4686d91700zoo_errorhandle_socket_error_msg1721 socket 12700142069 zk retcode4 errno112host is down failed while receiving a server response i0806 011847718889 17479 groupcpp418 lost connection to zookeeper attempting to reconnect  20140806 011847720174580x2b4687de6700zoo_errorhandle_socket_error_msg1721 socket 12700142069 zk retcode4 errno112host is down failed while receiving a server response i0806 011847720788 17484 groupcpp418 lost connection to zookeeper attempting to reconnect  i0806 011847724663 17458 zookeeper_test_servercpp122 shutdown zookeepertestserver on port 42069 20140806 011848798174580x2b468638b700zoo_warnzookeeper_interest1557 exceeded deadline by 4133ms 20140806 011848798174580x2b468638b700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011849720174580x2b4686d91700zoo_warnzookeeper_interest1557 exceeded deadline by 33ms 20140806 011849721174580x2b4686d91700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011849722174580x2b4687de6700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011850136174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client 20140806 011850800174580x2b468638b700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011851723174580x2b4686d91700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011851723174580x2b4687de6700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011852801174580x2b468638b700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client w0806 011852842553 17481 groupcpp456 timed out waiting to reconnect to zookeeper forcing zookeeper session sessionid147aa6601cf0000 expiration i0806 011852842911 17481 groupcpp472 zookeeper session expired i0806 011852843468 17485 detectorcpp126 the current leader id0 is lost i0806 011852843483 17485 detectorcpp138 detected a new leader none i0806 011852843618 17485 contendercpp196 membership cancelled 0 20140806 011852843174580x2b4679aa4700zoo_infozookeeper_close2522 freeing zookeeper resources for sessionid0x147aa6601cf0000 20140806 011852844174580x2b4679aa4700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011852844174580x2b4679aa4700zoo_infolog_env716 client environmenthostnamelucid 20140806 011852844174580x2b4679aa4700zoo_infolog_env723 client environmentosnamelinux 20140806 011852844174580x2b4679aa4700zoo_infolog_env724 client environmentosarch263264generic 20140806 011852844174580x2b4679aa4700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011852844174580x2b4679aa4700zoo_infolog_env733 client environmentusernamenull 20140806 011852844174580x2b4679aa4700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011852844174580x2b4679aa4700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011852844174580x2b4679aa4700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x1349ad0 flags0 20140806 011852844174580x2b468698f700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011853473174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client w0806 011853720684 17480 groupcpp456 timed out waiting to reconnect to zookeeper forcing zookeeper session sessionid147aa6601cf0001 expiration i0806 011853721132 17480 groupcpp472 zookeeper session expired i0806 011853721516 17479 detectorcpp126 the current leader id0 is lost i0806 011853721534 17479 detectorcpp138 detected a new leader none i0806 011853721696 17479 contendercpp196 membership cancelled 1 20140806 011853721174580x2b46798a3700zoo_infozookeeper_close2522 freeing zookeeper resources for sessionid0x147aa6601cf0001 20140806 011853722174580x2b46798a3700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011853722174580x2b46798a3700zoo_infolog_env716 client environmenthostnamelucid 20140806 011853722174580x2b46798a3700zoo_infolog_env723 client environmentosnamelinux 20140806 011853722174580x2b46798a3700zoo_infolog_env724 client environmentosarch263264generic 20140806 011853722174580x2b46798a3700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011853722174580x2b46798a3700zoo_infolog_env733 client environmentusernamenull 20140806 011853722174580x2b46798a3700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011853722174580x2b46798a3700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011853722174580x2b46798a3700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x16a0550 flags0 20140806 011853723174580x2b4686f92700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011853726174580x2b4687de6700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client w0806 011853730258 17479 groupcpp456 timed out waiting to reconnect to zookeeper forcing zookeeper session sessionid147aa6601cf0002 expiration i0806 011853730736 17479 groupcpp472 zookeeper session expired i0806 011853731081 17481 detectorcpp126 the current leader id0 is lost i0806 011853731132 17481 detectorcpp138 detected a new leader none 20140806 011853731174580x2b46796a2700zoo_infozookeeper_close2522 freeing zookeeper resources for sessionid0x147aa6601cf0002 20140806 011853731174580x2b46796a2700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20140806 011853731174580x2b46796a2700zoo_infolog_env716 client environmenthostnamelucid 20140806 011853731174580x2b46796a2700zoo_infolog_env723 client environmentosnamelinux 20140806 011853731174580x2b46796a2700zoo_infolog_env724 client environmentosarch263264generic 20140806 011853731174580x2b46796a2700zoo_infolog_env725 client environmentosversion128ubuntu smp tue jul 15 083240 utc 2014 20140806 011853731174580x2b46796a2700zoo_infolog_env733 client environmentusernamenull 20140806 011853731174580x2b46796a2700zoo_infolog_env741 client environmentuserhomehomejenkins 20140806 011853732174580x2b46796a2700zoo_infolog_env753 client environmentuserdirvarjenkinsworkspacemesosubuntu1004gccsrc 20140806 011853732174580x2b46796a2700zoo_infozookeeper_init786 initiating client connection host12700142069 sessiontimeout5000 watcher0x2b467450bc00 sessionid0 sessionpasswdnull context0x2b467c035f30 flags0 20140806 011853733174580x2b4687be5700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011854512174580x2b468698f700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011855393174580x2b4686f92700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011855403174580x2b4687be5700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011856301174580x2b468698f700zoo_warnzookeeper_interest1557 exceeded deadline by 122ms 20140806 011856302174580x2b468698f700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011856809174580x2b4687394700zoo_errorhandle_socket_error_msg1697 socket 12700136197 zk retcode4 errno111connection refused server refused to accept the client 20140806 011857939174580x2b4686f92700zoo_warnzookeeper_interest1557 exceeded deadline by 879ms 20140806 011857940174580x2b4686f92700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client 20140806 011857940174580x2b4687be5700zoo_warnzookeeper_interest1557 exceeded deadline by 870ms 20140806 011857940174580x2b4687be5700zoo_errorhandle_socket_error_msg1697 socket 12700142069 zk retcode4 errno111connection refused server refused to accept the client testsmaster_contender_detector_testscpp574 failure failed to wait 10secs for leaderreconnecting 20140806 011857941174580x2b46794a0120zoo_infozookeeper_close2522 freeing zookeeper resources for sessionid0 i0806 011857949972 17458 contendercpp186 now cancelling the membership 1 20140806 011,1
allocatortestframeworkreregistersfirst is flaky noformat gmock warning uninteresting mock function call  taking default action specified at mesossrctestsmesoshpp566 function call resourcesrecovered0x7f38f40043e8 20140806190304208117018636159245110000 0x7f38f40043c8 20140806190304208117018636159245110 0x7f38f40043b0  cpus2 mem1024 disk464204 ports3100032000  noformat,2
create user doc for framework rate limiting feature create a markdown doc under docs,2
expose metric for container destroy failures increment counter when container destroy fails,3
futurefailure should return a const string,1
the statsjson endpoint on the slave exposes registered as a string the slave is currently exposing a string value for the registered statistic this should be a number code slave5051statsjson  recovery_errors 0 registered 1 slaveexecutors_registering 0   code should be a pretty straightforward fix looks like this first originated back in 2013 code commit b8291304e1523eb67ea8dc5f195cdb0d8e7d8348 author vinod kone vinodtwittercom date wed jul 3 123736 2013 0700 added a registered keyvalue pair to slaves statsjson review httpsreviewsapacheorgr12256 diff git asrcslavehttpcpp bsrcslavehttpcpp index dc2955fdd51516 100644  asrcslavehttpcpp  bsrcslavehttpcpp  2816 2818  futureresponse slavehttpstatsconst request request objectvalueslost_tasks  slavestatstaskstask_lost objectvaluesvalid_status_updates  slavestatsvalidstatusupdates objectvaluesinvalid_status_updates  slavestatsinvalidstatusupdates  objectvaluesregistered  slavemaster  1  0  return okobject requestquerygetjsonp  code,1
improve reconciliation between master and slave as we update the master to keep tasks in memory until they are both terminal and acknowledged mesos1410 the lifetime of tasks in mesos will look as follows code master slave   tn   master receives task t nonterminal forwards to slave tn tn  slave receives task t nonterminal tn tt  task becomes terminal on slave update forwarded tt tt  master receives update forwards to framework  tt  master receives ack forwards to slave    slave receives ack code in the current form of reconciliation the slave sends to the master all tasks that are not both terminal and acknowledged at any point in the above lifecycle the slaves reregistration message can reach the master note the following properties 1 the master may have a nonterminal task not present in the slaves reregistration message 2 the master may have a nonterminal task present in the slaves reregistration message but in a different state 3 the slaves reregistration message may contain a terminal unacknowledged task unknown to the master in the current master  slave reconciliationhttpsgithubcomapachemesosblob0191srcmastermastercppl3146 code the master assumes that case 1 is because a launch task message was dropped and it sends task_lost weve seen above that 1 can happen even when the task reaches the slave correctly so this can lead to inconsistency after chatting with vinodkone were considering updating the reconciliation to occur as follows  slave sends all tasks that are not both terminal and acknowledged during reregistration this is the same as before  if the master sees tasks that are missing in the slave the master sends the tasks that need to be reconciled to the slave for the tasks this can be piggybacked on the reregistration message  the slave will send task_lost if the task is not known to it preferably in a retried manner unless we update socket closure on the slave to force a reregistration,3
make check segfaults observed this on apache ci httpsbuildsapacheorgjobmesostrunkubuntubuildoutofsrcsetjava_home2331consolefull it looks like the segfault happens before any tests are run so i suspect somewhere in the setup phase of the tests code mv f depsteststime_teststpo depsteststime_testspo binbash libtool tagcxx modelink g g g2 o2 wnounusedlocaltypedefs stdc11 o tests testsdecoder_testso testsencoder_testso testshttp_testso testsio_testso testsmaino testsmutex_testso testsmetrics_testso testsowned_testso testsprocess_testso testsqueue_testso testsreap_testso testssequence_testso testsshared_testso testsstatistics_testso testssubprocess_testso testssystem_testso teststimeseries_testso teststime_testso 3rdpartylibgmockla libprocessla 3rdpartyglog033libglogla 3rdpartylibry_http_parserla 3rdpartylibev415libevla lz lrt libtool link g g g2 o2 wnounusedlocaltypedefs stdc11 o tests testsdecoder_testso testsencoder_testso testshttp_testso testsio_testso testsmaino testsmutex_testso testsmetrics_testso testsowned_testso testsprocess_testso testsqueue_testso testsreap_testso testssequence_testso testsshared_testso testsstatistics_testso testssubprocess_testso testssystem_testso teststimeseries_testso teststime_testso 3rdpartylibslibgmocka libslibprocessa homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess3rdpartyglog033libslibgloga homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess3rdpartylibev415libslibeva 3rdpartyglog033libslibgloga lpthread 3rdpartylibslibry_http_parsera 3rdpartylibev415libslibeva lm lz lrt make5 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess make checklocal make5 entering directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess tests note google test filter   running 0 tests from 0 test cases  0 tests from 0 test cases ran 0 ms total  passed  0 tests you have 3 disabled tests make5  checklocal segmentation fault make5 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess make4  checkam error 2 make4 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess make3  checkrecursive error 1 make3 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdpartylibprocess make2  checkrecursive error 1 make2 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdparty make1  check error 2 make1 leaving directory homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcsetjava_homebuild3rdparty make  checkrecursive error 1 build step execute shell marked build as failure sending emails to devmesosapacheorg benjaminhindmangmailcom dhamontwittercom yujiejaygmailcom finished failure code,2
add document for network monitoring the doc should tell the user how to use the new network monitoring feature,2
better error message when replicated log hasnt been initialized aurora uses the mesos replicated log if you dont run mesoslog initialize before starting aurora youll get info messages in your aurora log code i0814 151838346638 25141 replicacpp633 replica in empty status received a broadcasted recover request i0814 151838346796 25132 recovercpp220 received a recover response from a replica in empty status code it is has been deemed too dangerous to automatically run mesoslog initialize for the user see aurora243 it would be helpful if that error message was made more friendly and at the error level the message could explain what the user should do and the implications of doing so links to the docs would be helpful see httpwildernessapacheorgchannelsfaurora201408141408055261 for context,1
subprocessteststatus sometimes flakes out its a pretty rare event but happened more then once  run  subprocessteststatus  aborted at 1408023909 unix time try date d 1408023909 if you are using gnu date  pc  0x35700094b1 unknown  sigterm 0x3e8000041d8 received by pid 16872 tid 0x7fa9ea426780 from pid 16856 stack trace   0x3570435cb0 unknown  0x35700094b1 unknown  0x3570009d9f unknown  0x357000e726 unknown  0x3570015185 unknown  0x5ead42 processchildmain  0x5ece8d std_function_handler_m_invoke  0x5eac9c processdefaultclone  0x5ebbd4 processsubprocess  0x55a229 processsubprocess  0x55a846 processsubprocess  0x54224c subprocesstest_status_testtestbody  0x7fa9ea460323 unknown  0x7fa9ea455b67 unknown  0x7fa9ea455c0e unknown  0x7fa9ea455d15 unknown  0x7fa9ea4593a8 unknown  0x7fa9ea459647 unknown  0x422466 main  0x3570421d65 unknown  0x4260bd unknown  ok  subprocessteststatus 153 ms,2
automate disallowing of commits mixing mesoslibprocessstout for various reasons we dont want to mix mesoslibprocessstout changes into a single commit typically it is up to the revieweereviewer to catch this it wold be nice to automate this via the precommit hook,2
the slave does not send pending tasks during reregistration in what looks like an oversight the pending tasks and executors in the slave frameworkpending are not sent in the reregistration message for tasks this can lead to spurious task_lost notifications being generated by the master when it falsely thinks the tasks are not present on the slave,3
the slave does not show pending tasks in the json endpoints the slave does not show pending tasks in the statejson endpoint this is a bit tricky to add since we rely on knowing the executor directory,1
command executor can overcommit the slave currently we give a small amount of resources to the command executor in addition to resources used by the command task httpsgithubcomapachemesosblob0200rc1srcslaveslavecppl2448 code title executorinfo slavegetexecutorinfo const frameworkid frameworkid const taskinfo task    add an allowance for the command executor this does lead to a  small overcommit of resources executormutable_resourcesmergefrom resourcesparse cpus  stringifydefault_executor_cpus    mem  stringifydefault_executor_memmegabytesget   code this leads to an overcommit of the slave ideally for command tasks we can transfer all of the task resources to the executor at the slave  isolation level,3
configure fails with configure line 18439 syntax error near unexpected token protobufprefix i followed the getting started documentation and did noformat  git clone httpgitwipusapacheorgreposasfmesosgit cd mesos  bootstrap  mkdir build cd build  configure noformat which aborts with noformat   checking whether we are using the gnu c compiler cached yes checking whether gcc accepts g cached yes checking for gcc option to accept iso c89 cached none needed checking dependency style of gcc cached gcc3 configure line 18439 syntax error near unexpected token protobufprefix configure line 18439  pkg_check_modulesprotobufprefix noformat,2
libprocess report bind parameters on failure when you attempt to start slave or master and theres another one already running there it is nice to report what are the actual parameters to bind call that failed,1
change the stout path utility to declare a single variadic join function instead of several separate declarations of various discrete arities,5
allow slave reconfiguration on restart make it so that either via a slave restart or a out of process reconfigure ping the attributes and resources of a slave can be updated to be a superset of what they used to be,3
masterzookeepertestlostzookeepercluster is flaky noformattitle testsmaster_testscpp1795 failure failed to wait 10secs for slaveregisteredmessage noformat should have placed the future_message that attempts to capture this messages before the slave starts,1
slaverecoverytestshutdownslave is flaky noformat  run  slaverecoverytest0shutdownslave using temporary directory tmpslaverecoverytest_0_shutdownslave_3o5eps i0828 212146206990 27625 leveldbcpp176 opened db in 24461837ms i0828 212146213706 27625 leveldbcpp183 compacted db in 6021499ms i0828 212146214047 27625 leveldbcpp198 created db iterator in 5566ns i0828 212146214313 27625 leveldbcpp204 seeked to beginning of db in 1433ns i0828 212146214515 27625 leveldbcpp273 iterated through 0 keys in the db in 723ns i0828 212146214826 27625 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0828 212146215409 27642 recovercpp425 starting replica recovery i0828 212146215718 27642 recovercpp451 replica is in empty status i0828 212146216264 27642 replicacpp638 replica in empty status received a broadcasted recover request i0828 212146216557 27642 recovercpp188 received a recover response from a replica in empty status i0828 212146216917 27642 recovercpp542 updating replica status to starting i0828 212146221271 27645 mastercpp286 master 20140828212146168428794561327625 saucy started on 12701145613 i0828 212146221812 27645 mastercpp332 master only allowing authenticated frameworks to register i0828 212146222038 27645 mastercpp337 master only allowing authenticated slaves to register i0828 212146222250 27645 credentialshpp36 loading credentials for authentication from tmpslaverecoverytest_0_shutdownslave_3o5epscredentials i0828 212146222585 27645 mastercpp366 authorization enabled i0828 212146222885 27642 leveldbcpp306 persisting metadata 8 bytes to leveldb took 5596969ms i0828 212146223085 27642 replicacpp320 persisted replica status to starting i0828 212146223424 27642 recovercpp451 replica is in starting status i0828 212146223933 27642 replicacpp638 replica in starting status received a broadcasted recover request i0828 212146224984 27642 recovercpp188 received a recover response from a replica in starting status i0828 212146225385 27642 recovercpp542 updating replica status to voting i0828 212146224750 27646 mastercpp1205 the newly elected leader is master12701145613 with id 20140828212146168428794561327625 i0828 212146226132 27646 mastercpp1218 elected as the leading master i0828 212146226349 27646 mastercpp1036 recovering from registrar i0828 212146226637 27646 registrarcpp313 recovering registrar i0828 212146224473 27641 mastercpp120 no whitelist given advertising offers for all slaves i0828 212146224431 27645 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master12701145613 i0828 212146240932 27642 leveldbcpp306 persisting metadata 8 bytes to leveldb took 15182422ms i0828 212146241453 27642 replicacpp320 persisted replica status to voting i0828 212146241926 27643 recovercpp556 successfully joined the paxos group i0828 212146242228 27642 recovercpp440 recover process terminated i0828 212146242501 27645 logcpp656 attempting to start the writer i0828 212146243247 27645 replicacpp474 replica received implicit promise request with proposal 1 i0828 212146253456 27645 leveldbcpp306 persisting metadata 8 bytes to leveldb took 995472ms i0828 212146253955 27645 replicacpp342 persisted promised to 1 i0828 212146254518 27645 coordinatorcpp230 coordinator attemping to fill missing position i0828 212146255234 27641 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0828 212146263128 27641 leveldbcpp343 persisting action 8 bytes to leveldb took 7484042ms i0828 212146263536 27641 replicacpp676 persisted action at 0 i0828 212146263806 27641 replicacpp508 replica received write request for position 0 i0828 212146263834 27641 leveldbcpp438 reading position from leveldb took 14063ns i0828 212146276149 27641 leveldbcpp343 persisting action 14 bytes to leveldb took 12295476ms i0828 212146276178 27641 replicacpp676 persisted action at 0 i0828 212146276319 27641 replicacpp655 replica received learned notice for position 0 i0828 212146285523 27641 leveldbcpp343 persisting action 16 bytes to leveldb took 9185244ms i0828 212146285552 27641 replicacpp676 persisted action at 0 i0828 212146285560 27641 replicacpp661 replica learned nop action at position 0 i0828 212146289685 27642 logcpp672 writer started with ending position 0 i0828 212146290166 27642 leveldbcpp438 reading position from leveldb took 14463ns i0828 212146297260 27642 registrarcpp346 successfully fetched the registry 0b i0828 212146297622 27642 registrarcpp422 attempting to update the registry i0828 212146298893 27645 logcpp680 attempting to append 118 bytes to the log i0828 212146299190 27645 coordinatorcpp340 coordinator attempting to write append action at position 1 i0828 212146299643 27645 replicacpp508 replica received write request for position 1 i0828 212146310351 27645 leveldbcpp343 persisting action 135 bytes to leveldb took 10349409ms i0828 212146310577 27645 replicacpp676 persisted action at 1 i0828 212146311039 27645 replicacpp655 replica received learned notice for position 1 i0828 212146322127 27645 leveldbcpp343 persisting action 137 bytes to leveldb took 10858061ms i0828 212146322614 27645 replicacpp676 persisted action at 1 i0828 212146322875 27645 replicacpp661 replica learned append action at position 1 i0828 212146323480 27645 registrarcpp479 successfully updated registry i0828 212146323874 27645 registrarcpp372 successfully recovered registrar i0828 212146323649 27639 logcpp699 attempting to truncate the log to 1 i0828 212146324465 27644 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0828 212146324988 27644 replicacpp508 replica received write request for position 2 i0828 212146325335 27643 mastercpp1063 recovered 0 slaves from the registry 82b  allowing 10mins for slaves to reregister i0828 212146335847 27644 leveldbcpp343 persisting action 16 bytes to leveldb took 10651398ms i0828 212146336320 27644 replicacpp676 persisted action at 2 i0828 212146336896 27644 replicacpp655 replica received learned notice for position 2 i0828 212146345854 27644 leveldbcpp343 persisting action 18 bytes to leveldb took 8540555ms i0828 212146346261 27644 leveldbcpp401 deleting 1 keys from leveldb took 30183ns i0828 212146346282 27644 replicacpp676 persisted action at 2 i0828 212146346315 27644 replicacpp661 replica learned truncate action at position 2 i0828 212146356840 27625 containerizercpp89 using isolation posixcpuposixmem i0828 212146361413 27644 slavecpp167 slave started on 4812701145613 i0828 212146361753 27644 credentialshpp84 loading credential for authentication from tmpslaverecoverytest_0_shutdownslave_umhrawcredential i0828 212146362046 27644 slavecpp274 slave using credential for testprincipal i0828 212146362810 27644 slavecpp287 slave resources cpus2 mem1024 disk1024 ports3100032000 i0828 212146363088 27644 slavecpp315 slave hostname saucy i0828 212146363301 27644 slavecpp316 slave checkpoint true i0828 212146363986 27644 statecpp33 recovering state from tmpslaverecoverytest_0_shutdownslave_umhrawmeta i0828 212146364308 27644 status_update_managercpp193 recovering status update manager i0828 212146364600 27644 containerizercpp252 recovering containerizer i0828 212146365325 27646 slavecpp3204 finished recovery i0828 212146365839 27646 slavecpp598 new master detected at master12701145613 i0828 212146366041 27646 slavecpp672 authenticating with master master12701145613 i0828 212146366317 27646 slavecpp645 detecting new master i0828 212146366569 27646 status_update_managercpp167 new master detected at master12701145613 i0828 212146366827 27646 authenticateehpp128 creating new client sasl connection i0828 212146367204 27646 mastercpp3637 authenticating slave4812701145613 i0828 212146367553 27646 authenticatorhpp156 creating new server sasl connection i0828 212146367857 27646 authenticateehpp219 received sasl authentication mechanisms crammd5 i0828 212146368031 27646 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0828 212146368228 27646 authenticatorhpp262 received sasl authentication start i0828 212146368444 27646 authenticatorhpp384 authentication requires more steps i0828 212146368648 27646 authenticateehpp265 received sasl authentication step i0828 212146368924 27646 authenticatorhpp290 received sasl authentication step i0828 212146369120 27646 auxpropcpp81 request to lookup properties for user testprincipal realm saucy server fqdn saucy sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0828 212146369350 27646 auxpropcpp153 looking up auxiliary property userpassword i0828 212146369544 27646 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0828 212146369730 27646 auxpropcpp81 request to lookup properties for user testprincipal realm saucy server fqdn saucy sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0828 212146369958 27646 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0828 212146370131 27646 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0828 212146370311 27646 authenticatorhpp376 authentication success i0828 212146370518 27646 authenticateehpp305 authentication success i0828 212146370637 27642 mastercpp3677 successfully authenticated principal testprincipal at slave4812701145613 i0828 212146371772 27641 slavecpp729 successfully authenticated with master master12701145613 i0828 212146371984 27641 slavecpp980 will retry registration in 15311045ms if necessary i0828 212146372643 27641 mastercpp2836 registering slave at slave4812701145613 saucy with id 201408282121461684287945613276250 i0828 212146373016 27641 registrarcpp422 attempting to update the registry i0828 212146374539 27641 logcpp680 attempting to append 289 bytes to the log i0828 212146374876 27641 coordinatorcpp340 coordinator attempting to write append action at position 3 i0828 212146375296 27641 replicacpp508 replica received write request for position 3 i0828 212146376046 27625 schedcpp137 version 0210 i0828 212146376374 27646 schedcpp233 new master detected at master12701145613 i0828 212146376595 27646 schedcpp283 authenticating with master master12701145613 i0828 212146376857 27646 authenticateehpp128 creating new client sasl connection i0828 212146377234 27646 mastercpp3637 authenticating schedulercb5a026423cc45d0bc4ca92fa530815812701145613 i0828 212146377496 27646 authenticatorhpp156 creating new server sasl connection i0828 212146377771 27646 authenticateehpp219 received sasl authentication mechanisms crammd5 i0828 212146377961 27646 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0828 212146378170 27646 authenticatorhpp262 received sasl authentication start i0828 212146378360 27646 authenticatorhpp384 authentication requires more steps i0828 212146378588 27639 authenticateehpp265 received sasl authentication step i0828 212146378789 27646 authenticatorhpp290 received sasl authentication step i0828 212146378942 27646 auxpropcpp81 request to lookup properties for user testprincipal realm saucy server fqdn saucy sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0828 212146379091 27646 auxpropcpp153 looking up auxiliary property userpassword i0828 212146379298 27646 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0828 212146379539 27646 auxpropcpp81 request to lookup properties for user testprincipal realm saucy server fqdn saucy sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0828 212146379720 27646 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0828 212146379935 27646 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0828 212146380089 27646 authenticatorhpp376 authentication success i0828 212146380306 27642 authenticateehpp305 authentication success i0828 212146382625 27642 schedcpp357 successfully authenticated with master master12701145613 i0828 212146383031 27642 schedcpp476 sending registration request to master12701145613 i0828 212146382928 27640 mastercpp3677 successfully authenticated principal testprincipal at schedulercb5a026423cc45d0bc4ca92fa530815812701145613 i0828 212146383651 27640 mastercpp1324 received registration request from schedulercb5a026423cc45d0bc4ca92fa530815812701145613 i0828 212146383846 27640 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0828 212146384184 27640 mastercpp1383 registering framework 201408282121461684287945613276250000 at schedulercb5a026423cc45d0bc4ca92fa530815812701145613 i0828 212146384464 27640 schedcpp407 framework registered with 201408282121461684287945613276250000 i0828 212146384764 27640 schedcpp421 schedulerregistered took 18266ns i0828 212146384600 27644 hierarchical_allocator_processhpp329 added framework 201408282121461684287945613276250000 i0828 212146385171 27644 hierarchical_allocator_processhpp691 no resources available to allocate i0828 212146385330 27644 hierarchical_allocator_processhpp653 performed allocation for 0 slaves in 160171ns i0828 212146386292 27641 leveldbcpp343 persisting action 308 bytes to leveldb took 10815384ms i0828 212146386492 27641 replicacpp676 persisted action at 3 i0828 212146386844 27641 replicacpp655 replica received learned notice for position 3 i0828 212146387980 27643 slavecpp980 will retry registration in 19851524ms if necessary i0828 212146388140 27639 mastercpp2824 ignoring register slave message from slave4812701145613 saucy as admission is already in progress i0828 212146396355 27641 leveldbcpp343 persisting action 310 bytes to leveldb took 9275034ms i0828 212146396641 27641 replicacpp676 persisted action at 3 i0828 212146396837 27641 replicacpp661 replica learned append action at position 3 i0828 212146397405 27641 registrarcpp479 successfully updated registry i0828 212146397528 27645 logcpp699 attempting to truncate the log to 3 i0828 212146397878 27645 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0828 212146398239 27645 replicacpp508 replica received write request for position 4 i0828 212146398597 27641 mastercpp2876 registered slave 201408282121461684287945613276250 at slave4812701145613 saucy i0828 212146398870 27641 mastercpp4110 adding slave 201408282121461684287945613276250 at slave4812701145613 saucy with cpus2 mem1024 disk1024 ports3100032000 i0828 212146399178 27639 slavecpp763 registered with master master12701145613 given slave id 201408282121461684287945613276250 i0828 212146399521 27639 slavecpp776 checkpointing slaveinfo to tmpslaverecoverytest_0_shutdownslave_umhrawmetaslaves201408282121461684287945613276250slaveinfo i0828 212146399961 27641 hierarchical_allocator_processhpp442 added slave 201408282121461684287945613276250 saucy with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0828 212146400316 27641 hierarchical_allocator_processhpp728 offering cpus2 mem1024 disk1024 ports3100032000 on slave 201408282121461684287945613276250 to framework 201408282121461684287945613276250000 i0828 212146400158 27644 slavecpp2333 received ping from slaveobserver4512701145613 i0828 212146400872 27639 masterhpp857 adding offer 201408282121461684287945613276250 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201408282121461684287945613276250 saucy i0828 212146401105 27639 mastercpp3584 sending 1 offers to framework 201408282121461684287945613276250000 i0828 212146401448 27639 schedcpp544 schedulerresourceoffers took 19056ns i0828 212146401700 27641 hierarchical_allocator_processhpp673 performed allocation for slave 201408282121461684287945613276250 in 1430159ms i0828 212146403659 27644 masterhpp867 removing offer 201408282121461684287945613276250 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201408282121461684287945613276250 saucy i0828 212146403903 27644 mastercpp2194 processing reply for offers  201408282121461684287945613276250  on slave 201408282121461684287945613276250 at slave4812701145613 saucy for framework 201408282121461684287945613276250000 i0828 212146404116 27644 mastercpp2277 authorizing framework principal testprincipal to launch task cf5afc1bc007435b8c36be8aa3659d3a as user jenkins i0828 212146404578 27644 masterhpp829 adding task cf5afc1bc007435b8c36be8aa3659d3a with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201408282121461684287945613276250 saucy i0828 212146404824 27644 mastercpp2343 launching task cf5afc1bc007435b8c36be8aa3659d3a of framework 201408282121461684287945613276250000 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201408282121461684287945613276250 at slave4812701145613 saucy i0828 212146405206 27644 slavecpp1011 got assigned task cf5afc1bc007435b8c36be8aa3659d3a for framework 201408282121461684287945613276250000 i0828 212146405462 27644 slavecpp3542 checkpointing frameworkinfo to tmpslaverecoverytest_0_shutdownslave_umhrawmetaslaves201408282121461684287945613276250frameworks201408282121461684287945613276250000frameworkinfo i0828 212146405840 27644 slavecpp3549 checkpointing framework pid schedulercb5a026423cc45d0bc4ca92fa530815812701145613 to tmpslaverecoverytest_0_shutdownslave_umhrawmetaslaves201408282121461684287945613276250frameworks201408282121461684287945613276250000frameworkpid i0828 212146406122 27645 leveldbcpp343 persisting action 16 bytes to leveldb took 7684731ms i0828 212146406288 27645 replicacpp676 persisted action at 4 i0828 212146406618 27645 replicacpp655 replica received learned notice for position 4 i0828 212146407562 27644 slavecpp1121 launching task cf5afc1bc007435b8c36be8aa3659d3a for framework 201408282121461684287945613276250000 i0828 212146409296 27644 slavecpp3858 checkpointing executorinfo to tmpslaverecoverytest_0_shutdownslave_umhrawmetaslaves20,2
request for statsjson cannot be fulfilled after stopping the framework request for statsjson to master from a test case doesnt work after calling frameworks driverstop however it works for statejson i think the problem is related to stats continuation _stats the following test illustrates the issue codetitletestcasecppborderstylesolid test_fmastertest requestafterdriverstop  trypidmaster  master  startmaster assert_somemaster trypidslave  slave  startslave assert_someslave mockscheduler sched mesosschedulerdriver driver sched default_framework_info masterget default_credential driverstart futureprocesshttpresponse response_before  processhttpgetmasterget statsjson await_readyresponse_before driverstop futureprocesshttpresponse response_after  processhttpgetmasterget statsjson await_readyresponse_after driverjoin shutdown  must shutdown before containerizer gets deallocated  code,5
allow variadic templates add variadic templates to the c11 configure check once there we can start using them in the codebase,1
freezer failure leads to lost task during container destruction in the past weve seen numerous issues around the freezer lately on the 2644 kernel weve seen issues where were unable to freeze the cgroup 1 an oom occurs 2 no indication of oom in the kernel logs 3 the slave is unable to freeze the cgroup 4 the task is marked as lost noformat i0903 164624956040 25469 memcpp575 memory limit exceeded requested 15488mb maximum used 15488mb memory statistics cache 7958691840 rss 8281653248 mapped_file 9474048 pgpgin 4487861 pgpgout 522933 pgfault 2533780 pgmajfault 11 inactive_anon 0 active_anon 8281653248 inactive_file 7631708160 active_file 326852608 unevictable 0 hierarchical_memory_limit 16240345088 total_cache 7958691840 total_rss 8281653248 total_mapped_file 9474048 total_pgpgin 4487861 total_pgpgout 522933 total_pgfault 2533780 total_pgmajfault 11 total_inactive_anon 0 total_active_anon 8281653248 total_inactive_file 7631728640 total_active_file 326852608 total_unevictable 0 i0903 164624956848 25469 containerizercpp1041 container bbb9732ad6004c1bb326846338c608c3 has reached its limit for resource mem162403e10 and will be terminated i0903 164624957427 25469 containerizercpp909 destroying container bbb9732ad6004c1bb326846338c608c3 i0903 164624958664 25481 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164634959529 25488 cgroupscpp2209 thawing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164634962070 25482 cgroupscpp1404 successfullly thawed cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 after 1710848ms i0903 164634962658 25479 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164644963349 25488 cgroupscpp2209 thawing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164644965631 25472 cgroupscpp1404 successfullly thawed cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 after 1588224ms i0903 164644966356 25472 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164654967254 25488 cgroupscpp2209 thawing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164656008447 25475 cgroupscpp1404 successfullly thawed cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 after 215296ms i0903 164656009071 25466 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164706010329 25488 cgroupscpp2209 thawing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164706012538 25467 cgroupscpp1404 successfullly thawed cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 after 1643008ms i0903 164706013216 25467 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164712516348 25480 slavecpp3030 current usage 957 max allowed age 5630238827780799days i0903 164716015192 25488 cgroupscpp2209 thawing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164716017043 25486 cgroupscpp1404 successfullly thawed cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 after 1511168ms i0903 164716017555 25480 cgroupscpp2192 freezing cgroup sysfscgroupfreezermesosbbb9732ad6004c1bb326846338c608c3 i0903 164719862746 25483 httpcpp245 http request for slave1statsjson e0903 164724960055 25472 slavecpp2557 termination of executor e of framework 20110407000400000025630000 failed failed to destroy container discarded future i0903 164724962054 25472 slavecpp2087 handling status update task_lost uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 from 00000 i0903 164724963470 25469 memcpp293 updated memorysoft_limit_in_bytes to 128mb for container bbb9732ad6004c1bb326846338c608c3 i0903 164724963541 25471 cpusharecpp338 updated cpushares to 256 cpus 025 for container bbb9732ad6004c1bb326846338c608c3 i0903 164724964756 25471 cpusharecpp359 updated cpucfs_period_us to 100ms and cpucfs_quota_us to 25ms cpus 025 for container bbb9732ad6004c1bb326846338c608c3 i0903 164743406610 25476 status_update_managercpp320 received status update task_lost uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 i0903 164743406991 25476 status_update_managerhpp342 checkpointing update for status update task_lost uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 i0903 164743410475 25476 status_update_managercpp373 forwarding status update task_lost uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 to masterscrubbed_ip5050 i0903 164743439923 25480 status_update_managercpp398 received status update acknowledgement uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 i0903 164743440115 25480 status_update_managerhpp342 checkpointing ack for status update task_lost uuid c0c1633b722140dc90a2660ef639f747 for task t of framework 20110407000400000025630000 i0903 164743443595 25480 slavecpp2709 cleaning up executor e of framework 20110407000400000025630000 noformat we should consider avoiding the freezer entirely in favor of a kill2 loop we dont have to wait for pid namespaces to remove the freezer dependency at the very least when the freezer fails we should proceed with a kill2 loop to ensure that we destroy the cgroup,2
masterauthorizationtestframeworkremovedbeforereregistration is flaky observed this on apache ci httpsbuildsapacheorgjobmesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebui2355changes code  run masterauthorizationtestframeworkremovedbeforereregistration using temporary directory tmpmasterauthorizationtest_frameworkremovedbeforereregistration_0tw16z i0903 220433520237 25565 leveldbcpp176 opened db in 49073821ms i0903 220433538331 25565 leveldbcpp183 compacted db in 18065051ms i0903 220433538363 25565 leveldbcpp198 created db iterator in 4826ns i0903 220433538377 25565 leveldbcpp204 seeked to beginning of db in 682ns i0903 220433538385 25565 leveldbcpp273 iterated through 0 keys in the db in 312ns i0903 220433538399 25565 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0903 220433538624 25593 recovercpp425 starting replica recovery i0903 220433538707 25598 recovercpp451 replica is in empty status i0903 220433540909 25590 mastercpp286 master 201409032204334537598844412225565 hemeraapacheorg started on 140211112744122 i0903 220433540932 25590 mastercpp332 master only allowing authenticated frameworks to register i0903 220433540936 25590 mastercpp337 master only allowing authenticated slaves to register i0903 220433540941 25590 credentialshpp36 loading credentials for authentication from tmpmasterauthorizationtest_frameworkremovedbeforereregistration_0tw16zcredentials i0903 220433541337 25590 mastercpp366 authorization enabled i0903 220433541508 25597 replicacpp638 replica in empty status received a broadcasted recover request i0903 220433542343 25582 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master140211112744122 i0903 220433542445 25592 mastercpp120 no whitelist given advertising offers for all slaves i0903 220433543175 25602 recovercpp188 received a recover response from a replica in empty status i0903 220433543637 25587 recovercpp542 updating replica status to starting i0903 220433544256 25579 mastercpp1205 the newly elected leader is master140211112744122 with id 201409032204334537598844412225565 i0903 220433544275 25579 mastercpp1218 elected as the leading master i0903 220433544282 25579 mastercpp1036 recovering from registrar i0903 220433544401 25579 registrarcpp313 recovering registrar i0903 220433558487 25593 leveldbcpp306 persisting metadata 8 bytes to leveldb took 14678563ms i0903 220433558531 25593 replicacpp320 persisted replica status to starting i0903 220433558653 25593 recovercpp451 replica is in starting status i0903 220433559867 25588 replicacpp638 replica in starting status received a broadcasted recover request i0903 220433560057 25602 recovercpp188 received a recover response from a replica in starting status i0903 220433561280 25584 recovercpp542 updating replica status to voting i0903 220433576900 25581 leveldbcpp306 persisting metadata 8 bytes to leveldb took 14712427ms i0903 220433576942 25581 replicacpp320 persisted replica status to voting i0903 220433577018 25581 recovercpp556 successfully joined the paxos group i0903 220433577108 25581 recovercpp440 recover process terminated i0903 220433577401 25581 logcpp656 attempting to start the writer i0903 220433578559 25589 replicacpp474 replica received implicit promise request with proposal 1 i0903 220433594611 25589 leveldbcpp306 persisting metadata 8 bytes to leveldb took 16029152ms i0903 220433594640 25589 replicacpp342 persisted promised to 1 i0903 220433595391 25584 coordinatorcpp230 coordinator attemping to fill missing position i0903 220433597512 25588 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0903 220433613037 25588 leveldbcpp343 persisting action 8 bytes to leveldb took 15502568ms i0903 220433613065 25588 replicacpp676 persisted action at 0 i0903 220433615435 25585 replicacpp508 replica received write request for position 0 i0903 220433615463 25585 leveldbcpp438 reading position from leveldb took 10743ns i0903 220433630801 25585 leveldbcpp343 persisting action 14 bytes to leveldb took 15320225ms i0903 220433630852 25585 replicacpp676 persisted action at 0 i0903 220433631126 25585 replicacpp655 replica received learned notice for position 0 i0903 220433647801 25585 leveldbcpp343 persisting action 16 bytes to leveldb took 16652951ms i0903 220433647830 25585 replicacpp676 persisted action at 0 i0903 220433647842 25585 replicacpp661 replica learned nop action at position 0 i0903 220433648548 25583 logcpp672 writer started with ending position 0 i0903 220433649235 25583 leveldbcpp438 reading position from leveldb took 25209ns i0903 220433650897 25591 registrarcpp346 successfully fetched the registry 0b i0903 220433650930 25591 registrarcpp422 attempting to update the registry i0903 220433652861 25601 logcpp680 attempting to append 138 bytes to the log i0903 220433653097 25586 coordinatorcpp340 coordinator attempting to write append action at position 1 i0903 220433655225 25590 replicacpp508 replica received write request for position 1 i0903 220433669618 25590 leveldbcpp343 persisting action 157 bytes to leveldb took 14337486ms i0903 220433669663 25590 replicacpp676 persisted action at 1 i0903 220433670045 25584 replicacpp655 replica received learned notice for position 1 i0903 220434414243 25584 leveldbcpp343 persisting action 159 bytes to leveldb took 15401247ms i0903 220434414300 25584 replicacpp676 persisted action at 1 i0903 220434414316 25584 replicacpp661 replica learned append action at position 1 i0903 220434414937 25589 registrarcpp479 successfully updated registry i0903 220434415069 25585 logcpp699 attempting to truncate the log to 1 i0903 220434415194 25589 registrarcpp372 successfully recovered registrar i0903 220434415284 25589 mastercpp1063 recovered 0 slaves from the registry 100b  allowing 10mins for slaves to reregister i0903 220434415362 25587 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0903 220434418926 25597 replicacpp508 replica received write request for position 2 i0903 220434434321 25597 leveldbcpp343 persisting action 16 bytes to leveldb took 15368147ms i0903 220434434352 25597 replicacpp676 persisted action at 2 i0903 220434435022 25582 replicacpp655 replica received learned notice for position 2 i0903 220434450331 25582 leveldbcpp343 persisting action 18 bytes to leveldb took 15284486ms i0903 220434450387 25582 leveldbcpp401 deleting 1 keys from leveldb took 25774ns i0903 220434450402 25582 replicacpp676 persisted action at 2 i0903 220434450412 25582 replicacpp661 replica learned truncate action at position 2 i0903 220434460691 25565 schedcpp137 version 0210 i0903 220434460927 25582 schedcpp233 new master detected at master140211112744122 i0903 220434460948 25582 schedcpp283 authenticating with master master140211112744122 i0903 220434461359 25582 authenticateehpp128 creating new client sasl connection i0903 220434461647 25582 mastercpp3637 authenticating scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434461801 25598 authenticatorhpp156 creating new server sasl connection i0903 220434462172 25598 authenticateehpp219 received sasl authentication mechanisms crammd5 i0903 220434462185 25598 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0903 220434462257 25598 authenticatorhpp262 received sasl authentication start i0903 220434462323 25598 authenticatorhpp384 authentication requires more steps i0903 220434462345 25598 authenticateehpp265 received sasl authentication step i0903 220434462417 25598 authenticatorhpp290 received sasl authentication step i0903 220434462522 25598 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0903 220434462529 25598 auxpropcpp153 looking up auxiliary property userpassword i0903 220434462538 25598 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0903 220434462543 25598 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0903 220434462548 25598 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0903 220434462550 25598 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0903 220434462558 25598 authenticatorhpp376 authentication success i0903 220434462635 25598 mastercpp3677 successfully authenticated principal testprincipal at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434462687 25590 authenticateehpp305 authentication success i0903 220434463219 25588 schedcpp357 successfully authenticated with master master140211112744122 i0903 220434463243 25588 schedcpp476 sending registration request to master140211112744122 i0903 220434463307 25588 mastercpp1324 received registration request from scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434463330 25588 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0903 220434463412 25588 mastercpp1383 registering framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434463577 25598 schedcpp407 framework registered with 2014090322043345375988444122255650000 i0903 220434463728 25587 hierarchical_allocator_processhpp329 added framework 2014090322043345375988444122255650000 i0903 220434463739 25587 hierarchical_allocator_processhpp697 no resources available to allocate i0903 220434463743 25587 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 5016ns i0903 220434463755 25598 schedcpp421 schedulerregistered took 165035ns i0903 220434465558 25583 schedcpp227 schedulerdisconnected took 6254ns i0903 220434465566 25583 schedcpp233 new master detected at master140211112744122 i0903 220434465575 25583 schedcpp283 authenticating with master master140211112744122 i0903 220434465642 25583 authenticateehpp128 creating new client sasl connection i0903 220434465790 25583 mastercpp1680 deactivating framework 2014090322043345375988444122255650000 i0903 220434465850 25583 mastercpp3637 authenticating scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434465879 25601 hierarchical_allocator_processhpp405 deactivated framework 2014090322043345375988444122255650000 i0903 220434466047 25600 authenticatorhpp156 creating new server sasl connection i0903 220434466315 25600 authenticateehpp219 received sasl authentication mechanisms crammd5 i0903 220434466326 25600 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0903 220434466346 25600 authenticatorhpp262 received sasl authentication start i0903 220434466418 25600 authenticatorhpp384 authentication requires more steps i0903 220434466436 25600 authenticateehpp265 received sasl authentication step i0903 220434466475 25600 authenticatorhpp290 received sasl authentication step i0903 220434466486 25600 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0903 220434466491 25600 auxpropcpp153 looking up auxiliary property userpassword i0903 220434466496 25600 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0903 220434466502 25600 auxpropcpp81 request to lookup properties for user testprincipal realm hemeraapacheorg server fqdn hemeraapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0903 220434466506 25600 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0903 220434466509 25600 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0903 220434466516 25600 authenticatorhpp376 authentication success i0903 220434466596 25588 mastercpp3677 successfully authenticated principal testprincipal at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220434466629 25597 authenticateehpp305 authentication success i0903 220434467062 25594 schedcpp357 successfully authenticated with master master140211112744122 i0903 220434467077 25594 schedcpp476 sending registration request to master140211112744122 i0903 220434467190 25588 mastercpp1448 received reregistration request from framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220436368134 25588 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0903 220434542999 25594 hierarchical_allocator_processhpp697 no resources available to allocate i0903 220435463639 25582 schedcpp476 sending registration request to master140211112744122 i0903 220436368185 25594 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 1825177748secs i0903 220436368302 25588 mastercpp1448 received reregistration request from framework 2014090322043345375988444122255650000 at scheduler04e0b5717e0c4ef3bb14c6bbfd8ac9a4140211112744122 i0903 220436368330 25588 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0903 220436368388 25582 schedcpp476 sending registration request to master140211112744122  failure mock function called more times than expected  returning default value function call authorize0x2ba11964c1b0 40byte object d0ed 3916 a12b 0000 0000 0000 0000 0000 006c 013c a12b 0000 3020 003c a12b 0000 0000 0000 0300 0000 the mock function has no default action set and its return type has no default value set  aborted at 1409781876 unix time try date d 1409781876 if you are using gnu date  i0903 220436368913 25598 schedcpp745 stopping framework 2014090322043345375988444122255650000 pc  0x2ba117a990d5 unknown  sigabrt 0x3ea000063dd received by pid 25565 tid 0x2ba11964d700 from pid 25565 stack trace   0x2ba117854cb0 unknown  0x2ba117a990d5 unknown  0x2ba117a9c83b unknown  0x9cba9d testinginternalgoogletestfailurereporterreportfailure  0x790091 testinginternalfunctionmockerbaseperformdefaultaction  0x790166 testinginternalfunctionmockerbaseuntypedperformdefaultaction  0x9c3daa testinginternaluntypedfunctionmockerbaseuntypedinvokewith  0x787279 mesosinternaltestsmockauthorizerauthorize  0x2ba1157c133d mesosinternalmastermastervalidate  0x2ba1157c2b7a mesosinternalmastermasterreregisterframework  0x2ba1157e0038 protobufprocesshandler2  0x2ba1157dde89 stdtr1_function_handler_m_invoke  0x2ba1157b15f7 mesosinternalmastermaster_visit  0x2ba1157bfa3e mesosinternalmastermastervisit  0x2ba115caf5e7 processprocessmanagerresume  0x2ba115cb027c processschedule  0x2ba11784ce9a start_thread  0x2ba117b5731d unknown code,1
use pid namespace to avoid freezing cgroup there is some known kernel issue when we freeze the whole cgroup upon oom mesos probably can just use pid namespace so that we will only need to kill the init of the pid namespace instead of freezing all the processes and killing them one by one but i am not quite sure if this would break the existing code,5
masterauthorizationtestduplicateregistration test is flaky code  run  masterauthorizationtestduplicateregistration using temporary directory tmpmasterauthorizationtest_duplicateregistration_pvjg7m i0905 155316398993 25769 leveldbcpp176 opened db in 2601036ms i0905 155316399566 25769 leveldbcpp183 compacted db in 546216ns i0905 155316399590 25769 leveldbcpp198 created db iterator in 2787ns i0905 155316399605 25769 leveldbcpp204 seeked to beginning of db in 500ns i0905 155316399617 25769 leveldbcpp273 iterated through 0 keys in the db in 185ns i0905 155316399633 25769 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0905 155316399817 25786 recovercpp425 starting replica recovery i0905 155316399952 25793 recovercpp451 replica is in empty status i0905 155316400683 25795 replicacpp638 replica in empty status received a broadcasted recover request i0905 155316400795 25787 recovercpp188 received a recover response from a replica in empty status i0905 155316401005 25783 recovercpp542 updating replica status to starting i0905 155316401470 25786 mastercpp286 master 2014090515531631259205794918825769 penatesapacheorg started on 671958118649188 i0905 155316401521 25786 mastercpp332 master only allowing authenticated frameworks to register i0905 155316401533 25786 mastercpp337 master only allowing authenticated slaves to register i0905 155316401543 25786 credentialshpp36 loading credentials for authentication from tmpmasterauthorizationtest_duplicateregistration_pvjg7mcredentials i0905 155316401558 25793 leveldbcpp306 persisting metadata 8 bytes to leveldb took 474683ns i0905 155316401582 25793 replicacpp320 persisted replica status to starting i0905 155316401667 25793 recovercpp451 replica is in starting status i0905 155316401669 25786 mastercpp366 authorization enabled i0905 155316401898 25795 mastercpp120 no whitelist given advertising offers for all slaves i0905 155316401936 25796 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master671958118649188 i0905 155316402160 25784 replicacpp638 replica in starting status received a broadcasted recover request i0905 155316402333 25790 mastercpp1205 the newly elected leader is master671958118649188 with id 2014090515531631259205794918825769 i0905 155316402359 25790 mastercpp1218 elected as the leading master i0905 155316402371 25790 mastercpp1036 recovering from registrar i0905 155316402472 25798 registrarcpp313 recovering registrar i0905 155316402529 25791 recovercpp188 received a recover response from a replica in starting status i0905 155316402782 25788 recovercpp542 updating replica status to voting i0905 155316403002 25795 leveldbcpp306 persisting metadata 8 bytes to leveldb took 116403ns i0905 155316403020 25795 replicacpp320 persisted replica status to voting i0905 155316403081 25791 recovercpp556 successfully joined the paxos group i0905 155316403197 25791 recovercpp440 recover process terminated i0905 155316403388 25796 logcpp656 attempting to start the writer i0905 155316403993 25784 replicacpp474 replica received implicit promise request with proposal 1 i0905 155316404147 25784 leveldbcpp306 persisting metadata 8 bytes to leveldb took 132156ns i0905 155316404167 25784 replicacpp342 persisted promised to 1 i0905 155316404542 25795 coordinatorcpp230 coordinator attemping to fill missing position i0905 155316405498 25787 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0905 155316405868 25787 leveldbcpp343 persisting action 8 bytes to leveldb took 347231ns i0905 155316405886 25787 replicacpp676 persisted action at 0 i0905 155316406553 25788 replicacpp508 replica received write request for position 0 i0905 155316406582 25788 leveldbcpp438 reading position from leveldb took 11402ns i0905 155316529067 25788 leveldbcpp343 persisting action 14 bytes to leveldb took 535803ns i0905 155316529088 25788 replicacpp676 persisted action at 0 i0905 155316529355 25784 replicacpp655 replica received learned notice for position 0 i0905 155316529784 25784 leveldbcpp343 persisting action 16 bytes to leveldb took 406036ns i0905 155316529806 25784 replicacpp676 persisted action at 0 i0905 155316529817 25784 replicacpp661 replica learned nop action at position 0 i0905 155316530108 25783 logcpp672 writer started with ending position 0 i0905 155316530597 25792 leveldbcpp438 reading position from leveldb took 14594ns i0905 155316532060 25787 registrarcpp346 successfully fetched the registry 0b i0905 155316532091 25787 registrarcpp422 attempting to update the registry i0905 155316533537 25785 logcpp680 attempting to append 140 bytes to the log i0905 155316533596 25785 coordinatorcpp340 coordinator attempting to write append action at position 1 i0905 155316533998 25798 replicacpp508 replica received write request for position 1 i0905 155316534397 25798 leveldbcpp343 persisting action 159 bytes to leveldb took 372452ns i0905 155316534416 25798 replicacpp676 persisted action at 1 i0905 155316534808 25793 replicacpp655 replica received learned notice for position 1 i0905 155316534996 25793 leveldbcpp343 persisting action 161 bytes to leveldb took 164609ns i0905 155316535014 25793 replicacpp676 persisted action at 1 i0905 155316535025 25793 replicacpp661 replica learned append action at position 1 i0905 155316535368 25784 registrarcpp479 successfully updated registry i0905 155316535419 25784 registrarcpp372 successfully recovered registrar i0905 155316535452 25785 logcpp699 attempting to truncate the log to 1 i0905 155316535555 25791 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0905 155316535553 25792 mastercpp1063 recovered 0 slaves from the registry 102b  allowing 10mins for slaves to reregister i0905 155316536038 25784 replicacpp508 replica received write request for position 2 i0905 155316536166 25784 leveldbcpp343 persisting action 16 bytes to leveldb took 101619ns i0905 155316536185 25784 replicacpp676 persisted action at 2 i0905 155316536497 25791 replicacpp655 replica received learned notice for position 2 i0905 155316536633 25791 leveldbcpp343 persisting action 18 bytes to leveldb took 109281ns i0905 155316536664 25791 leveldbcpp401 deleting 1 keys from leveldb took 14164ns i0905 155316536677 25791 replicacpp676 persisted action at 2 i0905 155316536689 25791 replicacpp661 replica learned truncate action at position 2 i0905 155316548408 25769 schedcpp137 version 0210 i0905 155316548627 25792 schedcpp233 new master detected at master671958118649188 i0905 155316548653 25792 schedcpp283 authenticating with master master671958118649188 i0905 155316548857 25797 authenticateehpp128 creating new client sasl connection i0905 155316548950 25797 mastercpp3637 authenticating scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155316549041 25797 authenticatorhpp156 creating new server sasl connection i0905 155316549120 25797 authenticateehpp219 received sasl authentication mechanisms crammd5 i0905 155316549141 25797 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0905 155316549180 25797 authenticatorhpp262 received sasl authentication start i0905 155316549229 25797 authenticatorhpp384 authentication requires more steps i0905 155316549268 25797 authenticateehpp265 received sasl authentication step i0905 155316549351 25787 authenticatorhpp290 received sasl authentication step i0905 155316549378 25787 auxpropcpp81 request to lookup properties for user testprincipal realm penatesapacheorg server fqdn penatesapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0905 155316549391 25787 auxpropcpp153 looking up auxiliary property userpassword i0905 155316549403 25787 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0905 155316549415 25787 auxpropcpp81 request to lookup properties for user testprincipal realm penatesapacheorg server fqdn penatesapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0905 155316549424 25787 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0905 155316549432 25787 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0905 155316549448 25787 authenticatorhpp376 authentication success i0905 155316549489 25787 authenticateehpp305 authentication success i0905 155316549525 25787 mastercpp3677 successfully authenticated principal testprincipal at scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155316549669 25783 schedcpp357 successfully authenticated with master master671958118649188 i0905 155316549690 25783 schedcpp476 sending registration request to master671958118649188 i0905 155316549751 25787 mastercpp1324 received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155316549782 25787 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0905 155316551250 25791 schedcpp233 new master detected at master671958118649188 i0905 155316551273 25791 schedcpp283 authenticating with master master671958118649188 i0905 155316551357 25788 authenticateehpp128 creating new client sasl connection i0905 155316551456 25791 mastercpp3637 authenticating scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155316551553 25788 authenticatorhpp156 creating new server sasl connection i0905 155316551673 25786 authenticateehpp219 received sasl authentication mechanisms crammd5 i0905 155316551697 25786 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0905 155316551755 25792 authenticatorhpp262 received sasl authentication start i0905 155316551808 25792 authenticatorhpp384 authentication requires more steps i0905 155316551856 25792 authenticateehpp265 received sasl authentication step i0905 155316551920 25786 authenticatorhpp290 received sasl authentication step i0905 155316551949 25786 auxpropcpp81 request to lookup properties for user testprincipal realm penatesapacheorg server fqdn penatesapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0905 155316551966 25786 auxpropcpp153 looking up auxiliary property userpassword i0905 155316551985 25786 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0905 155316551997 25786 auxpropcpp81 request to lookup properties for user testprincipal realm penatesapacheorg server fqdn penatesapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0905 155316552006 25786 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0905 155316552014 25786 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0905 155316552031 25786 authenticatorhpp376 authentication success i0905 155316552081 25792 authenticateehpp305 authentication success i0905 155316552100 25786 mastercpp3677 successfully authenticated principal testprincipal at scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155316552249 25792 schedcpp357 successfully authenticated with master master671958118649188 i0905 155317402861 25793 hierarchical_allocator_processhpp697 no resources available to allocate i0905 155318874348 25792 schedcpp476 sending registration request to master671958118649188 i0905 155318874364 25793 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 1471501003secs i0905 155318874420 25792 schedcpp476 sending registration request to master671958118649188 i0905 155318874451 25793 mastercpp1324 received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155318874480 25793 mastercpp1284 authorizing framework principal testprincipal to receive offers for role  i0905 155318874565 25793 mastercpp1324 received registration request from scheduler334303706af54c7bbbd8f6a43269ecf5671958118649188 i0905 155318874588 25793 mastercpp1284 authorizing framework principal testprincipal to receive offers for role   failure mock function called more times than expected  returning default value function call authorize0x2b9ed7fe9350 40byte object 90ba b4d4 9e2b 0000 0000 0000 0000 0000 a0fa 06f4 9e2b 0000 8017 09f4 9e2b 0000 0000 0000 0300 0000 the mock function has no default action set and its return type has no default value set  aborted at 1409932398 unix time try date d 1409932398 if you are using gnu date  pc  0x2b9ed6233f79 unknown  sigabrt 0x95c000064a9 received by pid 25769 tid 0x2b9ed7fea700 from pid 25769 stack trace   0x2b9ed5fef340 unknown  0x2b9ed6233f79 unknown  0x2b9ed6237388 unknown  0x93a5ec testinginternalgoogletestfailurereporterreportfailure  0x7296c5 testinginternalfunctionmockerbaseuntypedperformdefaultaction  0x933094 testinginternaluntypedfunctionmockerbaseuntypedinvokewith  0x71fbde mesosinternaltestsmockauthorizerauthorize  0x2b9ed4038caf mesosinternalmastermastervalidate  0x2b9ed4039763 mesosinternalmastermasterregisterframework  0x2b9ed40a0c0f protobufprocesshandler1  0x2b9ed4050c57 std_function_handler_m_invoke  0x2b9ed407d202 protobufprocessvisit  0x2b9ed402af1a mesosinternalmastermaster_visit  0x2b9ed4037eb8 mesosinternalmastermastervisit  0x2b9ed44cb792 processprocessmanagerresume  0x2b9ed44cba9c processschedule  0x2b9ed5fe7182 start_thread  0x2b9ed62f830d unknown code,2
introduce unique_ptr  add unique_ptr to the configure check  document use of unique_ptr in style guide  use when possible use stdmove when necessary  move raw pointers to owned to establish ownership  deprecate owned in favour of unique_ptr,1
design persistent resources,13
provide an option to validate flag value in stoutflags currently we can provide the default value for a flag but cannot check if the flag is set to a reasonable value and eg issue a warning passing an optional lambda checker to flagbaseadd can be a possible solution,3
allocatortest0frameworkexited is flaky noformattitle  run  allocatortest0frameworkexited using temporary directory tmpallocatortest_0_frameworkexited_b6wzng i0909 080235116555 18112 leveldbcpp176 opened db in 3164686ms i0909 080235126065 18112 leveldbcpp183 compacted db in 9449823ms i0909 080235126118 18112 leveldbcpp198 created db iterator in 5858ns i0909 080235126137 18112 leveldbcpp204 seeked to beginning of db in 1136ns i0909 080235126150 18112 leveldbcpp273 iterated through 0 keys in the db in 560ns i0909 080235126178 18112 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0909 080235126502 18133 recovercpp425 starting replica recovery i0909 080235126601 18133 recovercpp451 replica is in empty status i0909 080235127012 18133 replicacpp638 replica in empty status received a broadcasted recover request i0909 080235127094 18133 recovercpp188 received a recover response from a replica in empty status i0909 080235127223 18133 recovercpp542 updating replica status to starting i0909 080235226631 18133 leveldbcpp306 persisting metadata 8 bytes to leveldb took 99308134ms i0909 080235226690 18133 replicacpp320 persisted replica status to starting i0909 080235226812 18131 recovercpp451 replica is in starting status i0909 080235227246 18131 replicacpp638 replica in starting status received a broadcasted recover request i0909 080235227308 18131 recovercpp188 received a recover response from a replica in starting status i0909 080235227409 18131 recovercpp542 updating replica status to voting i0909 080235228540 18129 mastercpp286 master 20140909080235168428794400518112 precise started on 12701144005 i0909 080235228593 18129 mastercpp332 master only allowing authenticated frameworks to register i0909 080235228607 18129 mastercpp337 master only allowing authenticated slaves to register i0909 080235228620 18129 credentialshpp36 loading credentials for authentication from tmpallocatortest_0_frameworkexited_b6wzngcredentials i0909 080235228754 18129 mastercpp366 authorization enabled i0909 080235229560 18129 mastercpp120 no whitelist given advertising offers for all slaves i0909 080235229933 18129 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master12701144005 i0909 080235230057 18127 mastercpp1212 the newly elected leader is master12701144005 with id 20140909080235168428794400518112 i0909 080235230129 18127 mastercpp1225 elected as the leading master i0909 080235230144 18127 mastercpp1043 recovering from registrar i0909 080235230257 18127 registrarcpp313 recovering registrar i0909 080235232461 18131 leveldbcpp306 persisting metadata 8 bytes to leveldb took 4999384ms i0909 080235232489 18131 replicacpp320 persisted replica status to voting i0909 080235232544 18131 recovercpp556 successfully joined the paxos group i0909 080235232611 18131 recovercpp440 recover process terminated i0909 080235232727 18131 logcpp656 attempting to start the writer i0909 080235233012 18131 replicacpp474 replica received implicit promise request with proposal 1 i0909 080235238785 18131 leveldbcpp306 persisting metadata 8 bytes to leveldb took 5749504ms i0909 080235238818 18131 replicacpp342 persisted promised to 1 i0909 080235244056 18131 coordinatorcpp230 coordinator attemping to fill missing position i0909 080235244580 18131 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0909 080235250143 18131 leveldbcpp343 persisting action 8 bytes to leveldb took 5382351ms i0909 080235250319 18131 replicacpp676 persisted action at 0 i0909 080235250901 18131 replicacpp508 replica received write request for position 0 i0909 080235251137 18131 leveldbcpp438 reading position from leveldb took 18689ns i0909 080235256597 18131 leveldbcpp343 persisting action 14 bytes to leveldb took 5274169ms i0909 080235256764 18131 replicacpp676 persisted action at 0 i0909 080235263712 18126 replicacpp655 replica received learned notice for position 0 i0909 080235269613 18126 leveldbcpp343 persisting action 16 bytes to leveldb took 5417225ms i0909 080235351641 18126 replicacpp676 persisted action at 0 i0909 080235351655 18126 replicacpp661 replica learned nop action at position 0 i0909 080235351889 18126 logcpp672 writer started with ending position 0 i0909 080235352165 18126 leveldbcpp438 reading position from leveldb took 25215ns i0909 080235353163 18126 registrarcpp346 successfully fetched the registry 0b i0909 080235353185 18126 registrarcpp422 attempting to update the registry i0909 080235354152 18126 logcpp680 attempting to append 120 bytes to the log i0909 080235354195 18126 coordinatorcpp340 coordinator attempting to write append action at position 1 i0909 080235354416 18126 replicacpp508 replica received write request for position 1 i0909 080235351579 18127 hierarchical_allocator_processhpp697 no resources available to allocate i0909 080235354558 18127 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 2984795ms i0909 080235360254 18126 leveldbcpp343 persisting action 137 bytes to leveldb took 5811986ms i0909 080235360285 18126 replicacpp676 persisted action at 1 i0909 080235364126 18132 replicacpp655 replica received learned notice for position 1 i0909 080235369856 18132 leveldbcpp343 persisting action 139 bytes to leveldb took 5702756ms i0909 080235369899 18132 replicacpp676 persisted action at 1 i0909 080235369910 18132 replicacpp661 replica learned append action at position 1 i0909 080235370209 18132 registrarcpp479 successfully updated registry i0909 080235370311 18132 registrarcpp372 successfully recovered registrar i0909 080235370477 18132 logcpp699 attempting to truncate the log to 1 i0909 080235370553 18132 mastercpp1070 recovered 0 slaves from the registry 84b  allowing 10mins for slaves to reregister i0909 080235370594 18132 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0909 080235371201 18127 replicacpp508 replica received write request for position 2 i0909 080235376760 18127 leveldbcpp343 persisting action 16 bytes to leveldb took 5264501ms i0909 080235377105 18127 replicacpp676 persisted action at 2 i0909 080235377770 18127 replicacpp655 replica received learned notice for position 2 i0909 080235383363 18127 leveldbcpp343 persisting action 18 bytes to leveldb took 5272769ms i0909 080235383818 18127 leveldbcpp401 deleting 1 keys from leveldb took 28148ns i0909 080235384137 18127 replicacpp676 persisted action at 2 i0909 080235384399 18127 replicacpp661 replica learned truncate action at position 2 i0909 080235396512 18127 slavecpp167 slave started on 6412701144005 i0909 080235654770 18131 hierarchical_allocator_processhpp697 no resources available to allocate i0909 080235654847 18131 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 104933ns i0909 080235654974 18127 credentialshpp84 loading credential for authentication from tmpallocatortest_0_frameworkexited_xv9mk4credential i0909 080235655097 18127 slavecpp274 slave using credential for testprincipal i0909 080235655203 18127 slavecpp287 slave resources cpus3 mem1024 disk25116 ports3100032000 i0909 080235655274 18127 slavecpp315 slave hostname precise i0909 080235655285 18127 slavecpp316 slave checkpoint false i0909 080235655804 18127 statecpp33 recovering state from tmpallocatortest_0_frameworkexited_xv9mk4meta i0909 080235655913 18127 status_update_managercpp193 recovering status update manager i0909 080235656005 18127 slavecpp3202 finished recovery i0909 080235656251 18127 slavecpp598 new master detected at master12701144005 i0909 080235656285 18127 slavecpp672 authenticating with master master12701144005 i0909 080235656325 18127 slavecpp645 detecting new master i0909 080235656358 18127 status_update_managercpp167 new master detected at master12701144005 i0909 080235656389 18127 authenticateehpp128 creating new client sasl connection i0909 080235656563 18127 mastercpp3653 authenticating slave6412701144005 i0909 080235656651 18127 authenticatorhpp156 creating new server sasl connection i0909 080235656770 18127 authenticateehpp219 received sasl authentication mechanisms crammd5 i0909 080235656796 18127 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0909 080235656822 18127 authenticatorhpp262 received sasl authentication start i0909 080235656858 18127 authenticatorhpp384 authentication requires more steps i0909 080235656883 18127 authenticateehpp265 received sasl authentication step i0909 080235656924 18127 authenticatorhpp290 received sasl authentication step i0909 080235656960 18127 auxpropcpp81 request to lookup properties for user testprincipal realm precise server fqdn precise sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0909 080235656971 18127 auxpropcpp153 looking up auxiliary property userpassword i0909 080235656982 18127 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0909 080235656997 18127 auxpropcpp81 request to lookup properties for user testprincipal realm precise server fqdn precise sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0909 080235657004 18127 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0909 080235657008 18127 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0909 080235657019 18127 authenticatorhpp376 authentication success i0909 080235657047 18127 authenticateehpp305 authentication success i0909 080235657073 18127 mastercpp3693 successfully authenticated principal testprincipal at slave6412701144005 i0909 080235657145 18127 slavecpp729 successfully authenticated with master master12701144005 i0909 080235657183 18127 slavecpp980 will retry registration in 19238717ms if necessary i0909 080235657276 18128 mastercpp2843 registering slave at slave6412701144005 precise with id 201409090802351684287944005181120 i0909 080235657389 18128 registrarcpp422 attempting to update the registry i0909 080235658382 18130 logcpp680 attempting to append 295 bytes to the log i0909 080235658432 18130 coordinatorcpp340 coordinator attempting to write append action at position 3 i0909 080235658635 18130 replicacpp508 replica received write request for position 3 i0909 080235660959 18112 schedcpp137 version 0210 i0909 080235661093 18126 schedcpp233 new master detected at master12701144005 i0909 080235661111 18126 schedcpp283 authenticating with master master12701144005 i0909 080235661175 18126 authenticateehpp128 creating new client sasl connection i0909 080235661306 18126 mastercpp3653 authenticating schedulerfd92991870574fef923aed9d6fd355be12701144005 i0909 080235661376 18126 authenticatorhpp156 creating new server sasl connection i0909 080235661466 18126 authenticateehpp219 received sasl authentication mechanisms crammd5 i0909 080235661483 18126 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0909 080235661504 18126 authenticatorhpp262 received sasl authentication start i0909 080235661530 18126 authenticatorhpp384 authentication requires more steps i0909 080235661552 18126 authenticateehpp265 received sasl authentication step i0909 080235661579 18126 authenticatorhpp290 received sasl authentication step i0909 080235661592 18126 auxpropcpp81 request to lookup properties for user testprincipal realm precise server fqdn precise sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0909 080235661598 18126 auxpropcpp153 looking up auxiliary property userpassword i0909 080235661607 18126 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0909 080235661613 18126 auxpropcpp81 request to lookup properties for user testprincipal realm precise server fqdn precise sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0909 080235661619 18126 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0909 080235661623 18126 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0909 080235661633 18126 authenticatorhpp376 authentication success i0909 080235661653 18126 authenticateehpp305 authentication success i0909 080235661672 18126 mastercpp3693 successfully authenticated principal testprincipal at schedulerfd92991870574fef923aed9d6fd355be12701144005 i0909 080235661730 18126 schedcpp357 successfully authenticated with master master12701144005 i0909 080235661741 18126 schedcpp476 sending registration request to master12701144005 i0909 080235661782 18126 mastercpp1331 received registration request from schedulerfd92991870574fef923aed9d6fd355be12701144005 i0909 080235661798 18126 mastercpp1291 authorizing framework principal testprincipal to receive offers for role  i0909 080235661917 18126 mastercpp1390 registering framework 201409090802351684287944005181120000 at schedulerfd92991870574fef923aed9d6fd355be12701144005 i0909 080235662017 18126 schedcpp407 framework registered with 201409090802351684287944005181120000 i0909 080235662039 18126 schedcpp421 schedulerregistered took 9070ns i0909 080235662119 18126 hierarchical_allocator_processhpp329 added framework 201409090802351684287944005181120000 i0909 080235662130 18126 hierarchical_allocator_processhpp697 no resources available to allocate i0909 080235662135 18126 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 5558ns i0909 080235672230 18130 leveldbcpp343 persisting action 314 bytes to leveldb took 13567526ms i0909 080235672268 18130 replicacpp676 persisted action at 3 i0909 080235672483 18130 replicacpp655 replica received learned notice for position 3 i0909 080235677322 18132 slavecpp980 will retry registration in 14890338ms if necessary i0909 080235677399 18132 mastercpp2831 ignoring register slave message from slave6412701144005 precise as admission is already in progress i0909 080235680881 18130 leveldbcpp343 persisting action 316 bytes to leveldb took 8376798ms i0909 080235680908 18130 replicacpp676 persisted action at 3 i0909 080235680917 18130 replicacpp661 replica learned append action at position 3 i0909 080235681252 18130 registrarcpp479 successfully updated registry i0909 080235681330 18130 logcpp699 attempting to truncate the log to 3 i0909 080235681385 18130 mastercpp2883 registered slave 201409090802351684287944005181120 at slave6412701144005 precise i0909 080235681399 18130 mastercpp4126 adding slave 201409090802351684287944005181120 at slave6412701144005 precise with cpus3 mem1024 disk25116 ports3100032000 i0909 080235681504 18130 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0909 080235681570 18130 slavecpp763 registered with master master12701144005 given slave id 201409090802351684287944005181120 i0909 080235681689 18130 slavecpp2329 received ping from slaveobserver5012701144005 i0909 080235681753 18130 hierarchical_allocator_processhpp442 added slave 201409090802351684287944005181120 precise with cpus3 mem1024 disk25116 ports3100032000 and cpus3 mem1024 disk25116 ports3100032000 available i0909 080235681808 18130 hierarchical_allocator_processhpp734 offering cpus3 mem1024 disk25116 ports3100032000 on slave 201409090802351684287944005181120 to framework 201409090802351684287944005181120000 i0909 080235681892 18130 hierarchical_allocator_processhpp679 performed allocation for slave 201409090802351684287944005181120 in 109580ns i0909 080235681968 18130 masterhpp861 adding offer 201409090802351684287944005181120 with resources cpus3 mem1024 disk25116 ports3100032000 on slave 201409090802351684287944005181120 precise i0909 080235682014 18130 mastercpp3600 sending 1 offers to framework 201409090802351684287944005181120000 i0909 080235682443 18130 schedcpp544 schedulerresourceoffers took 254258ns i0909 080235682633 18130 masterhpp871 removing offer 201409090802351684287944005181120 with resources cpus3 mem1024 disk25116 ports3100032000 on slave 201409090802351684287944005181120 precise i0909 080235682684 18130 mastercpp2201 processing reply for offers  201409090802351684287944005181120  on slave 201409090802351684287944005181120 at slave6412701144005 precise for framework 201409090802351684287944005181120000 i0909 080235682708 18130 mastercpp2284 authorizing framework principal testprincipal to launch task 0 as user jenkins i0909 080235682971 18130 replicacpp508 replica received write request for position 4 i0909 080235683132 18132 masterhpp833 adding task 0 with resources cpus2 mem512 on slave 201409090802351684287944005181120 precise i0909 080235683159 18132 mastercpp2350 launching task 0 of framework 201409090802351684287944005181120000 with resources cpus2 mem512 on slave 201409090802351684287944005181120 at slave6412701144005 precise i0909 080235683363 18132 slavecpp1011 got assigned task 0 for framework 201409090802351684287944005181120000 i0909 080235683580 18132 slavecpp1121 launching task 0 for framework 201409090802351684287944005181120000 i0909 080235684833 18133 hierarchical_allocator_processhpp563 recovered cpus1 mem512 disk25116 ports3100032000 total allocatable cpus1 mem512 disk25116 ports3100032000 on slave 201409090802351684287944005181120 from framework 201409090802351684287944005181120000 i0909 080235684864 18133 hierarchical_allocator_processhpp599 framework 201409090802351684287944005181120000 filtered slave 201409090802351684287944005181120 for 5secs i0909 080235686401 18132 execcpp132 version 0210 i0909 080235686848 18128 execcpp182 executor started at executor812701144005 with pid 18112 i0909 080235687095 18132 slavecpp1231 queuing task 0 for executor executor1 of framework 201409090802351684287944005181120000 i0909 080235687302 18132 slavecpp552 successfully attached file tmpallocatortest_0_frameworkexited_xv9mk4slaves201409090802351684287944005181120frameworks201409090802351684287944005181120000executorsexecutor1runsc4458e43,1
design the semantics for updating frameworkinfo currently there is no easy way for frameworks to update their frameworkinfo resulting in issues like mesos703 and mesos1218 this ticket captures the design for doing frameworkinfo update without having to roll mastersslavestasksexecutors,3
add chown option to commandinfouri mesos fetcher always chowns the extracted executor uris as the executor user but sometimes this is not desirable eg setuid bit gets lost during chown if slavefetcher is running as root it would be nice to give frameworks the ability to skip the chown,2
reconciliation can send outoforder updates when a slave reregisters with the master it currently sends the latest task state for all tasks that are not both terminal and acknowledged however reconciliation assumes that we always have the latest unacknowledged state of the task represented in the master as a result outoforder updates are possible eg 1 slave has task t in task_finished with unacknowledged updates task_running task_finished 2 master fails over 3 new master reregisters the slave with t in task_finished 4 reconciliation request arrives master sends task_finished 5 slave sends task_running to master master sends task_running i think the fix here is to preserve the task state invariants in the master namely that the master has the latest unacknowledged state of the task this means when the slave reregisters it should instead send the latest acknowledged state of each task,3
disallow executors with cpu only or memory only resources currently master allows executors to be launched with either only cpus or only memory but we shouldnt allow that this is because executor is an actual unix process that is launched by the slave if an executor doesnt specify cpus what should do the cpu limits be for that executor when there are no tasks running on it if no cpu limits are set then it might starve other executorstasks on the slave violating isolation guarantees same goes with memory moreover the current containerizerisolator code will throw failures when using such an executor eg when the last task on the executor finishes and containerizerupdate is called with 0 cpus or 0 mem,3
expose rtt in container stats as we expose the bandwidth so we should expose the rtt as a measure of latency each container is experiencing we can use ss to get the persocket statistics and filter and aggregate accordingly to get a measure of rtt,3
reconcile disconnecteddeactivated semantics in the master code currently the master code treats a deactivated and disconnected slave similarly by setting disconnected variable in the slave struct this causes us to disconnect a slave in cases where we really only want to deactivate the slave eg authentication it would be nice to differentiate these semantics by adding a new variable active in the slave struct we might want to do the same with the framework struct for consistency,3
fail fast in example frameworks if task goes into unexpected state most of the example frameworks launch a bunch of tasks and exit if all of them reach finished state but if there is a bug in the code resulting in task_lost the framework waits forever instead the framework should abort if an unexpected task state is encountered,1
task attempted to use more offers than requested in example jave and python frameworks code  run  examplestestjavaframework using temporary directory tmpexamplestest_javaframework_2pcfch enabling authentication for the framework warning logging before initgooglelogging is written to stderr i0917 231435199069 31510 processcpp1771 libprocess is initialized on 12701134609 for 8 cpus i0917 231435199794 31510 loggingcpp177 logging to stderr i0917 231435225342 31510 leveldbcpp176 opened db in 22197149ms i0917 231435231133 31510 leveldbcpp183 compacted db in 5601897ms i0917 231435231498 31510 leveldbcpp198 created db iterator in 215441ns i0917 231435231608 31510 leveldbcpp204 seeked to beginning of db in 11488ns i0917 231435231722 31510 leveldbcpp273 iterated through 0 keys in the db in 14016ns i0917 231435231917 31510 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0917 231435233129 31526 recovercpp425 starting replica recovery i0917 231435233614 31526 recovercpp451 replica is in empty status i0917 231435234994 31526 replicacpp638 replica in empty status received a broadcasted recover request i0917 231435240116 31519 recovercpp188 received a recover response from a replica in empty status i0917 231435240782 31519 recovercpp542 updating replica status to starting i0917 231435242846 31524 mastercpp286 master 20140917231435168428793460931503 saucy started on 12701134609 i0917 231435243191 31524 mastercpp332 master only allowing authenticated frameworks to register i0917 231435243288 31524 mastercpp339 master allowing unauthenticated slaves to register i0917 231435243399 31524 credentialshpp36 loading credentials for authentication from tmpexamplestest_javaframework_2pcfchcredentials w0917 231435243588 31524 credentialshpp51 permissions on credentials file tmpexamplestest_javaframework_2pcfchcredentials are too open it is recommended that your credentials file is not accessible by others i0917 231435243846 31524 mastercpp366 authorization enabled i0917 231435244882 31520 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master12701134609 i0917 231435245224 31520 mastercpp120 no whitelist given advertising offers for all slaves i0917 231435246934 31524 mastercpp1211 the newly elected leader is master12701134609 with id 20140917231435168428793460931503 i0917 231435247234 31524 mastercpp1224 elected as the leading master i0917 231435247336 31524 mastercpp1042 recovering from registrar i0917 231435247542 31526 registrarcpp313 recovering registrar i0917 231435250555 31510 containerizercpp89 using isolation posixcpuposixmem i0917 231435252326 31510 containerizercpp89 using isolation posixcpuposixmem i0917 231435252821 31520 slavecpp169 slave started on 112701134609 i0917 231435253552 31520 slavecpp289 slave resources cpus1 mem1001 disk24988 ports3100032000 i0917 231435253906 31520 slavecpp317 slave hostname saucy i0917 231435254004 31520 slavecpp318 slave checkpoint true i0917 231435254818 31520 statecpp33 recovering state from tmpmesosw8snrw0meta i0917 231435255106 31519 leveldbcpp306 persisting metadata 8 bytes to leveldb took 1399622ms i0917 231435255235 31519 replicacpp320 persisted replica status to starting i0917 231435255419 31519 recovercpp451 replica is in starting status i0917 231435255834 31519 replicacpp638 replica in starting status received a broadcasted recover request i0917 231435256000 31519 recovercpp188 received a recover response from a replica in starting status i0917 231435256217 31519 recovercpp542 updating replica status to voting i0917 231435256641 31520 status_update_managercpp193 recovering status update manager i0917 231435257064 31520 containerizercpp252 recovering containerizer i0917 231435257725 31520 slavecpp3220 finished recovery i0917 231435258463 31520 slavecpp600 new master detected at master12701134609 i0917 231435258769 31524 status_update_managercpp167 new master detected at master12701134609 i0917 231435258885 31520 slavecpp636 no credentials provided attempting to register without authentication i0917 231435259024 31520 slavecpp647 detecting new master i0917 231435259863 31520 slavecpp169 slave started on 212701134609 i0917 231435260288 31520 slavecpp289 slave resources cpus1 mem1001 disk24988 ports3100032000 i0917 231435260493 31520 slavecpp317 slave hostname saucy i0917 231435260588 31520 slavecpp318 slave checkpoint true i0917 231435265127 31510 containerizercpp89 using isolation posixcpuposixmem i0917 231435265877 31519 leveldbcpp306 persisting metadata 8 bytes to leveldb took 9536278ms i0917 231435265983 31519 replicacpp320 persisted replica status to voting i0917 231435266324 31519 recovercpp556 successfully joined the paxos group i0917 231435266511 31519 recovercpp440 recover process terminated i0917 231435266978 31519 logcpp656 attempting to start the writer i0917 231435268165 31523 replicacpp474 replica received implicit promise request with proposal 1 i0917 231435269850 31525 slavecpp169 slave started on 312701134609 i0917 231435270365 31525 slavecpp289 slave resources cpus1 mem1001 disk24988 ports3100032000 i0917 231435270658 31525 slavecpp317 slave hostname saucy i0917 231435270781 31525 slavecpp318 slave checkpoint true i0917 231435271332 31525 statecpp33 recovering state from tmpmesosw8snrw2meta i0917 231435271580 31522 status_update_managercpp193 recovering status update manager i0917 231435271838 31522 containerizercpp252 recovering containerizer i0917 231435272238 31525 slavecpp3220 finished recovery i0917 231435273002 31525 slavecpp600 new master detected at master12701134609 i0917 231435273252 31521 status_update_managercpp167 new master detected at master12701134609 i0917 231435273360 31525 slavecpp636 no credentials provided attempting to register without authentication i0917 231435273507 31525 slavecpp647 detecting new master i0917 231435275413 31525 statecpp33 recovering state from tmpmesosw8snrw1meta i0917 231435278506 31523 leveldbcpp306 persisting metadata 8 bytes to leveldb took 10232514ms i0917 231435278712 31523 replicacpp342 persisted promised to 1 i0917 231435279585 31523 coordinatorcpp230 coordinator attemping to fill missing position i0917 231435280400 31523 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0917 231435280900 31526 status_update_managercpp193 recovering status update manager i0917 231435281282 31519 containerizercpp252 recovering containerizer i0917 231435281615 31520 slavecpp3220 finished recovery i0917 231435281891 31510 schedcpp137 version 0210 i0917 231435282306 31526 schedcpp233 new master detected at master12701134609 i0917 231435282464 31526 schedcpp283 authenticating with master master12701134609 i0917 231435282891 31526 authenticateehpp104 initializing client sasl i0917 231435284816 31526 authenticateehpp128 creating new client sasl connection i0917 231435285428 31519 mastercpp873 dropping mesosinternalauthenticatemessage message since not recovered yet i0917 231435288007 31521 slavecpp600 new master detected at master12701134609 i0917 231435288399 31521 slavecpp636 no credentials provided attempting to register without authentication i0917 231435288535 31521 slavecpp647 detecting new master i0917 231435288501 31519 status_update_managercpp167 new master detected at master12701134609 i0917 231435289625 31523 leveldbcpp343 persisting action 8 bytes to leveldb took 8997343ms i0917 231435289784 31523 replicacpp676 persisted action at 0 i0917 231435292667 31521 replicacpp508 replica received write request for position 0 i0917 231435293112 31521 leveldbcpp438 reading position from leveldb took 325638ns i0917 231435301774 31521 leveldbcpp343 persisting action 14 bytes to leveldb took 8576338ms i0917 231435301916 31521 replicacpp676 persisted action at 0 i0917 231435302289 31521 replicacpp655 replica received learned notice for position 0 i0917 231435310542 31521 leveldbcpp343 persisting action 16 bytes to leveldb took 8087789ms i0917 231435310675 31521 replicacpp676 persisted action at 0 i0917 231435310946 31521 replicacpp661 replica learned nop action at position 0 i0917 231435311254 31521 logcpp672 writer started with ending position 0 i0917 231435311957 31521 leveldbcpp438 reading position from leveldb took 35110ns i0917 231435320283 31521 registrarcpp346 successfully fetched the registry 0b i0917 231435320513 31521 registrarcpp422 attempting to update the registry i0917 231435322226 31525 logcpp680 attempting to append 118 bytes to the log i0917 231435322549 31525 coordinatorcpp340 coordinator attempting to write append action at position 1 i0917 231435322931 31525 replicacpp508 replica received write request for position 1 i0917 231435330169 31525 leveldbcpp343 persisting action 135 bytes to leveldb took 7133053ms i0917 231435330340 31525 replicacpp676 persisted action at 1 i0917 231435330890 31525 replicacpp655 replica received learned notice for position 1 i0917 231435339218 31525 leveldbcpp343 persisting action 137 bytes to leveldb took 8192024ms i0917 231435339380 31525 replicacpp676 persisted action at 1 i0917 231435339715 31525 replicacpp661 replica learned append action at position 1 i0917 231435340615 31525 registrarcpp479 successfully updated registry i0917 231435340802 31525 registrarcpp372 successfully recovered registrar i0917 231435341104 31525 logcpp699 attempting to truncate the log to 1 i0917 231435341351 31525 mastercpp1069 recovered 0 slaves from the registry 82b  allowing 10mins for slaves to reregister i0917 231435341527 31525 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0917 231435341964 31525 replicacpp508 replica received write request for position 2 i0917 231435352336 31525 leveldbcpp343 persisting action 16 bytes to leveldb took 10213086ms i0917 231435352494 31525 replicacpp676 persisted action at 2 i0917 231435356258 31523 replicacpp655 replica received learned notice for position 2 i0917 231435364992 31523 leveldbcpp343 persisting action 18 bytes to leveldb took 8606522ms i0917 231435365166 31523 leveldbcpp401 deleting 1 keys from leveldb took 48378ns i0917 231435365404 31523 replicacpp676 persisted action at 2 i0917 231435365537 31523 replicacpp661 replica learned truncate action at position 2 i0917 231435568366 31523 slavecpp994 will retry registration in 423208575ms if necessary i0917 231435568840 31522 mastercpp2870 registering slave at slave312701134609 saucy with id 201409172314351684287934609315030 i0917 231435569422 31522 registrarcpp422 attempting to update the registry i0917 231435572013 31522 logcpp680 attempting to append 289 bytes to the log i0917 231435572273 31519 coordinatorcpp340 coordinator attempting to write append action at position 3 i0917 231435572816 31519 replicacpp508 replica received write request for position 3 i0917 231435579784 31519 leveldbcpp343 persisting action 308 bytes to leveldb took 6809365ms i0917 231435579907 31519 replicacpp676 persisted action at 3 i0917 231435580512 31519 replicacpp655 replica received learned notice for position 3 i0917 231435588748 31519 leveldbcpp343 persisting action 310 bytes to leveldb took 8112519ms i0917 231435588888 31519 replicacpp676 persisted action at 3 i0917 231435588985 31519 replicacpp661 replica learned append action at position 3 i0917 231435589754 31519 registrarcpp479 successfully updated registry i0917 231435590070 31519 mastercpp2910 registered slave 201409172314351684287934609315030 at slave312701134609 saucy i0917 231435590255 31519 mastercpp4118 adding slave 201409172314351684287934609315030 at slave312701134609 saucy with cpus1 mem1001 disk24988 ports3100032000 i0917 231435590831 31519 slavecpp765 registered with master master12701134609 given slave id 201409172314351684287934609315030 i0917 231435589913 31523 logcpp699 attempting to truncate the log to 3 i0917 231435591414 31523 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0917 231435591815 31523 replicacpp508 replica received write request for position 4 i0917 231435591117 31521 hierarchical_allocator_processhpp442 added slave 201409172314351684287934609315030 saucy with cpus1 mem1001 disk24988 ports3100032000 and cpus1 mem1001 disk24988 ports3100032000 available i0917 231435592293 31521 hierarchical_allocator_processhpp679 performed allocation for slave 201409172314351684287934609315030 in 64364ns i0917 231435592953 31519 slavecpp778 checkpointing slaveinfo to tmpmesosw8snrw2metaslaves201409172314351684287934609315030slaveinfo i0917 231435593475 31519 slavecpp2347 received ping from slaveobserver112701134609 i0917 231435601356 31523 leveldbcpp343 persisting action 16 bytes to leveldb took 9420461ms i0917 231435601539 31523 replicacpp676 persisted action at 4 i0917 231435602325 31523 replicacpp655 replica received learned notice for position 4 i0917 231435610779 31523 leveldbcpp343 persisting action 18 bytes to leveldb took 834398ms i0917 231435611114 31523 leveldbcpp401 deleting 2 keys from leveldb took 66521ns i0917 231435611554 31523 replicacpp676 persisted action at 4 i0917 231435611690 31523 replicacpp661 replica learned truncate action at position 4 i0917 231436033941 31523 slavecpp994 will retry registration in 322705631ms if necessary i0917 231436034276 31521 mastercpp2870 registering slave at slave112701134609 saucy with id 201409172314351684287934609315031 i0917 231436034536 31521 registrarcpp422 attempting to update the registry i0917 231436035889 31521 logcpp680 attempting to append 454 bytes to the log i0917 231436036099 31524 coordinatorcpp340 coordinator attempting to write append action at position 5 i0917 231436036416 31524 replicacpp508 replica received write request for position 5 i0917 231436046672 31524 leveldbcpp343 persisting action 473 bytes to leveldb took 10160627ms i0917 231436047035 31524 replicacpp676 persisted action at 5 i0917 231436047613 31524 replicacpp655 replica received learned notice for position 5 i0917 231436053006 31524 leveldbcpp343 persisting action 475 bytes to leveldb took 5180742ms i0917 231436053246 31524 replicacpp676 persisted action at 5 i0917 231436053678 31524 replicacpp661 replica learned append action at position 5 i0917 231436060384 31524 registrarcpp479 successfully updated registry i0917 231436061328 31524 mastercpp2910 registered slave 201409172314351684287934609315031 at slave112701134609 saucy i0917 231436061537 31524 mastercpp4118 adding slave 201409172314351684287934609315031 at slave112701134609 saucy with cpus1 mem1001 disk24988 ports3100032000 i0917 231436061982 31524 slavecpp765 registered with master master12701134609 given slave id 201409172314351684287934609315031 i0917 231436062891 31524 slavecpp778 checkpointing slaveinfo to tmpmesosw8snrw0metaslaves201409172314351684287934609315031slaveinfo i0917 231436061050 31525 logcpp699 attempting to truncate the log to 5 i0917 231436063244 31525 coordinatorcpp340 coordinator attempting to write truncate action at position 6 i0917 231436063746 31525 replicacpp508 replica received write request for position 6 i0917 231436062386 31520 hierarchical_allocator_processhpp442 added slave 201409172314351684287934609315031 saucy with cpus1 mem1001 disk24988 ports3100032000 and cpus1 mem1001 disk24988 ports3100032000 available i0917 231436064352 31520 hierarchical_allocator_processhpp679 performed allocation for slave 201409172314351684287934609315031 in 35730ns i0917 231436065166 31524 slavecpp2347 received ping from slaveobserver212701134609 i0917 231436070137 31525 leveldbcpp343 persisting action 16 bytes to leveldb took 6242192ms i0917 231436070355 31525 replicacpp676 persisted action at 6 i0917 231436071005 31525 replicacpp655 replica received learned notice for position 6 i0917 231436076560 31525 leveldbcpp343 persisting action 18 bytes to leveldb took 5368532ms i0917 231436077137 31525 leveldbcpp401 deleting 2 keys from leveldb took 371245ns i0917 231436077241 31525 replicacpp676 persisted action at 6 i0917 231436077345 31525 replicacpp661 replica learned truncate action at position 6 i0917 231436141270 31522 slavecpp994 will retry registration in 1857205901secs if necessary i0917 231436141644 31522 mastercpp2870 registering slave at slave212701134609 saucy with id 201409172314351684287934609315032 i0917 231436141930 31522 registrarcpp422 attempting to update the registry i0917 231436143316 31521 logcpp680 attempting to append 619 bytes to the log i0917 231436143646 31521 coordinatorcpp340 coordinator attempting to write append action at position 7 i0917 231436143954 31521 replicacpp508 replica received write request for position 7 i0917 231436148875 31521 leveldbcpp343 persisting action 638 bytes to leveldb took 4787834ms i0917 231436149085 31521 replicacpp676 persisted action at 7 i0917 231436149673 31521 replicacpp655 replica received learned notice for position 7 i0917 231436155232 31521 leveldbcpp343 persisting action 640 bytes to leveldb took 5472209ms i0917 231436155522 31521 replicacpp676 persisted action at 7 i0917 231436155936 31521 replicacpp661 replica learned append action at position 7 i0917 231436156481 31521 registrarcpp479 successfully updated registry i0917 231436156663 31526 logcpp699 attempting to truncate the log to 7 i0917 231436156813 31526 coordinatorcpp340 coordinator attempting to write truncate action at position 8 i0917 231436157155 31526 replicacpp508 replica received write request for position 8 i0917 231436157510 31520 mastercpp2910 registered slave 201409172314351684287934609315032 at slave212701134609 saucy i0917 231436157645 31520 mastercpp4118 adding slave 201409172314351684287934609315032 at slave212701134609 saucy with cpus1 mem1001 disk24988 ports3100032000 i0917 231436157928 31520 slavecpp765 registered with master master12701134609 given slave id 201409172314351684287934609315032 i0917 231436158304 31520 slavecpp778 checkpointing slaveinfo to tmpmesosw8snrw1metaslaves201409172314351684287934609315032slaveinfo i0,2
create a guide to becoming a committer we have a committers guide but the process by which one becomes a committer is unclear we should set some guidelines and a process by which we can grow contributors into committers,3
completed tasks remains in task_running when framework is disconnected we have run into a problem that cause tasks which completes when a framework is disconnected and has a failover time to remain in a running state even though the tasks actually finishes this hogs the cluster and gives users a inconsistent view of the cluster state going to the slave the task is finished going to the master the task is still in a nonterminal state when the scheduler reattaches or the failover timeout expires the tasks finishes correctly the current workflow of this scheduler has a long failover timeout but may on the other hand never reattach here is a test framework we have been able to reproduce the issue with httpsgistgithubcomnqn9b9b1de9123a6e836f54 it launches many shortlived tasks 1 second sleep and when killing the framework instance the master reports the tasks as running even after several minutes httpcllyimage2r3719461e0tscreen20shot202014091020at203193920pmpng when clicking on one of the slaves where for example task 49 runs the slave knows that it completed httpcllyimage2p410l3m1o1nscreen20shot202014091020at203212920pmpng here is the log of a mesoslocal instance where i reproduced it httpsgistgithubcomnqnf7ee20601199d70787c0 here task 10 to 19 are stuck in running state there is a lot of output so here is a filtered log for task 10 httpsgistgithubcomnqna53e5ea05c5e41cd5a7d the problem turn out to be an issue with the ackcycle of status updates if the framework disconnects with a failover timeout set the status update manage on the slaves will keep trying to send the front of status update stream to the master which in turn forwards it to the framework if the first status update after the disconnect is terminal things work out fine the master pick the terminal state up removes the task and release the resources if on the other hand one nonterminal status is in the stream the master will never know that the task finished or failed before the framework reconnects during a discussion on the dev mailing list httpmailarchivesapacheorgmod_mboxmesosdev201409mbox3ccadkthhavr5mrq1s9hxw1bb_xfalxwwxjutp7mv4y3wpbhawgmailgmailcom3e we enumerated a couple of options to solve this problem first off having two ackcycles one between masters and slaves and one between masters and frameworks would be ideal we would be able to replay the statuses in order while keeping the master state current however this requires us to persist the master state in a replicated storage as a first pass we can make sure that the tasks caught in a running state doesnt hog the cluster when completed and the framework being disconnected here is a proofofconcept to work out of httpsgithubcomnqnmesostreeniklasstatusupdatedisconnect a new optional field have been added to the internal status update message httpsgithubcomnqnmesosblobniklasstatusupdatedisconnectsrcmessagesmessagesprotol68 which makes it possible for the status update manager to set the field if the latest status was terminal httpsgithubcomnqnmesosblobniklasstatusupdatedisconnectsrcslavestatus_update_managercppl501 i added a test which should highlight the issue as well httpsgithubcomnqnmesosblobniklasstatusupdatedisconnectsrctestsfault_tolerance_testscppl2478 i would love some input on the approach before moving on there are rough edges in the poc which of course should be addressed before bringing it for up review,2
expose master stats differentiating between mastergenerated and slavegenerated lost tasks the master exports a monotonicallyincreasing counter of tasks transitioned to task_lost this loses fidelity of the source of the lost task a first step in exposing the source of lost tasks might be to just differentiate between task_lost transitions initiated by the master vs the slave and maybe bad input from the scheduler,5
allocatortest0slavelost is flaky code  run  allocatortest0slavelost using temporary directory tmpallocatortest_0_slavelost_z2oazw i0929 165829484141 3486 leveldbcpp176 opened db in 604109ns i0929 165829484629 3486 leveldbcpp183 compacted db in 172697ns i0929 165829484912 3486 leveldbcpp198 created db iterator in 6429ns i0929 165829485133 3486 leveldbcpp204 seeked to beginning of db in 1618ns i0929 165829485337 3486 leveldbcpp273 iterated through 0 keys in the db in 752ns i0929 165829485595 3486 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0929 165829486017 3500 recovercpp425 starting replica recovery i0929 165829486304 3500 recovercpp451 replica is in empty status i0929 165829486793 3500 replicacpp638 replica in empty status received a broadcasted recover request i0929 165829487205 3500 recovercpp188 received a recover response from a replica in empty status i0929 165829487540 3500 recovercpp542 updating replica status to starting i0929 165829487911 3500 leveldbcpp306 persisting metadata 8 bytes to leveldb took 36629ns i0929 165829488173 3500 replicacpp320 persisted replica status to starting i0929 165829488438 3500 recovercpp451 replica is in starting status i0929 165829488891 3500 replicacpp638 replica in starting status received a broadcasted recover request i0929 165829489187 3500 recovercpp188 received a recover response from a replica in starting status i0929 165829489516 3500 recovercpp542 updating replica status to voting i0929 165829489887 3502 leveldbcpp306 persisting metadata 8 bytes to leveldb took 32099ns i0929 165829490124 3502 replicacpp320 persisted replica status to voting i0929 165829490381 3500 recovercpp556 successfully joined the paxos group i0929 165829490713 3500 recovercpp440 recover process terminated i0929 165829493401 3506 mastercpp312 master 201409291658292759502016556183486 fedora20 started on 19216812216455618 i0929 165829493700 3506 mastercpp358 master only allowing authenticated frameworks to register i0929 165829493921 3506 mastercpp363 master only allowing authenticated slaves to register i0929 165829494123 3506 credentialshpp36 loading credentials for authentication from tmpallocatortest_0_slavelost_z2oazwcredentials i0929 165829494500 3506 mastercpp392 authorization enabled i0929 165829495249 3506 mastercpp120 no whitelist given advertising offers for all slaves i0929 165829495728 3502 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master19216812216455618 i0929 165829496196 3506 mastercpp1241 the newly elected leader is master19216812216455618 with id 201409291658292759502016556183486 i0929 165829496469 3506 mastercpp1254 elected as the leading master i0929 165829496713 3506 mastercpp1072 recovering from registrar i0929 165829497020 3506 registrarcpp312 recovering registrar i0929 165829497486 3506 logcpp656 attempting to start the writer i0929 165829498105 3506 replicacpp474 replica received implicit promise request with proposal 1 i0929 165829498373 3506 leveldbcpp306 persisting metadata 8 bytes to leveldb took 27145ns i0929 165829498605 3506 replicacpp342 persisted promised to 1 i0929 165829500880 3500 coordinatorcpp230 coordinator attemping to fill missing position i0929 165829501404 3500 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i0929 165829501687 3500 leveldbcpp343 persisting action 8 bytes to leveldb took 57971ns i0929 165829501935 3500 replicacpp676 persisted action at 0 i0929 165829504905 3507 replicacpp508 replica received write request for position 0 i0929 165829505130 3507 leveldbcpp438 reading position from leveldb took 18418ns i0929 165829505377 3507 leveldbcpp343 persisting action 14 bytes to leveldb took 19998ns i0929 165829505571 3507 replicacpp676 persisted action at 0 i0929 165829505957 3507 replicacpp655 replica received learned notice for position 0 i0929 165829506186 3507 leveldbcpp343 persisting action 16 bytes to leveldb took 21648ns i0929 165829506433 3507 replicacpp676 persisted action at 0 i0929 165829506767 3507 replicacpp661 replica learned nop action at position 0 i0929 165829507199 3507 logcpp672 writer started with ending position 0 i0929 165829507730 3507 leveldbcpp438 reading position from leveldb took 11532ns i0929 165829508915 3507 registrarcpp345 successfully fetched the registry 0b i0929 165829509230 3507 registrarcpp421 attempting to update the registry i0929 165829510516 3500 logcpp680 attempting to append 130 bytes to the log i0929 165829510949 3500 coordinatorcpp340 coordinator attempting to write append action at position 1 i0929 165829511363 3500 replicacpp508 replica received write request for position 1 i0929 165829511697 3500 leveldbcpp343 persisting action 149 bytes to leveldb took 66530ns i0929 165829512039 3500 replicacpp676 persisted action at 1 i0929 165829512460 3500 replicacpp655 replica received learned notice for position 1 i0929 165829512778 3500 leveldbcpp343 persisting action 151 bytes to leveldb took 24121ns i0929 165829513013 3500 replicacpp676 persisted action at 1 i0929 165829513239 3500 replicacpp661 replica learned append action at position 1 i0929 165829513674 3500 logcpp699 attempting to truncate the log to 1 i0929 165829513954 3500 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0929 165829514385 3500 replicacpp508 replica received write request for position 2 i0929 165829514680 3500 leveldbcpp343 persisting action 16 bytes to leveldb took 65014ns i0929 165829514991 3500 replicacpp676 persisted action at 2 i0929 165829516978 3501 replicacpp655 replica received learned notice for position 2 i0929 165829517319 3501 leveldbcpp343 persisting action 18 bytes to leveldb took 24103ns i0929 165829517546 3501 leveldbcpp401 deleting 1 keys from leveldb took 16533ns i0929 165829517801 3501 replicacpp676 persisted action at 2 i0929 165829518039 3501 replicacpp661 replica learned truncate action at position 2 i0929 165829518539 3507 registrarcpp478 successfully updated registry i0929 165829518885 3507 registrarcpp371 successfully recovered registrar i0929 165829519201 3507 mastercpp1099 recovered 0 slaves from the registry 94b  allowing 10mins for slaves to reregister i0929 165829533073 3505 slavecpp169 slave started on 5719216812216455618 i0929 165829533500 3505 credentialshpp84 loading credential for authentication from tmpallocatortest_0_slavelost_xdxhfgcredential i0929 165829533834 3505 slavecpp276 slave using credential for testprincipal i0929 165829534168 3505 slavecpp289 slave resources cpus2 mem1024 disk752 ports3100032000 i0929 165829534751 3505 slavecpp317 slave hostname fedora20 i0929 165829534965 3505 slavecpp318 slave checkpoint false i0929 165829535557 3505 statecpp33 recovering state from tmpallocatortest_0_slavelost_xdxhfgmeta i0929 165829535951 3505 status_update_managercpp193 recovering status update manager i0929 165829536290 3505 slavecpp3271 finished recovery i0929 165829536782 3505 slavecpp598 new master detected at master19216812216455618 i0929 165829537122 3505 slavecpp672 authenticating with master master19216812216455618 i0929 165829537492 3505 slavecpp645 detecting new master i0929 165829537294 3506 status_update_managercpp167 new master detected at master19216812216455618 i0929 165829537642 3507 authenticateehpp128 creating new client sasl connection i0929 165829538769 3502 mastercpp3737 authenticating slave5719216812216455618 i0929 165829539091 3502 authenticatorhpp156 creating new server sasl connection i0929 165829539710 3503 authenticateehpp219 received sasl authentication mechanisms crammd5 i0929 165829539943 3503 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0929 165829540206 3502 authenticatorhpp262 received sasl authentication start i0929 165829540457 3502 authenticatorhpp384 authentication requires more steps i0929 165829540757 3502 authenticateehpp265 received sasl authentication step i0929 165829541121 3502 authenticatorhpp290 received sasl authentication step i0929 165829541368 3502 auxpropcpp81 request to lookup properties for user testprincipal realm fedora20 server fqdn fedora20 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0929 165829541599 3502 auxpropcpp153 looking up auxiliary property userpassword i0929 165829541874 3502 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0929 165829542129 3502 auxpropcpp81 request to lookup properties for user testprincipal realm fedora20 server fqdn fedora20 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0929 165829542333 3502 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0929 165829542553 3502 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0929 165829542785 3502 authenticatorhpp376 authentication success i0929 165829543047 3502 authenticateehpp305 authentication success i0929 165829543381 3502 slavecpp729 successfully authenticated with master master19216812216455618 i0929 165829543707 3502 slavecpp992 will retry registration in 11795692ms if necessary i0929 165829543179 3503 mastercpp3777 successfully authenticated principal testprincipal at slave5719216812216455618 i0929 165829544255 3503 mastercpp2930 registering slave at slave5719216812216455618 fedora20 with id 2014092916582927595020165561834860 i0929 165829544587 3503 registrarcpp421 attempting to update the registry i0929 165829545816 3500 logcpp680 attempting to append 299 bytes to the log i0929 165829546267 3500 coordinatorcpp340 coordinator attempting to write append action at position 3 i0929 165829546749 3500 replicacpp508 replica received write request for position 3 i0929 165829547030 3500 leveldbcpp343 persisting action 318 bytes to leveldb took 31759ns i0929 165829547236 3500 replicacpp676 persisted action at 3 i0929 165829548902 3506 replicacpp655 replica received learned notice for position 3 i0929 165829549139 3506 leveldbcpp343 persisting action 320 bytes to leveldb took 25595ns i0929 165829549343 3506 replicacpp676 persisted action at 3 i0929 165829549607 3506 replicacpp661 replica learned append action at position 3 i0929 165829550081 3506 logcpp699 attempting to truncate the log to 3 i0929 165829550497 3506 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0929 165829550943 3506 replicacpp508 replica received write request for position 4 i0929 165829551198 3506 leveldbcpp343 persisting action 16 bytes to leveldb took 20852ns i0929 165829551409 3506 replicacpp676 persisted action at 4 i0929 165829551795 3506 replicacpp655 replica received learned notice for position 4 i0929 165829552094 3506 leveldbcpp343 persisting action 18 bytes to leveldb took 22182ns i0929 165829552320 3506 leveldbcpp401 deleting 2 keys from leveldb took 18503ns i0929 165829552525 3506 replicacpp676 persisted action at 4 i0929 165829552781 3506 replicacpp661 replica learned truncate action at position 4 i0929 165829550289 3503 registrarcpp478 successfully updated registry i0929 165829553553 3503 mastercpp2970 registered slave 2014092916582927595020165561834860 at slave5719216812216455618 fedora20 i0929 165829553807 3503 mastercpp4180 adding slave 2014092916582927595020165561834860 at slave5719216812216455618 fedora20 with cpus2 mem1024 disk752 ports3100032000 i0929 165829554152 3503 slavecpp763 registered with master master19216812216455618 given slave id 2014092916582927595020165561834860 i0929 165829554455 3503 slavecpp2345 received ping from slaveobserver5619216812216455618 i0929 165829554707 3504 hierarchical_allocator_processhpp442 added slave 2014092916582927595020165561834860 fedora20 with cpus2 mem1024 disk752 ports3100032000 and cpus2 mem1024 disk752 ports3100032000 available i0929 165829555064 3504 hierarchical_allocator_processhpp679 performed allocation for slave 2014092916582927595020165561834860 in 13111ns i0929 165829558220 3486 schedcpp137 version 0210 i0929 165829558821 3501 schedcpp233 new master detected at master19216812216455618 i0929 165829559054 3501 schedcpp283 authenticating with master master19216812216455618 i0929 165829559360 3501 authenticateehpp128 creating new client sasl connection i0929 165829560096 3501 mastercpp3737 authenticating schedulerc8df3f3b2552476f9daf9aa2f012ad2819216812216455618 i0929 165829560430 3501 authenticatorhpp156 creating new server sasl connection i0929 165829561141 3501 authenticateehpp219 received sasl authentication mechanisms crammd5 i0929 165829561465 3501 authenticateehpp245 attempting to authenticate with mechanism crammd5 i0929 165829561743 3501 authenticatorhpp262 received sasl authentication start i0929 165829562098 3501 authenticatorhpp384 authentication requires more steps i0929 165829562353 3501 authenticateehpp265 received sasl authentication step i0929 165829562721 3507 authenticatorhpp290 received sasl authentication step i0929 165829563022 3507 auxpropcpp81 request to lookup properties for user testprincipal realm fedora20 server fqdn fedora20 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0929 165829563254 3507 auxpropcpp153 looking up auxiliary property userpassword i0929 165829563484 3507 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i0929 165829563736 3507 auxpropcpp81 request to lookup properties for user testprincipal realm fedora20 server fqdn fedora20 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0929 165829563976 3507 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0929 165829564188 3507 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0929 165829564415 3507 authenticatorhpp376 authentication success i0929 165829564673 3507 mastercpp3777 successfully authenticated principal testprincipal at schedulerc8df3f3b2552476f9daf9aa2f012ad2819216812216455618 i0929 165829568681 3501 authenticateehpp305 authentication success i0929 165829569046 3501 schedcpp357 successfully authenticated with master master19216812216455618 i0929 165829569286 3501 schedcpp476 sending registration request to master19216812216455618 i0929 165829569581 3507 mastercpp1360 received registration request from schedulerc8df3f3b2552476f9daf9aa2f012ad2819216812216455618 i0929 165829569846 3507 mastercpp1320 authorizing framework principal testprincipal to receive offers for role  i0929 165829570219 3507 mastercpp1419 registering framework 2014092916582927595020165561834860000 at schedulerc8df3f3b2552476f9daf9aa2f012ad2819216812216455618 i0929 165829570543 3506 schedcpp407 framework registered with 2014092916582927595020165561834860000 i0929 165829570811 3506 schedcpp421 schedulerregistered took 13811ns i0929 165829571135 3502 hierarchical_allocator_processhpp329 added framework 2014092916582927595020165561834860000 i0929 165829571393 3502 hierarchical_allocator_processhpp734 offering cpus2 mem1024 disk752 ports3100032000 on slave 2014092916582927595020165561834860 to framework 2014092916582927595020165561834860000 i0929 165829571723 3502 hierarchical_allocator_processhpp659 performed allocation for 1 slaves in 368547ns i0929 165829572125 3507 masterhpp868 adding offer 2014092916582927595020165561834860 with resources cpus2 mem1024 disk752 ports3100032000 on slave 2014092916582927595020165561834860 fedora20 i0929 165829572374 3507 mastercpp3679 sending 1 offers to framework 2014092916582927595020165561834860000 i0929 165829572841 3503 schedcpp544 schedulerresourceoffers took 114306ns i0929 165829573197 3507 masterhpp877 removing offer 2014092916582927595020165561834860 with resources cpus2 mem1024 disk752 ports3100032000 on slave 2014092916582927595020165561834860 fedora20 i0929 165829573457 3507 mastercpp2274 processing reply for offers  2014092916582927595020165561834860  on slave 2014092916582927595020165561834860 at slave5719216812216455618 fedora20 for framework 2014092916582927595020165561834860000 w0929 165829573717 3507 mastercpp1944 executor default for task 0 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0929 165829573953 3507 mastercpp1955 executor default for task 0 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0929 165829574177 3507 mastercpp2357 authorizing framework principal testprincipal to launch task 0 as user jenkins i0929 165829574745 3507 masterhpp845 adding task 0 with resources cpus2 mem512 on slave 2014092916582927595020165561834860 fedora20 i0929 165829574992 3507 mastercpp2423 launching task 0 of framework 2014092916582927595020165561834860000 with resources cpus2 mem512 on slave 2014092916582927595020165561834860 at slave5719216812216455618 fedora20 i0929 165829575315 3503 slavecpp1023 got assigned task 0 for framework 2014092916582927595020165561834860000 i0929 165829575724 3503 slavecpp1133 launching task 0 for framework 2014092916582927595020165561834860000 i0929 165829578129 3503 execcpp132 version 0210 i0929 165829578505 3504 execcpp182 executor started at executor3019216812216455618 with pid 3486 i0929 165829578867 3503 slavecpp1246 queuing task 0 for executor default of framework 2014092916582927595020165561834860000 i0929 165829579144 3503 slavecpp554 successfully attached file tmpallocatortest_0_slavelost_xdxhfgslaves2014092916582927595020165561834860frameworks2014092916582927595020165561834860000executorsdefaultrunsb0de97597054476390f4889ddc3a8524 i0929 165829579401 3503 slavecpp1756 got registration for executor default of framework 2014092916582927595020165561834860000 fro,1
remove proc and sys remounts from port_mapping isolator procnet reflects a new network namespace regardless and remount doesnt actually do what we expected anyway ie its not sufficient for a new pid namespace and a new mount is required,3
mesos 0201 doesnt compile the compilation of mesos 0201 fails on ubuntu trusty with the following error  slavecontainerizermesoscontainerizercpp fpic dpic o slavecontainerizermesoslibslibmesos_no_3rdparty_lacontainerizero in file included from linuxroutingfilteriphpp360 from slavecontainerizerisolatorsnetworkport_mappinghpp42 from slavecontainerizermesoscontainerizercpp44 linuxroutingfilterfilterhpp2943 fatal error linuxroutingfilterhandlehpp no such file or directory include linuxroutingfilterhandlehpp,1
support specifying libnl3 install location libnl_cflags uses a hardcoded path in the configure script instead of detecting the location,2
leaked file descriptors in statusupdatestream httpsgithubcomapachemesosblobmastersrcslavestatus_update_managerhppl180 we should set cloexec for fd,1
performance regression in the masters http metrics as part of the change to hold on to terminal unacknowledged tasks in the master we introduced a performance regression during the following patch httpsgithubcomapachemesoscommit0760b007ad65bc91e8cea377339978c78d36d247 noformat commit 0760b007ad65bc91e8cea377339978c78d36d247 author benjamin mahler bmahlertwittercom date thu sep 11 104820 2014 0700 minor cleanups to the master code review httpsreviewsapacheorgr25566 noformat rather than keeping a running count of allocated resources we now compute resources ondemand this was done in order to ignore terminal tasks resources as a result of this change the statsjson and metricssnapshot endpoints on the master have slowed down substantially on large clusters noformat  time curl localhost5050health real 0m0004s user 0m0001s sys 0m0002s  time curl localhost5050statsjson  devnull real 0m15402s user 0m0001s sys 0m0003s  time curl localhost5050metricssnapshot  devnull real 0m6059s user 0m0002s sys 0m0002s noformat perf top reveals some of the resource computation during a request to statsjson noformat perf top events 36k cycles 1053 libc25so  _int_free 990 libc25so  malloc 856 libmesos0210so  std_rb_treeprocessprocessbase processprocessbase std_identityprocessprocessbase stdlessprocessprocessbase stdallocatorprocessprocessbase  823 libc25so  _int_malloc 580 libstdcso608  std_rb_tree_incrementstd_rb_tree_node_base 533 kernel k _raw_spin_lock 313 libstdcso608  stdstringassignstdstring const 295 libmesos0210so  processsocketmanagerexitedprocessprocessbase 243 libmesos0210so  mesosresourcemergefrommesosresource const 188 libmesos0210so  mesosinternalmasterslaveused const 148 libstdcso608  __gnu_cxx__atomic_addint volatile int 145 kernel k find_busiest_group 141 libc25so  free 138 libmesos0210so  mesosvalue_rangemergefrommesosvalue_range const 113 libmesos0210so  mesosvalue_scalarmergefrommesosvalue_scalar const 112 libmesos0210so  mesosresourceshareddtor 107 libstdcso608  __gnu_cxx__exchange_and_addint volatile int 094 libmesos0210so  googleprotobufunknownfieldsetmergefromgoogleprotobufunknownfieldset const 092 libstdcso608  operator newunsigned long 088 libmesos0210so  mesosvalue_rangesmergefrommesosvalue_ranges const 075 libmesos0210so  mesosmatchesmesosresource const mesosresource const noformat,3
split launch tasks and decline offers metrics both launchtasks and declineoffers scheduler driver calls end up in messages_launch_tasks metric on the master it would be nice to split them for differentiating these two calls,1
redirect to the leader master when current master is not a leader some of the api endpoints for example mastertasksjson will return bogus information if you query a nonleading master code stevenanesthetize curl httpmaster1mesosvpcqaotenvcom5050mastertasksjson  jq   head n 10  tasks   stevenanesthetize curl httpmaster2mesosvpcqaotenvcom5050mastertasksjson  jq   head n 10  tasks   stevenanesthetize curl httpmaster3mesosvpcqaotenvcom5050mastertasksjson  jq   head n 10  tasks   executor_id  framework_id 20140724231003419644938505017070000 id ppguestcenterwebhealthmonitor606cd6ee4b5011e4825b5212e05f35db name ppguestcenterwebhealthmonitor606cd6ee4b5011e4825b5212e05f35db resources  cpus 025 disk 0 code this is very hard for endusers to work around for example if i query which master is leading followed by leader which tasks are running it is possible that the leader fails over in between leaving me with an incorrect answer and no way to know that this happened in my opinion the api should return the correct response by asking the current leader or an error 500 not the leader but its unacceptable to return a successful wrong answer,3
race between authenticator and authenticatorauthenticate can lead to schedulersslaves to never get authenticated the master might get a duplicate authenticate request while a previous authentication attempt is in progress depending on what the authenticatorprocess is executing at the time there are 2 possible race conditions which will cause schedulerslave to continuously retry authentication but never succeed we have seen both the race conditions in a heavily loaded production cluster race1   an authenticate event was dispatched to authenticatorprocess masterauthenticate called authenticatorauthenticate  a terminate event was then injected into the front of the authenticatorprocess queue duplicate masterauthenticate did authenticator before the above authenticate event was executed  due to the bug in libprocess the future returned by masterauthenticate was never transitioned to discarded master_authenticate was never called  this caused all the subsequent authentication retries to be enqueued on the master waiting for master_authenticate to be executed fix transition the dispatched future to discarded if the libprocess is terminated httpsreviewsapacheorgr25945 race 2   an authenticate event was dispatched to authenticatorprocess masterauthenticate called authenticatorauthenticate  authenticatorprocessauthenticate executed and set promiseondiscarddeferself selfdiscarded note the internal promise of authenticatorprocess is discarded in authenticatorprocessdiscarded  a terminate event was then injected into the front of the authenticatorprocess queue duplicate masterauthenticate did authenticator before the above discarded event was executed  authenticatorprocess is destructed without ever discarding the internal promise master_authenticate was never called  this caused all the subsequent authentication retries to be enqueued on the master waiting for master_authenticate to be executed fix the fix here is to discard the internal promise when the authenticatorprocess is destructed,2
updateframework message might reach the slave before reregistered message and get dropped in reregisterslave we send slavereregisteredmessage before we link the slave pid which means a temporary socket will be created and used subsequently after linking we send the updateframeworkmessage which creates and uses a persistent socket this might lead to outoforder delivery resulting in updateframeworkmessage reaching the slave before the slavereregisteredmessage and getting dropped because the slave is not yet reregistered,1
oskilltree incorrectly returns early if pid has terminated if groups  true andor sessions  true then oskilltree should continue to signal all processes in the process group andor session even if the leading pid has terminated,2
slave resources obtained from localhost5051statejson is not correct the resources field in slave is uninitialized also seems that attributes field in slave is redundant as we store slave info,2
add backoff to framework reregistration retries to avoid so many duplicate framework reregistration attempts and thus offer rescinds we should add backoff to reregistration retries,3
create libeventsslbacked socket implementation,13
make executors user owner of executors cgroup directory currently when cgroups are enabled and executor is spawned its mounted under for ex sysfscgroupcpumesosmesosid this directory in current implementation is only writable by root user this prevents process launched by executor to mount its child processes under this cgroup because the cgroup directory is only writable by root to enable a executor spawned process to mount its child processes under its cgroup directory the cgroup directory should be made writable by the user which spawns the executor,3
add event queue size metrics to scheduler driver in the master process we expose metrics for event queue sizes for various event types we should do the same for the scheduler driver process,2
specification for executor and task life cycles in slave we should create a precise specification of what the mesos source code is supposed to be implementing wrt the life cycle of executors and tasks and in addition we should document why certain design decisions have been made one way or another to provide guidance for future code changes with such a source codeindependent specification we could write unbiased regression and scale tests which would be instrumental in maintaining high quality furthermore this would make the source code more amenable why pick this particular area of the source code shouldnt more of mesos have a thorough specification probably so but we need to start somewhere and this area seems to be a good choice given both its intricacy and its importance,5
0210 release mesos release 0210 will include the following major features  provide state reconciliation for frameworks mesos1407httpsissuesapacheorgjirabrowsemesos1407 possible features to include  isolation of system directories tmp for mesos containers mesos1586httpsissuesapacheorgjirabrowsemesos1586  expose reason for task_killed 1930httpsissuesapacheorgjirabrowsemesos1930 this ticket will be used to track blockers to this release,5
test routingtestinetsockets fails on some machine noformat  run  routingtestinetsockets mesossrctestsrouting_testscpp238 failure infos input data out of range abort mesos3rdpartylibprocess3rdpartystoutincludestouttryhpp92 tryget but state  error input data out of range aborted at 1414000937 unix time try date d 1414000937 if you are using gnu date  pc  0x7f2c2d509fc5 __gi_raise  sigabrt 0x1b49000040b1 received by pid 16561 tid 0x7f2c31031720 from pid 16561 stack trace   0x7f2c2f0d4ca0 unknown  0x7f2c2d509fc5 __gi_raise  0x7f2c2d50ba70 __gi_abort  0x4cf782 _abort  0x4cf7bc _abort  0x99459e routingtest_inetsockets_testtestbody  0xa1c363 testinginternalhandleexceptionsinmethodifsupported  0xa13617 testingtestrun  0xa136be testingtestinforun  0xa137c5 testingtestcaserun  0xa13a68 testinginternalunittestimplrunalltests  0xa13cf7 testingunittestrun  0x49bc4b main  0x7f2c2d4f79f4 __libc_start_main  0x4aad79 unknown make3  checklocal aborted noformat,2
rbt only takes revision ranges as args for versions  06 the supportpostreviewspy script doesnt differentiate between rbt versions although the calling conventions for passing revision ranges are different,1
slave and offer ids are indistinguishable in the logs it is currently impossible to tell slave ids and offer ids apart when looking at logs adding some differentiator will make log reading a little simpler,1
move task_lost generations due to invalid tasks from scheduler driver to master as we move towards pure schedulerexecutor clients it is imperative that the scheduler driver doesnt do validation of tasks and generate task_lost messages itself all that logic should live in the master schedulers should reconcile dropped messages via reconciliation,3
refactor the c resources abstraction for diskinfo as we introduce diskinfo and reservation for resource we need to change the c resources abstraction to properly deal with mergesplit of resources with those additional fields also the existing c resources interfaces are poorly designed some of them are confusing and unintuitive some of them are overloaded with too many functionalities for instance noformat bool operator  const resource left const resource right noformat this interface in nonintuitive because a  b doesnt imply b  a noformat resource operator  const resource left const resource right noformat this one is also nonintuitive because if left is not compatible with right the result is left why not right similar for operator  noformat optionresource resourcesgetconst resource r const noformat this one assume resources is flattened but it might not be as we start to introduce persistent disk resources mesos1554 things will get more complicated for example one may want to get two types of disk functions one returns the ephemeral disk bytes with no disk info one returns the total disk bytes including ones that have disk info we may wanna introduce a concept about resource that indicates that a resource cannot be merged or split eg atomic since we need to change this class anyway i wanna take this chance to refactor it,8
documentation for egress control limit,1
container network stats reported by the port mapping isolator is the reverse of the actual network stats looks like the txrx network stats reported is the reverse of the actual network stats the reason is because we simply get txrx data from veth on the host since veth pair is a tunnel the ingress of veth on host is the egress of eth0 in container and vice versa therefore we need to flip the data we got from veth noformat jyu  sudo ip netns exec 24926 sbinip s link show dev eth0 2 eth0 broadcastmulticastuplower_up mtu 1500 qdisc pfifo_fast state up mode default qlen 1000 linkether f04da2757405 brd ffffffffffff rx bytes packets errors dropped overrun mcast 46030857691178 12561038581 0 0 0 0 tx bytes packets errors dropped carrier collsns 29792886058561 15036798198 0 0 0 0 jyu  ip s link show dev mesos24926 7412 mesos24926 broadcastmulticastuplower_up mtu 1500 qdisc pfifo_fast state up mode default qlen 1000 linkether f04da2757405 brd ffffffffffff rx bytes packets errors dropped overrun mcast 29793066979551 15036894749 0 0 0 0 tx bytes packets errors dropped carrier collsns 46031126366116 12561113732 0 0 0 0 noformat,1
allocatortest0slavereregistersfirst is flaky noformattitle  run  allocatortest0slavereregistersfirst using temporary directory tmpallocatortest_0_slavereregistersfirst_ype61d i1028 234822360447 31190 leveldbcpp176 opened db in 2192575ms i1028 234822361253 31190 leveldbcpp183 compacted db in 760753ns i1028 234822361320 31190 leveldbcpp198 created db iterator in 22188ns i1028 234822361340 31190 leveldbcpp204 seeked to beginning of db in 1950ns i1028 234822361351 31190 leveldbcpp273 iterated through 0 keys in the db in 345ns i1028 234822361403 31190 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1028 234822362185 31217 recovercpp437 starting replica recovery i1028 234822362764 31219 recovercpp463 replica is in empty status i1028 234822363955 31210 replicacpp638 replica in empty status received a broadcasted recover request i1028 234822364320 31217 recovercpp188 received a recover response from a replica in empty status i1028 234822364820 31211 recovercpp554 updating replica status to starting i1028 234822365365 31215 leveldbcpp306 persisting metadata 8 bytes to leveldb took 418991ns i1028 234822365391 31215 replicacpp320 persisted replica status to starting i1028 234822365617 31217 recovercpp463 replica is in starting status i1028 234822366328 31206 mastercpp312 master 2014102823482231930294435004331190 pietasapacheorg started on 671958119050043 i1028 234822366377 31206 mastercpp358 master only allowing authenticated frameworks to register i1028 234822366391 31206 mastercpp363 master only allowing authenticated slaves to register i1028 234822366402 31206 credentialshpp36 loading credentials for authentication from tmpallocatortest_0_slavereregistersfirst_ype61dcredentials i1028 234822366708 31206 mastercpp392 authorization enabled i1028 234822366886 31209 replicacpp638 replica in starting status received a broadcasted recover request i1028 234822367311 31208 mastercpp120 no whitelist given advertising offers for all slaves i1028 234822367312 31207 recovercpp188 received a recover response from a replica in starting status i1028 234822367686 31211 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master671958119050043 i1028 234822367863 31212 recovercpp554 updating replica status to voting i1028 234822368477 31218 leveldbcpp306 persisting metadata 8 bytes to leveldb took 375527ns i1028 234822368505 31218 replicacpp320 persisted replica status to voting i1028 234822368517 31204 mastercpp1242 the newly elected leader is master671958119050043 with id 2014102823482231930294435004331190 i1028 234822368549 31204 mastercpp1255 elected as the leading master i1028 234822368567 31204 mastercpp1073 recovering from registrar i1028 234822368621 31215 recovercpp568 successfully joined the paxos group i1028 234822368716 31219 registrarcpp313 recovering registrar i1028 234822369000 31215 recovercpp452 recover process terminated i1028 234822369523 31208 logcpp656 attempting to start the writer i1028 234822370909 31205 replicacpp474 replica received implicit promise request with proposal 1 i1028 234822371266 31205 leveldbcpp306 persisting metadata 8 bytes to leveldb took 325016ns i1028 234822371290 31205 replicacpp342 persisted promised to 1 i1028 234822371979 31218 coordinatorcpp230 coordinator attemping to fill missing position i1028 234822373378 31210 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i1028 234822373746 31210 leveldbcpp343 persisting action 8 bytes to leveldb took 329018ns i1028 234822373772 31210 replicacpp676 persisted action at 0 i1028 234822374897 31214 replicacpp508 replica received write request for position 0 i1028 234822374951 31214 leveldbcpp438 reading position from leveldb took 26002ns i1028 234822375272 31214 leveldbcpp343 persisting action 14 bytes to leveldb took 289094ns i1028 234822375298 31214 replicacpp676 persisted action at 0 i1028 234822375886 31204 replicacpp655 replica received learned notice for position 0 i1028 234822376258 31204 leveldbcpp343 persisting action 16 bytes to leveldb took 346650ns i1028 234822376277 31204 replicacpp676 persisted action at 0 i1028 234822376298 31204 replicacpp661 replica learned nop action at position 0 i1028 234822376843 31215 logcpp672 writer started with ending position 0 i1028 234822378056 31205 leveldbcpp438 reading position from leveldb took 28265ns i1028 234822380323 31217 registrarcpp346 successfully fetched the registry 0b in 1155584ms i1028 234822380466 31217 registrarcpp445 applied 1 operations in 50632ns attempting to update the registry i1028 234822382472 31217 logcpp680 attempting to append 139 bytes to the log i1028 234822382715 31210 coordinatorcpp340 coordinator attempting to write append action at position 1 i1028 234822383463 31210 replicacpp508 replica received write request for position 1 i1028 234822383857 31210 leveldbcpp343 persisting action 158 bytes to leveldb took 363758ns i1028 234822383875 31210 replicacpp676 persisted action at 1 i1028 234822384397 31218 replicacpp655 replica received learned notice for position 1 i1028 234822384840 31218 leveldbcpp343 persisting action 160 bytes to leveldb took 420161ns i1028 234822384862 31218 replicacpp676 persisted action at 1 i1028 234822384882 31218 replicacpp661 replica learned append action at position 1 i1028 234822385684 31211 registrarcpp490 successfully updated the registry in 5158144ms i1028 234822385818 31211 registrarcpp376 successfully recovered registrar i1028 234822385912 31214 logcpp699 attempting to truncate the log to 1 i1028 234822386101 31218 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i1028 234822386124 31211 mastercpp1100 recovered 0 slaves from the registry 101b  allowing 10mins for slaves to reregister i1028 234822387398 31209 replicacpp508 replica received write request for position 2 i1028 234822387758 31209 leveldbcpp343 persisting action 16 bytes to leveldb took 334969ns i1028 234822387776 31209 replicacpp676 persisted action at 2 i1028 234822388272 31204 replicacpp655 replica received learned notice for position 2 i1028 234822388453 31204 leveldbcpp343 persisting action 18 bytes to leveldb took 159390ns i1028 234822388501 31204 leveldbcpp401 deleting 1 keys from leveldb took 30409ns i1028 234822388516 31204 replicacpp676 persisted action at 2 i1028 234822388531 31204 replicacpp661 replica learned truncate action at position 2 i1028 234822400737 31207 slavecpp169 slave started on 34671958119050043 i1028 234822400786 31207 credentialshpp84 loading credential for authentication from tmpallocatortest_0_slavereregistersfirst_qppv21credential i1028 234822400996 31207 slavecpp276 slave using credential for testprincipal i1028 234822401304 31207 slavecpp289 slave resources cpus2 mem1024 disk370122e06 ports3100032000 i1028 234822401413 31207 slavecpp318 slave hostname pietasapacheorg i1028 234822401520 31207 slavecpp319 slave checkpoint false w1028 234822401535 31207 slavecpp321 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i1028 234822402349 31207 statecpp33 recovering state from tmpallocatortest_0_slavereregistersfirst_qppv21meta i1028 234822402678 31207 status_update_managercpp197 recovering status update manager i1028 234822403048 31211 slavecpp3456 finished recovery i1028 234822403815 31215 slavecpp602 new master detected at master671958119050043 i1028 234822403852 31215 slavecpp665 authenticating with master master671958119050043 i1028 234822403875 31206 status_update_managercpp171 pausing sending status updates i1028 234822403961 31215 slavecpp638 detecting new master i1028 234822404016 31211 authenticateehpp133 creating new client sasl connection i1028 234822404230 31204 mastercpp3853 authenticating slave34671958119050043 i1028 234822404464 31205 authenticatorhpp161 creating new server sasl connection i1028 234822404613 31211 authenticateehpp224 received sasl authentication mechanisms crammd5 i1028 234822404649 31211 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1028 234822404734 31211 authenticatorhpp267 received sasl authentication start i1028 234822404783 31211 authenticatorhpp389 authentication requires more steps i1028 234822404898 31215 authenticateehpp270 received sasl authentication step i1028 234822404999 31215 authenticatorhpp295 received sasl authentication step i1028 234822405030 31215 auxpropcpp81 request to lookup properties for user testprincipal realm pietasapacheorg server fqdn pietasapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1028 234822405047 31215 auxpropcpp153 looking up auxiliary property userpassword i1028 234822405086 31215 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1028 234822405109 31215 auxpropcpp81 request to lookup properties for user testprincipal realm pietasapacheorg server fqdn pietasapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1028 234822405122 31215 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1028 234822405129 31215 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1028 234822405146 31215 authenticatorhpp381 authentication success i1028 234822405243 31213 authenticateehpp310 authentication success i1028 234822405253 31214 mastercpp3893 successfully authenticated principal testprincipal at slave34671958119050043 i1028 234822405505 31213 slavecpp722 successfully authenticated with master master671958119050043 i1028 234822405619 31213 slavecpp1050 will retry registration in 17050994ms if necessary i1028 234822405819 31215 mastercpp3032 registering slave at slave34671958119050043 pietasapacheorg with id 2014102823482231930294435004331190s0 i1028 234822406262 31216 registrarcpp445 applied 1 operations in 52647ns attempting to update the registry i1028 234822406697 31190 schedcpp137 version 0210 i1028 234822407083 31211 schedcpp233 new master detected at master671958119050043 i1028 234822407114 31211 schedcpp283 authenticating with master master671958119050043 i1028 234822407290 31214 authenticateehpp133 creating new client sasl connection i1028 234822407424 31214 mastercpp3853 authenticating scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 i1028 234822407659 31207 authenticatorhpp161 creating new server sasl connection i1028 234822407757 31207 authenticateehpp224 received sasl authentication mechanisms crammd5 i1028 234822407774 31207 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1028 234822407830 31207 authenticatorhpp267 received sasl authentication start i1028 234822407868 31207 authenticatorhpp389 authentication requires more steps i1028 234822407927 31207 authenticateehpp270 received sasl authentication step i1028 234822408015 31212 authenticatorhpp295 received sasl authentication step i1028 234822408037 31212 auxpropcpp81 request to lookup properties for user testprincipal realm pietasapacheorg server fqdn pietasapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1028 234822408046 31212 auxpropcpp153 looking up auxiliary property userpassword i1028 234822408072 31212 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1028 234822408092 31212 auxpropcpp81 request to lookup properties for user testprincipal realm pietasapacheorg server fqdn pietasapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1028 234822408100 31212 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1028 234822408105 31212 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1028 234822408116 31212 authenticatorhpp381 authentication success i1028 234822408192 31210 authenticateehpp310 authentication success i1028 234822408210 31217 mastercpp3893 successfully authenticated principal testprincipal at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 i1028 234822408419 31210 schedcpp357 successfully authenticated with master master671958119050043 i1028 234822408460 31210 schedcpp476 sending registration request to master671958119050043 i1028 234822408568 31217 mastercpp1362 received registration request for framework default at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 i1028 234822408617 31217 mastercpp1321 authorizing framework principal testprincipal to receive offers for role  i1028 234822408937 31214 mastercpp1426 registering framework 20141028234822319302944350043311900000 default at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 i1028 234822409265 31213 schedcpp407 framework registered with 20141028234822319302944350043311900000 i1028 234822409267 31212 hierarchical_allocator_processhpp329 added framework 20141028234822319302944350043311900000 i1028 234822409312 31212 hierarchical_allocator_processhpp697 no resources available to allocate i1028 234822409324 31215 logcpp680 attempting to append 316 bytes to the log i1028 234822409333 31213 schedcpp421 schedulerregistered took 38591ns i1028 234822409327 31212 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 24107ns i1028 234822409518 31205 coordinatorcpp340 coordinator attempting to write append action at position 3 i1028 234822410127 31206 replicacpp508 replica received write request for position 3 i1028 234822410706 31206 leveldbcpp343 persisting action 335 bytes to leveldb took 554098ns i1028 234822410725 31206 replicacpp676 persisted action at 3 i1028 234822411151 31217 replicacpp655 replica received learned notice for position 3 i1028 234822411499 31217 leveldbcpp343 persisting action 337 bytes to leveldb took 326572ns i1028 234822411519 31217 replicacpp676 persisted action at 3 i1028 234822411533 31217 replicacpp661 replica learned append action at position 3 i1028 234822412292 31219 registrarcpp490 successfully updated the registry in 5972992ms i1028 234822412518 31218 logcpp699 attempting to truncate the log to 3 i1028 234822412621 31213 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i1028 234822412734 31219 slavecpp2522 received ping from slaveobserver38671958119050043 i1028 234822412787 31206 mastercpp3086 registered slave 2014102823482231930294435004331190s0 at slave34671958119050043 pietasapacheorg with cpus2 mem1024 disk370122e06 ports3100032000 i1028 234822412858 31219 slavecpp756 registered with master master671958119050043 given slave id 2014102823482231930294435004331190s0 i1028 234822412994 31210 status_update_managercpp178 resuming sending status updates i1028 234822413014 31211 hierarchical_allocator_processhpp442 added slave 2014102823482231930294435004331190s0 pietasapacheorg with cpus2 mem1024 disk370122e06 ports3100032000 and cpus2 mem1024 disk370122e06 ports3100032000 available i1028 234822413159 31211 hierarchical_allocator_processhpp734 offering cpus2 mem1024 disk370122e06 ports3100032000 on slave 2014102823482231930294435004331190s0 to framework 20141028234822319302944350043311900000 i1028 234822413290 31208 replicacpp508 replica received write request for position 4 i1028 234822413421 31211 hierarchical_allocator_processhpp679 performed allocation for slave 2014102823482231930294435004331190s0 in 346658ns i1028 234822413650 31208 leveldbcpp343 persisting action 16 bytes to leveldb took 336067ns i1028 234822413668 31208 replicacpp676 persisted action at 4 i1028 234822413797 31216 mastercpp3795 sending 1 offers to framework 20141028234822319302944350043311900000 default at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 i1028 234822414077 31212 replicacpp655 replica received learned notice for position 4 i1028 234822414356 31212 leveldbcpp343 persisting action 18 bytes to leveldb took 260401ns i1028 234822414403 31212 leveldbcpp401 deleting 2 keys from leveldb took 28541ns i1028 234822414417 31212 replicacpp676 persisted action at 4 i1028 234822414446 31212 replicacpp661 replica learned truncate action at position 4 i1028 234822414422 31207 schedcpp544 schedulerresourceoffers took 310278ns i1028 234822415086 31214 mastercpp2321 processing reply for offers  2014102823482231930294435004331190o0  on slave 2014102823482231930294435004331190s0 at slave34671958119050043 pietasapacheorg for framework 20141028234822319302944350043311900000 default at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 w1028 234822415163 31214 mastercpp1969 executor default for task 0 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w1028 234822415186 31214 mastercpp1980 executor default for task 0 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i1028 234822415256 31214 mastercpp2417 authorizing framework principal testprincipal to launch task 0 as user jenkins i1028 234822416033 31219 masterhpp877 adding task 0 with resources cpus1 mem500 on slave 2014102823482231930294435004331190s0 pietasapacheorg i1028 234822416084 31219 mastercpp2480 launching task 0 of framework 20141028234822319302944350043311900000 default at scheduler0aa33fc70d29487c80ebf933681f9c95671958119050043 with resources cpus1 mem500 on slave 2014102823482231930294435004331190s0 at slave34671958119050043 pietasapacheorg i1028 234822416317 31214 slavecpp1081 got assigned task 0 for framework 20141028234822319302944350043311900000 i1028 234822416679 31215 hierarchical_allocator_processhpp563 recovered cpus1 mem524 disk370122e06 ports3100032000 total allocatable cpus1 mem524 disk370122e06 ports3100032000 on slave 2014102823482231930294435004331190s0 from framework 20141028234822319302944350043311900000 i1028 234822416721 31215 hierarchical_allocator_processhpp599 framework 20141028234822319302944350043311900000 filtered slave 2,2
masterauthorizationtestduplicatereregistration is flaky noformattitle  run  masterauthorizationtestduplicatereregistration using temporary directory tmpmasterauthorizationtest_duplicatereregistration_dlomyx i1029 082526021766 32232 leveldbcpp176 opened db in 3066621ms i1029 082526022734 32232 leveldbcpp183 compacted db in 935019ns i1029 082526022766 32232 leveldbcpp198 created db iterator in 4350ns i1029 082526022785 32232 leveldbcpp204 seeked to beginning of db in 902ns i1029 082526022799 32232 leveldbcpp273 iterated through 0 keys in the db in 387ns i1029 082526022831 32232 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1029 082526023305 32248 recovercpp437 starting replica recovery i1029 082526023598 32248 recovercpp463 replica is in empty status i1029 082526025059 32260 replicacpp638 replica in empty status received a broadcasted recover request i1029 082526025320 32247 recovercpp188 received a recover response from a replica in empty status i1029 082526025585 32256 recovercpp554 updating replica status to starting i1029 082526026546 32249 mastercpp312 master 2014102908252631426977954069632232 pomonaapacheorg started on 671958118740696 i1029 082526026561 32261 leveldbcpp306 persisting metadata 8 bytes to leveldb took 694444ns i1029 082526026592 32249 mastercpp358 master only allowing authenticated frameworks to register i1029 082526026592 32261 replicacpp320 persisted replica status to starting i1029 082526026605 32249 mastercpp363 master only allowing authenticated slaves to register i1029 082526026639 32249 credentialshpp36 loading credentials for authentication from tmpmasterauthorizationtest_duplicatereregistration_dlomyxcredentials i1029 082526026877 32249 mastercpp392 authorization enabled i1029 082526026901 32260 recovercpp463 replica is in starting status i1029 082526027498 32261 mastercpp120 no whitelist given advertising offers for all slaves i1029 082526027541 32248 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master671958118740696 i1029 082526028055 32252 replicacpp638 replica in starting status received a broadcasted recover request i1029 082526028451 32247 recovercpp188 received a recover response from a replica in starting status i1029 082526028733 32249 mastercpp1242 the newly elected leader is master671958118740696 with id 2014102908252631426977954069632232 i1029 082526028764 32249 mastercpp1255 elected as the leading master i1029 082526028781 32249 mastercpp1073 recovering from registrar i1029 082526028904 32246 recovercpp554 updating replica status to voting i1029 082526029163 32257 registrarcpp313 recovering registrar i1029 082526029556 32251 leveldbcpp306 persisting metadata 8 bytes to leveldb took 485711ns i1029 082526029588 32251 replicacpp320 persisted replica status to voting i1029 082526029726 32253 recovercpp568 successfully joined the paxos group i1029 082526029932 32253 recovercpp452 recover process terminated i1029 082526030436 32250 logcpp656 attempting to start the writer i1029 082526032152 32248 replicacpp474 replica received implicit promise request with proposal 1 i1029 082526032778 32248 leveldbcpp306 persisting metadata 8 bytes to leveldb took 597030ns i1029 082526032807 32248 replicacpp342 persisted promised to 1 i1029 082526033481 32254 coordinatorcpp230 coordinator attemping to fill missing position i1029 082526035429 32247 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i1029 082526036154 32247 leveldbcpp343 persisting action 8 bytes to leveldb took 690208ns i1029 082526036181 32247 replicacpp676 persisted action at 0 i1029 082526037344 32249 replicacpp508 replica received write request for position 0 i1029 082526037395 32249 leveldbcpp438 reading position from leveldb took 22607ns i1029 082526038074 32249 leveldbcpp343 persisting action 14 bytes to leveldb took 647429ns i1029 082526038105 32249 replicacpp676 persisted action at 0 i1029 082526038683 32247 replicacpp655 replica received learned notice for position 0 i1029 082526039378 32247 leveldbcpp343 persisting action 16 bytes to leveldb took 664911ns i1029 082526039407 32247 replicacpp676 persisted action at 0 i1029 082526039433 32247 replicacpp661 replica learned nop action at position 0 i1029 082526040045 32252 logcpp672 writer started with ending position 0 i1029 082526041378 32251 leveldbcpp438 reading position from leveldb took 25625ns i1029 082526044642 32246 registrarcpp346 successfully fetched the registry 0b in 15433984ms i1029 082526044742 32246 registrarcpp445 applied 1 operations in 16444ns attempting to update the registry i1029 082526047538 32256 logcpp680 attempting to append 139 bytes to the log i1029 082526156330 32247 coordinatorcpp340 coordinator attempting to write append action at position 1 i1029 082526158460 32261 replicacpp508 replica received write request for position 1 i1029 082526159277 32261 leveldbcpp343 persisting action 158 bytes to leveldb took 782308ns i1029 082526159328 32261 replicacpp676 persisted action at 1 i1029 082526160267 32255 replicacpp655 replica received learned notice for position 1 i1029 082526161070 32255 leveldbcpp343 persisting action 160 bytes to leveldb took 750259ns i1029 082526161100 32255 replicacpp676 persisted action at 1 i1029 082526161125 32255 replicacpp661 replica learned append action at position 1 i1029 082526162199 32253 registrarcpp490 successfully updated the registry in 11740416ms i1029 082526162400 32253 registrarcpp376 successfully recovered registrar i1029 082526162724 32249 mastercpp1100 recovered 0 slaves from the registry 101b  allowing 10mins for slaves to reregister i1029 082526162757 32253 logcpp699 attempting to truncate the log to 1 i1029 082526162919 32256 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i1029 082526163949 32250 replicacpp508 replica received write request for position 2 i1029 082526164589 32250 leveldbcpp343 persisting action 16 bytes to leveldb took 603175ns i1029 082526164618 32250 replicacpp676 persisted action at 2 i1029 082526165385 32251 replicacpp655 replica received learned notice for position 2 i1029 082526166007 32251 leveldbcpp343 persisting action 18 bytes to leveldb took 594003ns i1029 082526166056 32251 leveldbcpp401 deleting 1 keys from leveldb took 23309ns i1029 082526166077 32251 replicacpp676 persisted action at 2 i1029 082526166100 32251 replicacpp661 replica learned truncate action at position 2 i1029 082526178493 32232 schedcpp137 version 0210 i1029 082526179029 32256 schedcpp233 new master detected at master671958118740696 i1029 082526179078 32256 schedcpp283 authenticating with master master671958118740696 i1029 082526179424 32246 authenticateehpp133 creating new client sasl connection i1029 082526179678 32259 mastercpp3853 authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526179970 32250 authenticatorhpp161 creating new server sasl connection i1029 082526180165 32250 authenticateehpp224 received sasl authentication mechanisms crammd5 i1029 082526180191 32250 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1029 082526180272 32250 authenticatorhpp267 received sasl authentication start i1029 082526180378 32250 authenticatorhpp389 authentication requires more steps i1029 082526180557 32260 authenticateehpp270 received sasl authentication step i1029 082526180704 32254 authenticatorhpp295 received sasl authentication step i1029 082526180737 32254 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1029 082526180748 32254 auxpropcpp153 looking up auxiliary property userpassword i1029 082526180780 32254 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1029 082526180804 32254 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1029 082526180816 32254 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1029 082526180824 32254 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1029 082526180841 32254 authenticatorhpp381 authentication success i1029 082526180937 32259 authenticateehpp310 authentication success i1029 082526180991 32260 mastercpp3893 successfully authenticated principal testprincipal at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526181422 32259 schedcpp357 successfully authenticated with master master671958118740696 i1029 082526181449 32259 schedcpp476 sending registration request to master671958118740696 i1029 082526181697 32260 mastercpp1362 received registration request for framework default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526181758 32260 mastercpp1321 authorizing framework principal testprincipal to receive offers for role  i1029 082526182063 32260 mastercpp1426 registering framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526182430 32248 hierarchical_allocator_processhpp329 added framework 20141029082526314269779540696322320000 i1029 082526182462 32248 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082526182462 32261 schedcpp407 framework registered with 20141029082526314269779540696322320000 i1029 082526182473 32248 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 15372ns i1029 082526182554 32261 schedcpp421 schedulerregistered took 60059ns i1029 082526185515 32260 schedcpp227 schedulerdisconnected took 16607ns i1029 082526185538 32260 schedcpp233 new master detected at master671958118740696 i1029 082526185567 32260 schedcpp283 authenticating with master master671958118740696 i1029 082526185783 32246 authenticateehpp133 creating new client sasl connection i1029 082526186218 32250 mastercpp3853 authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526186456 32247 authenticatorhpp161 creating new server sasl connection i1029 082526186594 32250 authenticateehpp224 received sasl authentication mechanisms crammd5 i1029 082526186621 32250 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1029 082526186745 32259 authenticatorhpp267 received sasl authentication start i1029 082526186800 32259 authenticatorhpp389 authentication requires more steps i1029 082526186936 32260 authenticateehpp270 received sasl authentication step i1029 082526187062 32249 authenticatorhpp295 received sasl authentication step i1029 082526187095 32249 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1029 082526187108 32249 auxpropcpp153 looking up auxiliary property userpassword i1029 082526187137 32249 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1029 082526187162 32249 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1029 082526187175 32249 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1029 082526187182 32249 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1029 082526187199 32249 authenticatorhpp381 authentication success i1029 082526187327 32249 authenticateehpp310 authentication success i1029 082526187366 32260 mastercpp3893 successfully authenticated principal testprincipal at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082526187631 32249 schedcpp357 successfully authenticated with master master671958118740696 i1029 082526187659 32249 schedcpp476 sending registration request to master671958118740696 i1029 082527028445 32251 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082528045682 32251 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 1017231941secs i1029 082528045760 32249 schedcpp476 sending registration request to master671958118740696 i1029 082528045900 32253 mastercpp1499 received reregistration request from framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082528045989 32253 mastercpp1321 authorizing framework principal testprincipal to receive offers for role  i1029 082528046455 32253 mastercpp1499 received reregistration request from framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082528046529 32253 mastercpp1321 authorizing framework principal testprincipal to receive offers for role  i1029 082528050155 32247 schedcpp233 new master detected at master671958118740696 i1029 082528050217 32247 schedcpp283 authenticating with master master671958118740696 i1029 082528050405 32252 mastercpp1552 reregistering framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082528050509 32253 authenticateehpp133 creating new client sasl connection i1029 082528050566 32252 mastercpp1592 allowing framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 to reregister with an already used id i1029 082528051084 32257 schedcpp449 framework reregistered with 20141029082526314269779540696322320000 i1029 082528051151 32252 mastercpp3853 authenticating scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082528051167 32257 schedcpp463 schedulerreregistered took 52801ns i1029 082528051723 32261 authenticatorhpp161 creating new server sasl connection i1029 082528052042 32249 authenticateehpp224 received sasl authentication mechanisms crammd5 i1029 082528052077 32249 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1029 082528052170 32249 mastercpp1534 dropping reregistration request of framework 20141029082526314269779540696322320000 default at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 because new authentication attempt is in progress i1029 082528052218 32257 authenticatorhpp267 received sasl authentication start i1029 082528052325 32257 authenticatorhpp389 authentication requires more steps i1029 082528052428 32257 authenticateehpp270 received sasl authentication step i1029 082528052641 32246 authenticatorhpp295 received sasl authentication step i1029 082528052685 32246 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1029 082528052701 32246 auxpropcpp153 looking up auxiliary property userpassword i1029 082528052739 32246 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1029 082528052767 32246 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1029 082528052779 32246 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1029 082528052788 32246 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1029 082528052804 32246 authenticatorhpp381 authentication success i1029 082528052947 32252 authenticateehpp310 authentication success i1029 082528053020 32246 mastercpp3893 successfully authenticated principal testprincipal at scheduler9ba6b80340b448b9bcef45a329f6b2a4671958118740696 i1029 082528053462 32247 schedcpp357 successfully authenticated with master master671958118740696 i1029 082529046855 32261 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082529046880 32261 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 35632ns i1029 082530047458 32253 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082530047487 32253 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 43031ns i1029 082531028373 32261 mastercpp120 no whitelist given advertising offers for all slaves i1029 082531048673 32249 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082531048702 32249 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 44769ns i1029 082532049576 32259 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082532049604 32259 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 51919ns i1029 082533050864 32249 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082533050896 32249 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 38019ns i1029 082534051961 32251 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082534051993 32251 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 64619ns i1029 082535052196 32249 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082535052223 32249 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 34475ns i1029 082536029101 32259 mastercpp120 no whitelist given advertising offers for all slaves i1029 082536053067 32249 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082536053095 32249 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 38354ns i1029 082537053506 32259 hierarchical_allocator_processhpp697 no resources available to allocate i1029 082537053536 32259 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 38249ns testsmaster_authorization_testscpp877 failure failed to wait 10secs for frameworkreregisteredmessage i1029 082538053241 32259 mastercpp768 framework 2014,2
segfault with pure virtual method called when tests fail the most recent one noformattitledrfallocatortestdrfallocatorprocess  run  drfallocatortestdrfallocatorprocess using temporary directory tmpdrfallocatortest_drfallocatorprocess_bi905j i1030 055506934813 24459 leveldbcpp176 opened db in 3175202ms i1030 055506935925 24459 leveldbcpp183 compacted db in 1077924ms i1030 055506935976 24459 leveldbcpp198 created db iterator in 16460ns i1030 055506935995 24459 leveldbcpp204 seeked to beginning of db in 2018ns i1030 055506936005 24459 leveldbcpp273 iterated through 0 keys in the db in 335ns i1030 055506936039 24459 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1030 055506936705 24480 recovercpp437 starting replica recovery i1030 055506937023 24480 recovercpp463 replica is in empty status i1030 055506938158 24475 replicacpp638 replica in empty status received a broadcasted recover request i1030 055506938859 24482 recovercpp188 received a recover response from a replica in empty status i1030 055506939486 24474 recovercpp554 updating replica status to starting i1030 055506940249 24489 leveldbcpp306 persisting metadata 8 bytes to leveldb took 591981ns i1030 055506940274 24489 replicacpp320 persisted replica status to starting i1030 055506940752 24481 recovercpp463 replica is in starting status i1030 055506940820 24489 mastercpp312 master 2014103005550631426977954042924459 pomonaapacheorg started on 671958118740429 i1030 055506940871 24489 mastercpp358 master only allowing authenticated frameworks to register i1030 055506940891 24489 mastercpp363 master only allowing authenticated slaves to register i1030 055506940908 24489 credentialshpp36 loading credentials for authentication from tmpdrfallocatortest_drfallocatorprocess_bi905jcredentials i1030 055506941215 24489 mastercpp392 authorization enabled i1030 055506941751 24475 mastercpp120 no whitelist given advertising offers for all slaves i1030 055506942227 24474 replicacpp638 replica in starting status received a broadcasted recover request i1030 055506942401 24476 hierarchical_allocator_processhpp299 initializing hierarchical allocator process with master  master671958118740429 i1030 055506942895 24483 recovercpp188 received a recover response from a replica in starting status i1030 055506943035 24474 mastercpp1242 the newly elected leader is master671958118740429 with id 2014103005550631426977954042924459 i1030 055506943063 24474 mastercpp1255 elected as the leading master i1030 055506943079 24474 mastercpp1073 recovering from registrar i1030 055506943313 24480 registrarcpp313 recovering registrar i1030 055506943455 24475 recovercpp554 updating replica status to voting i1030 055506944144 24474 leveldbcpp306 persisting metadata 8 bytes to leveldb took 536365ns i1030 055506944172 24474 replicacpp320 persisted replica status to voting i1030 055506944355 24489 recovercpp568 successfully joined the paxos group i1030 055506944576 24489 recovercpp452 recover process terminated i1030 055506945155 24486 logcpp656 attempting to start the writer i1030 055506947013 24473 replicacpp474 replica received implicit promise request with proposal 1 i1030 055506947854 24473 leveldbcpp306 persisting metadata 8 bytes to leveldb took 806463ns i1030 055506947883 24473 replicacpp342 persisted promised to 1 i1030 055506948547 24481 coordinatorcpp230 coordinator attemping to fill missing position i1030 055506950269 24479 replicacpp375 replica received explicit promise request for position 0 with proposal 2 i1030 055506950933 24479 leveldbcpp343 persisting action 8 bytes to leveldb took 603843ns i1030 055506950961 24479 replicacpp676 persisted action at 0 i1030 055506952180 24476 replicacpp508 replica received write request for position 0 i1030 055506952239 24476 leveldbcpp438 reading position from leveldb took 28437ns i1030 055506952896 24476 leveldbcpp343 persisting action 14 bytes to leveldb took 623980ns i1030 055506952926 24476 replicacpp676 persisted action at 0 i1030 055506953543 24485 replicacpp655 replica received learned notice for position 0 i1030 055506954082 24485 leveldbcpp343 persisting action 16 bytes to leveldb took 511807ns i1030 055506954107 24485 replicacpp676 persisted action at 0 i1030 055506954128 24485 replicacpp661 replica learned nop action at position 0 i1030 055506954710 24473 logcpp672 writer started with ending position 0 i1030 055506956215 24478 leveldbcpp438 reading position from leveldb took 33085ns i1030 055506959481 24475 registrarcpp346 successfully fetched the registry 0b in 1611904ms i1030 055506959616 24475 registrarcpp445 applied 1 operations in 28239ns attempting to update the registry i1030 055506962514 24487 logcpp680 attempting to append 139 bytes to the log i1030 055506962646 24474 coordinatorcpp340 coordinator attempting to write append action at position 1 i1030 055506964146 24486 replicacpp508 replica received write request for position 1 i1030 055506964962 24486 leveldbcpp343 persisting action 158 bytes to leveldb took 743389ns i1030 055506964993 24486 replicacpp676 persisted action at 1 i1030 055506965895 24473 replicacpp655 replica received learned notice for position 1 i1030 055506966531 24473 leveldbcpp343 persisting action 160 bytes to leveldb took 607242ns i1030 055506966555 24473 replicacpp676 persisted action at 1 i1030 055506966578 24473 replicacpp661 replica learned append action at position 1 i1030 055506967706 24481 registrarcpp490 successfully updated the registry in 8036096ms i1030 055506967895 24481 registrarcpp376 successfully recovered registrar i1030 055506967993 24482 logcpp699 attempting to truncate the log to 1 i1030 055506968258 24479 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i1030 055506968268 24475 mastercpp1100 recovered 0 slaves from the registry 101b  allowing 10mins for slaves to reregister i1030 055506969156 24476 replicacpp508 replica received write request for position 2 i1030 055506969678 24476 leveldbcpp343 persisting action 16 bytes to leveldb took 491913ns i1030 055506969703 24476 replicacpp676 persisted action at 2 i1030 055506970459 24478 replicacpp655 replica received learned notice for position 2 i1030 055506971060 24478 leveldbcpp343 persisting action 18 bytes to leveldb took 573076ns i1030 055506971124 24478 leveldbcpp401 deleting 1 keys from leveldb took 35339ns i1030 055506971145 24478 replicacpp676 persisted action at 2 i1030 055506971168 24478 replicacpp661 replica learned truncate action at position 2 i1030 055506980211 24459 containerizercpp100 using isolation posixcpuposixmem i1030 055506984153 24473 slavecpp169 slave started on 203671958118740429 i1030 055507055308 24473 credentialshpp84 loading credential for authentication from tmpdrfallocatortest_drfallocatorprocess_wulx31credential i1030 055506988750 24459 schedcpp137 version 0210 i1030 055507055521 24473 slavecpp276 slave using credential for testprincipal i1030 055507055726 24473 slavecpp289 slave resources cpus2 mem1024 disk0 ports3100032000 i1030 055507055865 24473 slavecpp318 slave hostname pomonaapacheorg i1030 055507055881 24473 slavecpp319 slave checkpoint false w1030 055507055889 24473 slavecpp321 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i1030 055507056172 24485 schedcpp233 new master detected at master671958118740429 i1030 055507056222 24485 schedcpp283 authenticating with master master671958118740429 i1030 055507056717 24485 statecpp33 recovering state from tmpdrfallocatortest_drfallocatorprocess_wulx31meta i1030 055507056851 24475 authenticateehpp133 creating new client sasl connection i1030 055507057003 24473 status_update_managercpp197 recovering status update manager i1030 055507057252 24488 mastercpp3853 authenticating schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055507057502 24489 containerizercpp281 recovering containerizer i1030 055507057524 24475 authenticatorhpp161 creating new server sasl connection i1030 055507057688 24475 authenticateehpp224 received sasl authentication mechanisms crammd5 i1030 055507057719 24475 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1030 055507057919 24481 authenticatorhpp267 received sasl authentication start i1030 055507057968 24481 authenticatorhpp389 authentication requires more steps i1030 055507058070 24473 authenticateehpp270 received sasl authentication step i1030 055507058199 24485 authenticatorhpp295 received sasl authentication step i1030 055507058223 24485 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1030 055507058233 24485 auxpropcpp153 looking up auxiliary property userpassword i1030 055507058259 24485 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1030 055507058290 24485 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1030 055507058302 24485 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1030 055507058307 24485 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1030 055507058320 24485 authenticatorhpp381 authentication success i1030 055507058467 24480 mastercpp3893 successfully authenticated principal testprincipal at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055507058493 24485 slavecpp3456 finished recovery i1030 055507058593 24478 authenticateehpp310 authentication success i1030 055507058838 24478 schedcpp357 successfully authenticated with master master671958118740429 i1030 055507058861 24478 schedcpp476 sending registration request to master671958118740429 i1030 055507058969 24475 slavecpp602 new master detected at master671958118740429 i1030 055507058969 24487 status_update_managercpp171 pausing sending status updates i1030 055507059026 24475 slavecpp665 authenticating with master master671958118740429 i1030 055507059061 24481 mastercpp1362 received registration request for framework framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055507059131 24481 mastercpp1321 authorizing framework principal testprincipal to receive offers for role role1 i1030 055507059171 24475 slavecpp638 detecting new master i1030 055507059214 24482 authenticateehpp133 creating new client sasl connection i1030 055507059550 24481 mastercpp3853 authenticating slave203671958118740429 i1030 055507059787 24487 authenticatorhpp161 creating new server sasl connection i1030 055507059922 24481 mastercpp1426 registering framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055507059996 24474 authenticateehpp224 received sasl authentication mechanisms crammd5 i1030 055507060034 24474 authenticateehpp250 attempting to authenticate with mechanism crammd5 i1030 055507060117 24474 authenticatorhpp267 received sasl authentication start i1030 055507060165 24474 authenticatorhpp389 authentication requires more steps i1030 055507060377 24476 hierarchical_allocator_processhpp329 added framework 20141030055506314269779540429244590000 i1030 055507060394 24488 schedcpp407 framework registered with 20141030055506314269779540429244590000 i1030 055507060403 24476 hierarchical_allocator_processhpp697 no resources available to allocate i1030 055507060431 24476 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 29857ns i1030 055507060443 24488 schedcpp421 schedulerregistered took 19407ns i1030 055507060545 24478 authenticateehpp270 received sasl authentication step i1030 055507060645 24478 authenticatorhpp295 received sasl authentication step i1030 055507060673 24478 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1030 055507060685 24478 auxpropcpp153 looking up auxiliary property userpassword i1030 055507060714 24478 auxpropcpp153 looking up auxiliary property cmusaslsecretcrammd5 i1030 055507060740 24478 auxpropcpp81 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1030 055507060760 24478 auxpropcpp103 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1030 055507060770 24478 auxpropcpp103 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1030 055507060788 24478 authenticatorhpp381 authentication success i1030 055507060920 24474 authenticateehpp310 authentication success i1030 055507060945 24485 mastercpp3893 successfully authenticated principal testprincipal at slave203671958118740429 i1030 055507061388 24489 slavecpp722 successfully authenticated with master master671958118740429 i1030 055507061504 24489 slavecpp1050 will retry registration in 4778336ms if necessary i1030 055507061718 24480 mastercpp3032 registering slave at slave203671958118740429 pomonaapacheorg with id 2014103005550631426977954042924459s0 i1030 055507062119 24489 registrarcpp445 applied 1 operations in 53691ns attempting to update the registry i1030 055507065182 24479 logcpp680 attempting to append 316 bytes to the log i1030 055507065337 24487 coordinatorcpp340 coordinator attempting to write append action at position 3 i1030 055507066359 24474 replicacpp508 replica received write request for position 3 i1030 055507066643 24474 leveldbcpp343 persisting action 335 bytes to leveldb took 249579ns i1030 055507066671 24474 replicacpp676 persisted action at 3 isrctestsallocator_testscpp120 failure failed to wait 10secs for offers1 1030 055507067101 24477 slavecpp1050 will retry registration in 2408243ms if necessary i1030 055507067140 24473 mastercpp3020 ignoring register slave message from slave203671958118740429 pomonaapacheorg as admission is already in progress i1030 055507067395 24488 replicacpp655 replica received learned notice for position 3 i1030 055507943416 24478 hierarchical_allocator_processhpp697 no resources available to allocate i1030 055519804687 24478 hierarchical_allocator_processhpp659 performed allocation for 0 slaves in 11861261123secs i1030 055511942713 24474 mastercpp120 no whitelist given advertising offers for all slaves i1030 055519805850 24488 leveldbcpp343 persisting action 337 bytes to leveldb took 1067224ms i1030 055519806012 24488 replicacpp676 persisted action at 3 srctestsallocator_testscpp115 failure actual function call count doesnt match expect_callsched1 resourceoffers_ _ expected to be called once actual never called  unsatisfied and active i1030 055519806144 24488 replicacpp661 replica learned append action at position 3 i1030 055519806695 24473 mastercpp768 framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 disconnected i1030 055519806726 24473 mastercpp1731 disconnecting framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055519806751 24473 mastercpp1747 deactivating framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055519806967 24473 mastercpp790 giving framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 0ns to failover srctestsallocator_testscpp94 failure actual function call count doesnt match expect_callallocator slaveadded_ _ _ expected to be called once actual never called  unsatisfied and active f1030 055519806967 24480 loggingcpp57 raw pure virtual method called i1030 055519807348 24488 mastercpp3665 framework failover timeout removing framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429 i1030 055519807370 24488 mastercpp4201 removing framework 20141030055506314269779540429244590000 framework1 at schedulerc98e7aacd03f464aaa7561208600e196671958118740429  aborted at 1414648519 unix time try date d 1414648519 if you are using gnu date  pc  0x91bc86 processpidpid  sigsegv 0x0 received by pid 24459 tid 0x2b86c919a700 from pid 0 stack trace  i1030 055519808631 24489 registrarcpp490 successfully updated the registry in 12746377984secs  0x2b86c55fc340 unknown i1030 055519808938 24473 logcpp699 attempting to truncate the log to 3  0x2b86c3327174 googlelogmessagefail i1030 055519809084 24481 coordinatorcpp340 coordinator attempting to write truncate action at position 4  0x91bc86 processpidpid  0x2b86c332c868 googlerawlog__ i1030 055519810191 24479 replicacpp508 replica received write request for position 4 i1030 055519810899 24479 leveldbcpp343 persisting action 16 bytes to leveldb took 678090ns i1030 055519810919 24479 replicacpp676 persisted action at 4  0x91bf24 processprocessself i1030 055519811635 24485 replicacpp655 replica received learned notice for position 4 i1030 055519812180 24485 leveldbcpp343 persisting action 18 bytes to leveldb took 523927ns i1030 055519812228 24485 leveldbcpp401 deleting 2 keys from leveldb took 29523ns i1030 055519812242 24485 replicacpp676 persisted action at 4 i  0x2b86c29d2a36 __cxa_pure_virtual 1030 055519812258 24485 replicacpp661 replica learned truncate action at position 4  0x1046936 testinginternaluntypedfunctionmockerbaseuntypedinvokewith i1030 055519829655 24474 slavecpp1050 will retry registration in 31785967ms if necessary  0x9c0633 testinginternalfunctionmockerbaseinvokewith  0x9b6152 testinginternalfunctionmockerinvoke  0x9abdeb mesosinternaltestsmockallocatorpr,5
allow slave to checkpoint resources the checkpointed resources are independent of the slave lifecycle in other words even if the slave host reboots itll still recover the checkpointed resources unlike other checkpointed data the slave needs to verify during startup that the checkpointed resources are compatible with the resources of the slave specified using resources flag,5
maintain persistent disk resources in master memory maintain an inmemory data structure to track persistent disk resources on each slave update this data structure when slaves registerreregisterdisconnect etc,3
manage persistent directories on slave whenever a slave sees a persistent disk resource in executorinfo or taskinfo that is new to it it will create a persistent directory which is for tasks to store persistent data the slave needs to do the following after its created 1 symlink into the executor sandbox so that tasksexecutor can see it 2 garbage collect it once it is released by the framework,5
update maintenance design to account for persistent resources with persistent resources and dynamic reservations frameworks need to know how long the resources will be unavailable for maintenance operations this is because for persistent resources the framework needs to understand how long the persistent resource will be unavailable for example if there will be a 10 minute reboot for a kernel upgrade the framework will not want to rereplicate all of its persistent data on the machine rather tolerating one unavailable replica for the maintenance window would be preferred id like to do a revisit of the design to ensure it works well for persistent resources as well,13
documentation for isolator filesystemshared,1
documentation for isolator namespacespid,1
add reason to containerizer proto termination when an isolator kills a task the reason is unknown as part of mesos1830 the reason is set to a general one but ideally we would have the termination reason to pass through to the status update,5
framework auth fail with timeout error and never get authenticated im facing this issue in master as of httpsgithubcomapachemesoscommit74ea59e144d131814c66972fb0cc14784d3503d4 as adammesos mentioned in irc this sounds similar to mesos1866 im running 1 master and 1 scheduler aurora the framework authentication fail due to time out error on mesos master code i1104 193717741449 8329 mastercpp3874 authenticating schedulerd2d4437bd3754467a583362152fe065ascheduler_ip8083 i1104 193717741585 8329 mastercpp3885 using default crammd5 authenticator i1104 193717742106 8336 authenticatorhpp169 creating new server sasl connection w1104 193722742959 8329 mastercpp3953 authentication timed out w1104 193722743548 8329 mastercpp3930 failed to authenticate schedulerd2d4437bd3754467a583362152fe065ascheduler_ip8083 authentication discarded code scheduler error code i1104 193857885486 49012 schedcpp283 authenticating with master mastermaster_ipport i1104 193857885928 49002 authenticateehpp133 creating new client sasl connection i1104 193857890581 49007 authenticateehpp224 received sasl authentication mechanisms crammd5 i1104 193857890656 49007 authenticateehpp250 attempting to authenticate with mechanism crammd5 w1104 193902891196 49005 schedcpp378 authentication timed out i1104 193902891850 49018 schedcpp338 failed to authenticate with master mastermaster_ipport authentication discarded code looks like 2 instances scheduler20f88a5359454977b5af28f6c52d3c94  schedulerd2d4437bd3754467a583362152fe065a of same framework is trying to authenticate and fail code w1104 193630769420 8319 mastercpp3930 failed to authenticate scheduler20f88a5359454977b5af28f6c52d3c94scheduler_ip8083 failed to communicate with authenticatee i1104 193642701441 8328 mastercpp3860 queuing up authentication request from schedulerd2d4437bd3754467a583362152fe065ascheduler_ip8083 because authentication is still in progress code restarting master and scheduler didnt fix it this particular issue happen with 1 master and 1 scheduler after mesos1866 is fixed,5
use one ip address per container for network isolation if there are enough ip addresses either ipv4 or ipv6 we should use one ip address per container instead of the ugly port range based solution one problem with this is the ip address management usually it is managed by a dhcp server maybe we need to manage them in mesos masterslave also maybe use macvlan instead of veth for better isolation,40
pull metrics struct out of master and slave to improve readability,2
runstaterecover should always recover completed runstaterecover will return partial state if it cannot find or open the libprocess pid file specifically it does not recover the completed flag however if the slave has removed the executor because launch failed or the executor failed to register the sentinel flag will be set and this fact should be recovered this ensures that container recovery is not attempted later this was discovered when the linuxlauncher failed to recover because it was asked to recover two containers with the same forkedpid investigation showed the executors both oomed before registering ie no libprocess pid file was present however the containerizer had detected the oom destroyed the container and notified the slave which cleaned everything up failing the task and calling removeexecutor which writes the completed sentinel file,1
mesoscontainerizerexecutetestioredirection test is flaky observed this on asf ci code  run  mesoscontainerizerexecutetestioredirection using temporary directory tmpmesoscontainerizerexecutetest_ioredirection_pbbn8a i1108 003425820514 30391 containerizercpp100 using isolation posixcpuposixmem i1108 003425821048 30411 containerizercpp424 starting container test_container for executor executor of framework  i1108 003425824015 30411 launchercpp137 forked child with pid 4221 for container test_container i1108 003425825438 30408 containerizercpp571 fetching uris for container test_container using command homejenkinsjenkinsslaveworkspacemesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebuibuildsrcmesosfetcher i1108 003425984254 30419 containerizercpp1117 executor for container test_container has exited i1108 003425984341 30419 containerizercpp946 destroying container test_container srctestscontainerizer_testscpp487 failure value of osreadpathjoindirectory stderrget actual i1108 003425872990 4224 loggingcpp177 logging to stderrnthis is stderrn expected errmsg  n which is this is stderrn  failed  mesoscontainerizerexecutetestioredirection 185 ms  1 test from mesoscontainerizerexecutetest 185 ms total code,1
refactor fetcher code in preparation for fetcher cache refactorrearrange fetcherrelated code so that cache functionality can be dropped in one could do both together in one go this is splitting up reviews into smaller chunks it will not immediately be obvious how this change will be used later but it will look betterfactored and still do the exact same thing as before in particular a download routine to be reused several times in launcherfetcher will be factored out and the remainder of fetcherrelated code can be moved from the containerizer realm into fetchercpp,1
concurrency control for fetcher cache having added a uri flag to commandinfo messages in mesos2069 that indicates caching caching files downloaded by the fetcher in a repository now ensure that when a uri is cached it is only ever downloaded once for the same user on the same slave as long as the slave keeps running this even holds if multiple tasks request the same uri concurrently if multiple requests for the same uri occur perform only one of them and reuse the result make concurrent requests for the same uri wait for the one download different uris from different commandinfos can be downloaded concurrently no cache eviction cleanup or failover will be handled for now additional tickets will be filed for these enhancements so dont use this feature in production until the whole epic is complete note that implementing this does not suffice for production use this ticket contains the main part of the fetcher logic though see the epic mesos336 for the rest of the features that lead to a fully functional fetcher cache the proposed general approach is to keep all bookkeeping about what is in which stage of being fetched and where it resides in the slaves mesoscontainerizerprocess so that all concurrent access is disambiguated and controlled by an actor aka libprocess process depends on mesos2056 and mesos2069,8
deprecate statsjson endpoints for master and slave with the introduction of the libprocess metricssnapshot endpoint metrics are now duplicated in the master and slave between this and statsjson we should deprecate the statsjson endpoints manual inspection of statsjson shows that all metrics are now covered by the new endpoint for master and slave,1
add inverseoffer protobuf message inverseoffer was defined as part of the maintenance work in mesos1474 design doc here httpsdocsgooglecomdocumentd16k0lvwpsgvoyxpsyxkmgcgbnmrlisnee4pfausojkedituspsharing code   a request to return some resources occupied by a framework  message inverseoffer  required offerid id  1 required frameworkid framework_id  2  a list of resources being requested back from the framework repeated resource resources  3  specified if the resources need to be released from a particular slave optional slaveid slave_id  4  the resources in this inverseoffer are part of a planned maintenance  schedule in the specified window any tasks running using these  resources may be killed when the window arrives optional interval unavailability  5  code this ticket is to capture the addition of the inverseoffer protobuf to mesosproto the necessary api changes for eventcall and the language bindings will be tracked separately,3
add inverseoffer to eventcall api the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos mesos1474 one way to add this is to tack it on to the offers event code message offers  repeated offer offers  1 repeated inverseoffer inverse_offers  2  code,3
add inverseoffer to c scheduler api the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos mesos1474 one way to add these to the c scheduler api is to add a new callback code virtual void inverseresourceoffers schedulerdriver driver const stdvectorinverseoffer inverseoffers  0 code libmesos compatibility will need to be figured out here we may want to leave the c binding untouched in favor of eventcall in order to not break api compatibility for schedulers,5
add inverseoffer to java scheduler api the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos mesos1474 one way to add these to the java scheduler api is to add a new callback code void inverseresourceoffers schedulerdriver driver listinverseoffer inverseoffers code jar  libmesos compatibility will need to be figured out here we may want to leave the java binding untouched in favor of eventcall in order to not break api compatibility for schedulers,5
add inverseoffer to python scheduler api the initial use case for inverseoffer in the framework api will be the maintenance primitives in mesos mesos1474 one way to add these to the python scheduler api is to add a new callback code def inverseresourceoffersself driver inverse_offers code egg  libmesos compatibility will need to be figured out here we may want to leave the python binding untouched in favor of eventcall in order to not break api compatibility for schedulers,5
add optional unavailability to resource offers to provide maintenance awareness in order to inform frameworks about upcoming maintenance on offered resources per mesos1474 wed like to add an optional unavailability information to offers code message interval  optional double start  1  time in seconds since the epoch optional double duration  2  time in seconds  message offer   existing fields   signifies that the resources in this offer are part of a planned  maintenance schedule in the specified window any tasks launched  using these resources may be killed when the window arrives  this field gives additional information about the maintenance  the maintenance may not necessarily start at exactly at this interval  nor last for exactly the duration of this interval optional interval unavailability  9  code,3
add http api to the master for maintenance operations based on mesos1474 wed like to provide an http api on the master for the maintenance primitives in mesos for the mvp well want something like this for manipulating the schedule code maintenanceschedule get  returns the schedule which will include the various maintenance windows post  create or update the schedule with a json blob see below maintenancestatus get  returns a list of machines and their maintenance mode maintenancestart post  transition a set of machines from draining into deactivated mode maintenancestop post  transition a set of machines from deactivated into normal mode maintenanceconsensus  not sure what the right name is matrix acceptance get  returns the latest info on which frameworks have accepted or declined the maintenance schedule code note the slashes in urls might not be supported yet a schedule might look like code  windows    machines    ip  19216801   hostname  localhost    unavailability   start  12345  epoch seconds duration  1000  seconds      code there should be firewall settings such that only those with access to master can use these endpoints,8
basic fetcher cache functionality add a flag to commandinfo uri protobufs that indicates that files downloaded by the fetcher shall be cached in a repository to be followed by mesos2057 for concurrency control also see mesos336 for the overall goals for the fetcher cache,8
implement simple slave recovery behavior for fetcher cache clean the fetcher cache completely upon slave restartrecovery this implements correct albeit not ideal behavior more efficient schemes that restore knowledge about cached files or even resume downloads can be added later,2
fetcher cache eviction delete files from the fetcher cache so that a given cache size is never exceeded succeed in doing so while concurrent downloads are on their way and new requests are pouring in idea measure the size of each download before it begins make enough room before the download this means that only download mechanisms that divulge the size before the main download will be supported afawk those in use so far have this property the calculation of how much space to free needs to be under concurrency control accumulating all space needed for competing incomplete download requests the python script that performs fetcher caching for aurora does not seem to implement this see httpsgistgithubcomzmanjif41df77510ef9d00265a imagine several of these programs running concurrently each ones _cache_eviction call succeeding each perceiving the same free space being available ultimately a conflict resolution strategy is needed if just the downloads underway already exceed the cache capacity then as a fallback direct download into the work directory will be used for some tasks tbd how to pick which task gets treated how at first only support copying of any downloaded files to the work directory for task execution this isolates the task life cycle after starting a task from cache eviction considerations later we can add symbolic links that avoid copying but then eviction of fetched files used by ongoing tasks must be blocked which adds complexity another future extension is mesos1667 extract from uri while downloading into work dir,8
fetcher cache test fixture to accelerate providing good test coverage for the fetcher cache mesos336 we can provide a framework that canonicalizes creating and running a number of tasks and allows easy parametrization with combinations of the following  whether to cache or not  whether make what has been downloaded executable or not  whether to extract from an archive or not  whether to download from a file system http or we can create a simple hhtp server in the test fixture to support the latter furthermore the tests need to be robust wrt varying numbers of statusupdate messages an accumulating update message sink that reports the final state is needed all this has already been programmed in this patch just needs to be rebased httpsreviewsapacheorgr21316,5
add maintenance information to the replicated registry to achieve faulttolerance for the maintenance primitives we will need to add the maintenance information to the registry the registry currently stores all of the slave information which is quite large  17mb for 50000 slaves from my testing which results in a protobuf object that is extremely expensive to copy as far as i can tell reads  writes to maintenance information is independent of reads  writes to the existing registry information so there are two approach here h4 add maintenance information to maintenance key  the advantage of this approach is that we dont further grow the large registry object  this approach assumes that writes to maintenance are independent of writes to the registry if these writes are not independent this approach requires that we add transactional support to the state abstraction  this approach requires adding compaction to logstorage  this approach likely requires some refactoring to the registrar h4 add maintenance information to registry key this is the chosen method  the advantage of this approach is that its the easiest to implement  this will further grow the single registry object but doesnt preclude it being split apart in the future  this approach may require using the diff support in logstorage andor adding compression support to logstorage snapshots to deal with the increased size of the registry,13
implement maintenance primitives in the master the master will need to do a number of things to implement the maintenance primitives  for machines that have a maintenance window  disambiguate machines to agents  for unused resources offers must be augmented with an unavailability  for used resources inverse offers must be sent  for inverse offers  filter them before sending them again  for declined inverse offers do something with the reason store or log  recover the maintenance information upon failover note some amount of this logic will need to be placed in the allocator,13
ensure that task_losts for a hard slave drain sigusr1 include a reason for maintenance sometimes operators will force the drain of a slave via sigusr1 when deemed safe eg noncritical tasks running andor necessary eg bad hardware to eliminate alerting noise wed like to add a reason that expresses the forced drain of the slave so that these are not considered to be a generic slave removal task_lost,3
scheduler driver may ack status updates when the scheduler threw an exception vinodkone discovered that this can happen if the scheduler calls schedulerdriverstop before or while handling schedulerstatusupdate in srcschedschedcpp the driver invokes statusupdate and later checks the aborted flag to determine whether to send an ack code void statusupdate const upid from const statusupdate update const upid pid   schedulerstatusupdatedriver status vlog1  schedulerstatusupdate took   stopwatchelapsed  note that we need to look at the volatile aborted here to  so that we dont acknowledge the update if the driver was  aborted during the processing of the update if aborted  vlog1  not sending status update acknowledgment message because   the driver is aborted return   code in srcjavajniorg_apache_mesos_mesosschedulerdrivercpp the statusupdate implementation checks for an exception and invokes driverabort code void jnischedulerstatusupdateschedulerdriver driver const taskstatus status  jvmattachcurrentthreadjnienv_castenv null jclass clazz  envgetobjectclassjdriver jfieldid scheduler  envgetfieldidclazz scheduler lorgapachemesosscheduler jobject jscheduler  envgetobjectfieldjdriver scheduler clazz  envgetobjectclassjscheduler  schedulerstatusupdatedriver status jmethodid statusupdate  envgetmethodidclazz statusupdate lorgapachemesosschedulerdriver lorgapachemesosprotostaskstatusv jobject jstatus  converttaskstatusenv status envexceptionclear envcallvoidmethodjscheduler statusupdate jdriver jstatus if envexceptioncheck  envexceptiondescribe envexceptionclear jvmdetachcurrentthread driverabort return  jvmdetachcurrentthread  code in srcschedschedcpp the abort implementation exits early if status  driver_running and does not set the aborted flag code status mesosschedulerdriverabort  lock lockmutex if status  driver_running  return status  checkprocess  null  we set the volatile aborted to true here to prevent any further  messages from being processed in the schedulerprocess however  if abort is called from another thread as the schedulerprocess  there may be at most one additional message processed  todobmahler use an atomic boolean processaborted  true  dispatching here ensures that we still process the outstanding  requests from the scheduler since those do proceed when  aborted is true dispatchprocess schedulerprocessabort return status  driver_aborted  code as a result the code will ack despite an exception being thrown,3
add master metrics for maintenance well need metrics in order to gain visibility into the maintenance functionality this will also allow operators to add alerting on these metrics in particular  number of scheduled hosts  number of active windows  number of expired windows  number of successful drains  number of failed drains as an example of an alert guideline we would want to know the number of expired windows as a gauge to ensure that it is not growing excessively this allows alerting to catch when operators are not properly unscheduling maintenance once it is complete,3
add safety constraints for maintenance primitives in order to ensure that the maintenance primitives can be used safely by operators we want to put a few safety mechanisms in place some ideas from the design dochttpsdocsgooglecomatwittercomdocumentd16k0lvwpsgvoyxpsyxkmgcgbnmrlisnee4pfausojk  prevent bad schedules from being constructed schedules with more than x overlap in slaves are rejected  prevent bad maintenance from proceeding unchecked if x of the slaves are not being unscheduled or are not reregistering cancel the schedule these will likely be configurable via flags,8
update the webui to include maintenance information the simplest thing here would probably be to include another tab in the header for maintenance information we could also consider adding maintenance information inline to the slaves table depending on how this is done the maintenance tab could actually be a subset of the slaves table only those slaves for which there is maintenance information,5
add documentation for maintenance primitives we should provide some guiding documentation around the upcoming maintenance primitives in mesos specifically we should ensure that general users framework developers and operators understand the notion of maintenance in mesos some guidance and recommendations for the latter two audiences will be necessary,8
add support encrypted and nonencrypted communication in parallel for cluster upgrade during cluster upgrade from nonencrypted to encrypted communication we need to support an interim where 1 a master can have connections to both encrypted and nonencrypted slaves 2 a slave that supports encrypted communication connects to a master that has not yet been upgraded 3 frameworks are encrypted but the master has not been upgraded yet 4 master has been upgraded but frameworks havent 5 a slave process has upgraded but running executor processes havent,13
update resource protobuf with diskinfo noformat message resource  required string name  1 required valuetype type  2 optional valuescalar scalar  3 optional valueranges ranges  4 optional valueset set  5 optional string role  6 default    used for describing persistent disk resource message diskinfo   a unique identifier for the persistent disk resource the id  needs to be unique within a role for a slave required string id  1  the volume mapping for the persistent disk resource required volume volume  2  optional diskinfo disk  8  noformat,1
update task validation to be after task authorization so that we can simply the task validation because we no longer need to check with pendingtasks,3
support acquiringreleasing resources with diskinfo in allocator the allocator needs to be changed because the resources are changing while we acquiring or releasing persistent disk resources resources with diskinfo for example when we release a persistent disk resource we are changing the release with diskinfo to a resource with the diskinfo,8
implement master to slave protocol for persistent disk resources we need to do the following 1 slave needs to send persisted resources when registering or reregistering 2 master needs to send total persisted resources to slave by either reusing runtaskupdateframeworkinfo or introduce new type of messages like updateresources,8
add the persistent resources release primitive to the framework api we are thinking about introducing a release protobuf message which specifies persistent disk resources w diskinfo to release the release message could be piggybacked on the launchdecline message this probably will overlap with the dynamic reservation work mesos2018,3
expose number of processes and threads in a container the cfs cpu statistics cpus_nr_throttled cpus_nr_periods cpus_throttled_time are difficult to interpret 1 nr_throttled is the number of intervals where any throttling occurred 2 throttled_time is the aggregate time across all runnable tasks tasks in the linux sense for example in a typical 60 second sampling interval nr_periods  600 nr_throttled could be 60 ie 10 of intervals but throttled_time could be much higher than 60600  60  6 seconds if there is more than one task that is runnable but throttled each throttled task contributes to the total throttled time small test to demonstrate throttled_time  nr_periods  quota_interval 5 x openssl speed running with quota100ms noformat cat cpustat  sleep 1  cat cpustat nr_periods 3228 nr_throttled 1276 throttled_time 528843772540 nr_periods 3238 nr_throttled 1286 throttled_time 531668964667 noformat all 10 intervals throttled 100 for total time of 28 seconds in 1 second more than 100 of the time interval it would be helpful to expose the number of processes and tasks in the container cgroup this would be at a very coarse granularity but would give some guidance,2
correct naming of cgroup memory statistics mem_rss_bytes is not rss but is the total memory usage memoryusage_in_bytes of the cgroup including file cache etc actual rss is reported as mem_anon_bytes these and others should be consistently named,3
add configure flag or environment variable to enable ssllibevent socket,1
configurable ping timeouts after a series of pingfailures the master considers the slave lost and calls shutdownslave requiring such a slave that reconnects to kill its tasks and reregister as a new slaveid on the other side after a similar timeout the slave will consider the master lost and try to detect a new master these timeouts are currently hardcoded constants 5  15s which may not be wellsuited for all scenarios  some clusters may tolerate a longer slave process restart period and wouldnt want tasks to be killed upon reconnect  some clusters may have higherlatency networks eg crossdatacenter or for volunteer computing efforts and would like to tolerate longer periods without communication we should provide flagsmechanisms on the master to control its tolerance for noncommunicative slaves and less importantly on the slave to tolerate missing masters,8
add socket tests add more socket specific tests to get coverage while doing libev to libevent w and wo ssl move,5
document changes in c resources api in changelog with the refactor introduced in mesos1974 we need to document those api changes in changelog,2
killtask should perform reconciliation for unknown tasks currently killtask uses its own reconciliation logic which has diverged from the reconciletasks logic specifically when the task is unknown and a nonstrict registry is in use killtask will not send task_lost whereas reconciletask will we should make these consistent,3
turning on cgroups_limit_swap effectively disables memory isolation our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether per httpsaccessredhatcomdocumentationenusred_hat_enterprise_linux6htmlresource_management_guidesecmemoryhtml it is important to set the memorylimit_in_bytes parameter before setting the memorymemswlimit_in_bytes parameter attempting to do so in the reverse order results in an error this is because memorymemswlimit_in_bytes becomes available only after all memory limitations previously set in memorylimit_in_bytes are exhausted looks like the flag sets memorymemswlimit_in_bytes if true and memorylimit_in_bytes if false but should always set memorylimit_in_bytes and in addition set memorymemswlimit_in_bytes if true otherwise the limits wont be set and enforced see httpsgithubcomapachemesosblobc8598f7f5a24a01b6a68e0f060b79662ee97af89srcslavecontainerizerisolatorscgroupsmemcppl365,2
support diskinfo in c resources we need to change the following functions 1 addable 2 subtractable 3 validate we probably shouldnt add two disk resources with the same persistence id because they must come from different namespaces we can add more checks in the validate functions for protobufs,3
expose percgroup memory pressure the cgroup memory controller can provide information on the memory pressure of a cgroup this is in the form of an event based notification where events of low medium critical are generated when the kernel makes specific actions to allocate memory this signal is probably more informative than comparing memory usage to memory limit,5
enable the master to handle reservation operations masters _accept function currently only handles create and destroy operations which exist for persistent volumes we need to handle the reserve and unreserve operations for dynamic reservations as well in addition we need to add validate functions for the reservation operations,5
segmentation fault in examplestestlowlevelschedulerpthread occured on review bot review of httpsreviewsapacheorgr28262review62333 the review doesnt touch code related to the test and doesnt break libprocess in general  run  examplestestlowlevelschedulerpthread srctestsscriptcpp83 failure failed low_level_scheduler_pthread_testsh terminated with signal segmentation fault  failed  examplestestlowlevelschedulerpthread 7561 ms the test,8
large number of connections slows statisticsjson responses we observed that in our production environment with network monitoring being turned on if there are many connections  104 in a container getting socket information is expensive it might take 1min to process all the socket information one of the reason is that the library we are using libnl is not so optimized cong wang has already submitted a patch httplistsinfradeadorgpipermaillibnl2014november001715html,2
add masterslaves and masterframeworksframeworktaskstask endpoints masterstatejson exports the entire state of the cluster and can for large clusters become massive tens of megabytes of json often a client only need information about subsets of the entire state for example all connected slaves or information registration info tasks etc belonging to a particular framework we can partition statejson into many smaller endpoints but for starters being able to get slave information and tasks information per framework would be useful,5
perfeventisolatortestroot_cgroups_sample requires perf to be installed the perfvalid relies on the perf command being installed this isnt always the case configure should probably check for the perf command exists,1
hierarchical allocator inconsistently accounts for reserved resources looking through the allocator code for mesos2099 i see an issue with respect to accounting reserved resources in the sorters within hierarchicalallocatorprocessallocate only unreserved resources are accounted for in the sorters whereas everywhere else addremove framework addremove slave we account for both reserved and unreserved from git blame it looks like this issue was introduced over a long course of refactoring and fixes to the allocator my guess is that this was never caught due to the lack of unittestability of the allocator unnecessarily requires a master pid to use an allocator from my understanding the two levels of the hierarchical sorter should have the following semantics  level 1 sorts across roles only unreserved resources are shared across roles and therefore the role sorter for level 1 should only account for the unreserved resource pool  level 2 sorts across frameworks within a role both unreserved and reserved resources are shared across frameworks within a role and therefore the framework sorters for level 2 should each account for the reserved resource pool for the role as well as the unreserved resources _allocated_ inside the role,5
performance issue in libprocess socketmanager noticed an issue in production under which the master is slow to respond after failover for 15 minutes after looking at some perf data the top offender is noformat 1202 mesosmaster libmesos0210rc3so  std_rb_treeprocessprocessbase processprocessbase std_identityprocessprocessbase stdlessprocessprocessbase stdallocatorprocessprocessbase eraseprocessprocessbase const  329 mesosmaster libmesos0210rc3so  processsocketmanagerexitedprocessprocessbase noformat it appears that in the socketmanager whenever an internal process exits we loop over all the links unnecessarily code void socketmanagerexitedprocessbase process   an exited event is enough to cause the process to get deleted  eg by the garbage collector which means we cant  dereference process or even use the address after we enqueue at  least one exited event thus we save the process pid const upid pid  processpid  likewise we need to save the current time of the process so we  can update the clocks of linked processes as appropriate const time time  clocknowprocess synchronized this   iterate through the links removing any links the process might  have had and creating exited events for any linked processes foreachpair const upid linkee setprocessbase processes links  processeseraseprocess if linkee  pid  foreach processbase linker processes  checklinker  process  process linked with itself synchronized timeouts  if clockpaused  clockupdatelinker time   linkerenqueuenew exitedeventlinkee    linkserasepid   code on clusters with 10000s of slaves this means we hold the socket manager lock for a very expensive loop erasing nothing from a set this is because the master contains links from the master process to each slave however when a random ephemeral process terminates we dont need to loop over each slave link while we hold this lock the following calls will block code class socketmanager  public socket acceptedint s void linkprocessbase process const upid to pidhttpproxy proxyconst socket socket void sendencoder encoder bool persist void sendconst response response const request request const socket socket void sendmessage message encoder nextint s void closeint s void exitedconst node node void exitedprocessbase process  code as a result the slave observers and the master can block calling send short term we will try to fix this issue by removing the unnecessary looping longer term it would be nice to avoid all this locking when sending on independent sockets,3
deprecate unused flag cgroups_subsystems cgroups_subsystems is a slave flag that is no longer used and should be deprecated,1
add containerid to the taskstatus message taskstatus provides the frameworks with certain information executorid slaveid etc which is useful when collecting statistics about cluster performance however it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself therefore it would be good to provide the framework scheduler with this information adding a new field in the taskstatus message see comments for a use case,3
failing test slavetestroot_runtaskwithcommandinfowithuser appears that running the executor as nobody is not supported nnielsen can you take a look executor log noformat roothostname build cat tmpslavetest_root_runtaskwithcommandinfowithuser_cxf1dyslaves2014121900520620811701866048711862s0frameworks20141219005206208117018660 487118620000executors1runslateststd sh homeidownesworkspacemesosbuildsrcmesosexecutor permission denied noformat test output noformat  running 1 test from 1 test case  global test environment setup  1 test from slavetest  run  slavetestroot_runtaskwithcommandinfowithuser srctestsslave_testscpp680 failure value of statusrunninggetstate actual task_failed expected task_running srctestsslave_testscpp682 failure failed to wait 10secs for statusfinished srctestsslave_testscpp673 failure actual function call count doesnt match expect_callsched statusupdatedriver _ expected to be called twice actual called once  unsatisfied and active  failed  slavetestroot_runtaskwithcommandinfowithuser 10641 ms  1 test from slavetest 10641 ms total  global test environment teardown  1 test from 1 test case ran 10658 ms total noformat,2
bogus docker images result in bad error message to scheduler when a scheduler specifies a bogus image in containerinfo mesos doesnt tell the scheduler that the docker pull failed or why this error is logged in the mesosslave log but it isnt given to the scheduler as far as i can tell noformat e1218 235055406230 8123 slavecpp2730 container 8f70784c3e4040729ca29daed23f15ff for executor thermos1418946354013xxxxxxcurl0f500cc41dd0a43388cbcd631cb588bb1 of framework 2014052221314517490045615050295120000 failed to start failed to docker pull dockerregistryexamplecomdoesntexisthello11latest exit status  exited with status 1 stderr  20141218 235055 error image doesntexisthello11 not found noformat if the docker image is not in the registry the scheduler should give the user an error message if docker pull failed because of networking issues it should be retried mesos should give the scheduler enough information to be able to make that decision,2
replicatestrestore fails with leveldb greater than v17 i wanted to configure mesos with system provided leveldb libraries when i ran into this issue apparently if one does configure withleveldbpathtoleveldb compilation succeeds however the replicatest_restore test fails with the following back trace code  run  replicatestrestore using temporary directory tmpreplicatest_restore_izbbrr i1222 141649517500 2927 leveldbcpp176 opened db in 10758917ms i1222 141649526495 2927 leveldbcpp183 compacted db in 8931146ms i1222 141649526523 2927 leveldbcpp198 created db iterator in 5787ns i1222 141649526531 2927 leveldbcpp204 seeked to beginning of db in 511ns i1222 141649526535 2927 leveldbcpp273 iterated through 0 keys in the db in 197ns i1222 141649526623 2927 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1222 141649530972 2945 leveldbcpp306 persisting metadata 8 bytes to leveldb took 3084458ms i1222 141649531008 2945 replicacpp320 persisted replica status to voting i1222 141649541263 2927 leveldbcpp176 opened db in 9980586ms i1222 141649551636 2927 leveldbcpp183 compacted db in 10348096ms i1222 141649551683 2927 leveldbcpp198 created db iterator in 3405ns i1222 141649551693 2927 leveldbcpp204 seeked to beginning of db in 3559ns i1222 141649551728 2927 leveldbcpp273 iterated through 1 keys in the db in 29722ns i1222 141649551751 2927 replicacpp741 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1222 141649551996 2947 replicacpp474 replica received implicit promise request with proposal 1 i1222 141649560921 2947 leveldbcpp306 persisting metadata 8 bytes to leveldb took 8899591ms i1222 141649560940 2947 replicacpp342 persisted promised to 1 i1222 141649561338 2943 replicacpp508 replica received write request for position 1 i1222 141649568677 2943 leveldbcpp343 persisting action 27 bytes to leveldb took 7287155ms i1222 141649568692 2943 replicacpp676 persisted action at 1 i1222 141649569042 2942 leveldbcpp438 reading position from leveldb took 26339ns f1222 141649569411 2927 replicacpp721 check_somestate io error lock tmpreplicatest_restore_izbbrrloglock already held by process failed to recover the log  check failure stack trace   0x7f7f6c53e688 googlelogmessagefail  0x7f7f6c53e5e7 googlelogmessagesendtolog  0x7f7f6c53dff8 googlelogmessageflush  0x7f7f6c540d2c googlelogmessagefatallogmessagefatal  0x90a520 _checkfatal_checkfatal  0x7f7f6c400f4d mesosinternallogreplicaprocessrestore  0x7f7f6c3fd763 mesosinternallogreplicaprocessreplicaprocess  0x7f7f6c401271 mesosinternallogreplicareplica  0xcd7ca3 replicatest_restore_testtestbody  0x10934b2 testinginternalhandlesehexceptionsinmethodifsupported  0x108e584 testinginternalhandleexceptionsinmethodifsupported  0x10768fd testingtestrun  0x1077020 testingtestinforun  0x10775a8 testingtestcaserun  0x107c324 testinginternalunittestimplrunalltests  0x1094348 testinginternalhandlesehexceptionsinmethodifsupported  0x108f2b7 testinginternalhandleexceptionsinmethodifsupported  0x107b1d4 testingunittestrun  0xd344a9 main  0x7f7f66fdfb45 __libc_start_main  0x8f3549 unknown  nil unknown 2 2927 abort core dumped glog_logtostderr1 gtest_v10 binmesostestssh verbose code the bundled version of leveldb is v14 i tested version 15 and that seems to work however v16 had some build issues and us unusable with mesos the next version v17 allows mesos to compile fine but results in the above error,3
add user documentation for reservations add a user guide for reservations which describes basic usage of them how acls are used to specify who can unreserve whose resources and few advanced usage cases,2
disallow special characters in role as we introduce persistent volumes in mesos1524 we will use roles as directory names on the slave httpsreviewsapacheorgr28562 as a result the master should disallow special characters like space and slash in role,2
the docker containerizer attempts to recover any task when checkpointing is enabled not just docker tasks once the slave restarts and recovers the task i see this error in the log for all tasks that were recovered every second or so note these were not docker tasks w0113 160100790323 773142 monitorcpp213 failed to get resource usage for container 7b729b89dc7e4d08af978cd1af560a21 for executor thermos1421085237813slipstreamprodagent38f7695141835415190d03f55dcc940dd of framework 2015010916171371535028250502907970000 failed to docker inspect mesos7b729b89dc7e4d08af978cd1af560a21 exit status  exited with status 1 stderr  error no such image or container mesos7b729b89dc7e4d08af978cd1af560a21 however the tasks themselves are still healthy and running the slave was launched with containerizersmesosdocker  more info it looks like the docker containerizer is a little too ambitious about recovering containers again this was not a docker task i0113 155959476145 773142 dockercpp814 recovering container 7b729b89dc7e4d08af978cd1af560a21 for executor thermos1421085237813slipstreamprodagent38f7695141835415190d03f55dcc940dd of framework 2015010916171371535028250502907970000 looking into the source it looks like the problem is that the composingcontainerize runs recover in parallel but neither the docker containerizer nor mesos containerizer check if they should recover the task or not because they were the ones that launched it perhaps this needs to be written into the checkpoint somewhere,8
add acls for the maintenance http endpoints in order to authorize the http endpoints for maintenance to be added in mesos2067 we will need to add an acl definition for performing maintenance operations,3
faulttolerancetestreregisterframeworkexitedexecutor is flaky observed this on internal ci code  run  faulttolerancetestreregisterframeworkexitedexecutor using temporary directory tmpfaulttolerancetest_reregisterframeworkexitedexecutor_ynprki i0114 185051461186 4720 leveldbcpp176 opened db in 4866948ms i0114 185051462057 4720 leveldbcpp183 compacted db in 472256ns i0114 185051462514 4720 leveldbcpp198 created db iterator in 42905ns i0114 185051462784 4720 leveldbcpp204 seeked to beginning of db in 21630ns i0114 185051463068 4720 leveldbcpp273 iterated through 0 keys in the db in 19967ns i0114 185051463485 4720 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0114 185051464555 4737 recovercpp449 starting replica recovery i0114 185051465188 4737 recovercpp475 replica is in empty status i0114 185051467324 4741 replicacpp641 replica in empty status received a broadcasted recover request i0114 185051470118 4736 recovercpp195 received a recover response from a replica in empty status i0114 185051475424 4739 recovercpp566 updating replica status to starting i0114 185051476553 4739 leveldbcpp306 persisting metadata 8 bytes to leveldb took 107545ns i0114 185051476862 4739 replicacpp323 persisted replica status to starting i0114 185051477309 4739 recovercpp475 replica is in starting status i0114 185051479109 4734 replicacpp641 replica in starting status received a broadcasted recover request i0114 185051481274 4738 recovercpp195 received a recover response from a replica in starting status i0114 185051482324 4738 recovercpp566 updating replica status to voting i0114 185051482913 4738 leveldbcpp306 persisting metadata 8 bytes to leveldb took 66011ns i0114 185051483186 4738 replicacpp323 persisted replica status to voting i0114 185051483608 4738 recovercpp580 successfully joined the paxos group i0114 185051484031 4738 recovercpp464 recover process terminated i0114 185051554949 4734 mastercpp262 master 201501141850512272962752570184720 fedora19 started on 19216812213557018 i0114 185051555785 4734 mastercpp308 master only allowing authenticated frameworks to register i0114 185051556046 4734 mastercpp313 master only allowing authenticated slaves to register i0114 185051556426 4734 credentialshpp36 loading credentials for authentication from tmpfaulttolerancetest_reregisterframeworkexitedexecutor_ynprkicredentials i0114 185051557003 4734 mastercpp357 authorization enabled i0114 185051558007 4737 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0114 185051558521 4741 whitelist_watchercpp65 no whitelist given i0114 185051562185 4734 mastercpp1219 the newly elected leader is master19216812213557018 with id 201501141850512272962752570184720 i0114 185051562680 4734 mastercpp1232 elected as the leading master i0114 185051562950 4734 mastercpp1050 recovering from registrar i0114 185051564506 4736 registrarcpp313 recovering registrar i0114 185051566162 4737 logcpp660 attempting to start the writer i0114 185051568691 4741 replicacpp477 replica received implicit promise request with proposal 1 i0114 185051569154 4741 leveldbcpp306 persisting metadata 8 bytes to leveldb took 106885ns i0114 185051569504 4741 replicacpp345 persisted promised to 1 i0114 185051573277 4740 coordinatorcpp230 coordinator attemping to fill missing position i0114 185051575623 4739 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0114 185051576133 4739 leveldbcpp343 persisting action 8 bytes to leveldb took 86360ns i0114 185051576449 4739 replicacpp679 persisted action at 0 i0114 185051586966 4736 replicacpp511 replica received write request for position 0 i0114 185051587666 4736 leveldbcpp438 reading position from leveldb took 60621ns i0114 185051588043 4736 leveldbcpp343 persisting action 14 bytes to leveldb took 81094ns i0114 185051588374 4736 replicacpp679 persisted action at 0 i0114 185051589418 4736 replicacpp658 replica received learned notice for position 0 i0114 185051590428 4736 leveldbcpp343 persisting action 16 bytes to leveldb took 106648ns i0114 185051590840 4736 replicacpp679 persisted action at 0 i0114 185051591104 4736 replicacpp664 replica learned nop action at position 0 i0114 185051592260 4734 logcpp676 writer started with ending position 0 i0114 185051594172 4739 leveldbcpp438 reading position from leveldb took 52163ns i0114 185051600744 4736 registrarcpp346 successfully fetched the registry 0b in 35968us i0114 185051601646 4736 registrarcpp445 applied 1 operations in 184502ns attempting to update the registry i0114 185051604329 4737 logcpp684 attempting to append 130 bytes to the log i0114 185051604966 4737 coordinatorcpp340 coordinator attempting to write append action at position 1 i0114 185051606449 4737 replicacpp511 replica received write request for position 1 i0114 185051606937 4737 leveldbcpp343 persisting action 149 bytes to leveldb took 84877ns i0114 185051607199 4737 replicacpp679 persisted action at 1 i0114 185051611934 4741 replicacpp658 replica received learned notice for position 1 i0114 185051612423 4741 leveldbcpp343 persisting action 151 bytes to leveldb took 113059ns i0114 185051612794 4741 replicacpp679 persisted action at 1 i0114 185051613056 4741 replicacpp664 replica learned append action at position 1 i0114 185051614598 4741 logcpp703 attempting to truncate the log to 1 i0114 185051615157 4741 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0114 185051616458 4737 replicacpp511 replica received write request for position 2 i0114 185051616902 4737 leveldbcpp343 persisting action 16 bytes to leveldb took 71716ns i0114 185051617168 4737 replicacpp679 persisted action at 2 i0114 185051618505 4740 replicacpp658 replica received learned notice for position 2 i0114 185051619031 4740 leveldbcpp343 persisting action 18 bytes to leveldb took 78481ns i0114 185051619567 4740 leveldbcpp401 deleting 1 keys from leveldb took 59638ns i0114 185051619832 4740 replicacpp679 persisted action at 2 i0114 185051620101 4740 replicacpp664 replica learned truncate action at position 2 i0114 185051621757 4736 registrarcpp490 successfully updated the registry in 1978496ms i0114 185051622658 4736 registrarcpp376 successfully recovered registrar i0114 185051623261 4736 mastercpp1077 recovered 0 slaves from the registry 94b  allowing 10mins for slaves to reregister i0114 185051670349 4739 slavecpp173 slave started on 11519216812213557018 i0114 185051671133 4739 credentialshpp84 loading credential for authentication from tmpfaulttolerancetest_reregisterframeworkexitedexecutor_onrvugcredential i0114 185051671685 4739 slavecpp282 slave using credential for testprincipal i0114 185051672245 4739 slavecpp300 slave resources cpus2 mem1024 disk1024 ports3100032000 i0114 185051673360 4739 slavecpp329 slave hostname fedora19 i0114 185051673660 4739 slavecpp330 slave checkpoint false w0114 185051674052 4739 slavecpp332 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0114 185051677234 4737 statecpp33 recovering state from tmpfaulttolerancetest_reregisterframeworkexitedexecutor_onrvugmeta i0114 185051684973 4739 status_update_managercpp197 recovering status update manager i0114 185051687644 4739 slavecpp3519 finished recovery i0114 185051688698 4737 slavecpp613 new master detected at master19216812213557018 i0114 185051688902 4734 status_update_managercpp171 pausing sending status updates i0114 185051689482 4737 slavecpp676 authenticating with master master19216812213557018 i0114 185051689910 4737 slavecpp681 using default crammd5 authenticatee i0114 185051690577 4741 authenticateehpp138 creating new client sasl connection i0114 185051691453 4737 slavecpp649 detecting new master i0114 185051691864 4741 mastercpp4130 authenticating slave11519216812213557018 i0114 185051692369 4741 mastercpp4141 using default crammd5 authenticator i0114 185051693208 4741 authenticatorhpp170 creating new server sasl connection i0114 185051694598 4738 authenticateehpp229 received sasl authentication mechanisms crammd5 i0114 185051694893 4738 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0114 185051695329 4741 authenticatorhpp276 received sasl authentication start i0114 185051695641 4741 authenticatorhpp398 authentication requires more steps i0114 185051696028 4736 authenticateehpp275 received sasl authentication step i0114 185051696486 4741 authenticatorhpp304 received sasl authentication step i0114 185051696753 4741 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0114 185051697041 4741 auxpropcpp171 looking up auxiliary property userpassword i0114 185051697343 4741 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0114 185051697685 4741 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0114 185051697998 4741 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0114 185051698251 4741 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0114 185051698580 4741 authenticatorhpp390 authentication success i0114 185051698927 4735 authenticateehpp315 authentication success i0114 185051705123 4741 mastercpp4188 successfully authenticated principal testprincipal at slave11519216812213557018 i0114 185051705847 4720 schedcpp151 version 0220 i0114 185051707159 4736 schedcpp248 new master detected at master19216812213557018 i0114 185051707523 4736 schedcpp304 authenticating with master master19216812213557018 i0114 185051707792 4736 schedcpp311 using default crammd5 authenticatee i0114 185051708412 4736 authenticateehpp138 creating new client sasl connection i0114 185051709316 4735 slavecpp747 successfully authenticated with master master19216812213557018 i0114 185051709723 4737 mastercpp4130 authenticating scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051710274 4737 mastercpp4141 using default crammd5 authenticator i0114 185051710739 4735 slavecpp1075 will retry registration in 17028024ms if necessary i0114 185051711304 4737 mastercpp3276 registering slave at slave11519216812213557018 fedora19 with id 201501141850512272962752570184720s0 i0114 185051711459 4738 authenticatorhpp170 creating new server sasl connection i0114 185051713142 4739 registrarcpp445 applied 1 operations in 100530ns attempting to update the registry i0114 185051713465 4738 authenticateehpp229 received sasl authentication mechanisms crammd5 i0114 185051715435 4738 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0114 185051715963 4740 authenticatorhpp276 received sasl authentication start i0114 185051716258 4740 authenticatorhpp398 authentication requires more steps i0114 185051716524 4740 authenticateehpp275 received sasl authentication step i0114 185051716784 4740 authenticatorhpp304 received sasl authentication step i0114 185051716979 4740 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0114 185051717139 4740 auxpropcpp171 looking up auxiliary property userpassword i0114 185051717315 4740 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0114 185051717542 4740 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0114 185051717703 4740 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0114 185051717864 4740 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0114 185051718040 4740 authenticatorhpp390 authentication success i0114 185051718292 4740 authenticateehpp315 authentication success i0114 185051718454 4738 mastercpp4188 successfully authenticated principal testprincipal at scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051719012 4740 schedcpp392 successfully authenticated with master master19216812213557018 i0114 185051719364 4740 schedcpp515 sending registration request to master19216812213557018 i0114 185051719702 4740 schedcpp548 will retry registration in 746539282ms if necessary i0114 185051719902 4735 mastercpp1417 received registration request for framework default at scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051720232 4735 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0114 185051722206 4735 mastercpp1481 registering framework 2015011418505122729627525701847200000 default at scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051720927 4737 logcpp684 attempting to append 300 bytes to the log i0114 185051722924 4737 coordinatorcpp340 coordinator attempting to write append action at position 3 i0114 185051724269 4737 replicacpp511 replica received write request for position 3 i0114 185051724817 4737 leveldbcpp343 persisting action 319 bytes to leveldb took 116638ns i0114 185051728560 4737 replicacpp679 persisted action at 3 i0114 185051726066 4736 schedcpp442 framework registered with 2015011418505122729627525701847200000 i0114 185051728879 4736 schedcpp456 schedulerregistered took 34885ns i0114 185051725520 4735 hierarchical_allocator_processhpp319 added framework 2015011418505122729627525701847200000 i0114 185051731864 4735 hierarchical_allocator_processhpp839 no resources available to allocate i0114 185051732038 4735 hierarchical_allocator_processhpp746 performed allocation for 0 slaves in 214728ns i0114 185051733106 4738 replicacpp658 replica received learned notice for position 3 i0114 185051733340 4738 leveldbcpp343 persisting action 321 bytes to leveldb took 83165ns i0114 185051733538 4738 replicacpp679 persisted action at 3 i0114 185051733705 4738 replicacpp664 replica learned append action at position 3 i0114 185051735610 4738 registrarcpp490 successfully updated the registry in 21936128ms i0114 185051735805 4739 logcpp703 attempting to truncate the log to 3 i0114 185051736445 4739 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0114 185051737664 4739 replicacpp511 replica received write request for position 4 i0114 185051738013 4739 leveldbcpp343 persisting action 16 bytes to leveldb took 72906ns i0114 185051738255 4739 replicacpp679 persisted action at 4 i0114 185051743397 4734 replicacpp658 replica received learned notice for position 4 i0114 185051743628 4734 leveldbcpp343 persisting action 18 bytes to leveldb took 78832ns i0114 185051743837 4734 leveldbcpp401 deleting 2 keys from leveldb took 63991ns i0114 185051744004 4734 replicacpp679 persisted action at 4 i0114 185051744168 4734 replicacpp664 replica learned truncate action at position 4 i0114 185051745537 4738 mastercpp3330 registered slave 201501141850512272962752570184720s0 at slave11519216812213557018 fedora19 with cpus2 mem1024 disk1024 ports3100032000 i0114 185051745968 4734 hierarchical_allocator_processhpp453 added slave 201501141850512272962752570184720s0 fedora19 with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0114 185051746070 4735 slavecpp781 registered with master master19216812213557018 given slave id 201501141850512272962752570184720s0 i0114 185051751437 4741 status_update_managercpp178 resuming sending status updates i0114 185051752428 4740 mastercpp4072 sending 1 offers to framework 2015011418505122729627525701847200000 default at scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051753764 4740 schedcpp605 schedulerresourceoffers took 751714ns i0114 185051754812 4740 mastercpp2541 processing reply for offers  201501141850512272962752570184720o0  on slave 201501141850512272962752570184720s0 at slave11519216812213557018 fedora19 for framework 2015011418505122729627525701847200000 default at scheduler092fbbec093843558187fb92e5174c6419216812213557018 i0114 185051755040 4740 mastercpp2647 authorizing framework principal testprincipal to launch task 0 as user jenkins w0114 185051756431 4741 mastercpp2124 executor default for task 0 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0114 185051756652 4741 mastercpp2136 executor default for task 0 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0114 185051757284 4741 masterhpp766 adding task 0 with resources cpus1 mem16 on slave 201501141850512272962752570184720s0 fedora19 i0114 185051757733 4734 hierarchical_allocator_processhpp764 performed allocation for slave 201501141850512272962752570184720s0 in 9535066ms i0114 185051758117 4735 slavecpp2588 received ping from slaveobserver9519216812213557018 i0114 185051758630 4741 mastercpp2897 launching task 0 of framework 2015011418505122729627525701847200000 default at scheduler092fbbec093843558187fb92e5174c6419216812213557018 with resources cpus1 mem16 on slave 201501141850512272962752570184720s0 at slave11519216812213557018 fedora19 i0114 185051759526 4741 hierarchical_allocator_processhpp610 updated allocation of framework 2015011418505122729627525701847200000 on slave 201501141850512272962752570184720s0 from cpus2 mem1024 disk1024 ports3100032000 to cpus2 mem1024 disk1024 ports3100032000 i0114 185051759796 4737 slavecpp1130 got assigned task 0 for framework 2015011418505122729627525701847200000 i0114 185051761184 4737 slavecpp1245 launching task 0 for framework 20150,2
hooktestverifyslavelaunchexecutorhook is flaky observed this on internal ci code  run  hooktestverifyslavelaunchexecutorhook using temporary directory tmphooktest_verifyslavelaunchexecutorhook_gjbgme i0114 185134659353 4720 leveldbcpp176 opened db in 1255951ms i0114 185134662112 4720 leveldbcpp183 compacted db in 596090ns i0114 185134662364 4720 leveldbcpp198 created db iterator in 177877ns i0114 185134662719 4720 leveldbcpp204 seeked to beginning of db in 19709ns i0114 185134663010 4720 leveldbcpp273 iterated through 0 keys in the db in 18208ns i0114 185134663312 4720 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0114 185134664266 4735 recovercpp449 starting replica recovery i0114 185134664908 4735 recovercpp475 replica is in empty status i0114 185134667842 4734 replicacpp641 replica in empty status received a broadcasted recover request i0114 185134669117 4735 recovercpp195 received a recover response from a replica in empty status i0114 185134677913 4735 recovercpp566 updating replica status to starting i0114 185134683157 4735 leveldbcpp306 persisting metadata 8 bytes to leveldb took 137939ns i0114 185134683507 4735 replicacpp323 persisted replica status to starting i0114 185134684013 4735 recovercpp475 replica is in starting status i0114 185134685554 4738 replicacpp641 replica in starting status received a broadcasted recover request i0114 185134696512 4736 recovercpp195 received a recover response from a replica in starting status i0114 185134700552 4735 recovercpp566 updating replica status to voting i0114 185134701128 4735 leveldbcpp306 persisting metadata 8 bytes to leveldb took 115624ns i0114 185134701478 4735 replicacpp323 persisted replica status to voting i0114 185134701817 4735 recovercpp580 successfully joined the paxos group i0114 185134702569 4735 recovercpp464 recover process terminated i0114 185134716439 4736 mastercpp262 master 201501141851342272962752570184720 fedora19 started on 19216812213557018 i0114 185134716913 4736 mastercpp308 master only allowing authenticated frameworks to register i0114 185134717136 4736 mastercpp313 master only allowing authenticated slaves to register i0114 185134717488 4736 credentialshpp36 loading credentials for authentication from tmphooktest_verifyslavelaunchexecutorhook_gjbgmecredentials i0114 185134718077 4736 mastercpp357 authorization enabled i0114 185134719238 4738 whitelist_watchercpp65 no whitelist given i0114 185134719755 4737 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0114 185134722584 4736 mastercpp1219 the newly elected leader is master19216812213557018 with id 201501141851342272962752570184720 i0114 185134722865 4736 mastercpp1232 elected as the leading master i0114 185134723310 4736 mastercpp1050 recovering from registrar i0114 185134723760 4734 registrarcpp313 recovering registrar i0114 185134725229 4740 logcpp660 attempting to start the writer i0114 185134727893 4739 replicacpp477 replica received implicit promise request with proposal 1 i0114 185134728425 4739 leveldbcpp306 persisting metadata 8 bytes to leveldb took 114781ns i0114 185134728662 4739 replicacpp345 persisted promised to 1 i0114 185134731271 4741 coordinatorcpp230 coordinator attemping to fill missing position i0114 185134733223 4734 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0114 185134734076 4734 leveldbcpp343 persisting action 8 bytes to leveldb took 87441ns i0114 185134734441 4734 replicacpp679 persisted action at 0 i0114 185134740272 4739 replicacpp511 replica received write request for position 0 i0114 185134740910 4739 leveldbcpp438 reading position from leveldb took 59846ns i0114 185134741672 4739 leveldbcpp343 persisting action 14 bytes to leveldb took 189259ns i0114 185134741919 4739 replicacpp679 persisted action at 0 i0114 185134743000 4739 replicacpp658 replica received learned notice for position 0 i0114 185134746844 4739 leveldbcpp343 persisting action 16 bytes to leveldb took 328487ns i0114 185134747118 4739 replicacpp679 persisted action at 0 i0114 185134747553 4739 replicacpp664 replica learned nop action at position 0 i0114 185134751344 4737 logcpp676 writer started with ending position 0 i0114 185134753504 4734 leveldbcpp438 reading position from leveldb took 61183ns i0114 185134762962 4737 registrarcpp346 successfully fetched the registry 0b in 38907904ms i0114 185134763610 4737 registrarcpp445 applied 1 operations in 67206ns attempting to update the registry i0114 185134766079 4736 logcpp684 attempting to append 130 bytes to the log i0114 185134766769 4736 coordinatorcpp340 coordinator attempting to write append action at position 1 i0114 185134768215 4741 replicacpp511 replica received write request for position 1 i0114 185134768759 4741 leveldbcpp343 persisting action 149 bytes to leveldb took 87970ns i0114 185134768995 4741 replicacpp679 persisted action at 1 i0114 185134770691 4736 replicacpp658 replica received learned notice for position 1 i0114 185134771273 4736 leveldbcpp343 persisting action 151 bytes to leveldb took 83590ns i0114 185134771579 4736 replicacpp679 persisted action at 1 i0114 185134771917 4736 replicacpp664 replica learned append action at position 1 i0114 185134773252 4738 logcpp703 attempting to truncate the log to 1 i0114 185134773756 4735 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0114 185134775552 4736 replicacpp511 replica received write request for position 2 i0114 185134775846 4736 leveldbcpp343 persisting action 16 bytes to leveldb took 71503ns i0114 185134776695 4736 replicacpp679 persisted action at 2 i0114 185134785259 4739 replicacpp658 replica received learned notice for position 2 i0114 185134786252 4737 registrarcpp490 successfully updated the registry in 22340864ms i0114 185134787094 4737 registrarcpp376 successfully recovered registrar i0114 185134787749 4737 mastercpp1077 recovered 0 slaves from the registry 94b  allowing 10mins for slaves to reregister i0114 185134787282 4739 leveldbcpp343 persisting action 18 bytes to leveldb took 707150ns i0114 185134788692 4739 leveldbcpp401 deleting 1 keys from leveldb took 60262ns i0114 185134789048 4739 replicacpp679 persisted action at 2 i0114 185134789329 4739 replicacpp664 replica learned truncate action at position 2 i0114 185134819548 4738 slavecpp173 slave started on 17119216812213557018 i0114 185134820530 4738 credentialshpp84 loading credential for authentication from tmphooktest_verifyslavelaunchexecutorhook_ayxnqecredential i0114 185134820952 4738 slavecpp282 slave using credential for testprincipal i0114 185134821516 4738 slavecpp300 slave resources cpus2 mem1024 disk1024 ports3100032000 i0114 185134822217 4738 slavecpp329 slave hostname fedora19 i0114 185134822502 4738 slavecpp330 slave checkpoint false w0114 185134822857 4738 slavecpp332 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0114 185134824998 4737 statecpp33 recovering state from tmphooktest_verifyslavelaunchexecutorhook_ayxnqemeta i0114 185134834015 4738 status_update_managercpp197 recovering status update manager i0114 185134834810 4738 slavecpp3519 finished recovery i0114 185134835906 4734 status_update_managercpp171 pausing sending status updates i0114 185134836423 4738 slavecpp613 new master detected at master19216812213557018 i0114 185134836908 4738 slavecpp676 authenticating with master master19216812213557018 i0114 185134837190 4738 slavecpp681 using default crammd5 authenticatee i0114 185134837820 4737 authenticateehpp138 creating new client sasl connection i0114 185134838784 4738 slavecpp649 detecting new master i0114 185134839306 4740 mastercpp4130 authenticating slave17119216812213557018 i0114 185134839957 4740 mastercpp4141 using default crammd5 authenticator i0114 185134841236 4740 authenticatorhpp170 creating new server sasl connection i0114 185134842681 4741 authenticateehpp229 received sasl authentication mechanisms crammd5 i0114 185134843118 4741 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0114 185134843581 4740 authenticatorhpp276 received sasl authentication start i0114 185134843962 4740 authenticatorhpp398 authentication requires more steps i0114 185134844357 4740 authenticateehpp275 received sasl authentication step i0114 185134844780 4740 authenticatorhpp304 received sasl authentication step i0114 185134845113 4740 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0114 185134845507 4740 auxpropcpp171 looking up auxiliary property userpassword i0114 185134845835 4740 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0114 185134846238 4740 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0114 185134846542 4740 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0114 185134846806 4740 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0114 185134847110 4740 authenticatorhpp390 authentication success i0114 185134847808 4734 authenticateehpp315 authentication success i0114 185134851029 4734 slavecpp747 successfully authenticated with master master19216812213557018 i0114 185134851608 4737 mastercpp4188 successfully authenticated principal testprincipal at slave17119216812213557018 i0114 185134854962 4720 schedcpp151 version 0220 i0114 185134856674 4734 slavecpp1075 will retry registration in 3085482ms if necessary i0114 185134857434 4739 schedcpp248 new master detected at master19216812213557018 i0114 185134861433 4739 schedcpp304 authenticating with master master19216812213557018 i0114 185134861693 4739 schedcpp311 using default crammd5 authenticatee i0114 185134857795 4737 mastercpp3276 registering slave at slave17119216812213557018 fedora19 with id 201501141851342272962752570184720s0 i0114 185134862951 4737 authenticateehpp138 creating new client sasl connection i0114 185134863919 4735 registrarcpp445 applied 1 operations in 120272ns attempting to update the registry i0114 185134864645 4738 mastercpp4130 authenticating schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134865033 4738 mastercpp4141 using default crammd5 authenticator i0114 185134866904 4738 authenticatorhpp170 creating new server sasl connection i0114 185134868840 4737 authenticateehpp229 received sasl authentication mechanisms crammd5 i0114 185134869125 4737 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0114 185134869523 4737 authenticatorhpp276 received sasl authentication start i0114 185134869835 4737 authenticatorhpp398 authentication requires more steps i0114 185134870213 4737 authenticateehpp275 received sasl authentication step i0114 185134870622 4737 authenticatorhpp304 received sasl authentication step i0114 185134870946 4737 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0114 185134871219 4737 auxpropcpp171 looking up auxiliary property userpassword i0114 185134871554 4737 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0114 185134871968 4737 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0114 185134872297 4737 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0114 185134872655 4737 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0114 185134873024 4737 authenticatorhpp390 authentication success i0114 185134873428 4737 authenticateehpp315 authentication success i0114 185134873632 4739 mastercpp4188 successfully authenticated principal testprincipal at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134875006 4740 schedcpp392 successfully authenticated with master master19216812213557018 i0114 185134875319 4740 schedcpp515 sending registration request to master19216812213557018 i0114 185134876200 4740 schedcpp548 will retry registration in 1952991346secs if necessary i0114 185134876729 4738 mastercpp1417 received registration request for framework default at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134877040 4738 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0114 185134878059 4738 mastercpp1481 registering framework 2015011418513422729627525701847200000 default at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134878473 4739 logcpp684 attempting to append 300 bytes to the log i0114 185134879464 4737 coordinatorcpp340 coordinator attempting to write append action at position 3 i0114 185134880116 4734 hierarchical_allocator_processhpp319 added framework 2015011418513422729627525701847200000 i0114 185134880470 4734 hierarchical_allocator_processhpp839 no resources available to allocate i0114 185134882331 4734 hierarchical_allocator_processhpp746 performed allocation for 0 slaves in 1901284ms i0114 185134884024 4741 schedcpp442 framework registered with 2015011418513422729627525701847200000 i0114 185134884454 4741 schedcpp456 schedulerregistered took 44320ns i0114 185134881965 4737 replicacpp511 replica received write request for position 3 i0114 185134885218 4737 leveldbcpp343 persisting action 319 bytes to leveldb took 134480ns i0114 185134885716 4737 replicacpp679 persisted action at 3 i0114 185134886034 4739 slavecpp1075 will retry registration in 22947772ms if necessary i0114 185134886291 4740 mastercpp3264 ignoring register slave message from slave17119216812213557018 fedora19 as admission is already in progress i0114 185134894690 4736 replicacpp658 replica received learned notice for position 3 i0114 185134898638 4736 leveldbcpp343 persisting action 321 bytes to leveldb took 215501ns i0114 185134899055 4736 replicacpp679 persisted action at 3 i0114 185134899416 4736 replicacpp664 replica learned append action at position 3 i0114 185134911782 4736 registrarcpp490 successfully updated the registry in 46176768ms i0114 185134912286 4740 logcpp703 attempting to truncate the log to 3 i0114 185134913108 4740 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0114 185134915027 4736 mastercpp3330 registered slave 201501141851342272962752570184720s0 at slave17119216812213557018 fedora19 with cpus2 mem1024 disk1024 ports3100032000 i0114 185134915642 4735 hierarchical_allocator_processhpp453 added slave 201501141851342272962752570184720s0 fedora19 with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0114 185134917809 4735 hierarchical_allocator_processhpp764 performed allocation for slave 201501141851342272962752570184720s0 in 514027ns i0114 185134916689 4738 replicacpp511 replica received write request for position 4 i0114 185134915784 4741 slavecpp781 registered with master master19216812213557018 given slave id 201501141851342272962752570184720s0 i0114 185134919293 4741 slavecpp2588 received ping from slaveobserver15619216812213557018 i0114 185134919775 4740 status_update_managercpp178 resuming sending status updates i0114 185134920374 4736 mastercpp4072 sending 1 offers to framework 2015011418513422729627525701847200000 default at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134920569 4738 leveldbcpp343 persisting action 16 bytes to leveldb took 1540136ms i0114 185134921092 4738 replicacpp679 persisted action at 4 i0114 185134927111 4735 replicacpp658 replica received learned notice for position 4 i0114 185134927299 4734 schedcpp605 schedulerresourceoffers took 1335524ms i0114 185134930418 4735 leveldbcpp343 persisting action 18 bytes to leveldb took 1596377ms i0114 185134930882 4735 leveldbcpp401 deleting 2 keys from leveldb took 67578ns i0114 185134931115 4735 replicacpp679 persisted action at 4 i0114 185134931529 4735 replicacpp664 replica learned truncate action at position 4 i0114 185134930356 4734 mastercpp2541 processing reply for offers  201501141851342272962752570184720o0  on slave 201501141851342272962752570184720s0 at slave17119216812213557018 fedora19 for framework 2015011418513422729627525701847200000 default at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 i0114 185134932834 4734 mastercpp2647 authorizing framework principal testprincipal to launch task 1 as user jenkins w0114 185134934442 4736 mastercpp2124 executor default for task 1 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0114 185134934960 4736 mastercpp2136 executor default for task 1 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0114 185134935878 4736 masterhpp766 adding task 1 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201501141851342272962752570184720s0 fedora19 i0114 185134939453 4738 hierarchical_allocator_processhpp610 updated allocation of framework 2015011418513422729627525701847200000 on slave 201501141851342272962752570184720s0 from cpus2 mem1024 disk1024 ports3100032000 to cpus2 mem1024 disk1024 ports3100032000 i0114 185134939950 4736 mastercpp2897 launching task 1 of framework 2015011418513422729627525701847200000 default at schedulerc45273e46eb544eebf4571b353db648f19216812213557018 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 201,3
slavetestmesosexecutorgracefulshutdown is flaky observed this on internal ci noformat  run  slavetestmesosexecutorgracefulshutdown using temporary directory tmpslavetest_mesosexecutorgracefulshutdown_awdtvj i0124 081404399211 7926 leveldbcpp176 opened db in 27364056ms i0124 081404402632 7926 leveldbcpp183 compacted db in 3357646ms i0124 081404402691 7926 leveldbcpp198 created db iterator in 23822ns i0124 081404402708 7926 leveldbcpp204 seeked to beginning of db in 1913ns i0124 081404402716 7926 leveldbcpp273 iterated through 0 keys in the db in 458ns i0124 081404402767 7926 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0124 081404403728 7951 recovercpp449 starting replica recovery i0124 081404404011 7951 recovercpp475 replica is in empty status i0124 081404407765 7950 replicacpp641 replica in empty status received a broadcasted recover request i0124 081404408710 7951 recovercpp195 received a recover response from a replica in empty status i0124 081404419666 7951 recovercpp566 updating replica status to starting i0124 081404429719 7953 mastercpp262 master 2015012408140416842879477877926 utopic started on 12701147787 i0124 081404429790 7953 mastercpp308 master only allowing authenticated frameworks to register i0124 081404429802 7953 mastercpp313 master only allowing authenticated slaves to register i0124 081404429826 7953 credentialshpp36 loading credentials for authentication from tmpslavetest_mesosexecutorgracefulshutdown_awdtvjcredentials i0124 081404430277 7953 mastercpp357 authorization enabled i0124 081404432682 7953 mastercpp1219 the newly elected leader is master12701147787 with id 2015012408140416842879477877926 i0124 081404432816 7953 mastercpp1232 elected as the leading master i0124 081404432894 7953 mastercpp1050 recovering from registrar i0124 081404433212 7950 registrarcpp313 recovering registrar i0124 081404434226 7951 leveldbcpp306 persisting metadata 8 bytes to leveldb took 14323302ms i0124 081404434270 7951 replicacpp323 persisted replica status to starting i0124 081404434489 7951 recovercpp475 replica is in starting status i0124 081404436164 7951 replicacpp641 replica in starting status received a broadcasted recover request i0124 081404439368 7947 recovercpp195 received a recover response from a replica in starting status i0124 081404440626 7947 recovercpp566 updating replica status to voting i0124 081404443667 7947 leveldbcpp306 persisting metadata 8 bytes to leveldb took 2698664ms i0124 081404443759 7947 replicacpp323 persisted replica status to voting i0124 081404443925 7947 recovercpp580 successfully joined the paxos group i0124 081404444160 7947 recovercpp464 recover process terminated i0124 081404444543 7949 logcpp660 attempting to start the writer i0124 081404446331 7949 replicacpp477 replica received implicit promise request with proposal 1 i0124 081404449329 7949 leveldbcpp306 persisting metadata 8 bytes to leveldb took 2690453ms i0124 081404449388 7949 replicacpp345 persisted promised to 1 i0124 081404450637 7947 coordinatorcpp230 coordinator attemping to fill missing position i0124 081404452271 7949 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0124 081404455124 7949 leveldbcpp343 persisting action 8 bytes to leveldb took 2593522ms i0124 081404455157 7949 replicacpp679 persisted action at 0 i0124 081404456594 7951 replicacpp511 replica received write request for position 0 i0124 081404456657 7951 leveldbcpp438 reading position from leveldb took 30358ns i0124 081404464860 7951 leveldbcpp343 persisting action 14 bytes to leveldb took 8164646ms i0124 081404464903 7951 replicacpp679 persisted action at 0 i0124 081404465947 7949 replicacpp658 replica received learned notice for position 0 i0124 081404471567 7949 leveldbcpp343 persisting action 16 bytes to leveldb took 5587838ms i0124 081404471601 7949 replicacpp679 persisted action at 0 i0124 081404471622 7949 replicacpp664 replica learned nop action at position 0 i0124 081404472682 7951 logcpp676 writer started with ending position 0 i0124 081404473919 7951 leveldbcpp438 reading position from leveldb took 28676ns i0124 081404491591 7951 registrarcpp346 successfully fetched the registry 0b in 58337024ms i0124 081404491704 7951 registrarcpp445 applied 1 operations in 28163ns attempting to update the registry i0124 081404493938 7953 logcpp684 attempting to append 118 bytes to the log i0124 081404494122 7953 coordinatorcpp340 coordinator attempting to write append action at position 1 i0124 081404495069 7953 replicacpp511 replica received write request for position 1 i0124 081404500089 7953 leveldbcpp343 persisting action 135 bytes to leveldb took 4989356ms i0124 081404500123 7953 replicacpp679 persisted action at 1 i0124 081404501271 7950 replicacpp658 replica received learned notice for position 1 i0124 081404505698 7950 leveldbcpp343 persisting action 137 bytes to leveldb took 4396221ms i0124 081404505734 7950 replicacpp679 persisted action at 1 i0124 081404505755 7950 replicacpp664 replica learned append action at position 1 i0124 081404507313 7950 registrarcpp490 successfully updated the registry in 1552896ms i0124 081404507478 7953 logcpp703 attempting to truncate the log to 1 i0124 081404507848 7953 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0124 081404508743 7953 replicacpp511 replica received write request for position 2 i0124 081404509214 7950 registrarcpp376 successfully recovered registrar i0124 081404509682 7946 mastercpp1077 recovered 0 slaves from the registry 82b  allowing 10mins for slaves to reregister i0124 081404514654 7953 leveldbcpp343 persisting action 16 bytes to leveldb took 5880031ms i0124 081404514689 7953 replicacpp679 persisted action at 2 i0124 081404515736 7953 replicacpp658 replica received learned notice for position 2 i0124 081404522014 7953 leveldbcpp343 persisting action 18 bytes to leveldb took 6245138ms i0124 081404522086 7953 leveldbcpp401 deleting 1 keys from leveldb took 37803ns i0124 081404522107 7953 replicacpp679 persisted action at 2 i0124 081404522128 7953 replicacpp664 replica learned truncate action at position 2 i0124 081404531460 7926 containerizercpp103 using isolation posixcpuposixmem i0124 081404547194 7951 slavecpp173 slave started on 20812701147787 i0124 081404555682 7951 credentialshpp84 loading credential for authentication from tmpslavetest_mesosexecutorgracefulshutdown_kb74xocredential i0124 081404556622 7951 slavecpp282 slave using credential for testprincipal i0124 081404557052 7951 slavecpp300 slave resources cpus2 mem1024 disk1024 ports3100032000 i0124 081404557842 7951 slavecpp329 slave hostname utopic i0124 081404558091 7951 slavecpp330 slave checkpoint false w0124 081404558352 7951 slavecpp332 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0124 081404566864 7948 statecpp33 recovering state from tmpslavetest_mesosexecutorgracefulshutdown_kb74xometa i0124 081404575711 7951 status_update_managercpp197 recovering status update manager i0124 081404575904 7951 containerizercpp300 recovering containerizer i0124 081404577112 7951 slavecpp3519 finished recovery i0124 081404577374 7926 schedcpp151 version 0220 i0124 081404578663 7950 schedcpp248 new master detected at master12701147787 i0124 081404578759 7950 schedcpp304 authenticating with master master12701147787 i0124 081404578781 7950 schedcpp311 using default crammd5 authenticatee i0124 081404579071 7950 authenticateehpp138 creating new client sasl connection i0124 081404579550 7947 mastercpp4129 authenticating scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404579582 7947 mastercpp4140 using default crammd5 authenticator i0124 081404580031 7947 authenticatorhpp170 creating new server sasl connection i0124 081404580402 7947 authenticateehpp229 received sasl authentication mechanisms crammd5 i0124 081404580430 7947 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0124 081404580538 7947 authenticatorhpp276 received sasl authentication start i0124 081404580581 7947 authenticatorhpp398 authentication requires more steps i0124 081404580651 7947 authenticateehpp275 received sasl authentication step i0124 081404580746 7947 authenticatorhpp304 received sasl authentication step i0124 081404580837 7947 authenticatorhpp390 authentication success i0124 081404580940 7947 authenticateehpp315 authentication success i0124 081404581009 7947 mastercpp4187 successfully authenticated principal testprincipal at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404581328 7947 schedcpp392 successfully authenticated with master master12701147787 i0124 081404581509 7947 mastercpp1420 received registration request for framework default at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404581585 7947 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0124 081404582033 7947 mastercpp1484 registering framework 20150124081404168428794778779260000 default at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404582595 7947 hierarchical_allocator_processhpp319 added framework 20150124081404168428794778779260000 i0124 081404583051 7947 schedcpp442 framework registered with 20150124081404168428794778779260000 i0124 081404584087 7951 slavecpp613 new master detected at master12701147787 i0124 081404584388 7951 slavecpp676 authenticating with master master12701147787 i0124 081404584564 7951 slavecpp681 using default crammd5 authenticatee i0124 081404584951 7951 slavecpp649 detecting new master i0124 081404585219 7951 status_update_managercpp171 pausing sending status updates i0124 081404585604 7951 authenticateehpp138 creating new client sasl connection i0124 081404587666 7953 mastercpp4129 authenticating slave20812701147787 i0124 081404587702 7953 mastercpp4140 using default crammd5 authenticator i0124 081404588434 7953 authenticatorhpp170 creating new server sasl connection i0124 081404588764 7953 authenticateehpp229 received sasl authentication mechanisms crammd5 i0124 081404588790 7953 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0124 081404588896 7953 authenticatorhpp276 received sasl authentication start i0124 081404588935 7953 authenticatorhpp398 authentication requires more steps i0124 081404589005 7953 authenticateehpp275 received sasl authentication step i0124 081404589082 7953 authenticatorhpp304 received sasl authentication step i0124 081404589140 7953 authenticatorhpp390 authentication success i0124 081404589232 7953 authenticateehpp315 authentication success i0124 081404589300 7953 mastercpp4187 successfully authenticated principal testprincipal at slave20812701147787 i0124 081404589587 7953 slavecpp747 successfully authenticated with master master12701147787 i0124 081404589913 7953 mastercpp3275 registering slave at slave20812701147787 utopic with id 2015012408140416842879477877926s0 i0124 081404590322 7953 registrarcpp445 applied 1 operations in 60404ns attempting to update the registry i0124 081404595336 7948 logcpp684 attempting to append 283 bytes to the log i0124 081404595552 7948 coordinatorcpp340 coordinator attempting to write append action at position 3 i0124 081404596535 7948 replicacpp511 replica received write request for position 3 i0124 081404597846 7951 mastercpp3263 ignoring register slave message from slave20812701147787 utopic as admission is already in progress i0124 081404602326 7948 leveldbcpp343 persisting action 302 bytes to leveldb took 5758211ms i0124 081404602363 7948 replicacpp679 persisted action at 3 i0124 081404603492 7951 replicacpp658 replica received learned notice for position 3 i0124 081404608952 7951 leveldbcpp343 persisting action 304 bytes to leveldb took 5427195ms i0124 081404608985 7951 replicacpp679 persisted action at 3 i0124 081404609007 7951 replicacpp664 replica learned append action at position 3 i0124 081404610643 7951 registrarcpp490 successfully updated the registry in 20258048ms i0124 081404610800 7948 logcpp703 attempting to truncate the log to 3 i0124 081404611184 7948 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0124 081404612076 7948 replicacpp511 replica received write request for position 4 i0124 081404613061 7946 mastercpp3329 registered slave 2015012408140416842879477877926s0 at slave20812701147787 utopic with cpus2 mem1024 disk1024 ports3100032000 i0124 081404613299 7946 hierarchical_allocator_processhpp453 added slave 2015012408140416842879477877926s0 utopic with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0124 081404613688 7946 slavecpp781 registered with master master12701147787 given slave id 2015012408140416842879477877926s0 i0124 081404614112 7946 mastercpp4071 sending 1 offers to framework 20150124081404168428794778779260000 default at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404614228 7946 status_update_managercpp178 resuming sending status updates i0124 081404617481 7947 mastercpp2677 processing accept call for offers  2015012408140416842879477877926o0  on slave 2015012408140416842879477877926s0 at slave20812701147787 utopic for framework 20150124081404168428794778779260000 default at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 i0124 081404617535 7947 mastercpp2513 authorizing framework principal testprincipal to launch task 7c16772d4aed471981c4658a2cc22543 as user jenkins i0124 081404618736 7947 masterhpp782 adding task 7c16772d4aed471981c4658a2cc22543 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015012408140416842879477877926s0 utopic i0124 081404618854 7947 mastercpp2885 launching task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 default at scheduler4a6c5cdec54a455aaaad6fc4e8ee99ef12701147787 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015012408140416842879477877926s0 at slave20812701147787 utopic i0124 081404619209 7947 slavecpp1130 got assigned task 7c16772d4aed471981c4658a2cc22543 for framework 20150124081404168428794778779260000 i0124 081404619472 7948 leveldbcpp343 persisting action 16 bytes to leveldb took 7364828ms i0124 081404619941 7948 replicacpp679 persisted action at 4 i0124 081404624851 7953 replicacpp658 replica received learned notice for position 4 i0124 081404625757 7947 slavecpp1245 launching task 7c16772d4aed471981c4658a2cc22543 for framework 20150124081404168428794778779260000 i0124 081404630590 7953 leveldbcpp343 persisting action 18 bytes to leveldb took 5705336ms i0124 081404630805 7953 leveldbcpp401 deleting 2 keys from leveldb took 51263ns i0124 081404630828 7953 replicacpp679 persisted action at 4 i0124 081404630851 7953 replicacpp664 replica learned truncate action at position 4 i0124 081404633968 7947 slavecpp3921 launching executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 in work directory tmpslavetest_mesosexecutorgracefulshutdown_kb74xoslaves2015012408140416842879477877926s0frameworks20150124081404168428794778779260000executors7c16772d4aed471981c4658a2cc22543runs53887a08f11d4a2fa659a715d9fcf3d2 i0124 081404634963 7951 containerizercpp445 starting container 53887a08f11d4a2fa659a715d9fcf3d2 for executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 w0124 081404636931 7951 containerizercpp296 commandinfograce_period flag is not set using default value 3secs i0124 081404655591 7947 slavecpp1368 queuing task 7c16772d4aed471981c4658a2cc22543 for executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 i0124 081404656992 7951 launchercpp137 forked child with pid 11030 for container 53887a08f11d4a2fa659a715d9fcf3d2 i0124 081404673646 7951 slavecpp2890 monitoring executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 in container 53887a08f11d4a2fa659a715d9fcf3d2 i0124 081404964946 11044 execcpp147 version 0220 i0124 081405113059 7948 slavecpp1912 got registration for executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 from executor112701149174 i0124 081405121086 7948 slavecpp2031 flushing queued task 7c16772d4aed471981c4658a2cc22543 for executor 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 i0124 081405266849 11062 execcpp221 executor registered on slave 2015012408140416842879477877926s0 shutdown timeout is set to 3secsregistered executor on utopic starting task 7c16772d4aed471981c4658a2cc22543 forked command at 11067 sh c sleep 1000 i0124 081405492084 7953 slavecpp2265 handling status update task_running uuid 54742a87ef024e72a19b83b0eeb62568 for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 from executor112701149174 i0124 081405492805 7953 status_update_managercpp317 received status update task_running uuid 54742a87ef024e72a19b83b0eeb62568 for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 i0124 081405493762 7953 slavecpp2508 forwarding the update task_running uuid 54742a87ef024e72a19b83b0eeb62568 for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 to master12701147787 i0124 081405493948 7953 slavecpp2441 sending acknowledgement for status update task_running uuid 54742a87ef024e72a19b83b0eeb62568 for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 to executor112701149174 i0124 081405495378 7949 mastercpp3652 forwarding status update task_running uuid 54742a87ef024e72a19b83b0eeb62568 for task 7c16772d4aed471981c4658a2cc22543 of framework 20150124081404168428794778779260000 i0124 081405495584 7949 mastercpp3624,3
update ratelimiter to allow the acquired future to be discarded currently there is no way for the future returned by ratelimiters acquire to be discarded by the user of the limiter this is useful in cases where the user is no longer interested in the permit see mesos1148 for an example use case,3
suppress mockallocatortransformallocation warnings after transforming allocated resources feature was added to allocator a number of warnings are popping out for allocator tests commits leading to this behaviour dacc88292cc13d4b08fe8cda4df71110a96cb12a 5a02d5bdc75d3b1149dcda519016374be06ec6bd corresponding reviews httpsreviewsapacheorgr29083 httpsreviewsapacheorgr29084 here is an example code  run  masterallocatortest0frameworkreregistersfirst gmock warning uninteresting mock function call  taking default action specified at srctestsmesoshpp719 function call transformallocation0x7fd3bb5274d8 20150115185632167776480059671441860000 0x7fd3bb5274f8 2015011518563216777648005967144186s0 0x1119140e0 16byte object f05e 52bb d37f 0000 c05f 52bb d37f 0000 stack trace  ok  masterallocatortest0frameworkreregistersfirst 204 ms code,3
run asf ci mesos builds inside docker there are several limitations to mesos projects current state of ci which is run on buildsao  only runs on ubuntu  doesnt run any tests that deal with cgroups  doesnt run any tests that need root permissions now that asf ci supports docker httpsissuesapacheorgjirabrowsebuilds25 it would be great for the mesos project to use it,5
diskusagecollectortestsymboliclink test is flaky observed this on a local machine running linux w sudo code  run  diskusagecollectortestsymboliclink srctestsdisk_quota_testscpp138 failure expected usage1get  kilobytes16 actual 24kb vs 8byte object 0040 0000 0000 0000  failed  diskusagecollectortestsymboliclink 201 ms code,1
version the operatoradmin api as a consumer of the mesos http api it is necessary for us to determine the current version of mesos so that we can parse the json documents returned correctly since they change from version to version currently were doing this by fetching statejson parsing it and pulling out the version field a more idiomatic way to do this would be to filter on the contenttype in the header itself to give a more concrete example currently the json documents returned by the http api return the following headers code http11 200 ok date fri 23 jan 2015 213137 gmt contentlength 9352 contenttype applicationjson code something like the following eg for masterstatejson would be easy to switch upon code http11 200 ok date fri 23 jan 2015 213137 gmt contentlength 9352 contenttype applicationvndmesosmasterstatev0201json charsetutf8 code the vnd prefix is typically used for vendor specific file types see httpenwikipediaorgwikiinternet_media_typeprefix_vnd charsetutf8 is required for json documents and is currently being omitted this contenttype would change for each document type for example code applicationvndmesosmasterstatev0201json charsetutf8 applicationvndmesosmasterstatsv0201json charsetutf8 applicationvndmesosslavestatev0201json charsetutf8 applicationvndmesosslavestatsv0201json charsetutf8 code alternatively the version could be appended as an extra field code applicationvndmesosmasterstatejson charsetutf8 versionv0201 applicationvndmesosmasterstatsjson charsetutf8 versionv0201 applicationvndmesosslavestatejson charsetutf8 versionv0201 applicationvndmesosslavestatsjson charsetutf8 versionv0201 code thanks,13
add tests target to makefile for buildingbutnotrunning tests make check allows one to build and run the test suite however often we just want to build the tests currently this is done by setting gtest_filter to an empty string it will be nice to have a dedicated target such as make tests that allows one to build the test suite without running it,1
document header include rules in style guide we have several ways of sorting grouping and ordering headers includes in mesos we should agree on a rule set and do a style scan,3
future callbacks should be cleared once the future has transitioned for example when a future has transitioned into ready state all ondiscard callbacks should be cleared to avoid potential cyclic dependency and memory leak for instance noformat promisenothing promise futurenothing f  promisefuture fondiscardlambdabindsomefunc f promisesetnothing noformat the above code has a cyclic dependency because fdata has a reference to the future inside an stdfunction which has a reference to fdata,2
deprecate plain text credential format currently two formats of credentials are supported json code credentials   principal sherman secret kitesurf  code and a new line file code principal1 secret1 pricipal2 secret2 code we should deprecate the new line format and remove support for the old format,3
slaverecoverytestreconcilekilltask is flaky saw this on an internal ci noformat  run  slaverecoverytest0reconcilekilltask using temporary directory tmpslaverecoverytest_0_reconcilekilltask_d5wswg i0126 191052005317 13291 leveldbcpp176 opened db in 978670ns i0126 191052006155 13291 leveldbcpp183 compacted db in 541346ns i0126 191052006494 13291 leveldbcpp198 created db iterator in 24562ns i0126 191052006798 13291 leveldbcpp204 seeked to beginning of db in 3254ns i0126 191052007036 13291 leveldbcpp273 iterated through 0 keys in the db in 949ns i0126 191052007369 13291 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0126 191052008362 13308 recovercpp449 starting replica recovery i0126 191052009141 13308 recovercpp475 replica is in empty status i0126 191052016494 13308 replicacpp641 replica in empty status received a broadcasted recover request i0126 191052017333 13309 recovercpp195 received a recover response from a replica in empty status i0126 191052018244 13309 recovercpp566 updating replica status to starting i0126 191052019064 13305 leveldbcpp306 persisting metadata 8 bytes to leveldb took 113577ns i0126 191052019487 13305 replicacpp323 persisted replica status to starting i0126 191052019937 13309 recovercpp475 replica is in starting status i0126 191052021492 13307 replicacpp641 replica in starting status received a broadcasted recover request i0126 191052022665 13309 recovercpp195 received a recover response from a replica in starting status i0126 191052027971 13312 recovercpp566 updating replica status to voting i0126 191052028590 13312 leveldbcpp306 persisting metadata 8 bytes to leveldb took 78452ns i0126 191052028869 13312 replicacpp323 persisted replica status to voting i0126 191052029252 13312 recovercpp580 successfully joined the paxos group i0126 191052030828 13307 recovercpp464 recover process terminated i0126 191052049947 13306 mastercpp262 master 2015012619105222729627523554513291 fedora19 started on 19216812213535545 i0126 191052050499 13306 mastercpp308 master only allowing authenticated frameworks to register i0126 191052050765 13306 mastercpp313 master only allowing authenticated slaves to register i0126 191052051048 13306 credentialshpp36 loading credentials for authentication from tmpslaverecoverytest_0_reconcilekilltask_d5wswgcredentials i0126 191052051589 13306 mastercpp357 authorization enabled i0126 191052052531 13305 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0126 191052052881 13311 whitelist_watchercpp65 no whitelist given i0126 191052055524 13306 mastercpp1219 the newly elected leader is master19216812213535545 with id 2015012619105222729627523554513291 i0126 191052056226 13306 mastercpp1232 elected as the leading master i0126 191052056639 13306 mastercpp1050 recovering from registrar i0126 191052057045 13307 registrarcpp313 recovering registrar i0126 191052058554 13312 logcpp660 attempting to start the writer i0126 191052060868 13309 replicacpp477 replica received implicit promise request with proposal 1 i0126 191052061691 13309 leveldbcpp306 persisting metadata 8 bytes to leveldb took 91680ns i0126 191052062261 13309 replicacpp345 persisted promised to 1 i0126 191052064559 13310 coordinatorcpp230 coordinator attemping to fill missing position i0126 191052069105 13311 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0126 191052069860 13311 leveldbcpp343 persisting action 8 bytes to leveldb took 94858ns i0126 191052070350 13311 replicacpp679 persisted action at 0 i0126 191052080348 13305 replicacpp511 replica received write request for position 0 i0126 191052081153 13305 leveldbcpp438 reading position from leveldb took 62247ns i0126 191052081676 13305 leveldbcpp343 persisting action 14 bytes to leveldb took 81487ns i0126 191052082053 13305 replicacpp679 persisted action at 0 i0126 191052083566 13309 replicacpp658 replica received learned notice for position 0 i0126 191052085734 13309 leveldbcpp343 persisting action 16 bytes to leveldb took 283144ns i0126 191052086067 13309 replicacpp679 persisted action at 0 i0126 191052086448 13309 replicacpp664 replica learned nop action at position 0 i0126 191052089784 13306 logcpp676 writer started with ending position 0 i0126 191052093415 13309 leveldbcpp438 reading position from leveldb took 66744ns i0126 191052104814 13306 registrarcpp346 successfully fetched the registry 0b in 47451136ms i0126 191052105731 13306 registrarcpp445 applied 1 operations in 42124ns attempting to update the registry i0126 191052111935 13305 logcpp684 attempting to append 131 bytes to the log i0126 191052112754 13305 coordinatorcpp340 coordinator attempting to write append action at position 1 i0126 191052114297 13308 replicacpp511 replica received write request for position 1 i0126 191052114908 13308 leveldbcpp343 persisting action 150 bytes to leveldb took 98332ns i0126 191052115387 13308 replicacpp679 persisted action at 1 i0126 191052117277 13305 replicacpp658 replica received learned notice for position 1 i0126 191052118142 13305 leveldbcpp343 persisting action 152 bytes to leveldb took 227799ns i0126 191052118621 13305 replicacpp679 persisted action at 1 i0126 191052118979 13305 replicacpp664 replica learned append action at position 1 i0126 191052121311 13305 registrarcpp490 successfully updated the registry in 15161088ms i0126 191052121548 13311 logcpp703 attempting to truncate the log to 1 i0126 191052122697 13311 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0126 191052124316 13307 replicacpp511 replica received write request for position 2 i0126 191052124913 13307 leveldbcpp343 persisting action 16 bytes to leveldb took 87281ns i0126 191052125334 13307 replicacpp679 persisted action at 2 i0126 191052127018 13311 replicacpp658 replica received learned notice for position 2 i0126 191052127835 13311 leveldbcpp343 persisting action 18 bytes to leveldb took 201050ns i0126 191052128232 13311 leveldbcpp401 deleting 1 keys from leveldb took 78012ns i0126 191052128835 13305 registrarcpp376 successfully recovered registrar i0126 191052128551 13311 replicacpp679 persisted action at 2 i0126 191052130105 13311 replicacpp664 replica learned truncate action at position 2 i0126 191052131479 13312 mastercpp1077 recovered 0 slaves from the registry 95b  allowing 10mins for slaves to reregister i0126 191052143465 13291 containerizercpp103 using isolation posixcpuposixmem i0126 191052170471 13309 slavecpp173 slave started on 10119216812213535545 i0126 191052171723 13309 credentialshpp84 loading credential for authentication from tmpslaverecoverytest_0_reconcilekilltask_qbguumcredential i0126 191052172286 13309 slavecpp282 slave using credential for testprincipal i0126 191052172821 13309 slavecpp300 slave resources cpus2 mem1024 disk1024 ports3100032000 i0126 191052173982 13309 slavecpp329 slave hostname fedora19 i0126 191052174505 13309 slavecpp330 slave checkpoint true i0126 191052179308 13309 statecpp33 recovering state from tmpslaverecoverytest_0_reconcilekilltask_qbguummeta i0126 191052180075 13308 status_update_managercpp197 recovering status update manager i0126 191052180611 13308 containerizercpp300 recovering containerizer i0126 191052182473 13309 slavecpp3519 finished recovery i0126 191052184403 13312 slavecpp613 new master detected at master19216812213535545 i0126 191052184916 13312 slavecpp676 authenticating with master master19216812213535545 i0126 191052185230 13312 slavecpp681 using default crammd5 authenticatee i0126 191052185715 13312 slavecpp649 detecting new master i0126 191052186420 13312 authenticateehpp138 creating new client sasl connection i0126 191052186002 13311 status_update_managercpp171 pausing sending status updates i0126 191052188293 13312 mastercpp4129 authenticating slave10119216812213535545 i0126 191052188748 13312 mastercpp4140 using default crammd5 authenticator i0126 191052189525 13312 authenticatorhpp170 creating new server sasl connection i0126 191052191082 13305 authenticateehpp229 received sasl authentication mechanisms crammd5 i0126 191052191550 13305 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0126 191052191990 13312 authenticatorhpp276 received sasl authentication start i0126 191052192365 13312 authenticatorhpp398 authentication requires more steps i0126 191052192800 13311 authenticateehpp275 received sasl authentication step i0126 191052193244 13312 authenticatorhpp304 received sasl authentication step i0126 191052193565 13312 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0126 191052193902 13312 auxpropcpp171 looking up auxiliary property userpassword i0126 191052194301 13312 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0126 191052195669 13312 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0126 191052196048 13312 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0126 191052196395 13312 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0126 191052196723 13312 authenticatorhpp390 authentication success i0126 191052197206 13305 authenticateehpp315 authentication success i0126 191052204121 13305 slavecpp747 successfully authenticated with master master19216812213535545 i0126 191052204676 13310 mastercpp4187 successfully authenticated principal testprincipal at slave10119216812213535545 i0126 191052205729 13305 slavecpp1075 will retry registration in 5608661ms if necessary i0126 191052206451 13310 mastercpp3275 registering slave at slave10119216812213535545 fedora19 with id 2015012619105222729627523554513291s0 i0126 191052210019 13310 registrarcpp445 applied 1 operations in 235087ns attempting to update the registry i0126 191052220736 13308 slavecpp1075 will retry registration in 928397ms if necessary i0126 191052221309 13311 mastercpp3263 ignoring register slave message from slave10119216812213535545 fedora19 as admission is already in progress i0126 191052224818 13307 logcpp684 attempting to append 302 bytes to the log i0126 191052225554 13307 coordinatorcpp340 coordinator attempting to write append action at position 3 i0126 191052227422 13305 replicacpp511 replica received write request for position 3 i0126 191052227969 13305 leveldbcpp343 persisting action 321 bytes to leveldb took 100350ns i0126 191052228276 13305 replicacpp679 persisted action at 3 i0126 191052232475 13312 replicacpp658 replica received learned notice for position 3 i0126 191052233280 13312 leveldbcpp343 persisting action 323 bytes to leveldb took 546567ns i0126 191052233726 13312 replicacpp679 persisted action at 3 i0126 191052234035 13312 replicacpp664 replica learned append action at position 3 i0126 191052236556 13310 registrarcpp490 successfully updated the registry in 26040064ms i0126 191052237330 13305 logcpp703 attempting to truncate the log to 3 i0126 191052238056 13311 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0126 191052239594 13311 replicacpp511 replica received write request for position 4 i0126 191052240129 13311 leveldbcpp343 persisting action 16 bytes to leveldb took 92868ns i0126 191052240458 13311 replicacpp679 persisted action at 4 i0126 191052241976 13308 replicacpp658 replica received learned notice for position 4 i0126 191052242645 13308 leveldbcpp343 persisting action 18 bytes to leveldb took 95635ns i0126 191052242990 13308 leveldbcpp401 deleting 2 keys from leveldb took 58066ns i0126 191052243337 13308 replicacpp679 persisted action at 4 i0126 191052243695 13308 replicacpp664 replica learned truncate action at position 4 i0126 191052245657 13291 schedcpp151 version 0220 i0126 191052247625 13305 mastercpp3329 registered slave 2015012619105222729627523554513291s0 at slave10119216812213535545 fedora19 with cpus2 mem1024 disk1024 ports3100032000 i0126 191052248942 13307 slavecpp781 registered with master master19216812213535545 given slave id 2015012619105222729627523554513291s0 i0126 191052250396 13307 slavecpp797 checkpointing slaveinfo to tmpslaverecoverytest_0_reconcilekilltask_qbguummetaslaves2015012619105222729627523554513291s0slaveinfo i0126 191052250731 13309 status_update_managercpp178 resuming sending status updates i0126 191052251765 13307 slavecpp2588 received ping from slaveobserver9919216812213535545 i0126 191052247951 13310 hierarchical_allocator_processhpp453 added slave 2015012619105222729627523554513291s0 fedora19 with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0126 191052252810 13310 hierarchical_allocator_processhpp831 no resources available to allocate i0126 191052254365 13310 hierarchical_allocator_processhpp756 performed allocation for slave 2015012619105222729627523554513291s0 in 1732701ms i0126 191052254137 13307 schedcpp248 new master detected at master19216812213535545 i0126 191052257863 13307 schedcpp304 authenticating with master master19216812213535545 i0126 191052258249 13307 schedcpp311 using default crammd5 authenticatee i0126 191052258908 13306 authenticateehpp138 creating new client sasl connection i0126 191052261397 13309 mastercpp4129 authenticating scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052261776 13309 mastercpp4140 using default crammd5 authenticator i0126 191052264528 13309 authenticatorhpp170 creating new server sasl connection i0126 191052266248 13312 authenticateehpp229 received sasl authentication mechanisms crammd5 i0126 191052266749 13312 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0126 191052267143 13312 authenticatorhpp276 received sasl authentication start i0126 191052267525 13312 authenticatorhpp398 authentication requires more steps i0126 191052267917 13312 authenticateehpp275 received sasl authentication step i0126 191052268404 13312 authenticatorhpp304 received sasl authentication step i0126 191052268725 13312 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0126 191052269078 13312 auxpropcpp171 looking up auxiliary property userpassword i0126 191052269498 13312 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0126 191052269881 13312 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0126 191052270385 13312 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0126 191052271015 13312 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0126 191052271599 13312 authenticatorhpp390 authentication success i0126 191052272126 13312 authenticateehpp315 authentication success i0126 191052272415 13305 mastercpp4187 successfully authenticated principal testprincipal at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052273998 13307 schedcpp392 successfully authenticated with master master19216812213535545 i0126 191052274415 13307 schedcpp515 sending registration request to master19216812213535545 i0126 191052274842 13307 schedcpp548 will retry registration in 674656506ms if necessary i0126 191052275235 13305 mastercpp1420 received registration request for framework default at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052276017 13305 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0126 191052277027 13305 mastercpp1484 registering framework 20150126191052227296275235545132910000 default at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052278285 13308 hierarchical_allocator_processhpp319 added framework 20150126191052227296275235545132910000 i0126 191052279575 13308 hierarchical_allocator_processhpp738 performed allocation for 1 slaves in 697902ns i0126 191052287966 13305 mastercpp4071 sending 1 offers to framework 20150126191052227296275235545132910000 default at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052288776 13307 schedcpp442 framework registered with 20150126191052227296275235545132910000 i0126 191052289373 13307 schedcpp456 schedulerregistered took 21674ns i0126 191052289932 13307 schedcpp605 schedulerresourceoffers took 76147ns i0126 191052293220 13311 mastercpp2677 processing accept call for offers  2015012619105222729627523554513291o0  on slave 2015012619105222729627523554513291s0 at slave10119216812213535545 fedora19 for framework 20150126191052227296275235545132910000 default at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 i0126 191052293586 13311 mastercpp2513 authorizing framework principal testprincipal to launch task 61eaeec3e8ca4e1582d6284c05c3bb6e as user jenkins i0126 191052295825 13311 masterhpp782 adding task 61eaeec3e8ca4e1582d6284c05c3bb6e with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015012619105222729627523554513291s0 fedora19 i0126 191052296272 13311 mastercpp2885 launching task 61eaeec3e8ca4e1582d6284c05c3bb6e of framework 20150126191052227296275235545132910000 default at scheduler6da85b48b57f4202b630c45f8f65232119216812213535545 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015012619105222729627523554513291s0 at slave10119216812213535545 fedora19 i0126 191052296886 13309 slavecpp1130 got assigned task 61eaeec3e8ca4e1582d6284c05c3bb6e for framework 20150126191052227296275235545132910000 i0126 191052297324 13309 slavecpp3846 checkpointing frameworkinfo to tmpslaverecoverytest_0_reconcilekilltask_qbguummetaslaves2015012619105222729627523554513291s0frameworks2015012619105222,1
design doc for the http api this tracks the design of the http api,13
move all scheduler driver validations to master with http api the scheduler driver will no longer exist and hence all the validations should move to the master,3
implement the scheduler endpoint on master,8
implement the events stream on master for call endpoint,8
implement the call endpoint on slave,8
implement the events stream on slave for call endpoint,8
add authentication support for http api since most of the communication between mesos components will happen through http with the arrival of the http apihttpsissuesapacheorgjirabrowsemesos2288 it makes sense to use http standard mechanisms to authenticate this communication,1
provide a java library for master detection when schedulers start interacting with mesos master via http endpoints they need a way to detect masters mesos should provide a master detection java library to make this easy for frameworks,5
faulttolerancetestschedulerfailoverframeworkmessage is flaky bad run noformat  run  faulttolerancetestschedulerfailoverframeworkmessage using temporary directory tmpfaulttolerancetest_schedulerfailoverframeworkmessage_f3jykr i0123 185011669674 15688 leveldbcpp176 opened db in 31920683ms i0123 185011678328 15688 leveldbcpp183 compacted db in 8580569ms i0123 185011678455 15688 leveldbcpp198 created db iterator in 38478ns i0123 185011678478 15688 leveldbcpp204 seeked to beginning of db in 3057ns i0123 185011678489 15688 leveldbcpp273 iterated through 0 keys in the db in 427ns i0123 185011678539 15688 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0123 185011682271 15705 recovercpp449 starting replica recovery i0123 185011682634 15705 recovercpp475 replica is in empty status i0123 185011684389 15708 replicacpp641 replica in empty status received a broadcasted recover request i0123 185011685132 15708 recovercpp195 received a recover response from a replica in empty status i0123 185011689842 15708 recovercpp566 updating replica status to starting i0123 185011702548 15708 leveldbcpp306 persisting metadata 8 bytes to leveldb took 12484558ms i0123 185011702615 15708 replicacpp323 persisted replica status to starting i0123 185011703531 15708 recovercpp475 replica is in starting status i0123 185011705080 15704 replicacpp641 replica in starting status received a broadcasted recover request i0123 185011712587 15708 recovercpp195 received a recover response from a replica in starting status i0123 185011722898 15708 recovercpp566 updating replica status to voting i0123 185011725427 15703 mastercpp262 master 20150123185011167773433752615688 localhostlocaldomain started on 12700137526 w0123 185011725464 15703 mastercpp266  master bound to loopback interface cannot communicate with remote schedulers or slaves you might want to set ip flag to a routable ip address  i0123 185011725502 15703 mastercpp308 master only allowing authenticated frameworks to register i0123 185011725513 15703 mastercpp313 master only allowing authenticated slaves to register i0123 185011725543 15703 credentialshpp36 loading credentials for authentication from tmpfaulttolerancetest_schedulerfailoverframeworkmessage_f3jykrcredentials i0123 185011725774 15703 mastercpp357 authorization enabled i0123 185011728428 15707 whitelist_watchercpp65 no whitelist given i0123 185011729169 15707 mastercpp1219 the newly elected leader is master12700137526 with id 20150123185011167773433752615688 i0123 185011729200 15707 mastercpp1232 elected as the leading master i0123 185011729223 15707 mastercpp1050 recovering from registrar i0123 185011729595 15706 registrarcpp313 recovering registrar i0123 185011730715 15703 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0123 185011737431 15708 leveldbcpp306 persisting metadata 8 bytes to leveldb took 14259597ms i0123 185011737511 15708 replicacpp323 persisted replica status to voting i0123 185011737768 15708 recovercpp580 successfully joined the paxos group i0123 185011737977 15708 recovercpp464 recover process terminated i0123 185011739083 15706 logcpp660 attempting to start the writer i0123 185011741236 15706 replicacpp477 replica received implicit promise request with proposal 1 i0123 185011750435 15706 leveldbcpp306 persisting metadata 8 bytes to leveldb took 8813783ms i0123 185011750514 15706 replicacpp345 persisted promised to 1 i0123 185011752239 15708 coordinatorcpp230 coordinator attemping to fill missing position i0123 185011754176 15706 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0123 185011763464 15706 leveldbcpp343 persisting action 8 bytes to leveldb took 8799822ms i0123 185011763535 15706 replicacpp679 persisted action at 0 i0123 185011765697 15709 replicacpp511 replica received write request for position 0 i0123 185011766293 15709 leveldbcpp438 reading position from leveldb took 54028ns i0123 185011776468 15709 leveldbcpp343 persisting action 14 bytes to leveldb took 9789169ms i0123 185011776561 15709 replicacpp679 persisted action at 0 i0123 185011777515 15709 replicacpp658 replica received learned notice for position 0 i0123 185011785459 15709 leveldbcpp343 persisting action 16 bytes to leveldb took 7897242ms i0123 185011785531 15709 replicacpp679 persisted action at 0 i0123 185011785565 15709 replicacpp664 replica learned nop action at position 0 i0123 185011786633 15709 logcpp676 writer started with ending position 0 i0123 185011788460 15709 leveldbcpp438 reading position from leveldb took 266087ns i0123 185011801141 15709 registrarcpp346 successfully fetched the registry 0b in 71491072ms i0123 185011801300 15709 registrarcpp445 applied 1 operations in 41795ns attempting to update the registry i0123 185011805186 15707 logcpp684 attempting to append 136 bytes to the log i0123 185011805454 15707 coordinatorcpp340 coordinator attempting to write append action at position 1 i0123 185011806677 15703 replicacpp511 replica received write request for position 1 i0123 185011815621 15703 leveldbcpp343 persisting action 155 bytes to leveldb took 889177ms i0123 185011815692 15703 replicacpp679 persisted action at 1 i0123 185011817358 15704 replicacpp658 replica received learned notice for position 1 i0123 185011825014 15704 leveldbcpp343 persisting action 157 bytes to leveldb took 7578558ms i0123 185011825088 15704 replicacpp679 persisted action at 1 i0123 185011825124 15704 replicacpp664 replica learned append action at position 1 i0123 185011827008 15705 registrarcpp490 successfully updated the registry in 25629952ms i0123 185011827143 15705 registrarcpp376 successfully recovered registrar i0123 185011827517 15705 mastercpp1077 recovered 0 slaves from the registry 98b  allowing 10mins for slaves to reregister i0123 185011828515 15704 logcpp703 attempting to truncate the log to 1 i0123 185011829074 15704 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0123 185011830546 15709 replicacpp511 replica received write request for position 2 i0123 185011837752 15709 leveldbcpp343 persisting action 16 bytes to leveldb took 7142431ms i0123 185011837826 15709 replicacpp679 persisted action at 2 i0123 185011839334 15709 replicacpp658 replica received learned notice for position 2 i0123 185011847069 15709 leveldbcpp343 persisting action 18 bytes to leveldb took 7116607ms i0123 185011847214 15709 leveldbcpp401 deleting 1 keys from leveldb took 74008ns i0123 185011847241 15709 replicacpp679 persisted action at 2 i0123 185011847295 15709 replicacpp664 replica learned truncate action at position 2 i0123 185011870337 15710 slavecpp173 slave started on 9412700137526 w0123 185011870980 15710 slavecpp176  slave bound to loopback interface cannot communicate with remote masters you might want to set ip flag to a routable ip address  i0123 185011871412 15710 credentialshpp84 loading credential for authentication from tmpfaulttolerancetest_schedulerfailoverframeworkmessage_tb8rh3credential i0123 185011871819 15710 slavecpp282 slave using credential for testprincipal i0123 185011873178 15710 slavecpp300 slave resources cpus2 mem1024 disk1024 ports3100032000 i0123 185011873620 15710 slavecpp329 slave hostname localhostlocaldomain i0123 185011873837 15710 slavecpp330 slave checkpoint false w0123 185011874068 15710 slavecpp332 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0123 185011879103 15705 statecpp33 recovering state from tmpfaulttolerancetest_schedulerfailoverframeworkmessage_tb8rh3meta w0123 185011882972 15688 schedcpp1246  scheduler driver bound to loopback interface cannot communicate with remote masters you might want to set libprocess_ip environment variable to use a routable ip address  i0123 185011884106 15709 status_update_managercpp197 recovering status update manager i0123 185011884703 15710 slavecpp3519 finished recovery i0123 185011892076 15704 status_update_managercpp171 pausing sending status updates i0123 185011892590 15710 slavecpp613 new master detected at master12700137526 i0123 185011892937 15710 slavecpp676 authenticating with master master12700137526 i0123 185011893165 15710 slavecpp681 using default crammd5 authenticatee i0123 185011893754 15708 authenticateehpp138 creating new client sasl connection i0123 185011894120 15708 mastercpp4129 authenticating slave9412700137526 i0123 185011894153 15708 mastercpp4140 using default crammd5 authenticator i0123 185011894628 15708 authenticatorhpp170 creating new server sasl connection i0123 185011894913 15708 authenticateehpp229 received sasl authentication mechanisms crammd5 i0123 185011894942 15708 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0123 185011895043 15708 authenticatorhpp276 received sasl authentication start i0123 185011895095 15708 authenticatorhpp398 authentication requires more steps i0123 185011895165 15708 authenticateehpp275 received sasl authentication step i0123 185011895261 15708 authenticatorhpp304 received sasl authentication step i0123 185011895292 15708 auxpropcpp99 request to lookup properties for user testprincipal realm localhostlocaldomain server fqdn localhostlocaldomain sasl_auxprop_override false sasl_auxprop_authzid false i0123 185011895305 15708 auxpropcpp171 looking up auxiliary property userpassword i0123 185011895354 15708 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0123 185011895881 15710 slavecpp649 detecting new master i0123 185011898449 15708 auxpropcpp99 request to lookup properties for user testprincipal realm localhostlocaldomain server fqdn localhostlocaldomain sasl_auxprop_override false sasl_auxprop_authzid true i0123 185011899024 15708 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0123 185011899106 15708 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0123 185011899190 15708 authenticatorhpp390 authentication success i0123 185011899569 15706 authenticateehpp315 authentication success i0123 185011902299 15706 slavecpp747 successfully authenticated with master master12700137526 i0123 185011902847 15706 slavecpp1075 will retry registration in 19809649ms if necessary i0123 185011903264 15705 mastercpp3214 queuing up registration request from slave9412700137526 because authentication is still in progress i0123 185011903497 15705 mastercpp4187 successfully authenticated principal testprincipal at slave9412700137526 i0123 185011903940 15705 mastercpp3275 registering slave at slave9412700137526 localhostlocaldomain with id 20150123185011167773433752615688s0 i0123 185011904398 15705 registrarcpp445 applied 1 operations in 63679ns attempting to update the registry i0123 185011917883 15688 schedcpp151 version 0220 i0123 185011919347 15703 logcpp684 attempting to append 315 bytes to the log i0123 185011921039 15703 coordinatorcpp340 coordinator attempting to write append action at position 3 i0123 185011919992 15706 schedcpp248 new master detected at master12700137526 i0123 185011921352 15706 schedcpp304 authenticating with master master12700137526 i0123 185011921408 15706 schedcpp311 using default crammd5 authenticatee i0123 185011921773 15706 authenticateehpp138 creating new client sasl connection i0123 185011922266 15706 mastercpp4129 authenticating scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011922301 15706 mastercpp4140 using default crammd5 authenticator i0123 185011923928 15703 replicacpp511 replica received write request for position 3 i0123 185011924285 15707 authenticatorhpp170 creating new server sasl connection i0123 185011925091 15707 authenticateehpp229 received sasl authentication mechanisms crammd5 i0123 185011925122 15707 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0123 185011925194 15707 authenticatorhpp276 received sasl authentication start i0123 185011925257 15707 authenticatorhpp398 authentication requires more steps i0123 185011925325 15707 authenticateehpp275 received sasl authentication step i0123 185011925442 15707 authenticatorhpp304 received sasl authentication step i0123 185011925473 15707 auxpropcpp99 request to lookup properties for user testprincipal realm localhostlocaldomain server fqdn localhostlocaldomain sasl_auxprop_override false sasl_auxprop_authzid false i0123 185011925487 15707 auxpropcpp171 looking up auxiliary property userpassword i0123 185011925532 15707 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0123 185011925559 15707 auxpropcpp99 request to lookup properties for user testprincipal realm localhostlocaldomain server fqdn localhostlocaldomain sasl_auxprop_override false sasl_auxprop_authzid true i0123 185011925571 15707 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0123 185011925580 15707 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0123 185011925595 15707 authenticatorhpp390 authentication success i0123 185011925695 15707 authenticateehpp315 authentication success i0123 185011925792 15707 mastercpp4187 successfully authenticated principal testprincipal at scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011926127 15707 schedcpp392 successfully authenticated with master master12700137526 i0123 185011926154 15707 schedcpp515 sending registration request to master12700137526 i0123 185011926215 15707 schedcpp548 will retry registration in 86681063ms if necessary i0123 185011926640 15707 mastercpp1420 received registration request for framework default at scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011926960 15707 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0123 185011927691 15707 mastercpp1484 registering framework 201501231850111677734337526156880000 default at scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011928292 15708 hierarchical_allocator_processhpp319 added framework 201501231850111677734337526156880000 i0123 185011928326 15708 hierarchical_allocator_processhpp839 no resources available to allocate i0123 185011928340 15708 hierarchical_allocator_processhpp746 performed allocation for 0 slaves in 21080ns i0123 185011934458 15707 schedcpp442 framework registered with 201501231850111677734337526156880000 i0123 185011934927 15707 schedcpp456 schedulerregistered took 112885ns i0123 185011935747 15709 slavecpp1075 will retry registration in 19609252ms if necessary i0123 185011935981 15709 mastercpp3263 ignoring register slave message from slave9412700137526 localhostlocaldomain as admission is already in progress i0123 185011938997 15703 leveldbcpp343 persisting action 334 bytes to leveldb took 10171709ms i0123 185011939049 15703 replicacpp679 persisted action at 3 i0123 185011940630 15709 replicacpp658 replica received learned notice for position 3 i0123 185011945473 15709 leveldbcpp343 persisting action 336 bytes to leveldb took 4804742ms i0123 185011945521 15709 replicacpp679 persisted action at 3 i0123 185011945550 15709 replicacpp664 replica learned append action at position 3 i0123 185011947105 15709 registrarcpp490 successfully updated the registry in 42637056ms i0123 185011948020 15703 mastercpp3329 registered slave 20150123185011167773433752615688s0 at slave9412700137526 localhostlocaldomain with cpus2 mem1024 disk1024 ports3100032000 i0123 185011948318 15703 hierarchical_allocator_processhpp453 added slave 20150123185011167773433752615688s0 localhostlocaldomain with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0123 185011948719 15703 hierarchical_allocator_processhpp764 performed allocation for slave 20150123185011167773433752615688s0 in 355831ns i0123 185011948813 15703 slavecpp781 registered with master master12700137526 given slave id 20150123185011167773433752615688s0 i0123 185011948969 15703 slavecpp2588 received ping from slaveobserver9212700137526 i0123 185011949324 15703 mastercpp4071 sending 1 offers to framework 201501231850111677734337526156880000 default at scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011949571 15706 status_update_managercpp178 resuming sending status updates i0123 185011950023 15709 logcpp703 attempting to truncate the log to 3 i0123 185011950810 15705 schedcpp605 schedulerresourceoffers took 135580ns i0123 185011952793 15708 mastercpp2677 processing accept call for offers  20150123185011167773433752615688o0  on slave 20150123185011167773433752615688s0 at slave9412700137526 localhostlocaldomain for framework 201501231850111677734337526156880000 default at scheduler2cecb105ca234048970712b1e4422e1112700137526 i0123 185011952852 15708 mastercpp2513 authorizing framework principal testprincipal to launch task 1 as user jenkins w0123 185011954649 15708 mastercpp2130 executor default for task 1 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0123 185011954988 15708 mastercpp2142 executor default for task 1 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0123 185011955579 15708 masterhpp782 adding task 1 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 20150123185011167773433752615688s0 localhostlocaldomain i0123 185011956035 15703 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0123 185011957592 15704 replicacpp511 replica received write request for position 4 i0123 185011958485 15708 mastercpp2885 launching task 1 of framework 201501231850111677734337526156880000 default at scheduler2cecb105ca234048970712b1e4422e1112700137526 with resources cpus,1
refactor validators in master there are several motivation for this we are in the process of adding dynamic reservations and persistent volumes support in master to do that master needs to validate relevant operations from the framework see offeroperation in mesosproto the existing validator style in master is hard to extend compose and reuse another motivation for this is for unit testing mesos1064 right now we write integration tests for those validators which is unfortunate,3
masterauthorizationtestframeworkremovedbeforereregistration is flaky good run noformat  run  masterauthorizationtestframeworkremovedbeforereregistration using temporary directory tmpmasterauthorizationtest_frameworkremovedbeforereregistration_zu7oad i0122 192306481690 17483 leveldbcpp176 opened db in 21058723ms i0122 192306488590 17483 leveldbcpp183 compacted db in 66715ms i0122 192306488816 17483 leveldbcpp198 created db iterator in 30034ns i0122 192306489053 17483 leveldbcpp204 seeked to beginning of db in 2908ns i0122 192306489073 17483 leveldbcpp273 iterated through 0 keys in the db in 492ns i0122 192306489148 17483 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0122 192306490272 17504 recovercpp449 starting replica recovery i0122 192306490900 17504 recovercpp475 replica is in empty status i0122 192306492422 17504 replicacpp641 replica in empty status received a broadcasted recover request i0122 192306492694 17504 recovercpp195 received a recover response from a replica in empty status i0122 192306493185 17504 recovercpp566 updating replica status to starting i0122 192306514881 17504 leveldbcpp306 persisting metadata 8 bytes to leveldb took 21459963ms i0122 192306514920 17504 replicacpp323 persisted replica status to starting i0122 192306515861 17501 mastercpp262 master 20150122192306168428794628317483 lucid started on 12701146283 i0122 192306515910 17501 mastercpp308 master only allowing authenticated frameworks to register i0122 192306515923 17501 mastercpp313 master only allowing authenticated slaves to register i0122 192306515946 17501 credentialshpp36 loading credentials for authentication from tmpmasterauthorizationtest_frameworkremovedbeforereregistration_zu7oadcredentials i0122 192306516150 17501 mastercpp357 authorization enabled i0122 192306517511 17501 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0122 192306517607 17501 whitelist_watchercpp65 no whitelist given i0122 192306518066 17498 mastercpp1219 the newly elected leader is master12701146283 with id 20150122192306168428794628317483 i0122 192306518095 17498 mastercpp1232 elected as the leading master i0122 192306518121 17498 mastercpp1050 recovering from registrar i0122 192306518333 17498 registrarcpp313 recovering registrar i0122 192306523987 17504 recovercpp475 replica is in starting status i0122 192306525090 17504 replicacpp641 replica in starting status received a broadcasted recover request i0122 192306525337 17504 recovercpp195 received a recover response from a replica in starting status i0122 192306525693 17504 recovercpp566 updating replica status to voting i0122 192306532680 17504 leveldbcpp306 persisting metadata 8 bytes to leveldb took 6810884ms i0122 192306532714 17504 replicacpp323 persisted replica status to voting i0122 192306532835 17504 recovercpp580 successfully joined the paxos group i0122 192306533004 17504 recovercpp464 recover process terminated i0122 192306533833 17500 logcpp660 attempting to start the writer i0122 192306535225 17500 replicacpp477 replica received implicit promise request with proposal 1 i0122 192306540340 17500 leveldbcpp306 persisting metadata 8 bytes to leveldb took 5086139ms i0122 192306540371 17500 replicacpp345 persisted promised to 1 i0122 192306541502 17504 coordinatorcpp230 coordinator attemping to fill missing position i0122 192306543021 17504 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0122 192306548140 17504 leveldbcpp343 persisting action 8 bytes to leveldb took 5083443ms i0122 192306548171 17504 replicacpp679 persisted action at 0 i0122 192306549746 17500 replicacpp511 replica received write request for position 0 i0122 192306549926 17500 leveldbcpp438 reading position from leveldb took 31962ns i0122 192306555033 17500 leveldbcpp343 persisting action 14 bytes to leveldb took 5065823ms i0122 192306555064 17500 replicacpp679 persisted action at 0 i0122 192306556094 17504 replicacpp658 replica received learned notice for position 0 i0122 192306558815 17504 leveldbcpp343 persisting action 16 bytes to leveldb took 2688382ms i0122 192306558847 17504 replicacpp679 persisted action at 0 i0122 192306558868 17504 replicacpp664 replica learned nop action at position 0 i0122 192306559917 17500 logcpp676 writer started with ending position 0 i0122 192306560995 17500 leveldbcpp438 reading position from leveldb took 27742ns i0122 192306563467 17500 registrarcpp346 successfully fetched the registry 0b in 45095936ms i0122 192306563551 17500 registrarcpp445 applied 1 operations in 19686ns attempting to update the registry i0122 192306566107 17500 logcpp684 attempting to append 118 bytes to the log i0122 192306566267 17500 coordinatorcpp340 coordinator attempting to write append action at position 1 i0122 192306567126 17500 replicacpp511 replica received write request for position 1 i0122 192306582588 17500 leveldbcpp343 persisting action 135 bytes to leveldb took 15425511ms i0122 192306582631 17500 replicacpp679 persisted action at 1 i0122 192306583425 17500 replicacpp658 replica received learned notice for position 1 i0122 192306589001 17500 leveldbcpp343 persisting action 137 bytes to leveldb took 5549486ms i0122 192306589200 17500 replicacpp679 persisted action at 1 i0122 192306589416 17500 replicacpp664 replica learned append action at position 1 i0122 192306596420 17500 registrarcpp490 successfully updated the registry in 32815104ms i0122 192306596551 17500 registrarcpp376 successfully recovered registrar i0122 192306596923 17500 mastercpp1077 recovered 0 slaves from the registry 82b  allowing 10mins for slaves to reregister i0122 192306597007 17500 logcpp703 attempting to truncate the log to 1 i0122 192306597239 17500 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0122 192306598464 17501 replicacpp511 replica received write request for position 2 i0122 192306604038 17501 leveldbcpp343 persisting action 16 bytes to leveldb took 5536264ms i0122 192306604084 17501 replicacpp679 persisted action at 2 i0122 192306608747 17503 replicacpp658 replica received learned notice for position 2 i0122 192306614094 17503 leveldbcpp343 persisting action 18 bytes to leveldb took 5315347ms i0122 192306614171 17503 leveldbcpp401 deleting 1 keys from leveldb took 33021ns i0122 192306614188 17503 replicacpp679 persisted action at 2 i0122 192306614208 17503 replicacpp664 replica learned truncate action at position 2 i0122 192306628820 17483 schedcpp151 version 0220 i0122 192306629879 17505 schedcpp248 new master detected at master12701146283 i0122 192306629973 17505 schedcpp304 authenticating with master master12701146283 i0122 192306629995 17505 schedcpp311 using default crammd5 authenticatee i0122 192306630314 17505 authenticateehpp138 creating new client sasl connection i0122 192306630722 17505 mastercpp4129 authenticating scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306630750 17505 mastercpp4140 using default crammd5 authenticator i0122 192306631115 17505 authenticatorhpp170 creating new server sasl connection i0122 192306631423 17505 authenticateehpp229 received sasl authentication mechanisms crammd5 i0122 192306631459 17505 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0122 192306631563 17505 authenticatorhpp276 received sasl authentication start i0122 192306631605 17505 authenticatorhpp398 authentication requires more steps i0122 192306631671 17505 authenticateehpp275 received sasl authentication step i0122 192306631748 17505 authenticatorhpp304 received sasl authentication step i0122 192306631774 17505 auxpropcpp99 request to lookup properties for user testprincipal realm lucid server fqdn lucid sasl_auxprop_override false sasl_auxprop_authzid false i0122 192306631784 17505 auxpropcpp171 looking up auxiliary property userpassword i0122 192306631822 17505 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0122 192306631856 17505 auxpropcpp99 request to lookup properties for user testprincipal realm lucid server fqdn lucid sasl_auxprop_override false sasl_auxprop_authzid true i0122 192306631870 17505 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0122 192306631877 17505 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0122 192306631892 17505 authenticatorhpp390 authentication success i0122 192306631988 17505 authenticateehpp315 authentication success i0122 192306632066 17505 mastercpp4187 successfully authenticated principal testprincipal at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306632359 17505 schedcpp392 successfully authenticated with master master12701146283 i0122 192306632382 17505 schedcpp515 sending registration request to master12701146283 i0122 192306632432 17505 schedcpp548 will retry registration in 598155756ms if necessary i0122 192306632575 17505 mastercpp1420 received registration request for framework default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306632639 17505 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0122 192306632912 17505 mastercpp1484 registering framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306633421 17505 hierarchical_allocator_processhpp319 added framework 201501221923061684287946283174830000 i0122 192306633448 17505 hierarchical_allocator_processhpp839 no resources available to allocate i0122 192306633458 17505 hierarchical_allocator_processhpp746 performed allocation for 0 slaves in 17704ns i0122 192306633919 17505 schedcpp442 framework registered with 201501221923061684287946283174830000 i0122 192306633980 17505 schedcpp456 schedulerregistered took 37063ns i0122 192306636554 17500 schedcpp242 schedulerdisconnected took 14843ns i0122 192306636579 17500 schedcpp248 new master detected at master12701146283 i0122 192306636625 17500 schedcpp304 authenticating with master master12701146283 i0122 192306636641 17500 schedcpp311 using default crammd5 authenticatee i0122 192306636914 17500 authenticateehpp138 creating new client sasl connection i0122 192306637313 17500 mastercpp4129 authenticating scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306637341 17500 mastercpp4140 using default crammd5 authenticator i0122 192306637675 17500 authenticatorhpp170 creating new server sasl connection i0122 192306638056 17501 authenticateehpp229 received sasl authentication mechanisms crammd5 i0122 192306638083 17501 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0122 192306638182 17501 authenticatorhpp276 received sasl authentication start i0122 192306638221 17501 authenticatorhpp398 authentication requires more steps i0122 192306638286 17501 authenticateehpp275 received sasl authentication step i0122 192306638360 17501 authenticatorhpp304 received sasl authentication step i0122 192306638383 17501 auxpropcpp99 request to lookup properties for user testprincipal realm lucid server fqdn lucid sasl_auxprop_override false sasl_auxprop_authzid false i0122 192306638393 17501 auxpropcpp171 looking up auxiliary property userpassword i0122 192306638422 17501 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0122 192306638447 17501 auxpropcpp99 request to lookup properties for user testprincipal realm lucid server fqdn lucid sasl_auxprop_override false sasl_auxprop_authzid true i0122 192306638458 17501 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0122 192306638464 17501 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0122 192306638478 17501 authenticatorhpp390 authentication success i0122 192306638566 17501 authenticateehpp315 authentication success i0122 192306638643 17501 mastercpp4187 successfully authenticated principal testprincipal at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306638919 17501 schedcpp392 successfully authenticated with master master12701146283 i0122 192306638942 17501 schedcpp515 sending registration request to master12701146283 i0122 192306638994 17501 schedcpp548 will retry registration in 489304713ms if necessary i0122 192306639169 17501 mastercpp1557 received reregistration request from framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306639242 17501 mastercpp1298 authorizing framework principal testprincipal to receive offers for role  i0122 192306639839 17483 schedcpp1471 asked to stop the driver i0122 192306640379 17499 schedcpp808 stopping framework 201501221923061684287946283174830000 i0122 192306640697 17499 mastercpp745 framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 disconnected i0122 192306640723 17499 mastercpp1789 disconnecting framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306640744 17499 mastercpp1805 deactivating framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306640806 17499 mastercpp767 giving framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 0ns to failover i0122 192306640951 17499 hierarchical_allocator_processhpp398 deactivated framework 201501221923061684287946283174830000 i0122 192306646342 17498 mastercpp1604 dropping reregistration request of framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 because it is not authenticated i0122 192306648844 17498 mastercpp3941 framework failover timeout removing framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306648871 17498 mastercpp4499 removing framework 201501221923061684287946283174830000 default at scheduler4156eae68d7f423a920a02b11b7bd1ba12701146283 i0122 192306649624 17498 hierarchical_allocator_processhpp352 removed framework 201501221923061684287946283174830000 i0122 192306656532 17483 mastercpp654 master terminating  ok  masterauthorizationtestframeworkremovedbeforereregistration 216 ms noformat bad run noformat  run  masterauthorizationtestframeworkremovedbeforereregistration using temporary directory tmpmasterauthorizationtest_frameworkremovedbeforereregistration_jdm2sm i0126 191955517570 2381 leveldbcpp176 opened db in 34341401ms i0126 191955529630 2381 leveldbcpp183 compacted db in 11824435ms i0126 191955529878 2381 leveldbcpp198 created db iterator in 26176ns i0126 191955530200 2381 leveldbcpp204 seeked to beginning of db in 3457ns i0126 191955530455 2381 leveldbcpp273 iterated through 0 keys in the db in 902ns i0126 191955530658 2381 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0126 191955531492 2397 recovercpp449 starting replica recovery i0126 191955531793 2397 recovercpp475 replica is in empty status i0126 191955533327 2397 replicacpp641 replica in empty status received a broadcasted recover request i0126 191955533608 2397 recovercpp195 received a recover response from a replica in empty status i0126 191955534101 2397 recovercpp566 updating replica status to starting i0126 191955550417 2397 leveldbcpp306 persisting metadata 8 bytes to leveldb took 16106821ms i0126 191955550472 2397 replicacpp323 persisted replica status to starting i0126 191955551434 2397 recovercpp475 replica is in starting status i0126 191955552846 2397 replicacpp641 replica in starting status received a broadcasted recover request i0126 191955553099 2397 recovercpp195 received a recover response from a replica in starting status i0126 191955553565 2397 recovercpp566 updating replica status to voting i0126 191955564590 2397 leveldbcpp306 persisting metadata 8 bytes to leveldb took 10719218ms i0126 191955564919 2397 replicacpp323 persisted replica status to voting i0126 191955565982 2397 recovercpp580 successfully joined the paxos group i0126 191955566231 2397 recovercpp464 recover process terminated i0126 191955567878 2401 mastercpp262 master 2015012619195516842879518622381 lucid started on 12701151862 i0126 191955567927 2401 mastercpp308 master only allowing authenticated frameworks to register i0126 191955567950 2401 mastercpp313 master only allowing authenticated slaves to register i0126 191955567978 2401 credentialshpp36 loading credentials for authentication from tmpmasterauthorizationtest_frameworkremovedbeforereregistration_jdm2smcredentials i0126 191955568220 2401 mastercpp357 authorization enabled i0126 191955569890 2401 hierarchical_allocator_processhpp285 initialized hierarchical allocator process i0126 191955569999 2401 whitelist_watchercpp65 no whitelist given i0126 191955570694 2401 mastercpp1219 the newly elected leader is master12701151862 with id 2015012619195516842879518622381 i0126 191955570721 2401 mastercpp1232 elected as the leading master i0126 191955570742 2401 mastercpp1050 recovering from registrar i0126 191955570977 2401 registrarcpp313 recovering registrar i0126 191955571959 2401 logcpp660 attempting to start the writer i0126 191955573441 2401 replicacpp477 replica received implicit promise request with proposal 1 i0126 191955590724 2401 leveldbcpp306 persisting metadata 8 bytes to leveldb took 17243964ms i0126 191955590785 2401 replicacpp345 persisted promised to 1 i0126 191955592140 2396 coordinatorcpp230 coordinator attemping to fill missing position i0126 191955593834 2396 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0126 191955603837 2396 leveldbcpp343 persisting action 8 bytes to leveldb took 9955824ms i0126 191955603902 2396 replicacpp679 persisted action at 0 i0126 191955606082 2401 replicacpp511 replica received write request for position 0 i0126 191955606331 2401 leveldbcpp438 reading position from leveldb took 44524ns i0126 191955612546 2401 leveldbcpp343 persisting action 14 bytes to level,1
mesos rejects executorinfo as incompatible when there is no functional difference in aurora1076 it was discovered that if an executorinfo was changed such that a previously unset optional field with a default value was changed to have the field set with the default value it would be rejected as not compatible for example if we have an executorinfo with a commandinfo with the shell attribute unset and then we change the commandinfo to set the shell attribute to true mesos will reject the task with noformat i0130 215005373389 50869 mastercpp3441 sending status update task_lost uuid 82ef615c0d59442795d580cf0e52b3fc for task systemgcc89c0c05200c462e958aecd7b9a76831 of framework 20110328224700000000190000 task has invalid executorinfo existing executorinfo with same executorid is not compatible noformat this is not intuitive because the default value of the shell attribute is true there should be no difference between not setting an optional field with a default value and setting that field to the default value,3
remove unnecessary constants in srcslavepathscpp a number of string constants are defined to describe the formats of various paths however given there is a 11 mapping between the string constant and the functions that build the paths the code would be more readable if the format strings were inline in the functions in the cases where one constant depends on another see the executor_info_path executor_path framework_path slave_path root_path chain for example the function calls can just be chained together this will have the added benefit of removing some statically constructed string constants which are dangerous,2
deprecate  remove commandinfocontainerinfo iiuc this has been deprecated and all current code except examplesdocker_no_executor_frameworkcpp uses the toplevel containerinfo,2
remove deprecated checkpointfalse code codys plan from mesos444 was 1 make it so the flag cant be changed at the command line 2 remove the checkpoint variable entirely from slaveflagshpp this is a fairly involved change since a number of unit tests depend on manually setting the flag as well as the default being noncheckpointing 3 remove logic around checkpointing in the slave remove logic inside the master 4 drop the flag from the slaveinfo struct will require a deprecation cycle,3
unable to set work_dir to a non tmp device when starting mesosslave with work_dir set to a directory which is not the same device as tmp results in mesosslave throwing a core dump code mesos  glog_v1 sbinmesosslave masterzk1017159832181mesos work_dirvarlibmesos warning logging before initgooglelogging is written to stderr i0204 182449274619 22922 processcpp958 libprocess is initialized on 10169146675051 for 8 cpus i0204 182449274978 22922 loggingcpp177 logging to stderr i0204 182449275111 22922 maincpp152 build 20150203 225930 by i0204 182449275233 22922 maincpp154 version 0220 i0204 182449275485 22922 containerizercpp103 using isolation posixcpuposixmem 20150204 182449275229220x7ffdd4d5c700zoo_infolog_env712 client environmentzookeeperversionzookeeper c client 345 20150204 182449275229220x7ffdd4d5c700zoo_infolog_env716 client environmenthostnameip1016914667ec2internal 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env723 client environmentosnamelinux 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env724 client environmentosarch3182 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env725 client environmentosversion2 smp tue jan 27 233436 utc 2015 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env733 client environmentusernamecore 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env741 client environmentuserhomeroot 20150204 182449276229220x7ffdd4d5c700zoo_infolog_env753 client environmentuserdiroptmesospheredcos0010120150203225612mesos 20150204 182449276229220x7ffdd4d5c700zoo_infozookeeper_init786 initiating client connection host1017159832181 sessiontimeout10000 watcher0x7ffdd97bccf0 sessionid0 sessionpasswdnull context0x7ffdc8000ba0 flags0 i0204 182449276793 22922 maincpp180 starting mesos slave 20150204 182449307229220x7ffdd151f700zoo_infocheck_events1703 initiated connection to server 1017159832181 i0204 182449307548 22922 slavecpp173 slave started on 110169146675051 i0204 182449307955 22922 slavecpp300 slave resources cpus1 mem2728 disk24736 ports3100032000 i0204 182449308404 22922 slavecpp329 slave hostname ip1016914667ec2internal i0204 182449308459 22922 slavecpp330 slave checkpoint true i0204 182449310431 22924 statecpp33 recovering state from varlibmesosmeta i0204 182449310583 22924 statecpp668 failed to find resources file varlibmesosmetaresourcesresourcesinfo i0204 182449310670 22924 statecpp74 failed to find the latest slave from varlibmesosmeta i0204 182449310803 22924 status_update_managercpp197 recovering status update manager i0204 182449310916 22924 containerizercpp300 recovering containerizer i0204 182449311110 22924 slavecpp3527 finished recovery f0204 182449311312 22924 slavecpp3537 check_somestatecheckpointpath bootidget failed to rename tmppshlqv to varlibmesosmetaboot_id invalid crossdevice link 20150204 182449310229220x7ffdd151f700zoo_infocheck_events1750 session establishment complete on server 1017159832181 sessionid0x14b51bc8506039a negotiated timeout10000  check failure stack trace   0x7ffdd9a6596d googlelogmessagefail i0204 182449313356 22930 groupcpp313 group process group110169146675051 connected to zookeeper  0x7ffdd9a677ad googlelogmessagesendtolog i0204 182449313786 22930 groupcpp790 syncing group operations queue size joins cancels datas  0 0 0 i0204 182449314487 22930 groupcpp385 trying to create path mesos in zookeeper i0204 182449323668 22930 groupcpp717 found nonsequence node log_replicas at mesos in zookeeper i0204 182449323806 22930 detectorcpp138 detected a new leader id1 i0204 182449323958 22930 groupcpp659 trying to get mesosinfo_0000000001 in zookeeper i0204 182449324595 22930 detectorcpp433 a new leading master upidmaster1017159835050 is detected  0x7ffdd9a6555c googlelogmessageflush  0x7ffdd9a680a9 googlelogmessagefatallogmessagefatal  0x7ffdd94b7179 _checkfatal_checkfatal  0x7ffdd96718e2 mesosinternalslaveslave__recover  0x7ffdd9a1524a processprocessmanagerresume  0x7ffdd9a1550c processschedule  0x7ffdd83832ad unknown  0x7ffdd80b834d unknown aborted core dumped code removing the work_dir option results in the slave starting successfully,2
masterallocatortest0outoforderdispatch is flaky noformattitle  run  masterallocatortest0outoforderdispatch using temporary directory tmpmasterallocatortest_0_outoforderdispatch_kjlb9b i0206 075544084333 15065 leveldbcpp175 opened db in 25006293ms i0206 075544089635 15065 leveldbcpp182 compacted db in 5256332ms i0206 075544089695 15065 leveldbcpp197 created db iterator in 23534ns i0206 075544089710 15065 leveldbcpp203 seeked to beginning of db in 2175ns i0206 075544089720 15065 leveldbcpp272 iterated through 0 keys in the db in 417ns i0206 075544089781 15065 replicacpp743 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0206 075544093750 15086 recovercpp448 starting replica recovery i0206 075544094044 15086 recovercpp474 replica is in empty status i0206 075544095473 15086 replicacpp640 replica in empty status received a broadcasted recover request i0206 075544095724 15086 recovercpp194 received a recover response from a replica in empty status i0206 075544096097 15086 recovercpp565 updating replica status to starting i0206 075544106575 15086 leveldbcpp305 persisting metadata 8 bytes to leveldb took 10289939ms i0206 075544106613 15086 replicacpp322 persisted replica status to starting i0206 075544108144 15086 recovercpp474 replica is in starting status i0206 075544109122 15086 replicacpp640 replica in starting status received a broadcasted recover request i0206 075544110879 15091 recovercpp194 received a recover response from a replica in starting status i0206 075544117267 15087 recovercpp565 updating replica status to voting i0206 075544124771 15087 leveldbcpp305 persisting metadata 8 bytes to leveldb took 266794ms i0206 075544124814 15087 replicacpp322 persisted replica status to voting i0206 075544124948 15087 recovercpp579 successfully joined the paxos group i0206 075544125095 15087 recovercpp463 recover process terminated i0206 075544126204 15087 mastercpp344 master 20150206075544168428793889515065 utopic started on 12701138895 i0206 075544126268 15087 mastercpp390 master only allowing authenticated frameworks to register i0206 075544126281 15087 mastercpp395 master only allowing authenticated slaves to register i0206 075544126307 15087 credentialshpp35 loading credentials for authentication from tmpmasterallocatortest_0_outoforderdispatch_kjlb9bcredentials i0206 075544126683 15087 mastercpp439 authorization enabled i0206 075544129329 15086 mastercpp1350 the newly elected leader is master12701138895 with id 20150206075544168428793889515065 i0206 075544129361 15086 mastercpp1363 elected as the leading master i0206 075544129389 15086 mastercpp1181 recovering from registrar i0206 075544129653 15088 registrarcpp312 recovering registrar i0206 075544130859 15088 logcpp659 attempting to start the writer i0206 075544132334 15088 replicacpp476 replica received implicit promise request with proposal 1 i0206 075544135187 15088 leveldbcpp305 persisting metadata 8 bytes to leveldb took 2825465ms i0206 075544135390 15088 replicacpp344 persisted promised to 1 i0206 075544138062 15091 coordinatorcpp229 coordinator attemping to fill missing position i0206 075544139576 15091 replicacpp377 replica received explicit promise request for position 0 with proposal 2 i0206 075544142156 15091 leveldbcpp342 persisting action 8 bytes to leveldb took 2545543ms i0206 075544142189 15091 replicacpp678 persisted action at 0 i0206 075544143414 15091 replicacpp510 replica received write request for position 0 i0206 075544143468 15091 leveldbcpp437 reading position from leveldb took 28872ns i0206 075544145982 15091 leveldbcpp342 persisting action 14 bytes to leveldb took 2480277ms i0206 075544146015 15091 replicacpp678 persisted action at 0 i0206 075544147050 15089 replicacpp657 replica received learned notice for position 0 i0206 075544154364 15089 leveldbcpp342 persisting action 16 bytes to leveldb took 7281644ms i0206 075544154400 15089 replicacpp678 persisted action at 0 i0206 075544154422 15089 replicacpp663 replica learned nop action at position 0 i0206 075544155506 15091 logcpp675 writer started with ending position 0 i0206 075544156746 15091 leveldbcpp437 reading position from leveldb took 30248ns i0206 075544173681 15091 registrarcpp345 successfully fetched the registry 0b in 43977984ms i0206 075544173821 15091 registrarcpp444 applied 1 operations in 30768ns attempting to update the registry i0206 075544176213 15086 logcpp683 attempting to append 119 bytes to the log i0206 075544176426 15086 coordinatorcpp339 coordinator attempting to write append action at position 1 i0206 075544177608 15088 replicacpp510 replica received write request for position 1 i0206 075544180059 15088 leveldbcpp342 persisting action 136 bytes to leveldb took 2415145ms i0206 075544180094 15088 replicacpp678 persisted action at 1 i0206 075544181324 15084 replicacpp657 replica received learned notice for position 1 i0206 075544183831 15084 leveldbcpp342 persisting action 138 bytes to leveldb took 2473724ms i0206 075544183866 15084 replicacpp678 persisted action at 1 i0206 075544183887 15084 replicacpp663 replica learned append action at position 1 i0206 075544185510 15084 registrarcpp489 successfully updated the registry in 11619072ms i0206 075544185678 15086 logcpp702 attempting to truncate the log to 1 i0206 075544186111 15086 coordinatorcpp339 coordinator attempting to write truncate action at position 2 i0206 075544186944 15086 replicacpp510 replica received write request for position 2 i0206 075544187492 15084 registrarcpp375 successfully recovered registrar i0206 075544188016 15087 mastercpp1208 recovered 0 slaves from the registry 83b  allowing 10mins for slaves to reregister i0206 075544189678 15086 leveldbcpp342 persisting action 16 bytes to leveldb took 2702559ms i0206 075544189713 15086 replicacpp678 persisted action at 2 i0206 075544190620 15086 replicacpp657 replica received learned notice for position 2 i0206 075544193383 15086 leveldbcpp342 persisting action 18 bytes to leveldb took 2737088ms i0206 075544193455 15086 leveldbcpp400 deleting 1 keys from leveldb took 37762ns i0206 075544193475 15086 replicacpp678 persisted action at 2 i0206 075544193496 15086 replicacpp663 replica learned truncate action at position 2 i0206 075544200028 15065 containerizercpp102 using isolation posixcpuposixmem i0206 075544212924 15088 slavecpp172 slave started on 4612701138895 i0206 075544213762 15088 credentialshpp83 loading credential for authentication from tmpmasterallocatortest_0_outoforderdispatch_runyvqcredential i0206 075544214251 15088 slavecpp281 slave using credential for testprincipal i0206 075544214653 15088 slavecpp299 slave resources cpus2 mem1024 disk24988 ports3100032000 i0206 075544214918 15088 slavecpp328 slave hostname utopic i0206 075544215116 15088 slavecpp329 slave checkpoint false w0206 075544215332 15088 slavecpp331 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0206 075544217061 15090 statecpp32 recovering state from tmpmasterallocatortest_0_outoforderdispatch_runyvqmeta i0206 075544235409 15088 status_update_managercpp196 recovering status update manager i0206 075544235601 15088 containerizercpp299 recovering containerizer i0206 075544236486 15088 slavecpp3526 finished recovery i0206 075544237709 15087 status_update_managercpp170 pausing sending status updates i0206 075544237890 15088 slavecpp620 new master detected at master12701138895 i0206 075544241575 15088 slavecpp683 authenticating with master master12701138895 i0206 075544247459 15088 slavecpp688 using default crammd5 authenticatee i0206 075544248617 15089 authenticateehpp137 creating new client sasl connection i0206 075544249099 15089 mastercpp3788 authenticating slave4612701138895 i0206 075544249137 15089 mastercpp3799 using default crammd5 authenticator i0206 075544249728 15089 authenticatorhpp169 creating new server sasl connection i0206 075544250285 15089 authenticateehpp228 received sasl authentication mechanisms crammd5 i0206 075544250496 15089 authenticateehpp254 attempting to authenticate with mechanism crammd5 i0206 075544250452 15088 slavecpp656 detecting new master i0206 075544251063 15091 authenticatorhpp275 received sasl authentication start i0206 075544251124 15091 authenticatorhpp397 authentication requires more steps i0206 075544251256 15089 authenticateehpp274 received sasl authentication step i0206 075544251451 15090 authenticatorhpp303 received sasl authentication step i0206 075544251575 15090 authenticatorhpp389 authentication success i0206 075544251687 15090 mastercpp3846 successfully authenticated principal testprincipal at slave4612701138895 i0206 075544253306 15089 authenticateehpp314 authentication success i0206 075544258015 15089 slavecpp754 successfully authenticated with master master12701138895 i0206 075544258468 15089 mastercpp2913 registering slave at slave4612701138895 utopic with id 20150206075544168428793889515065s0 i0206 075544259028 15089 registrarcpp444 applied 1 operations in 88902ns attempting to update the registry i0206 075544269492 15065 schedcpp149 version 0220 i0206 075544270539 15090 schedcpp246 new master detected at master12701138895 i0206 075544270614 15090 schedcpp302 authenticating with master master12701138895 i0206 075544270634 15090 schedcpp309 using default crammd5 authenticatee i0206 075544270900 15090 authenticateehpp137 creating new client sasl connection i0206 075544272300 15089 logcpp683 attempting to append 285 bytes to the log i0206 075544272552 15089 coordinatorcpp339 coordinator attempting to write append action at position 3 i0206 075544273609 15086 mastercpp3788 authenticating schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544273643 15086 mastercpp3799 using default crammd5 authenticator i0206 075544273955 15086 authenticatorhpp169 creating new server sasl connection i0206 075544274617 15090 authenticateehpp228 received sasl authentication mechanisms crammd5 i0206 075544274813 15090 authenticateehpp254 attempting to authenticate with mechanism crammd5 i0206 075544275171 15088 authenticatorhpp275 received sasl authentication start i0206 075544275215 15088 authenticatorhpp397 authentication requires more steps i0206 075544275408 15090 authenticateehpp274 received sasl authentication step i0206 075544275696 15084 authenticatorhpp303 received sasl authentication step i0206 075544275774 15084 authenticatorhpp389 authentication success i0206 075544275876 15084 mastercpp3846 successfully authenticated principal testprincipal at schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544277593 15090 authenticateehpp314 authentication success i0206 075544278201 15086 schedcpp390 successfully authenticated with master master12701138895 i0206 075544278548 15086 mastercpp1568 received registration request for framework framework1 at schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544278642 15086 mastercpp1429 authorizing framework principal testprincipal to receive offers for role  i0206 075544279157 15086 mastercpp1632 registering framework 201502060755441684287938895150650000 framework1 at schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544280081 15086 schedcpp440 framework registered with 201502060755441684287938895150650000 i0206 075544280320 15086 hierarchical_allocator_processhpp318 added framework 201502060755441684287938895150650000 i0206 075544281411 15089 replicacpp510 replica received write request for position 3 i0206 075544282289 15085 mastercpp2901 ignoring register slave message from slave4612701138895 utopic as admission is already in progress i0206 075544284984 15089 leveldbcpp342 persisting action 304 bytes to leveldb took 3368213ms i0206 075544285020 15089 replicacpp678 persisted action at 3 i0206 075544285893 15089 replicacpp657 replica received learned notice for position 3 i0206 075544288350 15089 leveldbcpp342 persisting action 306 bytes to leveldb took 2430449ms i0206 075544288384 15089 replicacpp678 persisted action at 3 i0206 075544288405 15089 replicacpp663 replica learned append action at position 3 i0206 075544290154 15089 registrarcpp489 successfully updated the registry in 31046912ms i0206 075544290307 15085 logcpp702 attempting to truncate the log to 3 i0206 075544290671 15085 coordinatorcpp339 coordinator attempting to write truncate action at position 4 i0206 075544291482 15085 replicacpp510 replica received write request for position 4 i0206 075544292559 15087 mastercpp2970 registered slave 20150206075544168428793889515065s0 at slave4612701138895 utopic with cpus2 mem1024 disk24988 ports3100032000 i0206 075544292940 15087 slavecpp788 registered with master master12701138895 given slave id 20150206075544168428793889515065s0 i0206 075544293298 15087 hierarchical_allocator_processhpp450 added slave 20150206075544168428793889515065s0 utopic with cpus2 mem1024 disk24988 ports3100032000 and cpus2 mem1024 disk24988 ports3100032000 available i0206 075544293684 15087 status_update_managercpp177 resuming sending status updates i0206 075544294085 15087 mastercpp3730 sending 1 offers to framework 201502060755441684287938895150650000 framework1 at schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544299957 15085 leveldbcpp342 persisting action 16 bytes to leveldb took 8442691ms i0206 075544300165 15085 replicacpp678 persisted action at 4 i0206 075544300698 15065 schedcpp1468 asked to stop the driver i0206 075544301127 15090 schedcpp806 stopping framework 201502060755441684287938895150650000 i0206 075544301503 15090 mastercpp1892 asked to unregister framework 201502060755441684287938895150650000 i0206 075544301535 15090 mastercpp4158 removing framework 201502060755441684287938895150650000 framework1 at schedulerd6cac0a1d4614a05b19d5cbdae239eb012701138895 i0206 075544302376 15090 slavecpp1592 asked to shut down framework 201502060755441684287938895150650000 by master12701138895 w0206 075544302407 15090 slavecpp1607 cannot shut down unknown framework 201502060755441684287938895150650000 i0206 075544302814 15090 hierarchical_allocator_processhpp397 deactivated framework 201502060755441684287938895150650000 i0206 075544302947 15090 hierarchical_allocator_processhpp351 removed framework 201502060755441684287938895150650000 i0206 075544309281 15086 hierarchical_allocator_processhpp642 recovered cpus2 mem1024 disk24988 ports3100032000 total allocatable cpus2 mem1024 disk24988 ports3100032000 on slave 20150206075544168428793889515065s0 from framework 201502060755441684287938895150650000 i0206 075544310158 15084 replicacpp657 replica received learned notice for position 4 i0206 075544313246 15084 leveldbcpp342 persisting action 18 bytes to leveldb took 3055049ms i0206 075544313328 15084 leveldbcpp400 deleting 2 keys from leveldb took 45270ns i0206 075544313349 15084 replicacpp678 persisted action at 4 i0206 075544313374 15084 replicacpp663 replica learned truncate action at position 4 i0206 075544329591 15065 schedcpp149 version 0220 i0206 075544330258 15088 schedcpp246 new master detected at master12701138895 i0206 075544330346 15088 schedcpp302 authenticating with master master12701138895 i0206 075544330368 15088 schedcpp309 using default crammd5 authenticatee i0206 075544330652 15088 authenticateehpp137 creating new client sasl connection i0206 075544331403 15088 mastercpp3788 authenticating scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 i0206 075544331717 15088 mastercpp3799 using default crammd5 authenticator i0206 075544332293 15088 authenticatorhpp169 creating new server sasl connection i0206 075544332655 15088 authenticateehpp228 received sasl authentication mechanisms crammd5 i0206 075544332684 15088 authenticateehpp254 attempting to authenticate with mechanism crammd5 i0206 075544332792 15088 authenticatorhpp275 received sasl authentication start i0206 075544332835 15088 authenticatorhpp397 authentication requires more steps i0206 075544332903 15088 authenticateehpp274 received sasl authentication step i0206 075544332983 15088 authenticatorhpp303 received sasl authentication step i0206 075544333056 15088 authenticatorhpp389 authentication success i0206 075544333153 15088 authenticateehpp314 authentication success i0206 075544333297 15091 mastercpp3846 successfully authenticated principal testprincipal at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 i0206 075544334326 15087 schedcpp390 successfully authenticated with master master12701138895 i0206 075544334645 15087 mastercpp1568 received registration request for framework framework2 at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 i0206 075544334722 15087 mastercpp1429 authorizing framework principal testprincipal to receive offers for role  i0206 075544335153 15087 mastercpp1632 registering framework 201502060755441684287938895150650001 framework2 at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 i0206 075544336019 15087 schedcpp440 framework registered with 201502060755441684287938895150650001 i0206 075544336156 15087 hierarchical_allocator_processhpp318 added framework 201502060755441684287938895150650001 i0206 075544336796 15087 mastercpp3730 sending 1 offers to framework 201502060755441684287938895150650001 framework2 at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 i0206 075544337725 15065 schedcpp1468 asked to stop the driver i0206 075544338002 15086 schedcpp806 stopping framework 201502060755441684287938895150650001 i0206 075544338297 15090 mastercpp1892 asked to unregister framework 201502060755441684287938895150650001 i0206 075544338353 15090 mastercpp4158 removing framework 201502060755441684287938895150650001 framework2 at scheduler7bdaa90beb9f4009bd5ad07fd3f24cec12701138895 srctestsmaster_allocator_testscpp300 failure mock function called more times than expected  taking default action specified at srctestsmesoshpp713 function call deactivateframework0x7fdb74008d70 201502060755441684287938895150650001 expected to be called once actual called twice  over,1
report percontainer metrics for network bandwidth throttling export metrics from the network isolation to identify scope and duration of container throttling packet loss can be identified from the overlimits and requeues fields of the htb qdisc report for the virtual interface eg noformat  tc s d qdisc show dev mesos19223 qdisc pfifo_fast 0 root refcnt 2 bands 3 priomap 1 2 2 2 1 2 0 0 1 1 1 1 1 1 1 1 sent 158213287452 bytes 1030876393 pkt dropped 0 overlimits 0 requeues 0 backlog 0b 0p requeues 0 qdisc ingress ffff parent fffffff1  sent 119381747824 bytes 1144549901 pkt dropped 2044879 overlimits 0 requeues 0 backlog 0b 0p requeues 0 noformat note that since a packet can be examined multiple times before transmission overlimits can exceed total packets sent add to the port_mapping isolator usage and the container statistics protobuf carefully consider the naming esp txrx  commenting of the protobuf fields so its clear what these represent and how they are different to the existing dropped packet counts from the network stack,5
mesos lifecycle modules a new kind of module that receives callbacks at significant life cycle events of its host libprocess process typically the latter is a mesos slave or master and the life time of the libprocess process coincides with the underlying os process h4 motivation and use cases we want to add customized and experimental capabilities that concern the life time of mesos components without protruding into mesos source code and without creating new build process dependencies for everybody example use cases 1 a slave or master life cycle module that gathers failover incidents and reports summaries thereof to a remote data sink 2 a slave module that observes host computer metrics and correlates these with task activity this can be used to find resources leaks and to prevent respectively guide oversubscription 3 upgrades and provisioning that require shutdown and restart h4 specifics the specific life cycle events that we want to get notified about and want to be able to act upon are  process is spawninginitializing  process is terminatingfinalizing in all these cases a reference to the process is passed as a parameter giving the module access for inspection and reaction h4 module classification unlike other named modules a life cycle module does not directly replace or provide essential mesos functionality such as an isolator module does unlike a decorator module it does not directly add or inject data into mesos core either,1
__init__py not getting installed in prefixlibpythonxysitepackagesmesos when doing a make install the srcpythonnativesrcmesos__init__py file is not getting installed in prefixlibpythonxysitepackagesmesos this makes it impossible to do the following import when pythonpath is set to the sitepackages directory code import mesosinterfacemesos_pb2 code the directories prefixlibpythonxysitepackagesmesosinterface native do have their corresponding __init__py files reproducing the bug code configure prefixhometestinstall  make install code,2
add ability to decode json serialized masterinfo from zk currently to discover the master a client needs the zk node location and access to the masterinfo protobuf so it can deserialize the binary blob in the node i think it would be nice to publish json like twitters serversets so clients are not tied to protobuf to do service discovery this ticket is an intermediate compatibility step we add in 023 the ability for the detector to understand json alongside protobuf serialized format this makes it compatible with both earlier versions as well a future one most likely 024 that will write the masterinfo information in json format,5
add ability for schedulers to explicitly acknowledge status updates on the driver in order for schedulers to be able to handle status updates in a scalable manner they need the ability to send acknowledgements through the driver this enables optimizations in schedulers eg process status updates asynchronously wo backing up the driver processingacking updates in batch without this an implicit reconciliation can overload a scheduler hence the motivation for mesos2308,8
provide a way to execute an arbitrary process in a mesoscontainerizer container context include a separate binary that when provided with a container_id path to an executable and optional arguments will find the container context enter it and exec the executable eg noformat mesoscontainerexec container_idabc123  pathtoexecutable arg1  noformat this need only support initially containers created with the mesoscontainerizer and will support all isolators shipped with mesos ie it should find and enter the cgroups and namespaces for the running executor of the specified container,5
add support for mesoscontainerizerlaunch to chroot to a specified path in preparation for the mesoscontainerizer to support a filesystem isolator the mesoscontainerizerlauncher must support chrooting optionally it should also configure the chroot environment by remounting special filesystems such as proc and sys and making device nodes such as devzero etc such that the chroot environment is functional,5
improve performance of the statejson endpoint for large clusters the masters statejson endpoint consistently takes a long time to compute the json result for large clusters noformat  time curl s o devnull localhost5050masterstatejson mon jan 26 223850 utc 2015 real 0m13174s user 0m0003s sys 0m0022s noformat this can cause the master to get backlogged if there are many statejson requests in flight looking at perf data it seems most of the time is spent doing memory allocation  deallocation this ticket will try to capture any low hanging fruit to speed this up possibly we can leverage moves if they are not already being used by the compiler,5
masterslavereconciliationtestreconcilelosttask is flaky httpsbuildsapacheorgjobmesostrunkubuntubuildoutofsrcdisablejavadisablepythondisablewebui2746changes code  run  masterslavereconciliationtestreconcilelosttask using temporary directory tmpmasterslavereconciliationtest_reconcilelosttask_rgb8ff i0218 015326881561 13918 leveldbcpp175 opened db in 2891605ms i0218 015326882547 13918 leveldbcpp182 compacted db in 953447ns i0218 015326882596 13918 leveldbcpp197 created db iterator in 20629ns i0218 015326882616 13918 leveldbcpp203 seeked to beginning of db in 2370ns i0218 015326882627 13918 leveldbcpp272 iterated through 0 keys in the db in 348ns i0218 015326882664 13918 replicacpp743 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0218 015326883124 13947 recovercpp448 starting replica recovery i0218 015326883625 13941 recovercpp474 replica is in 4 status i0218 015326884744 13945 replicacpp640 replica in 4 status received a broadcasted recover request i0218 015326885118 13939 recovercpp194 received a recover response from a replica in 4 status i0218 015326885565 13933 recovercpp565 updating replica status to 3 i0218 015326886548 13932 leveldbcpp305 persisting metadata 8 bytes to leveldb took 733223ns i0218 015326886574 13932 replicacpp322 persisted replica status to 3 i0218 015326886714 13943 mastercpp347 master 2015021801532631426977955726813918 pomonaapacheorg started on 671958118757268 i0218 015326886760 13943 mastercpp393 master only allowing authenticated frameworks to register i0218 015326886772 13943 mastercpp398 master only allowing authenticated slaves to register i0218 015326886798 13943 credentialshpp36 loading credentials for authentication from tmpmasterslavereconciliationtest_reconcilelosttask_rgb8ffcredentials i0218 015326886826 13934 recovercpp474 replica is in 3 status i0218 015326887151 13943 mastercpp440 authorization enabled i0218 015326887866 13944 replicacpp640 replica in 3 status received a broadcasted recover request i0218 015326887969 13942 whitelist_watchercpp78 no whitelist given i0218 015326888021 13940 hierarchicalhpp286 initialized hierarchical allocator process i0218 015326888178 13934 recovercpp194 received a recover response from a replica in 3 status i0218 015326889114 13943 mastercpp1354 the newly elected leader is master671958118757268 with id 2015021801532631426977955726813918 i0218 015327064930 13948 processcpp2117 dropped  lost event for pid hierarchicalallocator183671958118757268 i0218 015327911870 13943 mastercpp1367 elected as the leading master i0218 015327911911 13943 mastercpp1185 recovering from registrar i0218 015327912106 13948 processcpp2117 dropped  lost event for pid scheduler93f780065b69498bb4e387cdf8062263671958118757268 i0218 015327912255 13932 registrarcpp312 recovering registrar i0218 015327912307 13948 processcpp2117 dropped  lost event for pid hierarchicalallocator179671958118757268 i0218 015327912626 13940 hierarchicalhpp831 no resources available to allocate i0218 015327912658 13940 hierarchicalhpp738 performed allocation for 0 slaves in 60316ns i0218 015327912838 13947 recovercpp565 updating replica status to 1 i0218 015327913966 13947 leveldbcpp305 persisting metadata 8 bytes to leveldb took 921045ns i0218 015327913998 13947 replicacpp322 persisted replica status to 1 i0218 015327914106 13932 recovercpp579 successfully joined the paxos group i0218 015327914378 13932 recovercpp463 recover process terminated i0218 015327914916 13939 logcpp659 attempting to start the writer i0218 015327916374 13937 replicacpp476 replica received implicit promise request with proposal 1 i0218 015327916941 13937 leveldbcpp305 persisting metadata 8 bytes to leveldb took 534122ns i0218 015327916967 13937 replicacpp344 persisted promised to 1 i0218 015327917795 13936 coordinatorcpp229 coordinator attemping to fill missing position i0218 015327919147 13941 replicacpp377 replica received explicit promise request for position 0 with proposal 2 i0218 015327919492 13941 leveldbcpp342 persisting action 8 bytes to leveldb took 306270ns i0218 015327919517 13941 replicacpp678 persisted action at 0 i0218 015327920755 13934 replicacpp510 replica received write request for position 0 i0218 015327920819 13934 leveldbcpp437 reading position from leveldb took 33747ns i0218 015327921195 13934 leveldbcpp342 persisting action 14 bytes to leveldb took 340479ns i0218 015327921221 13934 replicacpp678 persisted action at 0 i0218 015327921916 13932 replicacpp657 replica received learned notice for position 0 i0218 015327922339 13932 leveldbcpp342 persisting action 16 bytes to leveldb took 392653ns i0218 015327922365 13932 replicacpp678 persisted action at 0 i0218 015327922386 13932 replicacpp663 replica learned 1 action at position 0 i0218 015327923009 13945 logcpp675 writer started with ending position 0 i0218 015327924167 13937 leveldbcpp437 reading position from leveldb took 29219ns i0218 015327927683 13932 registrarcpp345 successfully fetched the registry 0b in 15376128ms i0218 015327927789 13932 registrarcpp444 applied 1 operations in 23004ns attempting to update the registry i0218 015327929957 13947 logcpp683 attempting to append 139 bytes to the log i0218 015327930058 13936 coordinatorcpp339 coordinator attempting to write 2 action at position 1 i0218 015327930637 13934 replicacpp510 replica received write request for position 1 i0218 015327930954 13934 leveldbcpp342 persisting action 158 bytes to leveldb took 286664ns i0218 015327930975 13934 replicacpp678 persisted action at 1 i0218 015327931521 13942 replicacpp657 replica received learned notice for position 1 i0218 015327931813 13942 leveldbcpp342 persisting action 160 bytes to leveldb took 267316ns i0218 015327931833 13942 replicacpp678 persisted action at 1 i0218 015327931849 13942 replicacpp663 replica learned 2 action at position 1 i0218 015327932617 13935 registrarcpp489 successfully updated the registry in 4722944ms i0218 015327932726 13935 registrarcpp375 successfully recovered registrar i0218 015327932751 13940 logcpp702 attempting to truncate the log to 1 i0218 015327932865 13944 coordinatorcpp339 coordinator attempting to write 3 action at position 2 i0218 015327932998 13939 mastercpp1212 recovered 0 slaves from the registry 101b  allowing 10mins for slaves to reregister i0218 015327933732 13936 replicacpp510 replica received write request for position 2 i0218 015327934146 13936 leveldbcpp342 persisting action 16 bytes to leveldb took 386584ns i0218 015327934167 13936 replicacpp678 persisted action at 2 i0218 015327934708 13935 replicacpp657 replica received learned notice for position 2 i0218 015327935081 13935 leveldbcpp342 persisting action 18 bytes to leveldb took 350891ns i0218 015327935127 13935 leveldbcpp400 deleting 1 keys from leveldb took 24983ns i0218 015327935140 13935 replicacpp678 persisted action at 2 i0218 015327935158 13935 replicacpp663 replica learned 3 action at position 2 i0218 015327947561 13918 containerizercpp104 using isolation posixcpuposixmem i0218 015327948971 13941 slavecpp173 slave started on 150671958118757268 i0218 015327949003 13941 credentialshpp84 loading credential for authentication from tmpmasterslavereconciliationtest_reconcilelosttask_5no5rjcredential i0218 015327949167 13941 slavecpp280 slave using credential for testprincipal i0218 015327949465 13941 slavecpp298 slave resources cpus2 mem1024 disk1024 ports3100032000 i0218 015327949556 13941 slavecpp327 slave hostname pomonaapacheorg i0218 015327949575 13941 slavecpp328 slave checkpoint false w0218 015327949587 13941 slavecpp330 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0218 015327950536 13932 statecpp34 recovering state from tmpmasterslavereconciliationtest_reconcilelosttask_5no5rjmeta i0218 015327950783 13940 status_update_managercpp196 recovering status update manager i0218 015327953531 13944 containerizercpp301 recovering containerizer i0218 015327953944 13918 schedcpp151 version 0220 i0218 015327954617 13932 slavecpp3611 finished recovery i0218 015327954732 13935 schedcpp248 new master detected at master671958118757268 i0218 015327954833 13935 schedcpp304 authenticating with master master671958118757268 i0218 015327954856 13935 schedcpp311 using default crammd5 authenticatee i0218 015327955037 13947 authenticateehpp138 creating new client sasl connection i0218 015327955198 13944 status_update_managercpp170 pausing sending status updates i0218 015327955195 13941 slavecpp623 new master detected at master671958118757268 i0218 015327955238 13934 mastercpp3811 authenticating scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015327955270 13934 mastercpp3822 using default crammd5 authenticator i0218 015327955317 13941 slavecpp686 authenticating with master master671958118757268 i0218 015327955348 13941 slavecpp691 using default crammd5 authenticatee i0218 015327955518 13933 authenticatorhpp169 creating new server sasl connection i0218 015327955534 13939 authenticateehpp138 creating new client sasl connection i0218 015327955693 13935 authenticateehpp229 received sasl authentication mechanisms crammd5 i0218 015327955732 13935 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0218 015327955844 13932 authenticatorhpp275 received sasl authentication start i0218 015327955905 13932 authenticatorhpp397 authentication requires more steps i0218 015327955999 13935 authenticateehpp275 received sasl authentication step i0218 015327956120 13932 authenticatorhpp303 received sasl authentication step i0218 015327957321 13941 slavecpp659 detecting new master i0218 015327957473 13934 mastercpp3811 authenticating slave150671958118757268 i0218 015328009866 13948 processcpp2117 dropped  lost event for pid slave146671958118757268 i0218 015328592335 13932 auxpropcpp98 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0218 015328592350 13934 mastercpp3822 using default crammd5 authenticator i0218 015328592367 13932 auxpropcpp170 looking up auxiliary property userpassword i0218 015328592434 13932 auxpropcpp170 looking up auxiliary property cmusaslsecretcrammd5 i0218 015328592483 13932 auxpropcpp98 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0218 015328592501 13932 auxpropcpp120 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0218 015328592510 13932 auxpropcpp120 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0218 015328592530 13932 authenticatorhpp389 authentication success i0218 015328592646 13935 authenticateehpp315 authentication success i0218 015328592686 13948 processcpp2117 dropped  lost event for pid scheduler4eee5e93d6bb4af497950aec0916dfa5671958118757268 i0218 015328592800 13939 authenticatorhpp169 creating new server sasl connection i0218 015328592836 13948 processcpp2117 dropped  lost event for pid hierarchicalallocator180671958118757268 i0218 015328592864 13934 mastercpp3869 successfully authenticated principal testprincipal at scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015328592990 13933 authenticateehpp229 received sasl authentication mechanisms crammd5 i0218 015328593029 13933 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0218 015328593245 13933 authenticatorhpp275 received sasl authentication start i0218 015328593364 13933 authenticatorhpp397 authentication requires more steps i0218 015328593490 13941 schedcpp392 successfully authenticated with master master671958118757268 i0218 015328593519 13941 schedcpp515 sending registration request to master671958118757268 i0218 015328593531 13945 authenticateehpp275 received sasl authentication step i0218 015328593606 13941 schedcpp548 will retry registration in 1707160316secs if necessary i0218 015328593720 13933 authenticatorhpp303 received sasl authentication step i0218 015328593731 13939 mastercpp1572 received registration request for framework default at scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015328593757 13933 auxpropcpp98 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0218 015328593780 13933 auxpropcpp170 looking up auxiliary property userpassword i0218 015328593818 13939 mastercpp1433 authorizing framework principal testprincipal to receive offers for role  i0218 015328593823 13933 auxpropcpp170 looking up auxiliary property cmusaslsecretcrammd5 i0218 015328593891 13933 auxpropcpp98 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0218 015328593909 13933 auxpropcpp120 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0218 015328593919 13933 auxpropcpp120 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0218 015328593947 13933 authenticatorhpp389 authentication success i0218 015328594048 13945 authenticateehpp315 authentication success i0218 015328594140 13946 mastercpp3869 successfully authenticated principal testprincipal at slave150671958118757268 i0218 015328594383 13947 slavecpp757 successfully authenticated with master master671958118757268 i0218 015328594571 13947 slavecpp1089 will retry registration in 17484321ms if necessary i0218 015328594606 13946 mastercpp1636 registering framework 20150218015326314269779557268139180000 default at scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015328594995 13944 hierarchicalhpp320 added framework 20150218015326314269779557268139180000 i0218 015328595034 13944 hierarchicalhpp831 no resources available to allocate i0218 015328595057 13944 hierarchicalhpp738 performed allocation for 0 slaves in 35451ns i0218 015328595185 13937 schedcpp442 framework registered with 20150218015326314269779557268139180000 i0218 015328595232 13937 schedcpp456 schedulerregistered took 22922ns i0218 015328595273 13946 mastercpp2936 registering slave at slave150671958118757268 pomonaapacheorg with id 2015021801532631426977955726813918s0 i0218 015328595803 13934 registrarcpp444 applied 1 operations in 74798ns attempting to update the registry i0218 015328598387 13939 logcpp683 attempting to append 316 bytes to the log i0218 015328598578 13938 coordinatorcpp339 coordinator attempting to write 2 action at position 3 i0218 015328599488 13932 replicacpp510 replica received write request for position 3 i0218 015328599758 13932 leveldbcpp342 persisting action 335 bytes to leveldb took 234907ns i0218 015328599786 13932 replicacpp678 persisted action at 3 i0218 015328600777 13939 replicacpp657 replica received learned notice for position 3 i0218 015328601304 13939 leveldbcpp342 persisting action 337 bytes to leveldb took 503852ns i0218 015328601326 13939 replicacpp678 persisted action at 3 i0218 015328601346 13939 replicacpp663 replica learned 2 action at position 3 i0218 015328602901 13934 logcpp702 attempting to truncate the log to 3 i0218 015328603011 13938 coordinatorcpp339 coordinator attempting to write 3 action at position 4 i0218 015328603135 13932 registrarcpp489 successfully updated the registry in 7035904ms i0218 015328603687 13932 replicacpp510 replica received write request for position 4 i0218 015328603844 13934 slavecpp2666 received ping from slaveobserver147671958118757268 i0218 015328603945 13941 mastercpp2993 registered slave 2015021801532631426977955726813918s0 at slave150671958118757268 pomonaapacheorg with cpus2 mem1024 disk1024 ports3100032000 i0218 015328604046 13933 hierarchicalhpp452 added slave 2015021801532631426977955726813918s0 pomonaapacheorg with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0218 015328604112 13932 leveldbcpp342 persisting action 16 bytes to leveldb took 399822ns i0218 015328604131 13932 replicacpp678 persisted action at 4 i0218 015328605741 13933 hierarchicalhpp756 performed allocation for slave 2015021801532631426977955726813918s0 in 1649293ms i0218 015328605836 13934 slavecpp791 registered with master master671958118757268 given slave id 2015021801532631426977955726813918s0 i0218 015328606003 13933 replicacpp657 replica received learned notice for position 4 i0218 015328606037 13947 status_update_managercpp177 resuming sending status updates i0218 015328606075 13937 mastercpp3753 sending 1 offers to framework 20150218015326314269779557268139180000 default at scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015328606547 13933 leveldbcpp342 persisting action 18 bytes to leveldb took 517378ns i0218 015329008322 13933 leveldbcpp400 deleting 2 keys from leveldb took 86406ns i0218 015329008350 13933 replicacpp678 persisted action at 4 i0218 015329008380 13933 replicacpp663 replica learned 3 action at position 4 i0218 015328912961 13946 hierarchicalhpp831 no resources available to allocate i0218 015329008543 13946 hierarchicalhpp738 performed allocation for 1 slaves in 95683965ms i0218 015329008621 13944 schedcpp605 schedulerresourceoffers took 74896ns i0218 015329009996 13932 mastercpp2266 processing accept call for offers  2015021801532631426977955726813918o0  on slave 2015021801532631426977955726813918s0 at slave150671958118757268 pomonaapacheorg for framework 20150218015326314269779557268139180000 default at scheduler17aa8fa2195f43d685d787b949d4419b671958118757268 i0218 015329010035 13932 mastercpp2110 authorizing framework principal testprincipal to launch task 1 as user jenkins w0218 015329011081 13932 validationcpp326 executor default for task 1 uses less cpus none than the minimum required 001 please update your executor as this will be,1
improve slave resiliency in the face of orphan containers right now theres a case where a misbehaving executor can cause a slave process to flap paneltitlequote from jieyu quote 1 user tries to kill an instance 2 slave sends killtaskmessage to executor 3 executor sends kill signals to task processes 4 executor sends task_killed to slave 5 slave updates container cpu limit to be 001 cpus 6 a userprocess is still processing the kill signal 7 the task process cannot exit since it has too little cpu share and is throttled 8 executor itself terminates 9 slave tries to destroy the container but cannot because the userprocess is stuck in the exit path 10 slave restarts and is constantly flapping because it cannot kill orphan containers quote panel the slaves orphan container handling should be improved to deal with this case despite illbehaved users framework writers,5
test script for verifying compatibility between mesos components while our current unitintegration test suite catches functional bugs it doesnt catch compatibility bugs eg mesos2371 this is really crucial to provide operators the ability to do seamless upgrades on live clusters we should have a test suite  framework ideally running on ci vetting each review on rb that tests upgrade paths between master slave scheduler and executor,2
drfsorter needs to distinguish resources from different slaves currently the drfsorter aggregates total and allocated resources across multiple slaves which only works for scalar resources we need to distinguish resources from different slaves suppose we have 2 slaves and 1 framework the framework is allocated all resources from both slaves code resources slaveresources  resourcesparsecpus2mem512ports3100032000get drfsorter sorter sorteraddslaveresources  add slave1 resources sorteraddslaveresources  add slave2 resources  total resources in sorter at this point is  cpus4 mem1024 ports3100032000  the scalar resources get aggregated correctly but ports do not sorteraddf  the 2 calls to allocated only works because we simply do  allocationname  resources  without checking that the resources is available in the total sorterallocatedf slaveresources sorterallocatedf slaveresources  at this point sorterallocationf is  cpus4 mem1024 ports3100032000 code to provide some context this issue came up while trying to reserve all unreserved resources from every offer code for const offer offer  offers  resources unreserved  offerresourcesunreserved resources reserved  unreservedflattenrole resourceframework offeroperation reserve reserveset_typeofferoperationreserve reservemutable_reservemutable_resourcescopyfromreserved driveracceptoffersofferid reserve  code suppose the slave resources are the same as above quote slave1 cpus2 mem512 ports3100032000 slave2 cpus2 mem512 ports3100032000 quote initial incorrect total resources in the drfsorter is quote cpus4 mem1024 ports3100032000 quote we receive 2 offers 1 from each slave quote offer1 cpus2 mem512 ports3100032000 offer2 cpus2 mem512 ports3100032000 quote at this point the resources allocated for the framework is quote cpus4 mem1024 ports3100032000 quote after first reserve operation with offer1 the allocated resources for the framework becomes quote cpus2 mem512 cpusrole2 memrole512 portsrole3100032000 quote during second reserve operation with offer2 codetitlehierarchicalallocatorprocessupdateallocation   frameworksorter frameworksorter  frameworksortersframeworksframeworkidrole resources allocation  frameworksorterallocationframeworkidvalue  update the allocated resources tryresources updatedallocation  allocationapplyoperations check_someupdatedallocation   code allocation in the above code is quote cpus2 mem512 cpusrole2 memrole512 portsrole3100032000 quote we try to apply a reserve operation and we fail to find ports3100032000 which leads to the check fail at check_someupdatedallocation,2
replace unsafe find  xargs with find exec the problem exists in 1194srcmakefileam 47srctestsballoon_framework_testsh the current find  xargs rm rf in the makefile could potentially destroy data if mesos source was in a folder with a space in the name eg if you for some reason checkout mesos to  mesos the command in srcmakefileam would turn into a rm rf  find  xargs should be nul delimited with find print0  xargs 0 for safer execution or can just be replaced with the find buildin option find exec   which behaves similar to xargs there was a second occurrence of this in a test script though in that case it would only rmdir empty folders so is less critical i submitted a pr here httpsgithubcomapachemesospull36,1
slavetesttasklaunchcontainerizerupdatefails is flaky observed on internal ci code  run  slavetesttasklaunchcontainerizerupdatefails using temporary directory tmpslavetest_tasklaunchcontainerizerupdatefails_tujtci i0222 045956568491 21813 processcpp2117 dropped  lost event for pid slave521921681226839461 i0222 045956595433 21791 leveldbcpp175 opened db in 2759732ms i0222 045956603965 21791 leveldbcpp182 compacted db in 849192ms i0222 045956604019 21791 leveldbcpp197 created db iterator in 19206ns i0222 045956604037 21791 leveldbcpp203 seeked to beginning of db in 1802ns i0222 045956604046 21791 leveldbcpp272 iterated through 0 keys in the db in 467ns i0222 045956604081 21791 replicacpp743 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0222 045956607413 21809 recovercpp448 starting replica recovery i0222 045956607687 21809 recovercpp474 replica is in 4 status i0222 045956609011 21809 replicacpp640 replica in 4 status received a broadcasted recover request i0222 045956609262 21809 recovercpp194 received a recover response from a replica in 4 status i0222 045956609709 21809 recovercpp565 updating replica status to 3 i0222 045956610749 21811 mastercpp347 master 2015022204595611488892803946121791 centos7 started on 1921681226839461 i0222 045956610791 21811 mastercpp393 master only allowing authenticated frameworks to register i0222 045956610802 21811 mastercpp398 master only allowing authenticated slaves to register i0222 045956610821 21811 credentialshpp36 loading credentials for authentication from tmpslavetest_tasklaunchcontainerizerupdatefails_tujtcicredentials i0222 045956611042 21811 mastercpp440 authorization enabled i0222 045956612329 21811 hierarchicalhpp286 initialized hierarchical allocator process i0222 045956612416 21811 whitelist_watchercpp78 no whitelist given i0222 045956613005 21811 mastercpp1354 the newly elected leader is master1921681226839461 with id 2015022204595611488892803946121791 i0222 045956613034 21811 mastercpp1367 elected as the leading master i0222 045956613050 21811 mastercpp1185 recovering from registrar i0222 045956613229 21811 registrarcpp312 recovering registrar i0222 045956622866 21809 leveldbcpp305 persisting metadata 8 bytes to leveldb took 12988429ms i0222 045956622913 21809 replicacpp322 persisted replica status to 3 i0222 045956623118 21809 recovercpp474 replica is in 3 status i0222 045956624419 21809 replicacpp640 replica in 3 status received a broadcasted recover request i0222 045956624685 21809 recovercpp194 received a recover response from a replica in 3 status i0222 045956625200 21809 recovercpp565 updating replica status to 1 i0222 045956635154 21809 leveldbcpp305 persisting metadata 8 bytes to leveldb took 9799671ms i0222 045956635197 21809 replicacpp322 persisted replica status to 1 i0222 045956635296 21809 recovercpp579 successfully joined the paxos group i0222 045956635426 21809 recovercpp463 recover process terminated i0222 045956635812 21809 logcpp659 attempting to start the writer i0222 045956637075 21809 replicacpp476 replica received implicit promise request with proposal 1 i0222 045956648674 21809 leveldbcpp305 persisting metadata 8 bytes to leveldb took 11566146ms i0222 045956648717 21809 replicacpp344 persisted promised to 1 i0222 045956649456 21809 coordinatorcpp229 coordinator attemping to fill missing position i0222 045956650800 21809 replicacpp377 replica received explicit promise request for position 0 with proposal 2 i0222 045956659916 21809 leveldbcpp342 persisting action 8 bytes to leveldb took 9078258ms i0222 045956659981 21809 replicacpp678 persisted action at 0 i0222 045956661075 21809 replicacpp510 replica received write request for position 0 i0222 045956661129 21809 leveldbcpp437 reading position from leveldb took 26387ns i0222 045956671227 21809 leveldbcpp342 persisting action 14 bytes to leveldb took 10064302ms i0222 045956671262 21809 replicacpp678 persisted action at 0 i0222 045956671821 21809 replicacpp657 replica received learned notice for position 0 i0222 045956684200 21809 leveldbcpp342 persisting action 16 bytes to leveldb took 12346897ms i0222 045956684242 21809 replicacpp678 persisted action at 0 i0222 045956684262 21809 replicacpp663 replica learned 1 action at position 0 i0222 045956684875 21809 logcpp675 writer started with ending position 0 i0222 045956685932 21809 leveldbcpp437 reading position from leveldb took 27308ns i0222 045956688256 21809 registrarcpp345 successfully fetched the registry 0b in 74992128ms i0222 045956688344 21809 registrarcpp444 applied 1 operations in 19566ns attempting to update the registry i0222 045956690690 21809 logcpp683 attempting to append 129 bytes to the log i0222 045956690848 21809 coordinatorcpp339 coordinator attempting to write 2 action at position 1 i0222 045956691661 21809 replicacpp510 replica received write request for position 1 i0222 045956701247 21809 leveldbcpp342 persisting action 148 bytes to leveldb took 9550768ms i0222 045956701292 21809 replicacpp678 persisted action at 1 i0222 045956702066 21809 replicacpp657 replica received learned notice for position 1 i0222 045956712136 21809 leveldbcpp342 persisting action 150 bytes to leveldb took 10041696ms i0222 045956712175 21809 replicacpp678 persisted action at 1 i0222 045956712198 21809 replicacpp663 replica learned 2 action at position 1 i0222 045956713289 21809 registrarcpp489 successfully updated the registry in 24890112ms i0222 045956713397 21809 registrarcpp375 successfully recovered registrar i0222 045956713537 21809 logcpp702 attempting to truncate the log to 1 i0222 045956713795 21809 mastercpp1212 recovered 0 slaves from the registry 93b  allowing 10mins for slaves to reregister i0222 045956713871 21809 coordinatorcpp339 coordinator attempting to write 3 action at position 2 i0222 045956714879 21809 replicacpp510 replica received write request for position 2 i0222 045956725225 21809 leveldbcpp342 persisting action 16 bytes to leveldb took 10311704ms i0222 045956725270 21809 replicacpp678 persisted action at 2 i0222 045956726066 21809 replicacpp657 replica received learned notice for position 2 i0222 045956734110 21809 leveldbcpp342 persisting action 18 bytes to leveldb took 8012327ms i0222 045956734180 21809 leveldbcpp400 deleting 1 keys from leveldb took 36578ns i0222 045956734201 21809 replicacpp678 persisted action at 2 i0222 045956734221 21809 replicacpp663 replica learned 3 action at position 2 i0222 045956747556 21809 slavecpp173 slave started on 531921681226839461 i0222 045956747601 21809 credentialshpp84 loading credential for authentication from tmpslavetest_tasklaunchcontainerizerupdatefails_qkhajpcredential i0222 045956747774 21809 slavecpp280 slave using credential for testprincipal i0222 045956748021 21809 slavecpp298 slave resources cpus2 mem1024 disk1024 ports3100032000 i0222 045956748682 21809 slavecpp327 slave hostname centos7 i0222 045956748705 21809 slavecpp328 slave checkpoint false w0222 045956748714 21809 slavecpp330 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0222 045956749826 21809 statecpp34 recovering state from tmpslavetest_tasklaunchcontainerizerupdatefails_qkhajpmeta i0222 045956750191 21809 status_update_managercpp196 recovering status update manager i0222 045956750465 21809 slavecpp3775 finished recovery i0222 045956751260 21809 slavecpp623 new master detected at master1921681226839461 i0222 045956751349 21809 slavecpp686 authenticating with master master1921681226839461 i0222 045956751369 21809 slavecpp691 using default crammd5 authenticatee i0222 045956751502 21809 slavecpp659 detecting new master i0222 045956751596 21809 status_update_managercpp170 pausing sending status updates i0222 045956751668 21809 authenticateehpp138 creating new client sasl connection i0222 045956752781 21809 mastercpp3811 authenticating slave531921681226839461 i0222 045956752820 21809 mastercpp3822 using default crammd5 authenticator i0222 045956753124 21809 authenticatorhpp169 creating new server sasl connection i0222 045956755609 21809 authenticateehpp229 received sasl authentication mechanisms crammd5 i0222 045956755641 21809 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0222 045956755708 21809 authenticatorhpp275 received sasl authentication start i0222 045956755751 21809 authenticatorhpp397 authentication requires more steps i0222 045956755813 21809 authenticateehpp275 received sasl authentication step i0222 045956755887 21809 authenticatorhpp303 received sasl authentication step i0222 045956755920 21809 auxpropcpp98 request to lookup properties for user testprincipal realm centos7 server fqdn centos7 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0222 045956755934 21809 auxpropcpp170 looking up auxiliary property userpassword i0222 045956756005 21809 auxpropcpp170 looking up auxiliary property cmusaslsecretcrammd5 i0222 045956756036 21809 auxpropcpp98 request to lookup properties for user testprincipal realm centos7 server fqdn centos7 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0222 045956756047 21809 auxpropcpp120 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0222 045956756054 21809 auxpropcpp120 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0222 045956756068 21809 authenticatorhpp389 authentication success i0222 045956756155 21809 authenticateehpp315 authentication success i0222 045956756219 21809 mastercpp3869 successfully authenticated principal testprincipal at slave531921681226839461 i0222 045956756503 21809 slavecpp757 successfully authenticated with master master1921681226839461 i0222 045956756611 21809 slavecpp1089 will retry registration in 11221976ms if necessary i0222 045956756876 21809 mastercpp2936 registering slave at slave531921681226839461 centos7 with id 2015022204595611488892803946121791s0 i0222 045956757323 21809 registrarcpp444 applied 1 operations in 70787ns attempting to update the registry i0222 045956759790 21809 logcpp683 attempting to append 299 bytes to the log i0222 045956760000 21809 coordinatorcpp339 coordinator attempting to write 2 action at position 3 i0222 045956760920 21809 replicacpp510 replica received write request for position 3 i0222 045956762037 21791 schedcpp154 version 0220 i0222 045956762763 21806 schedcpp251 new master detected at master1921681226839461 i0222 045956762835 21806 schedcpp307 authenticating with master master1921681226839461 i0222 045956762856 21806 schedcpp314 using default crammd5 authenticatee i0222 045956763082 21806 authenticateehpp138 creating new client sasl connection i0222 045956763753 21806 mastercpp3811 authenticating schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956763784 21806 mastercpp3822 using default crammd5 authenticator i0222 045956764040 21806 authenticatorhpp169 creating new server sasl connection i0222 045956764624 21806 authenticateehpp229 received sasl authentication mechanisms crammd5 i0222 045956764653 21806 authenticateehpp255 attempting to authenticate with mechanism crammd5 i0222 045956764719 21806 authenticatorhpp275 received sasl authentication start i0222 045956764758 21806 authenticatorhpp397 authentication requires more steps i0222 045956764819 21806 authenticateehpp275 received sasl authentication step i0222 045956764889 21806 authenticatorhpp303 received sasl authentication step i0222 045956764911 21806 auxpropcpp98 request to lookup properties for user testprincipal realm centos7 server fqdn centos7 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0222 045956764922 21806 auxpropcpp170 looking up auxiliary property userpassword i0222 045956764974 21806 auxpropcpp170 looking up auxiliary property cmusaslsecretcrammd5 i0222 045956765005 21806 auxpropcpp98 request to lookup properties for user testprincipal realm centos7 server fqdn centos7 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0222 045956765017 21806 auxpropcpp120 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0222 045956765023 21806 auxpropcpp120 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0222 045956765036 21806 authenticatorhpp389 authentication success i0222 045956765120 21806 authenticateehpp315 authentication success i0222 045956765182 21806 mastercpp3869 successfully authenticated principal testprincipal at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956765442 21806 schedcpp395 successfully authenticated with master master1921681226839461 i0222 045956765465 21806 schedcpp518 sending registration request to master1921681226839461 i0222 045956765522 21806 schedcpp551 will retry registration in 1283564292secs if necessary i0222 045956765637 21806 mastercpp1572 received registration request for framework default at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956765699 21806 mastercpp1433 authorizing framework principal testprincipal to receive offers for role  i0222 045956766120 21806 mastercpp1636 registering framework 20150222045956114888928039461217910000 default at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956766572 21806 hierarchicalhpp320 added framework 20150222045956114888928039461217910000 i0222 045956766598 21806 hierarchicalhpp831 no resources available to allocate i0222 045956766609 21806 hierarchicalhpp738 performed allocation for 0 slaves in 15902ns i0222 045956766753 21806 schedcpp445 framework registered with 20150222045956114888928039461217910000 i0222 045956766790 21806 schedcpp459 schedulerregistered took 15076ns i0222 045956773710 21806 slavecpp1089 will retry registration in 3454005ms if necessary i0222 045956773900 21806 mastercpp2924 ignoring register slave message from slave531921681226839461 centos7 as admission is already in progress i0222 045956775297 21809 leveldbcpp342 persisting action 318 bytes to leveldb took 14319807ms i0222 045956775344 21809 replicacpp678 persisted action at 3 i0222 045956776139 21809 replicacpp657 replica received learned notice for position 3 i0222 045956778630 21806 slavecpp1089 will retry registration in 32764468ms if necessary i0222 045956778779 21806 mastercpp2924 ignoring register slave message from slave531921681226839461 centos7 as admission is already in progress i0222 045956783778 21809 leveldbcpp342 persisting action 320 bytes to leveldb took 7609533ms i0222 045956783828 21809 replicacpp678 persisted action at 3 i0222 045956783849 21809 replicacpp663 replica learned 2 action at position 3 i0222 045956785058 21809 registrarcpp489 successfully updated the registry in 27669248ms i0222 045956785274 21809 logcpp702 attempting to truncate the log to 3 i0222 045956785815 21809 mastercpp2993 registered slave 2015022204595611488892803946121791s0 at slave531921681226839461 centos7 with cpus2 mem1024 disk1024 ports3100032000 i0222 045956785913 21809 coordinatorcpp339 coordinator attempting to write 3 action at position 4 i0222 045956786267 21809 hierarchicalhpp452 added slave 2015022204595611488892803946121791s0 centos7 with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0222 045956786600 21809 hierarchicalhpp756 performed allocation for slave 2015022204595611488892803946121791s0 in 292298ns i0222 045956786684 21809 slavecpp791 registered with master master1921681226839461 given slave id 2015022204595611488892803946121791s0 i0222 045956786792 21809 slavecpp2830 received ping from slaveobserver521921681226839461 i0222 045956787230 21809 mastercpp3753 sending 1 offers to framework 20150222045956114888928039461217910000 default at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956787334 21809 status_update_managercpp177 resuming sending status updates i0222 045956788156 21809 schedcpp608 schedulerresourceoffers took 557128ns i0222 045956788936 21809 mastercpp2266 processing accept call for offers  2015022204595611488892803946121791o0  on slave 2015022204595611488892803946121791s0 at slave531921681226839461 centos7 for framework 20150222045956114888928039461217910000 default at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 i0222 045956789000 21809 mastercpp2110 authorizing framework principal testprincipal to launch task 0 as user jenkins w0222 045956790506 21809 validationcpp327 executor default for task 0 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0222 045956790546 21809 validationcpp339 executor default for task 0 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0222 045956790808 21809 masterhpp821 adding task 0 with resources cpus1 mem128 on slave 2015022204595611488892803946121791s0 centos7 i0222 045956790885 21809 mastercpp2543 launching task 0 of framework 20150222045956114888928039461217910000 default at schedulerd9c22c4e8dec42a6a350a984726428911921681226839461 with resources cpus1 mem128 on slave 2015022204595611488892803946121791s0 at slave531921681226839461 centos7 i0222 045956791201 21809 replicacpp510 replica received write request for position 4 i0222 045956791610 21806 slavecpp1120 got assigned task 0 for framework 20150222045956114888928039461217910000 i0222 045956792140 21806 slavecpp1230 launching task 0 for framework 20150222045956114888928039461217910000 i0222 045956794872 21806 slavecpp4177 launching executor default of framework 20150222045956114888928039461217910000 in work directory tmpslavetest_tasklaunchcontainerizerupdatefails_qkhajpslaves2015022204595611488892803946121791s0frameworks20150222045956114888928039461217910000executorsdefaultruns753232b543ff4fbfb29a0f76161132ab i0222 045956796846 21806 execcpp130 version 0220 i0222 045956797173 21806 slavecpp1377 queuing task 0 for execut,1
grouptestlabelledgroup segfaults observed this on internal ci not sure if it is due to grouptestlabelledgroup or an earlier test code i0219 010417980598 27766 zookeeper_test_servercpp117 shutting down zookeepertestserver on port 39597  ok  grouptestretryableerrors 30150 ms  run  grouptestlabelledgroup makefile6656 recipe for target checklocal failed make3  checklocal segmentation fault core dumped code,2
provide user doc for the new posix disk isolator in mesos containerizer we introduced a posix disk isolator for mesos containerizer in 0220 this isolator allows us to get container disk usage as well as enforcing container disk quota its based on du we need to document this feature,2
rate limit slaves removals during master recovery much like we rate limit slave removals in the common path mesos1148 we need to rate limit slave removals that occur during master recovery when a master recovers and is using a strict registry slaves that do not reregister within a timeout will be removed currently there is a safeguard in place to abort when too many slaves have not reregistered however in the case of a transient partition we dont want to remove large sections of slaves without rate limiting,3
create styleguide for documentation as of right now different pages in our documentation use quite different styles consider for example the different emphasis for note  noformat note httpmesosapacheorgdocumentationlatestslaverecoverynoformat  noformatnote httpmesosapacheorgdocumentationlatestupgrades noformat would be great to establish a common style for the documentation,2
improve nstestroot_setns  use symbol name directly to launch the subprocess instead of the hardcoded string  replaced the static string with char,1
mastertestshutdownframeworkwhiletaskrunning is flaky looks like the executorshutdowntimeout was called immediately after executorshutdown was called code  run  mastertestshutdownframeworkwhiletaskrunning using temporary directory tmpmastertest_shutdownframeworkwhiletaskrunning_sbd6vk i0224 185117385068 30213 leveldbcpp176 opened db in 1262442ms i0224 185117386360 30213 leveldbcpp183 compacted db in 985102ns i0224 185117387025 30213 leveldbcpp198 created db iterator in 78043ns i0224 185117387420 30213 leveldbcpp204 seeked to beginning of db in 25814ns i0224 185117387804 30213 leveldbcpp273 iterated through 0 keys in the db in 25025ns i0224 185117388270 30213 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0224 185117389760 30227 recovercpp449 starting replica recovery i0224 185117395699 30227 recovercpp475 replica is in 4 status i0224 185117398294 30227 replicacpp641 replica in 4 status received a broadcasted recover request i0224 185117398816 30227 recovercpp195 received a recover response from a replica in 4 status i0224 185117402415 30230 recovercpp566 updating replica status to 3 i0224 185117403473 30229 leveldbcpp306 persisting metadata 8 bytes to leveldb took 273857ns i0224 185117404093 30229 replicacpp323 persisted replica status to 3 i0224 185117404930 30229 recovercpp475 replica is in 3 status i0224 185117407995 30233 replicacpp641 replica in 3 status received a broadcasted recover request i0224 185117410697 30231 recovercpp195 received a recover response from a replica in 3 status i0224 185117415710 30230 recovercpp566 updating replica status to 1 i0224 185117416987 30227 leveldbcpp306 persisting metadata 8 bytes to leveldb took 221966ns i0224 185117417579 30227 replicacpp323 persisted replica status to 1 i0224 185117418803 30234 recovercpp580 successfully joined the paxos group i0224 185117419699 30227 recovercpp464 recover process terminated i0224 185117430594 30234 mastercpp349 master 2015022418511722729627524495030213 fedora19 started on 19216812213544950 i0224 185117431082 30234 mastercpp395 master only allowing authenticated frameworks to register i0224 185117431453 30234 mastercpp400 master only allowing authenticated slaves to register i0224 185117431828 30234 credentialshpp37 loading credentials for authentication from tmpmastertest_shutdownframeworkwhiletaskrunning_sbd6vkcredentials i0224 185117432740 30234 mastercpp442 authorization enabled i0224 185117434224 30229 hierarchicalhpp287 initialized hierarchical allocator process i0224 185117434994 30233 whitelist_watchercpp79 no whitelist given i0224 185117440687 30234 mastercpp1356 the newly elected leader is master19216812213544950 with id 2015022418511722729627524495030213 i0224 185117441764 30234 mastercpp1369 elected as the leading master i0224 185117442430 30234 mastercpp1187 recovering from registrar i0224 185117443053 30229 registrarcpp313 recovering registrar i0224 185117445468 30228 logcpp660 attempting to start the writer i0224 185117449970 30233 replicacpp477 replica received implicit promise request with proposal 1 i0224 185117451359 30233 leveldbcpp306 persisting metadata 8 bytes to leveldb took 339488ns i0224 185117451949 30233 replicacpp345 persisted promised to 1 i0224 185117456845 30235 processcpp2117 dropped  lost event for pid hierarchicalallocator15419216812213544950 i0224 185117461741 30231 coordinatorcpp230 coordinator attemping to fill missing position i0224 185117464686 30228 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0224 185117465515 30228 leveldbcpp343 persisting action 8 bytes to leveldb took 170261ns i0224 185117465991 30228 replicacpp679 persisted action at 0 i0224 185117470512 30229 replicacpp511 replica received write request for position 0 i0224 185117471437 30229 leveldbcpp438 reading position from leveldb took 139178ns i0224 185117472129 30229 leveldbcpp343 persisting action 14 bytes to leveldb took 141560ns i0224 185117472705 30229 replicacpp679 persisted action at 0 i0224 185117476305 30228 replicacpp658 replica received learned notice for position 0 i0224 185117477991 30228 leveldbcpp343 persisting action 16 bytes to leveldb took 208112ns i0224 185117478574 30228 replicacpp679 persisted action at 0 i0224 185117479044 30228 replicacpp664 replica learned 1 action at position 0 i0224 185117484371 30233 logcpp676 writer started with ending position 0 i0224 185117487396 30233 leveldbcpp438 reading position from leveldb took 96498ns i0224 185117498906 30233 registrarcpp346 successfully fetched the registry 0b in 55234048ms i0224 185117499781 30233 registrarcpp445 applied 1 operations in 97308ns attempting to update the registry i0224 185117503955 30231 logcpp684 attempting to append 131 bytes to the log i0224 185117505009 30231 coordinatorcpp340 coordinator attempting to write 2 action at position 1 i0224 185117507428 30228 replicacpp511 replica received write request for position 1 i0224 185117508517 30228 leveldbcpp343 persisting action 150 bytes to leveldb took 316570ns i0224 185117508985 30228 replicacpp679 persisted action at 1 i0224 185117512902 30229 replicacpp658 replica received learned notice for position 1 i0224 185117517261 30229 leveldbcpp343 persisting action 152 bytes to leveldb took 427860ns i0224 185117517470 30229 replicacpp679 persisted action at 1 i0224 185117517796 30229 replicacpp664 replica learned 2 action at position 1 i0224 185117532624 30232 registrarcpp490 successfully updated the registry in 3231104ms i0224 185117533957 30228 logcpp703 attempting to truncate the log to 1 i0224 185117534366 30228 coordinatorcpp340 coordinator attempting to write 3 action at position 2 i0224 185117536684 30227 replicacpp511 replica received write request for position 2 i0224 185117537406 30227 leveldbcpp343 persisting action 16 bytes to leveldb took 196455ns i0224 185117537946 30227 replicacpp679 persisted action at 2 i0224 185117537695 30232 registrarcpp376 successfully recovered registrar i0224 185117544136 30231 mastercpp1214 recovered 0 slaves from the registry 95b  allowing 10mins for slaves to reregister i0224 185117546041 30227 replicacpp658 replica received learned notice for position 2 i0224 185117546728 30227 leveldbcpp343 persisting action 18 bytes to leveldb took 192442ns i0224 185117547058 30227 leveldbcpp401 deleting 1 keys from leveldb took 61064ns i0224 185117547363 30227 replicacpp679 persisted action at 2 i0224 185117547669 30227 replicacpp664 replica learned 3 action at position 2 i0224 185117565460 30234 slavecpp174 slave started on 13819216812213544950 i0224 185117566038 30234 credentialshpp85 loading credential for authentication from tmpmastertest_shutdownframeworkwhiletaskrunning_lrugmscredential i0224 185117566584 30234 slavecpp281 slave using credential for testprincipal i0224 185117567198 30234 slavecpp299 slave resources cpus2 mem1024 disk1024 ports3100032000 i0224 185117567930 30234 slavecpp328 slave hostname fedora19 i0224 185117568172 30234 slavecpp329 slave checkpoint false w0224 185117568435 30234 slavecpp331 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0224 185117570539 30227 statecpp35 recovering state from tmpmastertest_shutdownframeworkwhiletaskrunning_lrugmsmeta i0224 185117573499 30232 status_update_managercpp197 recovering status update manager i0224 185117574209 30234 slavecpp3775 finished recovery i0224 185117576277 30229 status_update_managercpp171 pausing sending status updates i0224 185117576680 30234 slavecpp624 new master detected at master19216812213544950 i0224 185117577131 30234 slavecpp687 authenticating with master master19216812213544950 i0224 185117577385 30234 slavecpp692 using default crammd5 authenticatee i0224 185117577945 30228 authenticateehpp139 creating new client sasl connection i0224 185117578837 30234 slavecpp660 detecting new master i0224 185117579270 30228 mastercpp3813 authenticating slave13819216812213544950 i0224 185117579900 30228 mastercpp3824 using default crammd5 authenticator i0224 185117580572 30228 authenticatorhpp170 creating new server sasl connection i0224 185117581501 30231 authenticateehpp230 received sasl authentication mechanisms crammd5 i0224 185117581805 30231 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0224 185117582222 30228 authenticatorhpp276 received sasl authentication start i0224 185117582531 30228 authenticatorhpp398 authentication requires more steps i0224 185117582945 30230 authenticateehpp276 received sasl authentication step i0224 185117583351 30228 authenticatorhpp304 received sasl authentication step i0224 185117583643 30228 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0224 185117583911 30228 auxpropcpp171 looking up auxiliary property userpassword i0224 185117584241 30228 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0224 185117584517 30228 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0224 185117584787 30228 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0224 185117585075 30228 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0224 185117585358 30228 authenticatorhpp390 authentication success i0224 185117585750 30233 authenticateehpp316 authentication success i0224 185117586354 30232 mastercpp3871 successfully authenticated principal testprincipal at slave13819216812213544950 i0224 185117590953 30234 slavecpp758 successfully authenticated with master master19216812213544950 i0224 185117591686 30233 mastercpp2938 registering slave at slave13819216812213544950 fedora19 with id 2015022418511722729627524495030213s0 i0224 185117592718 30233 registrarcpp445 applied 1 operations in 100358ns attempting to update the registry i0224 185117595989 30227 logcpp684 attempting to append 302 bytes to the log i0224 185117596757 30227 coordinatorcpp340 coordinator attempting to write 2 action at position 3 i0224 185117599280 30227 replicacpp511 replica received write request for position 3 i0224 185117599481 30234 slavecpp1090 will retry registration in 12331173ms if necessary i0224 185117601940 30227 leveldbcpp343 persisting action 321 bytes to leveldb took 999045ns i0224 185117602339 30227 replicacpp679 persisted action at 3 i0224 185117612349 30229 replicacpp658 replica received learned notice for position 3 i0224 185117612934 30229 leveldbcpp343 persisting action 323 bytes to leveldb took 152139ns i0224 185117613471 30229 replicacpp679 persisted action at 3 i0224 185117613796 30229 replicacpp664 replica learned 2 action at position 3 i0224 185117615980 30229 mastercpp2926 ignoring register slave message from slave13819216812213544950 fedora19 as admission is already in progress i0224 185117614302 30233 slavecpp1090 will retry registration in 11014835ms if necessary i0224 185117617490 30234 registrarcpp490 successfully updated the registry in 24179968ms i0224 185117618989 30234 mastercpp2995 registered slave 2015022418511722729627524495030213s0 at slave13819216812213544950 fedora19 with cpus2 mem1024 disk1024 ports3100032000 i0224 185117619567 30233 hierarchicalhpp455 added slave 2015022418511722729627524495030213s0 fedora19 with cpus2 mem1024 disk1024 ports3100032000 and cpus2 mem1024 disk1024 ports3100032000 available i0224 185117621080 30233 hierarchicalhpp834 no resources available to allocate i0224 185117621441 30233 hierarchicalhpp759 performed allocation for slave 2015022418511722729627524495030213s0 in 544608ns i0224 185117619704 30229 slavecpp792 registered with master master19216812213544950 given slave id 2015022418511722729627524495030213s0 i0224 185117622195 30229 slavecpp2830 received ping from slaveobserver12519216812213544950 i0224 185117622385 30227 status_update_managercpp178 resuming sending status updates i0224 185117620266 30232 logcpp703 attempting to truncate the log to 3 i0224 185117623522 30232 coordinatorcpp340 coordinator attempting to write 3 action at position 4 i0224 185117624835 30229 replicacpp511 replica received write request for position 4 i0224 185117625727 30229 leveldbcpp343 persisting action 16 bytes to leveldb took 259831ns i0224 185117626122 30229 replicacpp679 persisted action at 4 i0224 185117627686 30227 replicacpp658 replica received learned notice for position 4 i0224 185117628228 30227 leveldbcpp343 persisting action 18 bytes to leveldb took 93777ns i0224 185117628785 30227 leveldbcpp401 deleting 2 keys from leveldb took 57660ns i0224 185117629176 30227 replicacpp679 persisted action at 4 i0224 185117629443 30227 replicacpp664 replica learned 3 action at position 4 i0224 185117636715 30213 schedcpp157 version 0230 i0224 185117638003 30229 schedcpp254 new master detected at master19216812213544950 i0224 185117638602 30229 schedcpp310 authenticating with master master19216812213544950 i0224 185117639024 30229 schedcpp317 using default crammd5 authenticatee i0224 185117639580 30228 authenticateehpp139 creating new client sasl connection i0224 185117640455 30235 processcpp2117 dropped  lost event for pid scheduler11bb6bcbcd514927a28bdbca9d63772f19216812213544950 i0224 185117641150 30228 mastercpp3813 authenticating schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117641597 30228 mastercpp3824 using default crammd5 authenticator i0224 185117642643 30228 authenticatorhpp170 creating new server sasl connection i0224 185117643698 30234 authenticateehpp230 received sasl authentication mechanisms crammd5 i0224 185117644296 30234 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0224 185117644739 30228 authenticatorhpp276 received sasl authentication start i0224 185117645143 30228 authenticatorhpp398 authentication requires more steps i0224 185117645654 30230 authenticateehpp276 received sasl authentication step i0224 185117646122 30228 authenticatorhpp304 received sasl authentication step i0224 185117646421 30228 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0224 185117646746 30228 auxpropcpp171 looking up auxiliary property userpassword i0224 185117647203 30228 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0224 185117647644 30228 auxpropcpp99 request to lookup properties for user testprincipal realm fedora19 server fqdn fedora19 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0224 185117648454 30228 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0224 185117648788 30228 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0224 185117649210 30228 authenticatorhpp390 authentication success i0224 185117649705 30231 authenticateehpp316 authentication success i0224 185117653314 30231 schedcpp398 successfully authenticated with master master19216812213544950 i0224 185117653766 30232 mastercpp3871 successfully authenticated principal testprincipal at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117654683 30231 schedcpp521 sending registration request to master19216812213544950 i0224 185117655138 30231 schedcpp554 will retry registration in 1028970132secs if necessary i0224 185117657112 30232 mastercpp1574 received registration request for framework default at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117658509 30232 mastercpp1435 authorizing framework principal testprincipal to receive offers for role  i0224 185117659765 30232 mastercpp1638 registering framework 20150224185117227296275244950302130000 default at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117660727 30233 hierarchicalhpp321 added framework 20150224185117227296275244950302130000 i0224 185117661730 30233 hierarchicalhpp741 performed allocation for 1 slaves in 529369ns i0224 185117662911 30229 schedcpp448 framework registered with 20150224185117227296275244950302130000 i0224 185117663374 30229 schedcpp462 schedulerregistered took 35637ns i0224 185117664552 30232 mastercpp3755 sending 1 offers to framework 20150224185117227296275244950302130000 default at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117668009 30234 schedcpp611 schedulerresourceoffers took 2574292ms i0224 185117671038 30232 mastercpp2268 processing accept call for offers  2015022418511722729627524495030213o0  on slave 2015022418511722729627524495030213s0 at slave13819216812213544950 fedora19 for framework 20150224185117227296275244950302130000 default at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 i0224 185117672071 30232 mastercpp2112 authorizing framework principal testprincipal to launch task 1 as user jenkins w0224 185117674675 30232 validationcpp326 executor default for task 1 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0224 185117675395 30232 validationcpp338 executor default for task 1 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0224 185117676460 30232 masterhpp822 adding task 1 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015022418511722729627524495030213s0 fedora19 i0224 185117677078 30232 mastercpp2545 launching task 1 of framework 20150224185117227296275244950302130000 default at schedulerfc72e828078341b69892ffc961e8567e19216812213544950 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 2015022418511722729627524495030213s0 at slave13819216812213544950 fedora19 i0224 185117678084 30230 slavecpp1121 got ass,1
mesoscontainerizerdestroytestlauncherdestroyfailure is flaky failed to osexecvpe in childmain never seen this one before code  run  mesoscontainerizerdestroytestlauncherdestroyfailure using temporary directory tmpmesoscontainerizerdestroytest_launcherdestroyfailure_qpjqen i0224 185549326912 21391 containerizercpp461 starting container test_container for executor executor of framework  i0224 185549332252 21391 launchercpp130 forked child with pid 23496 for container test_container abort srcsubprocesscpp165 failed to osexecvpe in childmain  aborted at 1424832949 unix time try date d 1424832949 if you are using gnu date  pc  0x2b178c5db0d5 unknown i0224 185549340955 21392 processcpp2117 dropped  lost event for pid scheduler509d37ac296f4429b101af433c1800e912701139647 i0224 185549342300 21386 containerizercpp911 destroying container test_container  sigabrt 0x3e800005bc8 received by pid 23496 tid 0x2b178f9f0700 from pid 23496 stack trace   0x2b178c397cb0 unknown  0x2b178c5db0d5 unknown  0x2b178c5de83b unknown  0x87a945 _abort  0x2b1789f610b9 processchildmain i0224 185549391793 21386 containerizercpp1120 executor for container test_container has exited i0224 185549400478 21391 processcpp2770 handling http event for process metrics with path metricssnapshot testscontainerizer_testscpp485 failure value of metricsvaluescontainerizermesoscontainer_destroy_errors actual 16byte object 0200 0000 172b 0000 e086 0e04 0000 0000 expected 1u which is 1  failed  mesoscontainerizerdestroytestlauncherdestroyfailure 89 ms code,2
masterallocatortest0frameworkreregistersfirst is flaky code  run  masterallocatortest0frameworkreregistersfirst using temporary directory tmpmasterallocatortest_0_frameworkreregistersfirst_vy5nml i0224 232231681670 30589 leveldbcpp176 opened db in 2943518ms i0224 232231682152 30619 processcpp2117 dropped  lost event for pid slave65671958118738391 i0224 232231682732 30589 leveldbcpp183 compacted db in 1029469ms i0224 232231682777 30589 leveldbcpp198 created db iterator in 15460ns i0224 232231682792 30589 leveldbcpp204 seeked to beginning of db in 1832ns i0224 232231682802 30589 leveldbcpp273 iterated through 0 keys in the db in 319ns i0224 232231682833 30589 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0224 232231683228 30605 recovercpp449 starting replica recovery i0224 232231683537 30605 recovercpp475 replica is in 4 status i0224 232231684624 30615 replicacpp641 replica in 4 status received a broadcasted recover request i0224 232231684978 30616 recovercpp195 received a recover response from a replica in 4 status i0224 232231685405 30610 recovercpp566 updating replica status to 3 i0224 232231686249 30609 mastercpp349 master 2015022423223131426977953839130589 pomonaapacheorg started on 671958118738391 i0224 232231686265 30617 leveldbcpp306 persisting metadata 8 bytes to leveldb took 717897ns i0224 232231686319 30617 replicacpp323 persisted replica status to 3 i0224 232231686336 30609 mastercpp395 master only allowing authenticated frameworks to register i0224 232231686357 30609 mastercpp400 master only allowing authenticated slaves to register i0224 232231686390 30609 credentialshpp37 loading credentials for authentication from tmpmasterallocatortest_0_frameworkreregistersfirst_vy5nmlcredentials i0224 232231686511 30606 recovercpp475 replica is in 3 status i0224 232231686563 30609 mastercpp442 authorization enabled i0224 232231686929 30607 whitelist_watchercpp79 no whitelist given i0224 232231686954 30603 hierarchicalhpp287 initialized hierarchical allocator process i0224 232231687134 30605 replicacpp641 replica in 3 status received a broadcasted recover request i0224 232231687731 30609 mastercpp1356 the newly elected leader is master671958118738391 with id 2015022423223131426977953839130589 i0224 232231839818 30609 mastercpp1369 elected as the leading master i0224 232231839834 30609 mastercpp1187 recovering from registrar i0224 232231839926 30605 registrarcpp313 recovering registrar i0224 232231840000 30613 recovercpp195 received a recover response from a replica in 3 status i0224 232231840504 30606 recovercpp566 updating replica status to 1 i0224 232231841599 30611 leveldbcpp306 persisting metadata 8 bytes to leveldb took 990330ns i0224 232231841627 30611 replicacpp323 persisted replica status to 1 i0224 232231841743 30611 recovercpp580 successfully joined the paxos group i0224 232231841904 30611 recovercpp464 recover process terminated i0224 232231842366 30608 logcpp660 attempting to start the writer i0224 232231843557 30607 replicacpp477 replica received implicit promise request with proposal 1 i0224 232231844312 30607 leveldbcpp306 persisting metadata 8 bytes to leveldb took 722368ns i0224 232231844337 30607 replicacpp345 persisted promised to 1 i0224 232231844889 30615 coordinatorcpp230 coordinator attemping to fill missing position i0224 232231846043 30614 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0224 232231846729 30614 leveldbcpp343 persisting action 8 bytes to leveldb took 660024ns i0224 232231846746 30614 replicacpp679 persisted action at 0 i0224 232231847671 30611 replicacpp511 replica received write request for position 0 i0224 232231847723 30611 leveldbcpp438 reading position from leveldb took 27349ns i0224 232231848429 30611 leveldbcpp343 persisting action 14 bytes to leveldb took 671461ns i0224 232231848454 30611 replicacpp679 persisted action at 0 i0224 232231849041 30615 replicacpp658 replica received learned notice for position 0 i0224 232231849762 30615 leveldbcpp343 persisting action 16 bytes to leveldb took 690386ns i0224 232231849787 30615 replicacpp679 persisted action at 0 i0224 232231849808 30615 replicacpp664 replica learned 1 action at position 0 i0224 232231850416 30612 logcpp676 writer started with ending position 0 i0224 232231851490 30615 leveldbcpp438 reading position from leveldb took 30659ns i0224 232231854452 30610 registrarcpp346 successfully fetched the registry 0b in 14491136ms i0224 232231854543 30610 registrarcpp445 applied 1 operations in 18024ns attempting to update the registry i0224 232231857095 30604 logcpp684 attempting to append 139 bytes to the log i0224 232231857208 30608 coordinatorcpp340 coordinator attempting to write 2 action at position 1 i0224 232231858073 30609 replicacpp511 replica received write request for position 1 i0224 232231858808 30609 leveldbcpp343 persisting action 158 bytes to leveldb took 701708ns i0224 232231858835 30609 replicacpp679 persisted action at 1 i0224 232231859508 30618 replicacpp658 replica received learned notice for position 1 i0224 232231860267 30618 leveldbcpp343 persisting action 160 bytes to leveldb took 731035ns i0224 232231860309 30618 replicacpp679 persisted action at 1 i0224 232231860332 30618 replicacpp664 replica learned 2 action at position 1 i0224 232231860983 30609 registrarcpp490 successfully updated the registry in 639616ms i0224 232231861071 30609 registrarcpp376 successfully recovered registrar i0224 232231861126 30608 logcpp703 attempting to truncate the log to 1 i0224 232231861249 30603 coordinatorcpp340 coordinator attempting to write 3 action at position 2 i0224 232231861248 30617 mastercpp1214 recovered 0 slaves from the registry 101b  allowing 10mins for slaves to reregister i0224 232231861831 30613 replicacpp511 replica received write request for position 2 i0224 232231862504 30613 leveldbcpp343 persisting action 16 bytes to leveldb took 648125ns i0224 232231862531 30613 replicacpp679 persisted action at 2 i0224 232231863067 30603 replicacpp658 replica received learned notice for position 2 i0224 232231863689 30603 leveldbcpp343 persisting action 18 bytes to leveldb took 602784ns i0224 232231863737 30603 leveldbcpp401 deleting 1 keys from leveldb took 28697ns i0224 232231863751 30603 replicacpp679 persisted action at 2 i0224 232231863767 30603 replicacpp664 replica learned 3 action at position 2 i0224 232231875962 30610 slavecpp174 slave started on 66671958118738391 i0224 232231876008 30610 credentialshpp85 loading credential for authentication from tmpmasterallocatortest_0_frameworkreregistersfirst_ikvxqmcredential i0224 232231876144 30610 slavecpp281 slave using credential for testprincipal i0224 232231876404 30610 slavecpp299 slave resources cpus2 mem1024 disk370122e06 ports3100032000 i0224 232231876489 30610 slavecpp328 slave hostname pomonaapacheorg i0224 232231876502 30610 slavecpp329 slave checkpoint false w0224 232231876507 30610 slavecpp331 disabling checkpointing is deprecated and the checkpoint flag will be removed in a future release please avoid using this flag i0224 232231877014 30603 statecpp35 recovering state from tmpmasterallocatortest_0_frameworkreregistersfirst_ikvxqmmeta i0224 232231877230 30610 status_update_managercpp197 recovering status update manager i0224 232231877495 30609 slavecpp3776 finished recovery i0224 232231877879 30607 status_update_managercpp171 pausing sending status updates i0224 232231877879 30604 slavecpp624 new master detected at master671958118738391 i0224 232231877959 30604 slavecpp687 authenticating with master master671958118738391 i0224 232231877975 30604 slavecpp692 using default crammd5 authenticatee i0224 232231878069 30604 slavecpp660 detecting new master i0224 232231878093 30608 authenticateehpp139 creating new client sasl connection i0224 232231878223 30604 mastercpp3813 authenticating slave66671958118738391 i0224 232231878244 30604 mastercpp3824 using default crammd5 authenticator i0224 232231878412 30613 authenticatorhpp170 creating new server sasl connection i0224 232231878525 30603 authenticateehpp230 received sasl authentication mechanisms crammd5 i0224 232231878551 30603 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0224 232231878625 30617 authenticatorhpp276 received sasl authentication start i0224 232231878662 30617 authenticatorhpp398 authentication requires more steps i0224 232231878727 30603 authenticateehpp276 received sasl authentication step i0224 232231878815 30617 authenticatorhpp304 received sasl authentication step i0224 232231878839 30617 auxpropcpp99 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0224 232231878847 30617 auxpropcpp171 looking up auxiliary property userpassword i0224 232231878875 30617 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0224 232231878891 30617 auxpropcpp99 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0224 232231878900 30617 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0224 232231878906 30617 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0224 232231878916 30617 authenticatorhpp390 authentication success i0224 232231880717 30589 schedcpp157 version 0230 i0224 232232017823 30611 authenticateehpp316 authentication success i0224 232232017901 30618 mastercpp3871 successfully authenticated principal testprincipal at slave66671958118738391 i0224 232232018156 30615 schedcpp254 new master detected at master671958118738391 i0224 232232018240 30615 schedcpp310 authenticating with master master671958118738391 i0224 232232018263 30615 schedcpp317 using default crammd5 authenticatee i0224 232232018496 30613 slavecpp758 successfully authenticated with master master671958118738391 i0224 232232018579 30611 authenticateehpp139 creating new client sasl connection i0224 232232018620 30613 slavecpp1090 will retry registration in 363167ns if necessary i0224 232232018811 30615 mastercpp2938 registering slave at slave66671958118738391 pomonaapacheorg with id 2015022423223131426977953839130589s0 i0224 232232019122 30615 mastercpp3813 authenticating scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232019156 30615 mastercpp3824 using default crammd5 authenticator i0224 232232019232 30612 registrarcpp445 applied 1 operations in 57599ns attempting to update the registry i0224 232232019394 30603 authenticatorhpp170 creating new server sasl connection i0224 232232019541 30611 authenticateehpp230 received sasl authentication mechanisms crammd5 i0224 232232019568 30611 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0224 232232019666 30605 authenticatorhpp276 received sasl authentication start i0224 232232019717 30605 authenticatorhpp398 authentication requires more steps i0224 232232019805 30615 authenticateehpp276 received sasl authentication step i0224 232232019942 30605 authenticatorhpp304 received sasl authentication step i0224 232232019979 30605 auxpropcpp99 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0224 232232019994 30605 auxpropcpp171 looking up auxiliary property userpassword i0224 232232020025 30605 auxpropcpp171 looking up auxiliary property cmusaslsecretcrammd5 i0224 232232020036 30610 slavecpp1090 will retry registration in 10850555ms if necessary i0224 232232020053 30605 auxpropcpp99 request to lookup properties for user testprincipal realm pomonaapacheorg server fqdn pomonaapacheorg sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0224 232232020102 30605 auxpropcpp121 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0224 232232020117 30605 auxpropcpp121 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0224 232232020133 30605 authenticatorhpp390 authentication success i0224 232232020151 30611 mastercpp2926 ignoring register slave message from slave66671958118738391 pomonaapacheorg as admission is already in progress i0224 232232020226 30603 authenticateehpp316 authentication success i0224 232232020256 30611 mastercpp3871 successfully authenticated principal testprincipal at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232020534 30615 schedcpp398 successfully authenticated with master master671958118738391 i0224 232232020561 30615 schedcpp521 sending registration request to master671958118738391 i0224 232232020635 30615 schedcpp554 will retry registration in 490035142ms if necessary i0224 232232020720 30613 mastercpp1574 received registration request for framework default at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232020787 30613 mastercpp1435 authorizing framework principal testprincipal to receive offers for role  i0224 232232021122 30607 mastercpp1638 registering framework 20150224232231314269779538391305890000 default at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232021502 30611 hierarchicalhpp321 added framework 20150224232231314269779538391305890000 i0224 232232021531 30611 hierarchicalhpp834 no resources available to allocate i0224 232232021543 30611 hierarchicalhpp741 performed allocation for 0 slaves in 18915ns i0224 232232021618 30609 schedcpp448 framework registered with 20150224232231314269779538391305890000 i0224 232232021673 30609 schedcpp462 schedulerregistered took 26310ns i0224 232232022400 30613 logcpp684 attempting to append 316 bytes to the log i0224 232232022523 30608 coordinatorcpp340 coordinator attempting to write 2 action at position 3 i0224 232232023232 30607 replicacpp511 replica received write request for position 3 i0224 232232024055 30607 leveldbcpp343 persisting action 335 bytes to leveldb took 798548ns i0224 232232024073 30607 replicacpp679 persisted action at 3 i0224 232232024651 30610 replicacpp658 replica received learned notice for position 3 i0224 232232025252 30610 leveldbcpp343 persisting action 337 bytes to leveldb took 580525ns i0224 232232025271 30610 replicacpp679 persisted action at 3 i0224 232232025297 30610 replicacpp664 replica learned 2 action at position 3 i0224 232232025995 30618 registrarcpp490 successfully updated the registry in 6586112ms i0224 232232026228 30604 logcpp703 attempting to truncate the log to 3 i0224 232232026360 30609 coordinatorcpp340 coordinator attempting to write 3 action at position 4 i0224 232232026669 30609 slavecpp2831 received ping from slaveobserver66671958118738391 i0224 232232026772 30609 slavecpp792 registered with master master671958118738391 given slave id 2015022423223131426977953839130589s0 i0224 232232026737 30603 mastercpp2995 registered slave 2015022423223131426977953839130589s0 at slave66671958118738391 pomonaapacheorg with cpus2 mem1024 disk370122e06 ports3100032000 i0224 232232026867 30603 status_update_managercpp178 resuming sending status updates i0224 232232026868 30617 hierarchicalhpp455 added slave 2015022423223131426977953839130589s0 pomonaapacheorg with cpus2 mem1024 disk370122e06 ports3100032000 and cpus2 mem1024 disk370122e06 ports3100032000 available i0224 232232026921 30615 replicacpp511 replica received write request for position 4 i0224 232232027276 30617 hierarchicalhpp759 performed allocation for slave 2015022423223131426977953839130589s0 in 351257ns i0224 232232027580 30615 leveldbcpp343 persisting action 16 bytes to leveldb took 624249ns i0224 232232027604 30615 replicacpp679 persisted action at 4 i0224 232232027642 30618 mastercpp3755 sending 1 offers to framework 20150224232231314269779538391305890000 default at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232028223 30617 replicacpp658 replica received learned notice for position 4 i0224 232232028621 30607 schedcpp611 schedulerresourceoffers took 648326ns i0224 232232028916 30617 leveldbcpp343 persisting action 18 bytes to leveldb took 662416ns i0224 232232028991 30617 leveldbcpp401 deleting 2 keys from leveldb took 47386ns i0224 232232029021 30617 replicacpp679 persisted action at 4 i0224 232232029044 30617 replicacpp664 replica learned 3 action at position 4 i0224 232232029534 30613 mastercpp2268 processing accept call for offers  2015022423223131426977953839130589o0  on slave 2015022423223131426977953839130589s0 at slave66671958118738391 pomonaapacheorg for framework 20150224232231314269779538391305890000 default at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 i0224 232232190521 30613 mastercpp2112 authorizing framework principal testprincipal to launch task 0 as user jenkins w0224 232232191864 30604 validationcpp328 executor default for task 0 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w0224 232232191905 30604 validationcpp340 executor default for task 0 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i0224 232232192206 30604 masterhpp822 adding task 0 with resources cpus1 mem500 on slave 2015022423223131426977953839130589s0 pomonaapacheorg i0224 232232192318 30604 mastercpp2545 launching task 0 of framework 20150224232231314269779538391305890000 default at scheduler9a3224ccaef049a7a2404b85b913ff44671958118738391 with resources cpus1 mem500 on slave 2015022423223131426977953839130589s0 at slave66671958118738391 pomonaapacheorg i0224 232232192659 30611 slavecpp1121 got assigned task 0 for framework 20150224232231314269779538391305890000 i0224 232232192847 30609 hierarchicalhpp648 recovered cpus1 mem524 disk370122e06 ports3100032000 total allocatable cpus1 mem524,2
add an example framework to test persistent volumes this serves two purposes 1 testing the new persistence feature 2 served as an example for others to use the new feature,3
add user doc for using persistent volumes,2
slave should garbage collect released persistent volumes this is tricky in the case when a persistence id is reused when a persistent volume is destroyed explicitly by the framework master deletes all information about this volume that mean the master no longer has the ability to check if the persistence id is reused and reject the later attempt on the slave side well use some gc policy to remove directories associated with deleted persistent volumes similar to how we gc sandboxes that means the persistent volume directory wont be deleted immediately when the volume is destroyed by the framework explicitly when the same persistence id is reused well see the persistent volume still exists and we need to cancel the gc of that directory similar to what we cancel the gc for meta directories during runtask,5
use fq_codel qdisc for egress network traffic isolation,8
add java binding for the acceptoffers api we introduced the new acceptoffers api in c driver we need to provide java binding for this api as well,2
add python bindings for the acceptoffers api we introduced the new acceptoffers api in c driver we need to provide python binding for this api as well,2
improve support for streaming http responses in libprocess currently libprocess httpresponse supports a pipe construct for doing streaming responses code struct response    either provide a body an absolute path to a file or a  pipe for streaming a response distinguish between the cases  using type below   body uses body as the body of the response these may be  encoded using gzip for efficiency if contentencoding is not  already specified   path attempts to perform a sendfile operation on the file  found at path   pipe splices data from pipe using transferencodingchunked  note that the read end of the pipe will be closed by libprocess  either after the write end has been closed or if the socket the  data is being spliced to has been closed ie nobody is  listening any longer this can cause writes to the pipe to  generate a sigpipe which will terminate your program unless you  explicitly ignore them or handle them   in all cases body path pipe you are expected to properly  specify the contenttype header but the contentlength and  or transferencoding headers will be filled in for you enum  none body path pipe  type   code this interface is too low level and difficult to program against  connection closure is signaled with sigpipe which is difficult for callers to deal with must suppress sigpipe locally or globally in order to get epipe instead  pipes are generally for interprocess communication and the pipe has finite size with a blocking pipe the caller must deal with blocking when the pipes buffer limit is exceeded with a nonblocking pipe the caller must deal with retrying the write well want to consider a few use cases  sending an httpresponse with streaming data  making a request with httpget and httppost in which the data is returned in a streaming manner  making a request in which the request content is streaming this ticket will focus on 1 as it is required for the http api,8
mesos replicated log does not log the action type name this is a regression introduced during the internal namespace refactor 0210 master noformat i0224 024329806895 50982 replicacpp661 replica learned append action at position 1655 noformat 0220 master noformat i0303 214539406929 1302 replicacpp664 replica learned 2 action at position 2079 noformat,1
the recovered executor directory points to the meta directory the bug was introduced in this review httpsreviewsapacheorgr29687 runstatedirectory points to the metadata directory this would cause the posixdiskisolator to report incorrect disk usages after slave recovery we also need a test to test the slave recovery path for the posixdiskisolator,2
add support for procselfmountinfo on linux procselfmountinfo provides mount information specific to the calling process this includes information on optional fields describing mount propagation eg sharedslave mounts initially add this to linuxfs then perhaps move existing users of mounttable to use the mountinfo deprecating and removing the mostly but not entirely redundant code,3
add operator endpoints to createdestroy persistent volumes persistent volumes will not be released automatically so we probably need an endpoint for operators to forcefully release persistent volumes we probably need to add principal to persistence struct and use acls to control who can release what additionally it would be useful to have an endpoint for operators to create persistent volumes,3
slave should provide details on processes running in its cgroups the slave can optionally be put into its own cgroups for a list of subsystems eg for monitoring of memory and cpu see the slave flag slave_subsystems it currently refuses to start if there are any processes in its cgroups  this could be another slave or some subprocess started by a previous slave  and only logs the pids of those processes improve this to log details about the processes suggest at least the process command uid running it and perhaps its start time,1
add option for subprocess to set a death signal for the forked child currently children forked by the slave including those through subprocess will continue running if the slave exits for some processes including helper processes like the fetcher du or perf wed like them to be terminated when the slave exits add support to subprocess to optionally set a deathsig for the child eg setting sigterm would mean the child would get sigterm when the slave terminates this can be done after forking with pr_set_deathsig see man prctl it is preserved through an exec call,3
authentication failure may lead to slave crash when slave authentication fails the following attempt to transmit a unregisterslavemessage may cause a crash within the slave noformat e0309 010834819758 336699392 slavecpp740 master master192168178205050 refused authentication i0309 010834819787 336699392 slavecpp538 master refused authentication unregistering and shutting down libprotobuf fatal googleprotobufmessage_litecc273 check failed isinitialized cant serialize message of type mesosinternalunregisterslavemessage because it is missing required fields slave_idvalue libprocess slave1192168178205051 terminating due to check failed isinitialized cant serialize message of type mesosinternalunregisterslavemessage because it is missing required fields slave_idvalue noformat the problem here is the following code noformat unregisterslavemessage message_ message_mutable_slave_idmergefrominfoid noformat authentication happens before registration infoid is an optional member of slaveinfo and not known yet it is set later while registering so slave_id will remain unset,1
write documentation for all the libprocess_ environment variables libprocess uses a set of environment variables to modify its behaviour however these variables are not documented anywhere nor it is defined where the documentation should be what would be needed is a decision whether the environment variables should be documented a new doc file or reusing an existing one and then add the documentation there after searching in the code these are the variables which need to be documented  libprocess_ip  libprocess_port  libprocess_advertise_ip  libprocess_advertise_port,2
allow resources flag to take json currently we used a customized format for resources flag as we introduce more and more stuffs eg persistence reservation in resource object we need a more generic way to specify resources for backward compatibility we can scan the first character if it is  then we invoke the json parser otherwise we use the existing parser,3
mesos masterslave should be able to bind to 127001 if explicitly requested with the current refactoring to ip it looks like master and slave can no longer bind to 127001 even if explicitly requested via ip flag among other things this breaks the balloon framework test which uses this flag,1
add the resourcereservationinfo protobuf message the resourcereservationinfo protobuf message encapsulates information needed to keep track of reservations its named reservationinfo rather than reservation to keep consistency with resourcediskinfo heres what it will look like code message reservationinfo   indicates the principal of the operator or framework that created the  reservation this is used to determine whether this resource can be  unreserved by an operator or a framework by checking the  unreserve acl required string principal   if this is set this resource was dynamically reserved by an  operator or a framework otherwise this resource was  statically configured by an operator via the resources flag optional reservationinfo reservation code,2
enable resources to handle resourcereservationinfo after mesos2475httpsissuesapacheorgjirabrowsemesos2475 our c resources class needs to know how to handle resource protobuf messages that have the reservation field set,2
enable resourcesapply to handle reservation operations resourcesapply currently only handles create and destroy operations which exist for persistent volumes we need to handle the reserve and unreserve operations for dynamic reservations as well,3
add ability to distinguish slave removals metrics by reason currently we only expose a single removal metric masterslave_removals which makes it difficult to distinguish between removal reasons in the alerting currently a slave can be removed for the following reasons  health checks failed  slave unregistered  slave was replaced by a new slave on the same endpoint in the case of 2 we expect this to be due to maintenance and dont want to be notified as strongly as with health check failures,3
enable a framework to perform reservation operations h3 goal this is the first step to supporting dynamic reservations the goal of this task is to enable a framework to reply to a resource offer with reserve and unreserve offer operations as defined by offeroperation in mesosproto h3 overview its divided into a few subtasks so that its clear what the small chunks to be addressed are in summary we need to introduce the resourcereservationinfo protobuf message to encapsulate the reservation information enable the c resources class to handle it then enable the master to handle reservation operations h3 expected outcome  the framework will be able to send back reservation operations to unreserve resources  the reservations are kept only in the master since we dont send the checkpointresources message to checkpoint the reservations on the slave yet  the reservations are considered to be reserved for the frameworks role,4
persist the reservation state on the slave h3 goal the goal for this task is to persist the reservation state stored on the master on the corresponding slave the needcheckpointing predicate is used to capture the condition for which a resource needs to be checkpointed currently the only condition is ispersistentvolume well update this to include dynamically reserved resources h3 expected outcome  the dynamically reserved resources will be persisted on the slave,5
create synchronous validations for calls call endpoint will return a 202 accepted code but has to do some basic validations before in case of invalidation it will return a 4xx code we have to create a mechanism that will validate the request and send back the appropriate code,8
doxygen setup for libprocess goals  initial doxygen setup  enable interested developers to generate already available doxygen content locally in their workspace and view it  form the basis for future contributions of more doxygen content 1 devise a way to use doxygen with mesos source code for example solve this by adding optional brewaptget installation to the getting started doc 2 create a make target for libprocess documentation that can be manually triggered 3 create initial library top level documentation 4 enhance one header file with doxygen make sure the generated output has all necessary links to navigate from the lib to the file and back etc,2
doxygen style for libprocess create a description of the doxygen style to use for libprocess documentation it is expected that this will later also become the doxygen style for stout and mesos but we are working on libprocess only for now possible outcome a file named docsdoxygenstylemd we hope for much input and expect a lot of discussion,1
performance issue in the master when a large number of slaves are registering for large clusters when a lot of slaves are registering the master gets backlogged processing registration requests perf revealed the following code events 14k cycles 2544 libmesos0220xso  mesosinternalmastermasterregisterslaveprocessupid const mesosslaveinfo const stdvectormesosresource stdallocatormesosresource  cons 1118 libmesos0220xso  pipecb 588 libc25so  malloc_consolidate 533 libc25so  _int_free 525 libc25so  malloc 523 libc25so  _int_malloc 411 libstdcso608  stdstringassignstdstring const 322 libmesos0220xso  mesosresourceshareddtor 310 kernel k _raw_spin_lock 197 libmesos0220xso  mesosattributeshareddtor 128 libc25so  memcmp 108 libc25so  free code this is likely because we loop over all the slaves for each registration code void masterregisterslave const upid from const slaveinfo slaveinfo const vectorresource checkpointedresources const string version     check if this slave is already registered because it retries foreachvalue slave slave slavesregistered  if slavepid  from         code,5
fetchertestextractnotexecutable is flaky observed in our internal ci code  run  fetchertestextractnotexecutable using temporary directory tmpfetchertest_extractnotexecutable_r5r7cn tar removing leading  from member names i0316 185548509306 14678 fetchercpp155 starting to fetch uris for container de1e516582b4434b91498667cf652c64 directory tmpfetchertest_extractnotexecutable_r5r7cn i0316 185548509845 14678 fetchercpp238 fetching uris using command varjenkinsworkspacemesosfedora20gccsrcmesosfetcher i0316 185548568611 15028 loggingcpp177 logging to stderr i0316 185548574928 15028 fetchercpp214 fetching uri tmpdijmjvtargz i0316 185548575166 15028 fetchercpp194 copying resource from tmpdijmjvtargz to tmpfetchertest_extractnotexecutable_r5r7cn tar this does not look like a tar archive tar exiting with failure status due to previous errors failed to extract tmpfetchertest_extractnotexecutable_r5r7cndijmjvtargzfailed to extract command tar c tmpfetchertest_extractnotexecutable_r5r7cn xf tmpfetchertest_extractnotexecutable_r5r7cndijmjvtargz exited with status 512 testsfetcher_testscpp686 failure fetchfailure failed to fetch uris for container de1e516582b4434b91498667cf652c64with exit status 256  failed  fetchertestextractnotexecutable 208 ms code,2
change the default leaf qdisc to fq_codel inside containers when we enable bandwidth cap htb is used on egress side inside containers however the default leaf qdisc for a htb class is still pfifo_fast which is known to have buffer bloat change the default leaf qdisc to fq_codel too tc qd add dev eth0 parent 11 fq_codel i can no longer see packet drops after this change,1
log ip addresses from http requests querying masterstatejson is an expensive operation when a cluster is large and its possible to dos the master via frequent and repeated queries which is a separate problem querying the endpoint results in a log entry being written but the entry lacks useful information such as an ip address response code and response size these details are useful for tracking down whowhat is querying the endpoint consider adding these details to the log entry or even writing a separate accesshttpshttpdapacheorgdocstrunklogshtmlaccesslog loghttpshttpdapacheorgdocstrunklogshtmlcommon also consider writing log entries for _all_ http requests metricssnapshot produces no log entries noformattitlesample log entry i0319 180618824846 10521 httpcpp478 http request for masterstatejson noformat,3
symlink the namespace handle with containerid for the port mapping isolator this serves two purposes 1 allows us to enter the network namespace using container id instead of pid ip netns exec containerid commands args 2 allows us to get container id for orphan containers during recovery this will be helpful for solving mesos2367 the challenge here is to solve it in a backward compatible way i propose to create symlinks under varrunnetns for example varrunnetnscontaineridxxxx  varrunnetns12345 12345 is the pid the old code will only remove the bind mounts and leave the symlinks which i think is fine since containerid is globally unique uuid,3
support http checks in mesos health check program currently only commands are supported but our health check protobuf enables users to encode http checks as well we should wire up this in the health check program or remove the http field from the protobuf,8
perftestroot_sampleinit test fails from mesos2300 as well it looks like this test is not reliable code  run  perftestroot_sampleinit srctestsperf_testscpp147 failure expected 0u  statisticsgetcycles actual 0 vs 0 srctestsperf_testscpp150 failure expected 00  statisticsgettask_clock code it looks like this test samples pid 1 which is either init or systemd per a chat with idownes this should probably sample something that is guaranteed to be consuming cycles,2
remove unnecessary default flags from portmappingmesostest as all the explicitly set flags are defaults we can remove them and simplify the code mesos2375 removed other occurrences of these default flags,1
developer guide for libprocess create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in mesos with examples this could be similar to stoutreadmemd,2
cleanup stale bind mounts for port mapping isolator during slave recovery leaked bind mount under varrunnetns for port mapping isolator is a known issue there are many ways it can get leaked for example if the slave crashes after creating the bind mount but before creating the veth the bind mount will be leaked also if the detached unmount does not finish in time and the subsequent osrm fails the bind mount will be leaked as well since leaked bind mount is inevitable we need to clean them up during startup slave recovery,2
new make distcheck failures inside a docker container after the commits code change 21 category none changed by jie yu yujiejaygmailcom changed at wed 25 mar 2015 001214 repository httpsgitwipusapacheorgreposasfmesosgit branch master revision 6c6473febac40be1e01c9ab005cca20ad2a48e18 comments disallowed multiple cgroups base hierarchies in tests review httpsreviewsapacheorgr32452 changed files srctestsmesoscpp change 22 category none changed by jie yu yujiejaygmailcom changed at wed 25 mar 2015 001537 repository httpsgitwipusapacheorgreposasfmesosgit branch master revision 212b88c4d20a89dcd9f319b3be984f5646a47499 comments allowed mesoscontainerizer to take empty isolation flag review httpsreviewsapacheorgr32467 code numerous tests inside our internal ci started failing code  run  slaverecoverytest0recoverslavestate srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0recoverslavestate where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0recoverstatusupdatemanager srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0recoverstatusupdatemanager where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0reconnectexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0reconnectexecutor where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0recoverunregisteredexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0recoverunregisteredexecutor where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0recoverterminatedexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0recoverterminatedexecutor where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0recovercompletedexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0recovercompletedexecutor where typeparam  mesosinternalslavemesoscontainerizer 23 ms  run  slaverecoverytest0cleanupexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0cleanupexecutor where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0removenoncheckpointingframework srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0removenoncheckpointingframework where typeparam  mesosinternalslavemesoscontainerizer 25 ms  run  slaverecoverytest0noncheckpointingframework srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0noncheckpointingframework where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0killtask srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0killtask where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0reboot 20150325 003256830405960x7f7cbf4f4700zoo_errorhandle_socket_error_msg1697 socket 12700132810 zk retcode4 errno111connection refused server refused to accept the client srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0reboot where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0gcexecutor srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0gcexecutor where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0shutdownslave srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0shutdownslave where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0shutdownslavesigusr1 srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0shutdownslavesigusr1 where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0registerdisconnectedslave srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0registerdisconnectedslave where typeparam  mesosinternalslavemesoscontainerizer 25 ms  run  slaverecoverytest0reconcilekilltask srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0reconcilekilltask where typeparam  mesosinternalslavemesoscontainerizer 24 ms  run  slaverecoverytest0reconcileshutdownframework srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0reconcileshutdownframework where typeparam  mesosinternalslavemesoscontainerizer 23 ms  run  slaverecoverytest0reconciletasksmissingfromslave srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0reconciletasksmissingfromslave where typeparam  mesosinternalslavemesoscontainerizer 25 ms  run  slaverecoverytest0schedulerfailover srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0schedulerfailover where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0partitionedslave srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0partitionedslave where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0masterfailover srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0masterfailover where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0multipleframeworks srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0multipleframeworks where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0multipleslaves srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0multipleslaves where typeparam  mesosinternalslavemesoscontainerizer 26 ms  run  slaverecoverytest0restartbeforecontainerizerlaunch srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  slaverecoverytest0restartbeforecontainerizerlaunch where typeparam  mesosinternalslavemesoscontainerizer 26 ms  24 tests from slaverecoverytest0 596 ms total  4 tests from mesoscontainerizerslaverecoverytest  run  mesoscontainerizerslaverecoverytestresourcestatistics srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  mesoscontainerizerslaverecoverytestresourcestatistics 25 ms  run  mesoscontainerizerslaverecoverytestcgroups_root_perfrollforward srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  mesoscontainerizerslaverecoverytestcgroups_root_perfrollforward 24 ms  run  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespaceforward srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespaceforward 25 ms  run  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespacebackward srctestsmesoscpp555 failure value of _basehierarchy actual sysfscgroupcpu expected basehierarchy which is sysfscgroup  multiple cgroups base hierarchies detected sysfscgroup sysfscgroupcpu mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems   failed  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespacebackward 24 ms  4 tests from mesoscontainerizerslaverecoverytest 98 ms total code code  failed  28 tests listed below  failed  slaverecoverytest0recoverslavestate where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0recoverstatusupdatemanager wh,1
c scheduler library should send call messages to master currently the c library sends different messages to master instead of a single call message to vet the new call api it should send call messages master should be updated to handle all types of calls,8
c scheduler library should send http calls to master once the scheduler library sends call messages we should update it to send calls as http requests to call endpoint on master,3
document issue with slave recovery when using systemd as the problem encountered in mesos2419 is a common problem with the default systemd configuration it would make sense to document this in the upgrade guide or somewhere else in the documentation,1
do not use runtaskmessageframework_id assume that frameworkinfoid is always set and so need to readset runtaskmessageframework_id this should land after httpsissuesapacheorgjirabrowsemesos2558 has been shipped,1
0240 release the main feature of this release is going to be v1 beta release of the http scheduler api part of mesos2288 epic unresolved issues tracker httpsissuesapacheorgjiraissuesjqlproject203d20mesos20and20status203d20resolved20and2022target20version2fs22203d20024020order20by20status20desc,5
expose memory pressure in memisolator,3
add memory statistics tests,5
use memory test helper to improve some test code,2
namespace handle symlinks in port_mapping isolator should not be under varrunnetns consider putting symlinks under varrunmessonetns this is because ip command assumes all files under varrunnetns are valid namespaces without duplication and it has command like ip all netns exec ip link to list all links for each network namespace,3
add  on newline for function declarations in style checker similar to mesos2577 another common style mistake is to not move curly braces on a newline for function and class declarations code class foo  void bar     code vs code class foo  void bar     code this should be easy to check with our style checker too,1
0221 release,1
document tips best practices guidelines for doing code reviews we currently have a committers guidehttpsgithubcomapachemesosblob0220docscommittersguidemd however most of this information is relevant to all contributors looking to be participating in the code review process im proposing we extract much of this information into a more general code reviewing document and include additional tips best practices lessons learned from members of the community this would be a great prerequisite for onboarding more committers and adding maintainershttpmailarchivesapacheorgmod_mboxmesosdev201502mbox3cca8rcoreugmvqoopsnb8wgybela5fhwpajyhje22iwzvsbeqmailgmailcom3e the committers guide can be more specific to our expectations of committers so we may want to make this into a committership document to help set expectations for contributors looking to become committers,3
create optional release step update pypi repositories one of the build artifacts for a release is the python package mesosinterface that needs to be uploaded to pypi along with a release to allow for users of python frameworks to use that version of mesos,2
let the slave control the duration of the perf sampler instead of relying on a sleep command right now we use a sleep command to control the duration of perf sampling noformat sudo perf stat a x logfd 1 pid 10940  sleep 10 noformat this causes an additional process ie the sleep process to be forked and causes troubles for us to terminate the perf sampler once the slave exits see mesos2462 seems that the additional sleep process is not necessary the slave can just monitor the duration and send a sigint to the perf process when duration elapsed this will cause the perf process to output the stats and terminate,3
refactor launchhelper and statisticshelper in port_mapping_tests to allow reuse refactor launchhelper and statisticshelper in port_mapping_tests to allow reuse,2
create docker executor currently were reusing the command executor to wait on the progress of the docker executor but has the following drawback  we need to launch a seperate docker log process just to forward logs where we can just simply reattach stdoutstderr if we create a specific executor for docker  in general mesos slave is assuming that the executor is the one starting the actual task but the current docker containerizer the containerizer is actually starting the docker container first then launches the command executor to wait on it this can cause problems if the container failed before the command executor was able to launch as slave will try to update the limits of the containerizer on executor registration but then the docker containerizer will fail to do so since the container failed overall its much simpler to tie the container lifecycle with the executor and simplfies logic and log management,8
update allocator docs once allocator interface changes so does the way of writing new allocators this should be reflected in mesos docs the modules doc should mention how to write and use allocator modules configuration doc should mention the new allocator flag,2
slave statejson frameworksexecutorsqueued_tasks wrong format queued_tasksexecutor_id is expected to be a string and not a complete json object it should have the very same format as the tasks array on the same level example directly taken from slave noformat  queued_tasks   data  executor_id  command  argv  uris   executable false value httpdownloadsfooioorchestrastormmesos092incubating47ovhbb373df1cstormmesos092incubatingtgz   value cd stormmesos  python binstorm supervisor stormmesosmesossupervisor  data assignmentidsrv4hwca1foocomsupervisoridsrv4hwca1foocomstageingestionstatsslave1111428421145 executor_id stageingestionstatsslave1111428421145 framework_id 20150401160104251662508505021970002 name  resources  cpus 05 disk 0 mem 1000   id srv4hwca1foocom31708 name worker srv4hwca1foocom31708 resources  cpus 1 disk 0 mem 5120 ports 3170831708  slave_id 2015032702555321810807650504122s0    noformat,3
add reserve and unreserve endpoints on the master for dynamic reservation enable operators to manage dynamic reservations by introducing the reserve and unreserve http endpoints on the master,5
notify dev  user mailing list of the upcoming mem stat renames in 0230,2
change docker rm command right now it seems mesos is using docker rm f id to delete containers so bind mounts are not deleted this means thousands of dirs in varlibdockervfsdir i would like to have the option to change it to docker rm f v id this deletes bind mounts but not persistant volumes best mike,2
pipe updateframework path from master to allocator to support framework reregistration pipe the updateframework call from the master through the allocator as described in the design doc in the epic mesos703,1
document the semantic change in decorator return values in order to enable decorator modules to _remove_ metadata environment variables or labels we changed the meaning of the return value for decorator hooks the resultt return values means statebeforeafter errorerror is propagated to the callsiteno change nonethe result of the decorator is not appliedno change somethe result of the decorator is appendedthe result of the decorator overwrites the final labelsenvironment object,1
examplestestpersistentvolumeframework is flaky this just failed for the first time on our os x bot far less frequent flaky than the other examplestest but still flaky while compiling master at commit f6620f851f635b3346c6ebf878152f38b3932ad9 there werent any commits which touched  changed anything in the test in the set code  run  examplestestpersistentvolumeframework srctestsscriptcpp83 failure failed persistent_volume_framework_testsh terminated with signal abort trap 6  failed  examplestestpersistentvolumeframework 7865 ms code,1
update style guide to disallow capture by reference of temporaries we modify the style guide to disallow constant references to temporaries as a whole this means disallowing both 1 and 2 below h3 background 1 constant references to simple expression temporaries do extend the lifetime of the temporary till end of function scope  temporary returned by function code  see full example below t fconst char s  return ts   const t good  fok  use of good is ok  code  temporary constructed as simple expression code  see full example below  const t good  tok  use of good is ok  code 2 constant references to expressions that result in a reference to a temporary do not extend the lifetime of the temporary  temporary returned by function code  see full example below t fconst char s  return ts   const t bad  fbadmember  use of bad is invalid  code  temporary constructed as simple expression code  see full example below  const t bad  tbadmember  use of bad is invalid  code h3 mesos case  in mesos we use futuret a lot many of our functions return futures by value code class socket  futuresocket accept futuresize_t recvchar data size_t size   code  sometimes we capture these futures code  const futuresocket accepted  socketaccept  valid c propose we disallow  code  sometimes we chain these futures code  socketacceptthenlambdabind_accepted  temporary will be valid during then expression evaluation  code  sometimes we do both code  const futuresocket accepted  socketacceptthenlambdabind_accepted  dangerous accepted lifetime will not be valid till end of scope disallow  code h3 reasoning  although 1 is ok and considered a featurehttpherbsuttercom20080101gotw88acandidateforthemostimportantconst 2 is extremely dangerous and leads to hard to track bugs  if we explicitly allow 1 but disallow 2 then my worry is that someone coming along to maintain the code later on may accidentally turn 1 into 2 without recognizing the severity of this mistake for example code  original code const t val  t stdcout  val  stdendl  new code const t val  tremovewhitespace stdcout  val  stdendl  val could be corrupted since the destructor has been invoked and ts memory freed code  if we disallow both cases it will be easier to catch these mistakes early on in code reviews and avoid these painful bugs at the same cost of introducing a new style guide rule h3 performance implications  benh suggests c developers are commonly taught to capture by constant reference to hint to the compiler that the copy can be elided  modern compilers use a data flow graph to make optimizations such as  inplaceconstruction leveraged by rvo and nrvo to construct the object in place on the stack similar to placement new httpenwikipediaorgwikiplacement_syntax  rvo return value optimization httpenwikipediaorgwikireturn_value_optimization  nrvo named return value optimization httpsmsdnmicrosoftcomenuslibraryms36405728vvs8029aspx  since modern compilers perform these optimizations we no longer need to hint to the compiler that the copies can be elided h3 example program code include stdioh class t  public tconst char str  strstr  printf tsn str  t  printf tsn str  const t member const  return this  private const char str  t fconst char s  return ts  int main  const t good  tok const t good_f  fok function const t bad  tbadmember const t bad_f  tbad functionmember printfend of function scopen  code output code  tok  tok function  tbad  tbad  tbad function  tbad function end of function scope  tok function  tok code,1
remove capture by reference of temporaries in stout,1
remove capture by reference of temporaries in libprocess,1
move implementations of framework struct functions out of masterhpp to help reduce compile time and keep the header easy to read lets move the implementations of the framework struct functions out of masterhpp,1
segfault in inline tryip getipconst stdstring hostname int family we saw a segfault in production attaching the coredump we see core was generated by usrlocalsbinmesosslave port5051 resourcescpus23mem70298ports31 program terminated with signal 11 segmentation fault 0 0x00007f639867c77e in free  from lib64libcso6 gdb bt 0 0x00007f639867c77e in free  from lib64libcso6 1 0x00007f63986c25d0 in freeaddrinfo  from lib64libcso6 2 0x00007f6399deeafa in netgetip hostnameredacted family2 at 3rdpartystoutincludestoutnethpp201 3 0x00007f6399e1f273 in processinitialize delegateunhandled dwarf expression opcode 0xf3  at srcprocesscpp837 4 0x000000000042342f in main,1
consolidate foo bar  string constants in test and example code we are using foo bar  string constants and pairs in srctestsmaster_testscpp srctestsslave_testscpp srctestshook_testscpp and srcexamplestest_hook_modulecpp for label and hooks tests these values should be stored in local variables to avoid the possibility of assignment getting out of sync with checking for that same value,2
design doc for resource oversubscription,13
update master to send revocable resources in separate offers master will send separate offers for revocable and nonrevocableregular resources this allows master to rescind revocable offers eg when a new oversubscribed resources estimate comes from the slave without impacting regular offers,3
slave should validate tasks using oversubscribed resources the latest oversubscribed resource estimate might render a revocable task launch invalid slave should check this and send task_lost with appropriate reason we need to add a new reason for this reason_resource_oversubscribed,5
update resource monitor to return resource usage add usage api call to return usage of all containers,3
implement resource estimator resource estimator is the component in the slave that estimates the amount of oversubscribable resources this needs to be integrated with the slave and resource monitor,5
modularize the resource estimator modularizing the resource estimator opens up the door for org specific implementations test the estimator module,3
implement qos controller this is a component of the slave that informs the slave about the possible corrections that need to be performed eg shutdown container using recoverable resources this needs to be integrated with the resource monitor need to figure out the metrics used for sending corrections eg scheduling latency usage informed by executorscheduler we also need to figure out the feedback loop between the qos controller and the resource estimator code class qoscontroller  public qoscontrollerresourcemonitor monitor processqueueqoscorrection correction  code,3
update mesos containerizer to understand revocable cpu resources the cpu isolator needs to properly set limits for revocable and nonrevocable containers the proposed strategy is to use a twoway split of the cpu cgroup hierarchy  normal nonrevocable and low priority revocable subtrees  and to use a biased split of cfs cpushares across the subtrees eg a 201 split tbd containers would be present in only one of the subtrees cfs quotas will not be set on subtree roots only cpushares each container would set cfs quota and shares as done currently,5
slave should act on correction events from qos controller slave might want to kill revocable tasks based on correction events from the qos controller the qos controller communicates corrections through a stream or processqueue to the slave which corrections it needs to carry out in order to mitigate interference with production tasks the correction is communicated through a message code message qoscorrection  enum correctiontype  killexecutor  1  killtask  2  resize throttle task  optional string reason  x optional executorid executor_id  x  optional taskid task_id  x  code and the slave will setup a handler to process these events initially only executor termination is supported and cause the slave to issue containerizerdestroy,8
update frameworkinfo to opt in to revocable resources add a new field to frameworkinfo that lets the frameworks explicitly choose revocable offers for backwards compatibility,1
implement a stand alone test framework that uses revocable cpu resources ideally this would be an example framework or stand alone binary like load generator framework that helps us evaluate oversubscription in a real cluster we need to come up with metrics that need to be exposed by this framework for evaluation eg how many revocable offers rescinds preemptions etc,5
root_cgroups_listen and root_increaserss tests are flaky  running 1 test from 1 test case  global test environment setup  1 test from cgroupsanyhierarchywithcpumemorytest  run  cgroupsanyhierarchywithcpumemorytestroot_cgroups_listen failed to allocate rss memory failed to lock memory mlock resource temporarily unavailablemesossrctestscgroups_testscpp571 failure failed to wait 15secs for future  failed  cgroupsanyhierarchywithcpumemorytestroot_cgroups_listen 15121 ms  1 test from cgroupsanyhierarchywithcpumemorytest 15121 ms total  global test environment teardown  1 test from 1 test case ran 15174 ms total  passed  0 tests  failed  1 test listed below  failed  cgroupsanyhierarchywithcpumemorytestroot_cgroups_listen,3
fix queuing discipline wrapper in linuxroutingqueueing qdisc search function is dependent on matching a single hard coded handle and does not correctly test for interface making the implementation fragile additionally the current setup scripts using dynamically created shell commands do not match the hard coded handles,5
port mapping isolator causes sigabrt during slave recovery there is a bug in the code if there are namespaces created by other party say ip netns the slave recovery will abort,1
containerizertestroot_cgroups_balloonframework flaky noformat i0429 005835267629 2086 slavecpp3210 executor default of framework 2015042900583016777343543220230000 terminated with signal aborted i0429 005835270761 2086 slavecpp2512 handling status update task_lost uuid f969e3506f914fa9980e1852554bd704 for task 1 of framework 201 5042900583016777343543220230000 from 00000 i0429 005835270983 2086 slavecpp4604 terminating task 1 w0429 005835271574 2080 containerizercpp903 ignoring update for unknown container 1298549aa3d246ffaad09dbc777affcc i0429 005835272541 2074 status_update_managercpp317 received status update task_lost uuid f969e3506f914fa9980e1852554bd704 for task 1 o f framework 2015042900583016777343543220230000 i0429 005835272624 2074 status_update_managercpp494 creating statusupdate stream for task 1 of framework 20150429005830167773435432202300 00 i0429 005835273217 2053 mastercpp3493 executor default of framework 2015042900583016777343543220230000 on slave 2015042900583016777343 54322023s0 at slave11035121245051 smfdaki27sr1develtwittercom terminated with signal aborted noformat which is from code 60  we use mlock and memset here to make sure that the memory 61  actually gets paged in and thus accounted for 62 if mlockbuffer chunk  0  63 perrorfailed to lock memory mlock 64 abort 65  66 67 if memsetbuffer 1 chunk  buffer  68 perrorfailed to fill memory memset 69 abort 70  code this is the same as mesos2660 ive confirmed that swapping them fixed it,1
follow google style guide for header file include order completely the header include order for mesos actually follows the google styleguide but omits step 1 without mentioning this exception in the mesos styleguide this proposal suggests to adapt to the include order explained in the google styleguide ie include the direct headers first in the cpp files implementing them a gist of the proposal can be found here httpsgistgithubcomjoerg8465cb9611d24b2e35b69b the corresponding review board review can be found here httpsreviewsapacheorgr33646,5
update modules doc with hook usage example modules doc states the possibility of using hooks but doesnt refer to necessary flags and usage example,1
add a slave flag to enable oversubscription slave sends oversubscribable resources to master only when the flag is enabled,2
slave should kill revocable tasks if oversubscription is disabled if oversubscription is disabled on a restarted slave that had it previously enabled it should kill revocable tasks slave knows this information from the resources of a container that it checkpoints and recovers add a new reason oversubscription_disabled,3
slave should forward oversubscribable resources to the master slave simply forwards resource estimates from resourceestimator to the master use a new message and handler on the master a slave flag for the interval between the messages,5
update resource message to include revocable resources need to update resource message with a new subtype that indicates that the resource is revocable it might also need to specify why it is revocable eg oversubscribed also need to make sure all the operations on resources takes this new message into account,3
printing a resource should show information about reservation disk etc while new fields like diskinfo and reservationinfo have been added to resource protobuf the output stream operator hasnt been updated to show these this is valuable information to have in the logs during debugging,1
add master flag to enabledisable oversubscription this flag lets an operator control cluster level oversubscription the master should send revocable offers to framework if this flag is enabled and the framework opts in to receive them master should ignore revocable resources from slaves if the flag is disabled need tests for all these scenarios,5
explore exposing stats from kernel exploratory work additional tickets to follow,5
add a teardown endpoint on master to teardown a framework we plan to rename shutdown endpoint to teardown to be compatible with the new api shutdown will be deprecated in 0230 or later,2
determine cfs behavior with biased cpushares subtrees see this tickethttpsissuesapacheorgjirabrowsemesos2652 for context  understand the relationship between cpushares and cfs quota  determine range of possible bias splits  determine how to achieve bias eg should 201 be 204801024 or 102450  rigorous testing of behavior with varying loads particularly the combination of latency sensitive loads for high biased tasks nonrevokable and cpu intensive loads for the low biased tasks revokable  discover any performance edge cases,13
implement bilevel cpushares subtrees in cgroupscpu isolator see this tickethttpsissuesapacheorgjirabrowsemesos2652 for context  configurable bias  change cgroup layout  implement rollforward migration path in isolator recover  document rollback migration path,8
compare splitflattened cgroup hierarchy for cpu oversubscription investigate if a flat hierarchy is sufficient for oversubscription of cpu or if a twoway split is necessarypreferred,3
modularize the qos controller modularize the qos controller to enable custom correction policies,3
add tests for qos controller corrections,5
add correct format template declarations to the styleguide the general rule to format templates is to declare them as code template typename t  notice the space between template and  class foo    code however the style is not documented anywhere nor it is inherited from the google style guide,1
incorrect zh uri scheme causes slave to segfault i have 4 slave nodes with the same hardware operating system and mesos configuration few minutes ago all 4 nodes were functioning well i tried to change the config of master from _10172230695050_ to _zh10172230692181mesos_ and restarted them in turn the other three had started normally but the last one got a segmentation fault as you can see below code rootiz25to7d407z  mesosslave masterzh10172230692181mesos hostname1235742237 containerizersdockermesos quiet  1 1216 rootiz25to7d407z   aborted at 1431085131 unix time try date d 1431085131 if you are using gnu date  pc  0x3aede7b53c unknown  sigsegv 0x0 received by pid 1216 tid 0x7f12f984b820 from pid 0 stack trace   0x3aee20f710 unknown  0x3aede7b53c unknown  0x3aedecf630 unknown  0x7f12fce1593f netgetip  0x7f12fce507ae processoperator  0x7f12fce50107 processupidupid  0x7f12fc52af71 mesosinternalmasterdetectorcreate  0x4b1290 main  0x3aede1ed5d unknown  0x4b00b9 unknown 1 segmentation fault mesosslave masterzh10172230692181mesos hostname1235742237 containerizersdockermesos quiet code,2
design doc for the executor http api this tracks the design of the executor http api,2
design master discovery functionality for httponly clients when building clients that do not bind to libmesos and only use the http api via pure language bindings  eg javaonly there is no simple way to discover the masters ip address to connect to rather than relying on outofband configuration mechanisms we would like to enable the ability of interrogating the zookeeper ensemble to discover the masters ip address and possibly other information to which the http api requests can be addressed to,3
deprecating json extension in master endpoints urls add an endpoint for each master endpoint with a json extension such as masterstatsjson so it becomes masterstats after a deprecation cycle,1
implement protobufs for master operator endpoints we should define protobufs for master operator endpoints so as to provide a structure we can refer to for each possible return from an endpoint,2
architecture document for percontainer ip assignment enforcement and isolation there are many ways in which we can go around wiring up percontainer ips in mesos as there are multiple underlying mechanisms and systems for keeping track of ip pools we probably need to aim for a very flexible architecture similar to the oversubscription project there are a couple of folks companies and vendors interested in getting this capability into mesos asap to provide a stronger networking story httpswwwmailarchivecomdevmesosapacheorgmsg32353html so lets start discussing and architecting this,13
create access to the mesos state abstraction that does not require linking with libmesos see srcstatestatehpp and srcjavasrcorgapachemesosstatejava for what the state abstraction is with the new http api see mesos2288 mesos2289 there will be no need to link to libmesos to a framework for it to communicate with a mesos master however if a framework uses the mesos state abstraction either directly in c or through other language bindings eg java it still needs to link with libmesos so in order to achieve libmesosfree frameworks that can leverage all apis mesos has to offer we need a different way to access the state abstraction  one approach is to provide an http api for state queries that get routed through the mesos master which relays them by making calls into libmesos details tbd including how separate this will be from the general http api,13
add support for enabling network namespace without enabling the network isolator following the discussion kapil started it is currently not possible to enable the linux network namespace for a container without enabling the network isolator which requires certain kernel capabilities and dependencies following the pattern of enabling pid namespaces isolationnamespacespid one possible solution could be to add another one for network ie namespacesnetwork,13
update drf sorter to update total resources drf sorter currently keeps track of allocated resources and total resources but there is no way to update the total resources for oversubscription we need the ability to update total resources because total oversubscribed resources change overtime,2
add a new api call to the allocator to update oversubscribed resources this tracks just the work of adding the api call to the allocator interface master makes this call on the allocator whenever it gets a new oversubscribed resources estimate from the slave,2
update master to handle oversubscribed resource estimate from the slave whenever the master gets a new oversubscribed resources estimate from the slave it should rescind any outstanding revocable offers with oversubscribed resources from that slave it should then call the allocator to update the oversubscribed resources,3
update allocator to allocate revocable resources the simplest way to add support for oversubscribed resources to the allocator is to simply add them to the already existing slavetotal and slaveavailable variables it is easy to distinguish the revocable resources by doing a revocable filter,5
change the interaction between the slave and the resource estimator from polling to pushing this will make the semantics more clear the resource estimator can control the speed of sending resources estimation to the slave to avoid cyclic dependency slave will register a callback with the resource estimator and the resource estimator will simply invoke that callback when theres a new estimation ready the callback will be a defer to the slaves main event queue,3
upgrade the design of masterinfo currently the masterinfo pb only supports an ip field as an int32 beyond making it harder and opaque open to subtle bugs for languages other than cc to decode into an ipv4 octets this does not allow mesos to support ipv6 master nodes we should consider ways to upgrade it in ways that permit us to support both ipv4  ipv6 nodes and possibly in a way that makes it easy for languages such as javapython that already have pb support so could easily deserialize this information see also mesos2709 for more info,3
add documentation for maintainers in order to scale the number of committers in the project we proposed the concept of maintainers here httpmarkmailorgthreadcjmdn3d7qfzbxhpm to follow up on that proposal well need some documentation to capture the concept of maintainers both how contributors can benefit from maintainer feedback and the expectations of maintainership in order to not enforce an excessive amount of process maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback,3
reported used resources for tasks in frameworks do not match slave tally rcorral recently observed that according to the masters and the slaves statejson summing up the resources allocated to tasks from different frameworks on a slave does not always match the total that is reported for the slave the latter number is sometimes higher it would be desirable for tools that display allocation statistics to find balanced tallies,3
exposing resources along with resourcestatistics from resource monitor right now the resource monitor returns a usage which contains containerid executorinfo and resourcestatistics in order for resource estimatorqos controller to calculate usage slack or tell if a container is using revokable resources or not we need to expose the resources that are currently assigned to the container this requires us the change the containerizer interface to get the resources as well while calling usage,5
architecture doc on global resources,3
include executorinfos for custom executors in masterstatejson the slavestatejson already reports executorinfos httpsgithubcomapachemesosblob0221srcslavehttpcppl215219 would be great to see this in the masterstatejson as well so external tools dont have to query each slave to find out executor resources sandbox directories etc,3
as a framework user i want to be able to discover my tasks ip the information exposed by the framework via the webuiurl does not always resolves to a routable endpoint eg when the hostname is not publicly resolvable or resolvable at all in order to facilitate service discovery via eg marathon ui we want to add the information in frameworkspid via the statesummary endpoint,3
help generated links point to wrong urls as reported by michael lunøe mlunoemesosphereio see also mesos329 and mesos913 for background quote in mesos3rdpartylibprocesssrchelpcpp a markdown file is created which is then converted to html through a javascript library all endpoints point to help they need to work dynamically for reverse proxy to do its thing mesoshelp works and displays the endpoints but they each need to go to their respective help endpoint note that this needs to work both for master and for slaves i think the route to slaves help is something like this mesosslaves20150518210216169502762850501366s0help but please double check this quote the fix appears to be not too complex as it would require to simply manipulate the generated url but a quick skim of the code would suggest that something more substantial may be desirable too,2
extend queueing discipline wrappers to expose network isolator statistics export traffic control statistics in queueing library to enable reporting out impact of network bandwidth statistics,3
add htb queueing discipline wrapper class network isolator uses a hierarchical token bucket htb traffic control discipline on the egress filter inside each container as the root for adding traffic filters a htb wrapper is needed to access the network statistics for this interface,3
master should validate tasks using oversubscribed resources current implementation out for reviewhttpsreviewsapacheorgr34310 only supports setting the priority of containers with revocable cpu if its specified in the initial executor info resources this should be enforced at the master also master should make sure that oversubscribed resources used by the task are valid,3
reduce multiple use of string literals we have several instances of string literals eg mesoscontainerizer net_tcprtt_microseconds_p50 being used in multiple locations where mismatches would result in correctness issues we should replace these with a single definition to reduce the risk,1
update style guide avoid object slicing in order to improve the safety of our code base lets augment the style guide to disallow public construction of base classes so that we can avoid the object slicing problem this is a good pattern to follow in general as it prevents subtle semantic bugs like the following codetitleobjectslicingcppborderstylesolid include stdioh include vector class base  public baseint _v  v_v  virtual int get const  return v  protected int v  class derived  public base  public derivedint _v  base_v  virtual int get const  return v  1   int main  base b5 derived d5 stdvectorbase vec vecpush_backb vecpush_backd for const auto v  vec  printfdn vget   code,1
add  operator for optiont tryt resultt futuret lets add operator overloads to optiont to allow access to the underlying t using the  operator,3
reflect in documentation that isolator flags are only relevant for mesos containerizer the isolator flags are only relevant when using the mesos containerizer we should reflect this in the flag description to avoid confusion,1
add correction message to inform slave about qos controller actions the qos controller informs the slave about correcting actions kill resize throttle besteffort containers tasks and so forth through a protobuf message called a qoscorrection this ticket tracks designing and creating this message for example code message qoscorrection   note in future we can define more actions like  resize or freeze but for now we have  1 kill  terminate the executor or task enum type  kill  1  kill action which will be performed on an executor message kill  optional executorid executor_id  1  required type action  1 optional string reason  2 optional double timestamp  3 optional kill kill  4  code,1
delegating constructors are not allowed by styleguide as of right now the styleguide does not allow delegating constructors being a c 11 feature they are already used in the code base eg stoutoptionhpp are supported by all relevant compiler gcc 47 and clang 30 and enhance readability therefore we should officially whitelist them in the styleguide,1
explicitlydefaulted functions are not allowed by styleguide as of right now the styleguide does not allow explicitly defaulted functions being a c 11 feature they enhance readability are supported by all relevant compiler gcc 44 and clang 30 and are introduced by some patches eg httpsreviewsapacheorgr34277 therefore we should officially whitelist them in the styleguide,1
consolidate functionality in stoutnet and processhttp stoutnethpp and processhttphpp offer overlapping functionality that could be consolidated in one place presumably the latter since it is more elaborate to begin with this would also remove the dependency of the former on libcurl while we are at it we could then turn netcontentlength into a generalized asynchronous processhttphead call prerequisite mesos2247 with the suggestion to enhance processhttp not stout see a comment in that jira,8
allow resource estimator to get resource usage information this includes two things 1 we need to expose resourcemonitorusage so that module writers can access it we could define a protobuf message for that 2 we need to allow resourceestimator to call resourcemonitorusages we could either expose the resourcemonitor or pass in a lambda to the resources estimator,5
add validation behavior to flagsbase in every launcher file ie those containing some variation on main there is a minor variation on code if flagshelp  cout  flagsusage  endl  arguably this is not an error the user asked for help  and she got it  the program execution ought to be  considered successful return exit_success  code as this is default behavior and weve added support for the help flag in the flagsbase class we should add this too there and remove it from everywhere else additionally a recurring behavior is checking for the presence of a required flag code if flagsmasterisnone  exitexit_failure  flagsusagemaster is required  code or some variation thereof we should add automatic validation for required flags during parsing this follows the dry principle,1
metric for cpu scheduling latency from all components the metric will provide statistics on the scheduling latency for processesthreads in a container ie statistics on the delay before application code can run this will be the aggregate effect of the normal scheduling period contention from other threadsprocesses both in the container and on the system and any effects from the cfs bandwidth control if enabled or other cpu isolation strategies,8
slave should forward total amount of oversubscribed resources to the master in addition to the unallocated oversubscribed resources the slave should also send the oversubscribed resources that are already allocated this is needed by the masterallocator to accurately calculate the available oversubscribed resources to offer,3
sigsegv received during resourcemonitorprocessusage observed in production noformattitleslave log i0523 170359830229 56587 port_mappingcpp2616 freed ephemeral ports 3379234816 for container with pid 47791 i0523 170359849773 56587 port_mappingcpp2764 successfully performed cleanup for pid 47791  aborted at 1432400641 unix time try date d 1432400641 if you are using gnu date  pc  0x7f100fcbfd85 _znst17_function_handlerifvrksseznk7process6futurein5mesos8internal5slave15resourcemonitor5usageee8onfailedizns7_22resourcemonitorprocess5usageens5_11containerideeuls1_e_veerksa_ot_nsa_6prefereeuls1_e_e9_m_invokeerkst9_any_datas1_ i0523 170359898959 56587 slavecpp3246 executor thermos1432400210944mesostestexhaust_diskspace54744d0fbe0a14e40bb2256bd5cbd9524 of framework 20110328224700000000190000 terminated with signal killed i0523 170403419869 56587 slavecpp2547 handling status update task_failed uuid 3be19404f7374a70a330d1d924a85dbb for task 1432400210944mesostestexhaust_diskspace54744d0fbe0a14e40bb2256bd5cbd9524 of framework 20110328224700000000190000 from 00000 i0523 170403773061 56587 slavecpp4077 received a new estimation of the oversubscribable resources i0523 170403773907 56587 slavecpp4077 received a new estimation of the oversubscribable resources i0523 170403774683 56587 slavecpp4077 received a new estimation of the oversubscribable resources i0523 170403776345 56587 slavecpp4077 received a new estimation of the oversubscribable resources  sigsegv 0x0 received by pid 56573 tid 0x7f100a190940 from pid 0 stack trace   0x7f100f181ca0 unknown  0x7f100fcbfd85 _znst17_function_handlerifvrksseznk7process6futurein5mesos8internal5slave15resourcemonitor5usageee8onfailedizns7_22resourcemonitorprocess5usageens5_11containerideeuls1_e_veerksa_ot_nsa_6prefereeuls1_e_e9_m_invokeerkst9_any_datas1_  0x7f100fb01506 processinternalrun  0x7f100fcc701b processfuturefail  0x7f100fccfbde processinternalthenf  0x7f100fd64bee _zn7process8internal3runist8functionifvrkns_6futurein5mesos18resourcestatisticseeeeejrs6_eeevrkst6vectorit_saisd_eedpot0_  0x7f100fd656dd processfuturefail  0x7f100fd6c332 processpromiseassociate  0x7f100fe2777e _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchin5mesos18resourcestatisticsens5_8internal5slave25mesoscontainerizerprocesserkns5_11containeridesa_eens0_6futureit_eerkns0_3pidit0_eemsh_fsf_t1_et2_euls2_e_e9_m_invokeerkst9_any_datas2_  0x7f101015561a processprocessmanagerresume  0x7f10101558dc processschedule  0x7f100f17983d start_thread  0x7f100e96bfcd clone usrlocalbinmesosslavesh line 102 56573 segmentation fault core dumped debug usrlocalsbinmesosslave mesos_flags slave exit status 139 noformat noformattitlegdb core dump thread 20 thread 0x7f100a190940 lwp 56574 0 _m_data __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482bitsbasic_stringh293 1 _m_rep __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482bitsbasic_stringh301 2 size __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482bitsbasic_stringh716 3 operator char stdchar_traitschar stdallocatorchar  __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482bitsbasic_stringh2758 4 operator __functorunhandled dwarf expression opcode 0xf3  at includemesostype_utilshpp267 5 operator __functorunhandled dwarf expression opcode 0xf3  at slavemonitorcpp129 6 operator __functorunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp220 7 std_function_handlervoidconst stdbasic_stringchar stdchar_traitschar stdallocatorchar  processfuturetonfailedf processfuturetprefer const with f  mesosinternalslaveresourcemonitorprocessusagemesoscontainerid__lambda180 templateparameter22  void t  mesosinternalslaveresourcemonitorusage__lambda2_m_invokeconst std_any_data  const stdbasic_stringchar stdchar_traitschar stdallocatorchar   __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482functional2071 8 0x00007f100fb01506 in processinternalrunstdfunctionvoidconst stdbasic_stringchar stdbasic_stringchar stdchar_traitschar stdallocatorchar const stdvectorstdfunctionvoidconst stdbasic_stringchar stdchar_traitschar stdallocatorchar  stdallocatorstdfunctionvoidconst stdbasic_stringchar stdchar_traitschar stdallocatorchar     callbacksstdvector of length 1 capacity 1   at 3rdpartylibprocessincludeprocessfuturehpp420 9 0x00007f100fcc701b in processfuturemesosinternalslaveresourcemonitorusagefail this0x7f0ffc185ca8 _messageunknown container c0ab6cd3fe4f49bd8dd632b388fcfab2 at 3rdpartylibprocessincludeprocessfuturehpp1406 10 0x00007f100fccfbde in fail funhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp649 11 processinternalthenfmesosresourcestatistics mesosinternalslaveresourcemonitorusageconst stdfunctionprocessfuturemesosinternalslaveresourcemonitorusageconst mesosresourcestatistics  const stdshared_ptrprocesspromisemesosinternalslaveresourcemonitorusage   const processfuturemesosresourcestatistics  funhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp1193 12 0x00007f100fd64bee in operator callbacksstdvector of length 1 capacity 1   at optrhdevtoolset2rootusrincludec482functional2464 13 processinternalrunstdfunctionvoidconst processfuturemesosresourcestatistics processfuturemesosresourcestatisticsconst stdvectorstdfunctionvoidconst processfuturemesosresourcestatistics stdallocatorstdfunctionvoidconst processfuturemesosresourcestatistics    callbacksstdvector of length 1 capacity 1   at 3rdpartylibprocessincludeprocessfuturehpp420 14 0x00007f100fd656dd in processfuturemesosresourcestatisticsfail this0x7f0ff8046230 _messageunknown container c0ab6cd3fe4f49bd8dd632b388fcfab2 at 3rdpartylibprocessincludeprocessfuturehpp1407 15 0x00007f100fd6c332 in onfailed thisunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp1121 16 onfailedstd_bindstd_mem_fnbool processfuturemesosresourcestatisticsconst stdbasic_stringcharprocessfuturemesosresourcestatistics std_placeholder1 bool thisunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp221 17 onfailedstd_bindstd_mem_fnbool processfuturemesosresourcestatisticsconst stdbasic_stringcharprocessfuturemesosresourcestatistics std_placeholder1  thisunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp270 18 processpromisemesosresourcestatisticsassociate thisunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessfuturehpp635 19 0x00007f100fe2777e in operator __functorunhandled dwarf expression opcode 0xf3  at 3rdpartylibprocessincludeprocessdispatchhpp239 20 std_function_handlervoidprocessprocessbase processdispatchconst processpidt processfuturet tp0 a0 with r  mesosresourcestatistics t  mesosinternalslavemesoscontainerizerprocess p0  const mesoscontainerid a0  mesoscontainerid__lambda21_m_invokeconst std_any_data  processprocessbase  __functorunhandled dwarf expression opcode 0xf3  at optrhdevtoolset2rootusrincludec482functional2071 21 0x00007f101015561a in processprocessmanagerresume this0xc24d20 process0x7f0ffc0169b0 at srcprocesscpp2172 22 0x00007f10101558dc in processschedule argunhandled dwarf expression opcode 0xf3  at srcprocesscpp602 23 0x00007f100f17983d in start_thread  from lib64libpthreadso0 24 0x00007f100e96bfcd in clone  from lib64libcso6 noformat,1
define protobuf for resourcemonitorusage we need to expose resourcemonitorusage so that module writers can access it we will define a protobuf message for that,1
slave should expose metrics about oversubscribed resources metricssnapshot should expose metrics on oversubscribed resources allocated and available,2
master should expose metrics about oversubscribed resources metricssnapshot should expose metrics on oversubscribed resources allocated and available,5
nonpod static variables used in fq_codel and ingress we declare const nonpod static variables for the following fq_codelhandle ingressroot ingresshandle we can eliminate the risk of indeterminate initialization by converting to c11 constexpr,1
getqdisc function in routingqueueinginternalcpp returns incorrect qdisc the getqdisc function ignores the passed link parameter and returns the first qdisc of the required type from any available interface,1
document the fetcher for framework developers specifically mesos provides a fetcher to move binaries this needs mvp documentation  what is it  how does it help  what protocols or schemas are supported  can it be extended this is important to get framework developers over the hump of learning to code against mesos and grow the ecosystem,5
added constexpr to c11 whitelist constexpr is currently used to eliminate initialization dependency issues for nonpod objects we should add it to the whitelist of acceptable c11 features in the style guide,1
create a fixedresourceestimator to return fixed amount of oversubscribable resources this will be useful for testing oversubscription in a real environment also it will be useful for people who has a prior knowledge about the amount of resources that can be safely oversubscribed on each slave,5
remove duplicate literals in ingress  fq_codel queueing disciplines fq_codel and ingress queueing disciplines include multiple uses of the string literals ingress and fq_codel any mismatch in these would cause runtime errors which can be prevented at compile time,1
add support for container rootfs to mesos isolators mesos containers can have a different rootfs to the host update isolator interface to pass rootfs during isolatorprepare update isolators where necessary,1
implement filesystem isolators move persistent volume support from mesos containerizer to separate filesystem isolators including support for container rootfs where possible use symlinks for posix systems without container rootfs use bind mounts for linux withwithout container rootfs,13
introduce filesystem provisioner abstraction optional filesystem provisioner component for the mesos containerizer that can provision percontainer filesystems this is different to a filesystem isolators because it just provisions a root filesystem for a container and doesnt actually do any isolation eg through a mount namespace  pivot or chroot,5
implement appc image provisioner implement a filesystem provisioner that can provision container images compliant with the application container image aci specificationhttpsgithubcomappcspec,5
export statistics on unevictable memory,1
rename optiontgetconst t _t to getorelse and refactor the original function as suggested if we want to change the name then we should refactor the original function as opposed to having 2 copies if we did have 2 versions of the same function would it make more sense to delegate one of them to the other as of today there is only one file need to be refactor 3rdpartylibprocess3rdpartystoutincludestoutososxhpp at line 151 161,3
remove dynamic allocation from futuret remove the dynamic allocation of t inside futuredata,3
log framework capabilities in the master now that capabilities has been added to frameworkinfo we should log these in the master when a framework reregisters ie which capabilities are enabled and disabled this would make debugging easier for framework developers ideally folding in the old checkpoint capability and logging that as well in the past the fact that checkpoint defaults to false has tripped up a lot of developers,1
make synchronized as primary form of synchronization reorganize synchronized to allow synchronizedm to work on 1 stdmutex 2 stdrecursive_mutex 3 stdatomic_flag move synchronizedhpp into stout so that developers dont think its part of the utility suite for actors in libprocess remove references to internalhpp and replace them with stdatomic_flag synchronization,8
jira workflow appears inconsistent see attached screenshot  the story is in the accepted state so it should now have a start progress button but it has a stop progress one instead also when in the in progress it has an accept button i think or something similar also other states appear inconsistent this story is about first looking at the workflow ensuring the stories and their statuses are consistent that button in the ui are consistently applied and then correct any issues that may have been identified the assumption here is that the workflow is noformat open  accepted  progress  reviewable  resolved  closed accept start ready resolve close noformat and at each stage it can be moved back by one unaccept stop progress unresolve and that at any stage it can be moved to closed for whatever reason,2
as a developer i need an easy way to convert masterinfo protobuf tofrom json as a preliminary to mesos2340 this requires the implementation of a simple deserialization mechanism to json fromto masterinfo protobuf,3
slave should call into resource estimator whenever it wants to forward oversubscribed resources currently the polling of resource estimator is decoupled from the loop in the slave that forwards oversubscribed resources now that the slave only sends updates when there is a change from the previous estimate it can just poll the resource estimator whenever it wants to send an estimate one advantage with this is that if the estimator is slow to respond the slave doesnt keep forwarding estimates with the stale oversubscribable value causing more revocable tasks to be unintentionally launched,3
osread should have one implementation in master there are currently three implementations of the function httpsgithubcomapachemesosblobmaster3rdpartylibprocess3rdpartystoutincludestoutosreadhppl42 httpsgithubcomapachemesosblobmaster3rdpartylibprocess3rdpartystoutincludestoutosreadhppl82 httpsgithubcomapachemesosblobmaster3rdpartylibprocess3rdpartystoutincludestoutosreadhppl42 all of them have fairly radically different implementations one uses c read one uses c ifstream one uses c fopen the read based one does an excess  unnecessary copy  buffer allocation it is going to read into one temporary buffer then copy into the result string would be more efficient to do a reserve on the result string and then fill the result buffer the ifstreamifstreambuf_iterator ignores that you can have an error partially through reading a file  doesnt find the error or propagate it up the fopen variant reads one newline separated line at a time this could produce interesting  unexpected reading in the context of a binary file it also causes glibc to insert null bytes at the end of the buffer it reads excess computation result isnt preallocated to be the right length meaning that most of the continually read lines will result in realloc and a lot of memory copies which will be inefficient on large files,3
flaky test fetchercachehttptesthttpcachedserialized fetchercachehttptesthttpcachedserialized has been observed to fail once so far but normally works fine here is the failure output  run  fetchercachehttptesthttpcachedserialized gmock warning uninteresting mock function call  returning directly function call resourceoffers0x3cca8e0 0x2b1053422b20  128byte object d0e1 5949 102b 0000 0000 0000 0000 0000 10a1 0068 102b 0000 a0de 0068 102b 0000 20df 0068 102b 0000 409c 0068 102b 0000 902d 0068 102b 0000 0400 0000 0400 0000 0400 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 0000 102b 0000 0000 0000 0f00 0000  stack trace f0604 130816377907 6813 fetcher_cache_testscpp354 check_readyoffers is pending failed to wait for resource offers  check failure stack trace   0x2b10488ff6c0 googlelogmessagefail  0x2b10488ff60c googlelogmessagesendtolog  0x2b10488ff00e googlelogmessageflush  0x2b1048901f22 googlelogmessagefatallogmessagefatal  0x9721e4 _checkfatal_checkfatal  0xb4da86 mesosinternaltestsfetchercachetestlaunchtask  0xb53f8d mesosinternaltestsfetchercachehttptest_httpcachedserialized_testtestbody  0x116ac21 testinginternalhandlesehexceptionsinmethodifsupported  0x1165e1e testinginternalhandleexceptionsinmethodifsupported  0x114e1df testingtestrun  0x114e902 testingtestinforun  0x114ee8a testingtestcaserun  0x1153b54 testinginternalunittestimplrunalltests  0x116ba93 testinginternalhandlesehexceptionsinmethodifsupported  0x1166b0f testinginternalhandleexceptionsinmethodifsupported  0x1152a60 testingunittestrun  0xcbc50f main  0x2b104af78ec5 unknown  0x867559 unknown make4  checklocal aborted make4 leaving directory homejenkinsjenkinsslaveworkspacemesosreviewbotmesos0230_buildsrc make3  checkam error 2 make3 leaving directory homejenkinsjenkinsslaveworkspacemesosreviewbotmesos0230_buildsrc make2  check error 2 make2 leaving directory homejenkinsjenkinsslaveworkspacemesosreviewbotmesos0230_buildsrc make1  checkrecursive error 1 make1 leaving directory homejenkinsjenkinsslaveworkspacemesosreviewbotmesos0230_build make  distcheck error 1,2
support revocablenonrevocable cpu updates in mesos containerizer mesos2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container improve this to support updates tofrom revocable cpu note any revocable cpu will result in the entire containers cpu being treated as revocable at the cpu isolator level higher level logic is responsible for addingremoving based on some policy,3
pass allocated resources for each executor to the resource estimator resource estimator obviously need this information to calculate say the usage slack now the question is how there are two approaches 1 pass in the allocated resources for each executor through the oversubscribable interface 2 let containerizer return total resources allocated for each container when usages are invoked i would suggest to take route 1 for several reasons 1 eventually well need to pass in slaves total resources to the resource estimator so that re can calculate allocation slack there is no way that we can get that from containerizer the slaves total resources keep changing due to dynamic reservation so we cannot pass in the slave total resources during initialization 2 the current implementation of usages might skip some containers if it fails to get statistics for that container not an error this will cause incomplete information to the re 3 we may want to calculate unallocated  total  allocated so that we can send allocation slack as well getting total and allocated from two different components might result in inconsistent value remember that total keeps changing due to dynamic reservation,3
document and consolidate qdisc handles the structure of traffic control qdiscs and filters in nontrivial with the knowledge of which handles are the parents of which filters or qdiscs are in the create and recovery functions and will be needed to collect statistics on the links lets pull out the constants and document them,1
add expect_no_future_dispatches macro for tests we already have expect_no_future_messages expect_no_future_dispatches should be done the same way we already have a use case for it httpsgithubcomapachemesosblobmastersrctestsmaster_contender_detector_testscppl251,1
pass callback to the qos controller to retrieve resourceusage from resource monitor on demand we need to allow qos controller to call resourcemonitorusages we will pass it in a lambda,2
support prefetching images default container images can be specified with the default_container_info flag to the slave this may be a large image that will take a long time to initially fetchhashextract when the first container is provisioned add optional support to start fetching the image when the slave starts and consider not registering until the fetch is complete to extend that we should support an operator endpoint so that operators can specify images to prefetch,5
add an endpoint to slaves to allow launching system administration tasks as a system administrator often times i need to run a organizationmandated task on every machine in the cluster ideally i could do this within the framework of mesos resources if it is a cleanup or auditing task but sometimes i just have to run something and run it now regardless if a machine has unaccounted resources ex addingremoving a user currently to do this i have to completely bypass mesos and ssh to the box ideally i could tell a mesos slave with proper authentication to run a container with the limited special permissions needed to get the task done,8
enable configuring mesos with environment variables without having them leak to tasks launched currently if mesos is configured with environment variables mesos_modules those show up in every task which is launched unless the executor explicitly cleans them up if the task being launched happens to be something libprocess  mesos based this can often prevent the task from starting up a scheduler has issues loading a module intended for the slave there are also cases where it would be nice to be able to change what the path is that tasks launch with the host may have more in the path than tasks are supposed to  allowed to depend upon,8
support different perf output formats the output format of perf changes in 314 inserting an additional field and in again in 41 appending additional fields see kernel commits 410136f5dd96b6013fe6d1011b523b1c247e1ccb d73515c03c6a2706e088094ff6095a3abefd398b update the perfparse function to understand all these formats,3
report percontainer metrics for network bandwidth throttling to the slave report percontainer metrics for network bandwidth throttling to the slave in the output of mesosnetworkhelper,1
decode network statistics from mesosnetworkhelper decode network statistics from mesosnetworkhelper and output to slave statisticsjson,1
in resources json model resources of the same name overwrite each other as shown here httpsgithubcomapachemesosblob8559d7b7356ec91795e564767588c6f4519653a5srccommonhttpcppl50 so if there are two cpus of different roles whichever comes later will overwrite the previous we should instead aggregate different resources of the same name however in the presence of revocable resources in order to maintain backwards compatibility we should exclude revocable resources,2
frameworkinfo should include a labels field to support arbitrary lightweight metadata a framework instance may offer specific capabilities to the cluster storage smartlybalanced request handling across deployed tasks access to 3rd party services outside of the cluster etc these capabilities may or may not be utilized by all or even most mesos clusters however it should be possible for processes running in the cluster to discover capabilities or features of frameworks in order to achieve a higher level of functionality and a more seamless integration experience across the cluster a rich discovery api attached to the frameworkinfo could result in some form of early lockin there are probably many ways to realize crossframework integration and external services integration that we havent considered yet rather than overspecify a discovery info message type at the framework level i think frameworkinfo should expose a very generic way to supply metadata for interested consumers other processes tasks etc adding a labels field to frameworkinfo reuses an existing message type and seems to fit well with the overall intent attaching generic metadata to a framework instance these labels should be visible when querying a mesos masters statejson endpoint,8
add and document new labels field to framework info add and document new labels field to framework info code message frameworkinfo   used to determine the unix user that an executor or task should  be launched as if the user field is set to an empty string mesos  will automagically set it to the current user required string user  1  name of the framework that shows up in the mesos web ui required string name  2  note that id is only available after a framework has  registered however it is included here in order to facilitate  scheduler failover ie if it is set then the  mesosschedulerdriver expects the scheduler is performing  failover optional frameworkid id  3   this field allows a framework to advertise its set of  capabilities eg ability to receive offers for revocable  resources repeated capability capabilities  10 optional labels labels  11  code,1
local filesystem docker image discovery given a docker image name and the local directory where images can be found creates a uri with a path to the corresponding image done when system successfully checks for the image untars the image if necessary and returns the proper uri to the image,2
implement docker local image store given a local docker image name and path to the image or image tarball fetches the images dependent layers untarring if necessary it will also parse the image layers configuration json and place the layers and image into persistent store done when a docker image can be successfully stored and retrieved using put and get methods,5
implement docker image provisioner provisions a docker image provisions all its dependent layers fetch an image from persistent store and also destroy an image done when tested for local discovery and copy backend,3
add docker image type to protobuf api,1
report percontainer metrics from host egress filter export in statisticsjson the fq_codel flow statistics for each container,1
resourcesparse allows different resources of the same name to have different types so code like this doesnt raise error code resourcesparsefoorole11foorole201 code doesnt look like allowing this adds value and this complicates resource mathsvalidationreporting we should disallow this,2
fetchercachetestlocalcachedextract is flaky from jenkins noformat  run  fetchercachetestlocalcachedextract using temporary directory tmpfetchercachetest_localcachedextract_cwdcdj i0610 200448591573 24561 leveldbcpp176 opened db in 3512525ms i0610 200448592456 24561 leveldbcpp183 compacted db in 828630ns i0610 200448592512 24561 leveldbcpp198 created db iterator in 32992ns i0610 200448592531 24561 leveldbcpp204 seeked to beginning of db in 8967ns i0610 200448592545 24561 leveldbcpp273 iterated through 0 keys in the db in 7762ns i0610 200448592604 24561 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0610 200448593438 24587 recovercpp449 starting replica recovery i0610 200448593698 24587 recovercpp475 replica is in empty status i0610 200448595641 24580 replicacpp641 replica in empty status received a broadcasted recover request i0610 200448596086 24590 recovercpp195 received a recover response from a replica in empty status i0610 200448596607 24590 recovercpp566 updating replica status to starting i0610 200448597507 24590 leveldbcpp306 persisting metadata 8 bytes to leveldb took 717888ns i0610 200448597535 24590 replicacpp323 persisted replica status to starting i0610 200448597697 24590 recovercpp475 replica is in starting status i0610 200448599165 24584 replicacpp641 replica in starting status received a broadcasted recover request i0610 200448599434 24584 recovercpp195 received a recover response from a replica in starting status i0610 200448599915 24590 recovercpp566 updating replica status to voting i0610 200448600545 24590 leveldbcpp306 persisting metadata 8 bytes to leveldb took 432335ns i0610 200448600574 24590 replicacpp323 persisted replica status to voting i0610 200448600659 24590 recovercpp580 successfully joined the paxos group i0610 200448600797 24590 recovercpp464 recover process terminated i0610 200448602905 24594 mastercpp363 master 2015061020044838755414203290724561 dbade881e927 started on 17217023132907 i0610 200448602957 24594 mastercpp365 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 credentialstmpfetchercachetest_localcachedextract_cwdcdjcredentials framework_sorterdrf helpfalse initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0230_instsharemesoswebui work_dirtmpfetchercachetest_localcachedextract_cwdcdjmaster zk_session_timeout10secs i0610 200448603374 24594 mastercpp410 master only allowing authenticated frameworks to register i0610 200448603392 24594 mastercpp415 master only allowing authenticated slaves to register i0610 200448603404 24594 credentialshpp37 loading credentials for authentication from tmpfetchercachetest_localcachedextract_cwdcdjcredentials i0610 200448603751 24594 mastercpp454 using default crammd5 authenticator i0610 200448604928 24594 mastercpp491 authorization enabled i0610 200448606034 24593 hierarchicalhpp309 initialized hierarchical allocator process i0610 200448606106 24593 whitelist_watchercpp79 no whitelist given i0610 200448607430 24594 mastercpp1476 the newly elected leader is master17217023132907 with id 2015061020044838755414203290724561 i0610 200448607466 24594 mastercpp1489 elected as the leading master i0610 200448607481 24594 mastercpp1259 recovering from registrar i0610 200448607712 24594 registrarcpp313 recovering registrar i0610 200448608543 24588 logcpp661 attempting to start the writer i0610 200448610231 24588 replicacpp477 replica received implicit promise request with proposal 1 i0610 200448611335 24588 leveldbcpp306 persisting metadata 8 bytes to leveldb took 1086439ms i0610 200448611382 24588 replicacpp345 persisted promised to 1 i0610 200448612303 24588 coordinatorcpp230 coordinator attemping to fill missing position i0610 200448613883 24593 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0610 200448619205 24593 leveldbcpp343 persisting action 8 bytes to leveldb took 5228235ms i0610 200448619257 24593 replicacpp679 persisted action at 0 i0610 200448621919 24593 replicacpp511 replica received write request for position 0 i0610 200448621987 24593 leveldbcpp438 reading position from leveldb took 49394ns i0610 200448622689 24593 leveldbcpp343 persisting action 14 bytes to leveldb took 668412ns i0610 200448622716 24593 replicacpp679 persisted action at 0 i0610 200448623507 24584 replicacpp658 replica received learned notice for position 0 i0610 200448624155 24584 leveldbcpp343 persisting action 16 bytes to leveldb took 612283ns i0610 200448624186 24584 replicacpp679 persisted action at 0 i0610 200448624215 24584 replicacpp664 replica learned nop action at position 0 i0610 200448625144 24593 logcpp677 writer started with ending position 0 i0610 200448626724 24589 leveldbcpp438 reading position from leveldb took 72013ns i0610 200448629276 24591 registrarcpp346 successfully fetched the registry 0b in 21520128ms i0610 200448629663 24591 registrarcpp445 applied 1 operations in 129587ns attempting to update the registry i0610 200448632237 24579 logcpp685 attempting to append 131 bytes to the log i0610 200448632624 24579 coordinatorcpp340 coordinator attempting to write append action at position 1 i0610 200448633739 24579 replicacpp511 replica received write request for position 1 i0610 200448634351 24579 leveldbcpp343 persisting action 150 bytes to leveldb took 583937ns i0610 200448634382 24579 replicacpp679 persisted action at 1 i0610 200448635073 24583 replicacpp658 replica received learned notice for position 1 i0610 200448635442 24583 leveldbcpp343 persisting action 152 bytes to leveldb took 357122ns i0610 200448635469 24583 replicacpp679 persisted action at 1 i0610 200448635494 24583 replicacpp664 replica learned append action at position 1 i0610 200448636337 24583 registrarcpp490 successfully updated the registry in 6534144ms i0610 200448636725 24594 logcpp704 attempting to truncate the log to 1 i0610 200448636858 24583 registrarcpp376 successfully recovered registrar i0610 200448637073 24594 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0610 200448637789 24594 mastercpp1286 recovered 0 slaves from the registry 95b  allowing 10mins for slaves to reregister i0610 200448638630 24583 replicacpp511 replica received write request for position 2 i0610 200448639127 24583 leveldbcpp343 persisting action 16 bytes to leveldb took 396272ns i0610 200448639153 24583 replicacpp679 persisted action at 2 i0610 200448639804 24583 replicacpp658 replica received learned notice for position 2 i0610 200448640965 24583 leveldbcpp343 persisting action 18 bytes to leveldb took 1147322ms i0610 200448641054 24583 leveldbcpp401 deleting 1 keys from leveldb took 72395ns i0610 200448641197 24583 replicacpp679 persisted action at 2 i0610 200448641345 24583 replicacpp664 replica learned truncate action at position 2 i0610 200448652274 24561 containerizercpp111 using isolation posixcpuposixmem i0610 200448658994 24590 slavecpp188 slave started on 4217217023132907 i0610 200448659049 24590 slavecpp189 flags at startup authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpfetchercachetest_localcachedextract_lchuumcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_remove_delay6hrs docker_sandbox_directorymntmesossandbox docker_socketvarrundockersock docker_stop_timeout0ns enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpfetchercachetest_localcachedextract_lchuumfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0230_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resource_monitoring_interval1secs resourcescpus1000 mem1000 revocable_cpu_low_prioritytrue stricttrue switch_usertrue versionfalse work_dirtmpfetchercachetest_localcachedextract_lchuum i0610 200448659570 24590 credentialshpp85 loading credential for authentication from tmpfetchercachetest_localcachedextract_lchuumcredential i0610 200448659803 24590 slavecpp319 slave using credential for testprincipal i0610 200448660441 24590 slavecpp352 slave resources cpus1000 mem1000 disk370122e06 ports3100032000 i0610 200448660555 24590 slavecpp382 slave hostname dbade881e927 i0610 200448660578 24590 slavecpp387 slave checkpoint true i0610 200448661550 24588 statecpp35 recovering state from tmpfetchercachetest_localcachedextract_lchuummeta i0610 200448661913 24590 status_update_managercpp201 recovering status update manager i0610 200448662253 24590 containerizercpp312 recovering containerizer i0610 200448663207 24581 slavecpp3950 finished recovery i0610 200448663761 24581 slavecpp4104 querying resource estimator for oversubscribable resources i0610 200448664077 24581 slavecpp678 new master detected at master17217023132907 i0610 200448664088 24586 status_update_managercpp175 pausing sending status updates i0610 200448664245 24581 slavecpp741 authenticating with master master17217023132907 i0610 200448664388 24581 slavecpp746 using default crammd5 authenticatee i0610 200448664611 24581 slavecpp714 detecting new master i0610 200448664647 24594 authenticateehpp139 creating new client sasl connection i0610 200448664813 24581 slavecpp4125 received oversubscribable resources from the resource estimator i0610 200448665060 24581 slavecpp4129 no master detected requerying resource estimator after 15secs i0610 200448665096 24594 mastercpp4181 authenticating slave4217217023132907 i0610 200448665247 24581 authenticatorcpp406 starting authentication session for crammd5_authenticatee13017217023132907 i0610 200448665657 24581 authenticatorcpp92 creating new server sasl connection i0610 200448666013 24581 authenticateehpp230 received sasl authentication mechanisms crammd5 i0610 200448666159 24581 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0610 200448666443 24592 authenticatorcpp197 received sasl authentication start i0610 200448666591 24592 authenticatorcpp319 authentication requires more steps i0610 200448666779 24592 authenticateehpp276 received sasl authentication step i0610 200448667007 24585 authenticatorcpp225 received sasl authentication step i0610 200448667043 24585 auxpropcpp101 request to lookup properties for user testprincipal realm dbade881e927 server fqdn dbade881e927 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0610 200448667058 24585 auxpropcpp173 looking up auxiliary property userpassword i0610 200448667110 24585 auxpropcpp173 looking up auxiliary property cmusaslsecretcrammd5 i0610 200448667142 24585 auxpropcpp101 request to lookup properties for user testprincipal realm dbade881e927 server fqdn dbade881e927 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0610 200448667155 24585 auxpropcpp123 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0610 200448667163 24585 auxpropcpp123 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0610 200448667181 24585 authenticatorcpp311 authentication success i0610 200448667331 24585 authenticateehpp316 authentication success i0610 200448667414 24585 mastercpp4211 successfully authenticated principal testprincipal at slave4217217023132907 i0610 200448667505 24585 authenticatorcpp424 authentication session cleanup for crammd5_authenticatee13017217023132907 i0610 200448667809 24585 slavecpp812 successfully authenticated with master master17217023132907 i0610 200448667982 24585 slavecpp1146 will retry registration in 7257154ms if necessary i0610 200448668226 24585 mastercpp3157 registering slave at slave4217217023132907 dbade881e927 with id 2015061020044838755414203290724561s0 i0610 200448668737 24585 registrarcpp445 applied 1 operations in 90255ns attempting to update the registry i0610 200448672297 24585 logcpp685 attempting to append 305 bytes to the log i0610 200448672541 24585 coordinatorcpp340 coordinator attempting to write append action at position 3 i0610 200448673528 24593 replicacpp511 replica received write request for position 3 i0610 200448674321 24593 leveldbcpp343 persisting action 324 bytes to leveldb took 766804ns i0610 200448674355 24593 replicacpp679 persisted action at 3 i0610 200448675138 24587 replicacpp658 replica received learned notice for position 3 i0610 200448675866 24587 leveldbcpp343 persisting action 326 bytes to leveldb took 714643ns i0610 200448675897 24587 replicacpp679 persisted action at 3 i0610 200448675922 24587 replicacpp664 replica learned append action at position 3 i0610 200448677471 24587 registrarcpp490 successfully updated the registry in 8656128ms i0610 200448677759 24587 logcpp704 attempting to truncate the log to 3 i0610 200448678423 24593 coordinatorcpp340 coordinator attempting to write truncate action at position 4 i0610 200448678621 24587 mastercpp3214 registered slave 2015061020044838755414203290724561s0 at slave4217217023132907 dbade881e927 with cpus1000 mem1000 disk370122e06 ports3100032000 i0610 200448678959 24593 hierarchicalhpp496 added slave 2015061020044838755414203290724561s0 dbade881e927 with cpus1000 mem1000 disk370122e06 ports3100032000 and cpus1000 mem1000 disk370122e06 ports3100032000 available i0610 200448679157 24593 hierarchicalhpp933 no resources available to allocate i0610 200448679183 24593 hierarchicalhpp852 performed allocation for slave 2015061020044838755414203290724561s0 in 175519ns i0610 200448679805 24593 replicacpp511 replica received write request for position 4 i0610 200448684160 24587 slavecpp846 registered with master master17217023132907 given slave id 2015061020044838755414203290724561s0 i0610 200448684229 24587 fetchercpp77 clearing fetcher cache i0610 200448684666 24587 slavecpp869 checkpointing slaveinfo to tmpfetchercachetest_localcachedextract_lchuummetaslaves2015061020044838755414203290724561s0slaveinfo i0610 200448687366 24587 slavecpp2895 received ping from slaveobserver4217217023132907 i0610 200448687453 24584 status_update_managercpp182 resuming sending status updates i0610 200448690901 24593 leveldbcpp343 persisting action 16 bytes to leveldb took 3385583ms i0610 200448690975 24593 replicacpp679 persisted action at 4 i0610 200448692137 24593 replicacpp658 replica received learned notice for position 4 i0610 200448692603 24593 leveldbcpp343 persisting action 18 bytes to leveldb took 449838ns i0610 200448692674 24593 leveldbcpp401 deleting 2 keys from leveldb took 52471ns i0610 200448692699 24593 replicacpp679 persisted action at 4 i0610 200448692726 24593 replicacpp664 replica learned truncate action at position 4 i0610 200448693544 24561 schedcpp157 version 0230 i0610 200448695550 24590 schedcpp254 new master detected at master17217023132907 i0610 200448697090 24590 schedcpp310 authenticating with master master17217023132907 i0610 200448697136 24590 schedcpp317 using default crammd5 authenticatee i0610 200448697511 24586 authenticateehpp139 creating new client sasl connection i0610 200448697937 24586 mastercpp4181 authenticating scheduler51f5c1b5bb504118bde84dcdfd69205d17217023132907 i0610 200448698185 24584 authenticatorcpp406 starting authentication session for crammd5_authenticatee13117217023132907 i0610 200448698575 24584 authenticatorcpp92 creating new server sasl connection i0610 200448698807 24584 authenticateehpp230 received sasl authentication mechanisms crammd5 i0610 200448699898 24584 authenticateehpp256 attempting to authenticate with mechanism crammd5 i0610 200448700040 24584 authenticatorcpp197 received sasl authentication start i0610 200448700119 24584 authenticatorcpp319 authentication requires more steps i0610 200448700193 24584 authenticateehpp276 received sasl authentication step i0610 200448700287 24584 authenticatorcpp225 received sasl authentication step i0610 200448700320 24584 auxpropcpp101 request to lookup properties for user testprincipal realm dbade881e927 server fqdn dbade881e927 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0610 200448700333 24584 auxpropcpp173 looking up auxiliary property userpassword i0610 200448700392 24584 auxpropcpp173 looking up auxiliary property cmusaslsecretcrammd5 i0610 200448700425 24584 auxpropcpp101 request to lookup properties for user testprincipal realm dbade881e927 server fqdn dbade881e927 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0610 200448700439 24584 auxpropcpp123 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0610 200448700448 24584 auxpropcpp123 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0610 200448700467 24584 authenticatorcpp311 authentication success i0610 200448700640 24584 authenticateehpp316 authentication success i0610 200448700742 24584 authenticatorcpp424 authentication session cleanup for crammd5_authenticatee13117217023132907 i0610 200448701282 24590 schedcpp398 successfully authenticated with master master17217023132907 i0610 200448701315 24590 schedcpp521 sending registration request to master17217023132907 i0610 200448701386 24590 schedcpp554 will retry registration in 1128089605secs if necessary,1
create the basic infrastructure to handle scheduler endpoint this is the first basic step in ensuring the basic call functionality processing a noformat post call noformat and returning  202 if all goes well  401 if not authorized and  403 if the request is malformed well get more sophisticated as the work progressed eg supporting 415 if the contenttype is not of the right kind,3
mesosfetcher wont fetch uris which begin with a   discovered while running mesos with marathon on top if i launch a marathon task with a uri which is  httpapacheosuoslorgmesos0221mesos0221targz mesos will log to stderr code i0611 223922815636 35673 loggingcpp177 logging to stderr i0611 223925643889 35673 fetchercpp214 fetching uri  httpapacheosuoslorgmesos0221mesos0221targz i0611 223925648111 35673 fetchercpp94 hadoop client not available skipping fetch with hadoop client failed to fetch httpapacheosuoslorgmesos0221mesos0221targz failed to synchronize with slave its probably exited code it would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed,2
slave should send oversubscribed resource information after master failover after a master failover if the total amount of oversubscribed resources does not change then the slave will not send the updateslave message to the new master the slave needs to send the information to the new master regardless of this,3
oversubscriptiontestfixedresourceestimator is flaky came up in httpsreviewsapacheorgr35395 code  run  oversubscriptiontestfixedresourceestimator i0613 134102604904 19367 execcpp132 version 0230 i0613 134102610995 19398 execcpp206 executor registered on slave 2015061313410231426977954829513678s0 registered executor on pomonaapacheorg starting task 7d78a3ef2de946c9811cb2c0e2d50578 forked command at 19410 sh c sleep 1000 srctestsoversubscription_testscpp579 failure mock function called more times than expected  returning directly function call statusupdate0x7ffffbc0c4e0 0x2ade2bffa910 96byte object 503e d722 de2a 0000 0000 0000 0000 0000 d0c4 0048 de2a 0000 5071 ac01 0000 0000 0100 0000 0200 0000 5071 ac01 0000 0000 b066 0048 de2a 0000 0000 0000 0000 0000 0000 0000 002a 0000 e717 a8bb 0c5f d541 1031 0148 de2a 0000 0000 0000 4b03 0000 expected to be called once actual called twice  oversaturated and active  failed  oversubscriptiontestfixedresourceestimator 714 ms code,1
style hook prevents valid markdown files from getting committed according to the original markdown specificationhttpdaringfireballnetprojectsmarkdownsyntaxp and to the most recent standarizationhttpspeccommonmarkorg020hardlinebreaks effort two spaces at the end of a line create a hard line break it breaks the line without starting a new paragraph similar to the html code br however theres a hook in mesos which prevent files with trailing whitespace to be committed,1
convert portmappingstatistics to use automatic json encodingdecoding simplify portmappingstatistics by using jsonprotocol and protobufparse to convert resourcestatistics tofrom line format this change will simplify the implementation of mesos2332,2
random recursive_mutex errors in when running make check while running make check on os x from time to time recursive_mutex errors appear after running all the test successfully just one of the experience messages actually stops make check reporting an error the following error messages have been experienced code libcabidylib libcabidylib libcabidylib libcabidylib libcabidylib libcabidylib terminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argument  aborted at 1434553937 unix time try date d 1434553937 if you are using gnu date  code code libcabidylib terminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argument  aborted at 1434557001 unix time try date d 1434557001 if you are using gnu date  libcabidylib pc  0x7fff93855286 __pthread_kill libcabidylib  sigabrt 0x7fff93855286 received by pid 88060 tid 0x10fc40000 stack trace   0x7fff8e1d6f1a _sigtramp libcabidylib  0x10fc3f1a8 unknown libcabidylib  0x7fff979deb53 abort libcabidylib libcabidylib libcabidylib terminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentterminating with uncaught exception of type std__1system_error recursive_mutex lock failed invalid argumentmaking check in include code code assertion failed e  0 function recursive_mutex file sourcecachelibcxxlibcxx120srcmutexcpp line 82  aborted at 1434555685 unix time try date d 1434555685 if you are using gnu date  pc  0x7fff93855286 __pthread_kill  sigabrt 0x7fff93855286 received by pid 60235 tid 0x7fff7ebdc300 stack trace   0x7fff8e1d6f1a _sigtramp  0x10b512350 googlechecknotnull  0x7fff979deb53 abort  0x7fff979a6c39 __assert_rtn  0x7fff9bffdcc9 std__1recursive_mutexrecursive_mutex  0x10b881928 processprocessmanagerprocessmanager  0x10b874445 processprocessmanagerprocessmanager  0x10b874418 processfinalize  0x10b2f7aec main  0x7fff98edc5c9 start make5  checklocal abort trap 6 make4  checkam error 2 make3  checkrecursive error 1 make2  checkrecursive error 1 make1  check error 2 make  checkrecursive error 1 code,1
do not call hook manager if no hooks installed hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator environment decorator etc often the call into such a decorator hooks results in a new copy of labels environment etc being returned to the call site this is an unnecessary overhead if there are no hooks installed the proper way would be to call decorators via the hook manager only if there are some hooks installed this would prevent unnecessary copying overhead if no hooks are available,2
allow isolators to specify required namespaces currently the linuxlauncher looks into slaveflags to compute the namespaces that should be enabled when launching the executor this means that a custom isolator module doesnt have any way to specify dependency on a set of namespaces the proposed solution is to extend the isolator interface to also export the namespaces dependency this way the mesoscontainerizer can directly query all loaded isolators inbuilt and custom modules to compute the set of namespaces required by the executor this set of namespaces is then passed on to the linuxlauncher,5
capture some testing patterns we use in a doc in mesos tests we use some tricks and patterns to express certain expectations these are not always obvious and not documented the intent of the ticket is to kickstart the document with the description of those tricks for posterity,1
add ssl socket tests commit beac384c77d4a9c235a813e9286716f4509bdd55 author joris van remoortere jorisvanremoorteregmailcom date fri jun 26 183012 2015 0700 add ssl tests review httpsreviewsapacheorgr35889,5
add ssl switch to python configuration the python egg requires explicit dependencies for ssl add these to the python configuration if ssl is enabled,3
sandbox url doesnt work in webui when using ssl the links to the sandbox in the web ui dont work when ssl is enabled this can happen if the certificate for the master and the slave do not match this is a consequence of the redirection that happens when serving files the resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves,3
performance regression in hierarchical allocator for large clusters the 0230 allocator cannot keep up with the volume of slaves after the following slave was reregistered it took the allocator a long time to work through the backlog of slaves to add noformattitle45 minute delay i0618 185540738399 10172 mastercpp3419 reregistered slave 20150422211121214834689050503253s4695 i0618 194014960636 10164 hierarchicalhpp496 added slave 20150422211121214834689050503253s4695 noformat empirically addslavehttpsgithubcomapachemesosblobdda49e688c7ece603ac7a04a977fc7085c713dd1srcmasterallocatormesoshierarchicalhppl462 and updateslavehttpsgithubcomapachemesosblobdda49e688c7ece603ac7a04a977fc7085c713dd1srcmasterallocatormesoshierarchicalhppl533 have become expensive some timings from a production cluster reveal that the allocator spending in the low tens of milliseconds for each call to addslave and updateslave when there are tens of thousands of slaves this amounts to the large delay seen above we also saw a slow steady increase in memory consumption hinting further at a queue backup in the allocator a synthetic benchmark like we did for the registrar would be prudent here along with visibility into the allocators queue size,3
add benchmark for hierarchical allocator in light of the performance regression in mesos2891 wed like to have a synthetic benchmark of the allocator code in order to analyze and direct improvements,3
add queue size metrics for the allocator in light of the performance regression in mesos2891 wed like to have visibility into the queue size of the allocator this will enable alerting on performance problems we currently have no metrics in the allocator i will also look into mesos1286 now that we have gcc 48 current queue size gauges require a trip through the process queue,1
write tests for new json zookeeper functionality follow up from mesos2340 need to ensure this does not break the zookeeper discovery functionality,2
enable mesos to use arbitrary script  module to figure out ip hostname currently mesos tries to guess the ip hostname by doing a reverse dns lookup this doesnt work on a lot of clouds as we want things like public ips which arent the default dns there arent fqdn names azure or the correct way to figure it out is to call some cloudspecific endpoint if mesos  libprocess could load a mesosmodule or run a script which is provided percloud we can figure out perfectly the ip  hostname for the given environment it also means we can ship one identical set of files to all hosts in a given provider which doesnt happen to have the dns scheme  hostnames that libprocessmesos expects currently we have to generate hostspecific config files which mesos uses to guess the hostspecific files break  fall apart if machines change ip  hostname without being reinstalled,5
network isolator should not fail when target state already exists network isolator has multiple instances of the following pattern noformat trybool something  create if somethingiserror  metricssomething_errors return failurefailed to create something   else if icmpvethtoeth0get  metricsadding_veth_icmp_filters_already_exist return failuresomething already exists  noformat these failures have occurred in operation due to the failure to recover or delete an orphan causing the slave to remain on line but unable to create new resources we should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested,3
add slave metric to count container launch failures we have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state for example unexpected network configuration adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state,1
slave  synchronous validation for calls call endpoint on the slave will return a 202 accepted code but has to do some basic validations before in case of invalidation it will return a badrequest back to the client  we need to create the required infrastructure to validate the request and then process it similar to srcmastervalidationcpp in the namespace scheduler ie check if the protobuf is properly initialized has the required attributes set pertaining to the call message etc,3
agent  create basic functionality to handle call endpoint this is the first basic step in ensuring the basic call functionality  set up the route on the agent for apiv1executor endpoint  the endpoint should perform basic headerprotobuf validation and return 501 notimplemented for now  introduce initial tests in executor_api_testscpp that just verify the status code,5
add version field to registerframeworkmessage and reregisterframeworkmessage in the same way we added version field to registerslavemessage and reregisterslavemessage we should do it framework reregistration messages this would help master determine which version of scheduler driver it is talking to we want this so that master can start sending event messages to the scheduler driver and scheduler library in the long term master will send a streaming response to the libraries but in the meantime we can test the event protobufs by sending event messages,3
add an event message handler to scheduler driver adding this handler lets master send event messages to the driver see mesos2909 for additional context,8
add an event message handler to scheduler library adding this handler lets master send event messages to the library see mesos2909 for additional context this ticket only tracks the installation of the handler and maybe handling of a single event for testing additional events handling will be captured in a different tickets,3
provide a python library for master detection when schedulers start interacting with mesos master via http endpoints they need a way to detect masters mesos should provide a master detection python library to make this easy for frameworks,5
scheduler driver should send call messages to the master to vet the new call protobufs it is prudent to have the scheduler driver schedcpp send call messages to the master similar to what we are doing with the scheduler library,8
port mapping isolator should cleanup unknown orphan containers after all known orphan containers are recovered during recovery otherwise the icmparp filter on host eth0 might be removed as a result of _cleanup if infos is empty causing subsequent _cleanup to fail on both knownunknown orphan containers noformat i0612 174651518501 16308 containerizercpp314 recovering containerizer i0612 174651520612 16308 port_mappingcpp1567 discovered network namespace handle symlink ddcb8397355244f9bc99b5b69aa72944  31607 i0612 174651521183 16308 port_mappingcpp1567 discovered network namespace handle symlink d8c48a4afdfb47ddb8d807188c21600d  41020 i0612 174651521883 16308 port_mappingcpp1567 discovered network namespace handle symlink 8953fc7f9fca4931b0cb2f4959ddee74  3302 i0612 174651522542 16308 port_mappingcpp1567 discovered network namespace handle symlink 50f9986febbc440d86a79fa1a7c55a75  19805 i0612 174651523643 16308 port_mappingcpp2597 removing ip packet filters with ports 3379234815 for container with pid 52304 i0612 174651525063 16308 port_mappingcpp2616 freed ephemeral ports 3379234816 for container with pid 52304 i0612 174651547696 16308 port_mappingcpp2762 successfully performed cleanup for pid 52304 i0612 174651550027 16308 port_mappingcpp1698 network isolator recovery complete i0612 174651550946 16329 containerizercpp449 removing orphan container 111ea69c61844da1a0e9c34e8c6deb30 i0612 174651552686 16329 containerizercpp449 removing orphan container ddcb8397355244f9bc99b5b69aa72944 i0612 174651552734 16309 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos111ea69c61844da1a0e9c34e8c6deb30 i0612 174651554932 16329 containerizercpp449 removing orphan container 8953fc7f9fca4931b0cb2f4959ddee74 i0612 174651555032 16309 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesosddcb8397355244f9bc99b5b69aa72944 i0612 174651555629 16308 cgroupscpp1420 successfully froze cgroup sysfscgroupfreezermesos111ea69c61844da1a0e9c34e8c6deb30 after 1730304ms i0612 174651557507 16329 containerizercpp449 removing orphan container 50f9986febbc440d86a79fa1a7c55a75 i0612 174651557611 16309 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174651557896 16313 cgroupscpp1420 successfully froze cgroup sysfscgroupfreezermesosddcb8397355244f9bc99b5b69aa72944 after 1685248ms i0612 174651559412 16310 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos111ea69c61844da1a0e9c34e8c6deb30 i0612 174651561564 16329 containerizercpp449 removing orphan container d8c48a4afdfb47ddb8d807188c21600d i0612 174651562489 16315 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos50f9986febbc440d86a79fa1a7c55a75 i0612 174651562988 16313 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesosddcb8397355244f9bc99b5b69aa72944 i0612 174651563303 16310 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos111ea69c61844da1a0e9c34e8c6deb30 after 2076928ms i0612 174651566052 16308 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesosd8c48a4afdfb47ddb8d807188c21600d i0612 174651566102 16313 slavecpp3911 finished recovery w0612 174651566432 16323 diskcpp299 ignoring cleanup for unknown container 111ea69c61844da1a0e9c34e8c6deb30 i0612 174651566651 16317 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesosddcb8397355244f9bc99b5b69aa72944 after 212096ms i0612 174651566987 16313 slavecpp3944 garbage collecting old slave 201503192131332080910346505057551s3314 i0612 174651567777 16318 cgroupscpp1420 successfully froze cgroup sysfscgroupfreezermesosd8c48a4afdfb47ddb8d807188c21600d after 1323008ms w0612 174651568042 16323 port_mappingcpp2544 ignoring cleanup for unknown container 111ea69c61844da1a0e9c34e8c6deb30 i0612 174651569522 16311 gccpp56 scheduling varlibmesosslaves201503192131332080910346505057551s3314 for gc 699999341503407days in the future w0612 174651569725 16329 diskcpp299 ignoring cleanup for unknown container ddcb8397355244f9bc99b5b69aa72944 i0612 174651570911 16325 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesosd8c48a4afdfb47ddb8d807188c21600d i0612 174651573581 16316 port_mappingcpp2597 removing ip packet filters with ports 3584036863 for container with pid 31607 i0612 174651575127 16316 port_mappingcpp2616 freed ephemeral ports 3584036864 for container with pid 31607 i0612 174651588284 16330 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesosd8c48a4afdfb47ddb8d807188c21600d after 14503936ms e0612 174651622140 16310 containerizercpp480 failed to clean up an isolator when destroying orphan container ddcb8397355244f9bc99b5b69aa72944 the icmp packet filter on host eth0 does not exist the arp packet filter on host eth0 does not exist w0612 174651773123 16313 diskcpp299 ignoring cleanup for unknown container d8c48a4afdfb47ddb8d807188c21600d i0612 174651774153 16325 port_mappingcpp2597 removing ip packet filters with ports 3276833791 for container with pid 41020 i0612 174651775167 16325 port_mappingcpp2616 freed ephemeral ports 3276833792 for container with pid 41020 e0612 174651817221 16323 containerizercpp480 failed to clean up an isolator when destroying orphan container d8c48a4afdfb47ddb8d807188c21600d the icmp packet filter on host eth0 does not exist the arp packet filter on host eth0 does not exist i0612 174651872231 16314 cgroupscpp1420 successfully froze cgroup sysfscgroupfreezermesos50f9986febbc440d86a79fa1a7c55a75 after 30833792ms i0612 174651874572 16314 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos50f9986febbc440d86a79fa1a7c55a75 i0612 174651876566 16314 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos50f9986febbc440d86a79fa1a7c55a75 after 1593344ms 20150612 174654833163070x7f172eb07940zoo_infoauth_completion_func1286 authentication scheme digest succeeded i0612 174654835737 16321 groupcpp385 trying to create path homemesosprodmaster in zookeeper i0612 174654839110 16321 detectorcpp138 detected a new leader id1 i0612 174654840276 16330 groupcpp659 trying to get homemesosprodmasterinfo_0000000001 in zookeeper i0612 174654842350 16330 detectorcpp452 a new leading master upidmaster1044141325050 is detected i0612 174654843297 16330 slavecpp653 new master detected at master1044141325050 i0612 174654843298 16312 status_update_managercpp171 pausing sending status updates i0612 174654844091 16330 slavecpp678 no credentials provided attempting to register without authentication i0612 174654845087 16330 slavecpp689 detecting new master i0612 174701561920 16309 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174701564687 16309 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 after 1924096ms i0612 174701565467 16309 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 w0612 174709978946 16326 diskcpp299 ignoring cleanup for unknown container 50f9986febbc440d86a79fa1a7c55a75 i0612 174709979818 16327 port_mappingcpp2597 removing ip packet filters with ports 3481635839 for container with pid 19805 i0612 174709981474 16327 port_mappingcpp2616 freed ephemeral ports 3481635840 for container with pid 19805 e0612 174710278715 16325 containerizercpp480 failed to clean up an isolator when destroying orphan container 50f9986febbc440d86a79fa1a7c55a75 the icmp packet filter on host eth0 does not exist the arp packet filter on host eth0 does not exist i0612 174711568151 16326 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174711570915 16326 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 after 1987072ms i0612 174711571728 16326 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174714549536 16316 slavecpp821 registered with master master1044141325050 given slave id 201506021901002215521290505039399s23257 i0612 174714550220 16318 status_update_managercpp178 resuming sending status updates i0612 174721574513 16319 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174721576817 16319 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 after 1587968ms i0612 174721577466 16319 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174731580281 16310 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174731582365 16310 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 after 1410048ms i0612 174731582895 16310 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174741585619 16322 cgroupscpp2394 thawing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174741587703 16322 cgroupscpp1449 successfullly thawed cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 after 1418752ms i0612 174741588436 16322 cgroupscpp2377 freezing cgroup sysfscgroupfreezermesos8953fc7f9fca4931b0cb2f4959ddee74 i0612 174751515689 16330 slavecpp3733 current disk usage 1143 max allowed age 5499938588861215days e0612 174751557831 16330 containerizercpp468 failed to destroy orphan container 8953fc7f9fca4931b0cb2f4959ddee74 timed out after 1mins noformat,3
specify correct libnl version for configure check currently configureac lists 3224 as the required libnl version however httpsreviewsapacheorgr31503 caused the minimum required version to be bumped to 3226 the configure check thus fails to error out during execution and the dependency is captured only during the build step,1
framework can overcommit oversubscribable resources during master failover this is due to a bug in the hierarchical allocator here is the sequence of events 1 slave uses a fixed resource estimator which advertise 4 revocable cpus 2 a framework a launches a task that uses all the 4 revocable cpus 3 master fails over 4 slave reregisters with the new master and sends updateslavemessage with 4 revocable cpus as oversubscribed resources 5 framework a hasnt registered yet therefore the slaves available resources will be 4 revocable cpus 6 framework a registered and will receive an additional 4 revocable cpus so it can launch another task with 4 revocable cpus that means 8 total the problem is due to the way we calculate allocated resource in allocator when updateslave if the framework is not registered the allocation below is not accurate check that if block in addslave code template class rolesorter class frameworksorter void hierarchicalallocatorprocessrolesorter frameworksorterupdateslave const slaveid slaveid const resources oversubscribed  checkinitialized checkslavescontainsslaveid  check that all the oversubscribed resources are revocable check_eqoversubscribed oversubscribedrevocable  update the total resources  first remove the old oversubscribed resources from the total slavesslaveidtotal  slavesslaveidtotalrevocable  now add the new estimate of oversubscribed resources slavesslaveidtotal  oversubscribed  now update the total resources in the role sorter rolesorterupdate slaveid slavesslaveidtotalunreserved  calculate the current allocation of oversubscribed resources resources allocation foreachkey const stdstring role roles  allocation  rolesorterallocationrole slaveidrevocable   update the available resources  first remove the old oversubscribed resources from available slavesslaveidavailable  slavesslaveidavailablerevocable  now add the new estimate of available oversubscribed resources slavesslaveidavailable  oversubscribed  allocation loginfo  slave   slaveid     slavesslaveidhostname   updated with oversubscribed resources   oversubscribed   total   slavesslaveidtotal   available   slavesslaveidavailable   allocateslaveid  template class rolesorter class frameworksorter void hierarchicalallocatorprocessrolesorter frameworksorteraddslave const slaveid slaveid const slaveinfo slaveinfo const resources total const hashmapframeworkid resources used  checkinitialized checkslavescontainsslaveid rolesorteraddslaveid totalunreserved foreachpair const frameworkid frameworkid const resources allocated used  if frameworkscontainsframeworkid  const stdstring role  frameworksframeworkidrole  todobmahler validate that the reserved resources have the  frameworks role rolesorterallocatedrole slaveid allocatedunreserved frameworksortersroleaddslaveid allocated frameworksortersroleallocated frameworkidvalue slaveid allocated     code,3
add move constructors  assignment to try now that we have c11 lets add move constructors and move assignment operators for try similarly to what was done for option,3
add move constructors  assignment to result now that we have c11 lets add move constructors and move assignment operators for result similarly to what was done for option,3
add move constructors  assignment to future now that we have c11 lets add move constructors and move assignment operators for future similarly to what was done for option there is currently one move constructor for futuret but not for t u and no assignment operator,3
fetchercpp  problem with certificates mesos 02200221 built and installed from sources accordingly to the instructions given herehttpmesosapacheorggettingstarted has some problem with certificates every time i try to deploy something that requires downloading any resource via https with uri specified via marathon such deployment fails and i get this message in failed apps sandbox code e0617 095844339409 12380 fetchercpp138 error downloading resource problem with the ssl ca cert path access rights code trying to download the same resource on the same slave with curl or wget works without problems moreover when i install exactly the same version of mesos from mesospheres debs on identical machines ie set up by the same ansible scripts everything works fine as well i guess it must be something related to the way how mesos is built  maybe some missing switch for configure or make any ideas,2
invalid usage of atomic_flag_init in member initialization the c specification states the macro atomic_flag_init shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state the macro can be used in the form atomic_flag guard  atomic_flag_init it is unspecified whether the macro can be used in other initialization contexts clang catches this although reports it erroneously as a braced scaled init issue and refuses to compile libprocess,1
extend mesosstylepycpplintpy to check include files cpplintpy provides the capability to enforce the style guide requirements for including everything you use and ordering files based on type but it does not work for mesos because we do use include  for project files where it expects include  we should update the style checker to support our include usage and then turn it on by default in the commit hook,1
update stout include headers update stout to include headers for symbols we rely on and reorder to comply with the style guide,2
create a design document for quota support in master create a design document for the quota feature support in mesos master excluding allocator to be shared with the mesos community design doc httpsdocsgooglecomdocumentd16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq9iedituspsharing,8
initial design document for quota support in allocator create a design document for the quota feature support in the builtin hierarchical drf allocator to be shared with the mesos community,5
linux docker inspect crashes on linux when a simple task is being executed on docker container executor the sandbox stderr shows a backtrace  aborted at 1435254156 unix time try date d 1435254156 if you are using gnu date  pc  0x7ffff2b1364d unknown  sigsegv 0xfffffffffffffff8 received by pid 88424 tid 0x7fffe88fb700 from pid 18446744073709551608 stack trace   0x7ffff25a4340 unknown  0x7ffff2b1364d unknown  0x7ffff2b724df unknown  0x4a6466 dockercontainercontainer  0x7ffff5bfa49a optionoption  0x7ffff5c15989 optionoperator  0x7ffff5c09e9f tryoperator  0x7ffff5c09ee3 resultoperator  0x7ffff5c0a938 processfutureset  0x7ffff5bff412 processpromiseset  0x7ffff5be53e3 docker___inspect  0x7ffff5be3cf8 _zzn6docker9__inspecterkssrkn7process5ownedins2_7promiseins_9containereeeeerk6optioni8durationens2_6futureisseerkns2_10subprocesseenkulrksg_e1_clesl_  0x7ffff5be91e9 _zznk7process6futureisse5onanyizn6docker9__inspecterkssrkns_5ownedins_7promiseins3_9containereeeeerk6optioni8durationes1_rkns_10subprocesseeulrks1_e1_veesm_ot_ns1_6prefereenulsm_e_clesm_  0x7ffff5be9d9d _znst17_function_handlerifvrkn7process6futureisseeeznks2_5onanyizn6docker9__inspecterkssrkns0_5ownedins0_7promiseins7_9containereeeeerk6optioni8durationes2_rkns0_10subprocesseeuls4_e1_vees4_ot_ns2_6prefereeuls4_e_e9_m_invokeerkst9_any_datas4_  0x7ffff5c1eadd stdfunctionoperator  0x7ffff5c15e07 processfutureonany  0x7ffff5be93a1 _znk7process6futureisse5onanyizn6docker9__inspecterkssrkns_5ownedins_7promiseins3_9containereeeeerk6optioni8durationes1_rkns_10subprocesseeulrks1_e1_veesm_ot_ns1_6prefere  0x7ffff5be87f6 _znk7process6futureisse5onanyizn6docker9__inspecterkssrkns_5ownedins_7promiseins3_9containereeeeerk6optioni8durationes1_rkns_10subprocesseeulrks1_e1_eesm_ot_  0x7ffff5be459c docker__inspect  0x7ffff5be337c _zzn6docker8_inspecterkssrkn7process5ownedins2_7promiseins_9containereeeeerk6optioni8durationeenkulve_clev  0x7ffff5be8c5a _zznk7process6futurei6optioniiee5onanyizn6docker8_inspecterkssrkns_5ownedins_7promiseins5_9containereeeeerks1_i8durationeeulve_veerks3_ot_ns3_10lessprefereenulsl_e_clesl_  0x7ffff5be9b36 _znst17_function_handlerifvrkn7process6futurei6optioniieeeeznks4_5onanyizn6docker8_inspecterkssrkns0_5ownedins0_7promiseins9_9containereeeeerks2_i8durationeeulve_vees6_ot_ns4_10lessprefereeuls6_e_e9_m_invokeerkst9_any_datas6_  0x7ffff5c1e9b3 stdfunctionoperator  0x7ffff6184a1a _zn7process8internal3runist8functionifvrkns_6futurei6optioniieeeeejrs6_eeevrkst6vectorit_saisd_eedpot0_  0x7ffff617e64d processfutureset  0x7ffff6752e46 processpromiseset  0x7ffff675faec processinternalcleanup  0x7ffff6765293 _znst5_bindifpfvrkn7process6futurei6optioniieeepns0_7promiseis3_eerkns0_10subprocesseest12_placeholderili1ees9_sa_ee6__callivis6_eilm0elm1elm2eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_eee  0x7ffff6764bcd _znst5_bindifpfvrkn7process6futurei6optioniieeepns0_7promiseis3_eerkns0_10subprocesseest12_placeholderili1ees9_sa_eeclijs6_eveet0_dpot_  0x7ffff67642a5 _zznk7process6futurei6optioniiee5onanyist5_bindifpfvrks3_pns_7promiseis2_eerkns_10subprocesseest12_placeholderili1eesa_sb_eevees7_ot_ns3_6prefereenuls7_e_cles7_  0x7ffff676531d _znst17_function_handlerifvrkn7process6futurei6optioniieeeeznks4_5onanyist5_bindifpfvs6_pns0_7promiseis3_eerkns0_10subprocesseest12_placeholderili1eesc_sd_eevees6_ot_ns4_6prefereeuls6_e_e9_m_invokeerkst9_any_datas6_  0x7ffff5c1e9b3 stdfunctionoperator end,1
testing the new workflow this is a simple test story to try out the new workflow unfortunately testing and getting it to work seems to be something that actually does take up time so im tracking this here,3
reconciliation is expensive for large numbers of tasks weve observed that both implicit and explicit reconciliation are expensive for large numbers of tasks noformat titleexplicit o100000 tasks 70secs i0625 205523716320 21937 mastercpp3863 performing explicit task state reconciliation for n tasks of framework f name at sipport i0625 205634812464 21937 mastercpp5041 removing task t with resources r of framework f on slave s at slave1ipport host noformat noformat titleimplicit with o100000 tasks 60secs i0625 202522310601 21936 mastercpp3802 performing implicit task state reconciliation for framework f name at sipport i0625 202623874528 21921 mastercpp218 scheduling shutdown of slave s due to health check timeout noformat lets add a benchmark to see if there are any bottlenecks here and to guide improvements,3
add a benchmark for task reconciliation per mesos2940 it would be great to have a benchmark for task reconciliation given large numbers of tasks this can guide attempts at improving performance,1
create documentation for using ssl,5
mesos fails to compile under mac when libssl and libevent are enabled configure enabledebug enablelibevent enablessl  make produces the following error pollcpp  echo 3rdpartylibprocesssrclibevent_pollcpp libtool compile g dpackage_namelibprocess dpackage_tarnamelibprocess dpackage_version001 dpackage_stringlibprocess 001 dpackage_bugreport dpackage_url dpackagelibprocess dversion001 dstdc_headers1 dhave_sys_types_h1 dhave_sys_stat_h1 dhave_stdlib_h1 dhave_string_h1 dhave_memory_h1 dhave_strings_h1 dhave_inttypes_h1 dhave_stdint_h1 dhave_unistd_h1 dhave_dlfcn_h1 dlt_objdirlibs dhave_apr_pools_h1 dhave_libapr_11 dhave_svn_version_h1 dhave_libsvn_subr_11 dhave_svn_delta_h1 dhave_libsvn_delta_11 dhave_libcurl1 dhave_event2_event_h1 dhave_libevent1 dhave_event2_thread_h1 dhave_libevent_pthreads1 dhave_openssl_ssl_h1 dhave_libssl1 dhave_libcrypto1 dhave_event2_bufferevent_ssl_h1 dhave_libevent_openssl1 duse_ssl_socket1 dhave_pthread_prio_inherit1 dhave_pthread1 dhave_libz1 dhave_libdl1 i i3rdpartylibprocess i3rdpartylibprocessinclude i3rdpartylibprocess3rdpartystoutinclude i3rdpartyboost1530 i3rdpartylibev415 i3rdpartypicojson4f93734 i3rdpartyglog033src i3rdpartyryhttpparser1c3624a iusrlocaloptopensslinclude iusrlocaloptlibeventinclude iusrlocaloptsubversionincludesubversion1 iusrincludeapr1 iusrincludeapr10 g1 o0 stdc11 stdliblibc dgtest_use_own_tr1_tuple1 mt libprocess_lalibevent_polllo md mp mf depslibprocess_lalibevent_polltpo c 3rdpartylibprocesssrclibevent_pollcpp fnocommon dpic o libprocess_lalibevent_pollo mv f depslibprocess_lasockettpo depslibprocess_lasocketplo mv f depslibprocess_lasubprocesstpo depslibprocess_lasubprocessplo mv f depslibprocess_lalibeventtpo depslibprocess_lalibeventplo mv f depslibprocess_lametricstpo depslibprocess_lametricsplo in file included from 3rdpartylibprocesssrclibevent_ssl_socketcpp11 in file included from 3rdpartylibprocessincludeprocessqueuehpp9 3rdpartylibprocessincludeprocessfuturehpp8497 error no viable conversion from const processfutureconst processfutureprocessnetworksocket  to const processnetworksocket setu  3rdpartylibprocesssrclibevent_ssl_socketcpp76910 note in instantiation of function template specialization processfutureprocessnetworksocketfutureprocessfutureconst processfutureprocessnetworksocket   requested here return accept_queueget  3rdpartylibprocessincludeprocesssockethpp217 note candidate constructor the implicit move constructor not viable no known conversion from const processfutureconst processfutureprocessnetworksocket  to processnetworksocket  for 1st argument class socket  3rdpartylibprocessincludeprocesssockethpp217 note candidate constructor the implicit copy constructor not viable no known conversion from const processfutureconst processfutureprocessnetworksocket  to const processnetworksocket  for 1st argument class socket  3rdpartylibprocessincludeprocessfuturehpp41121 note passing argument to parameter _t here bool setconst t _t  1 error generated make4  libprocess_lalibevent_ssl_socketlo error 1 make4  waiting for unfinished jobs mv f depslibprocess_lalibevent_polltpo depslibprocess_lalibevent_pollplo mv f depslibprocess_laopenssltpo depslibprocess_laopensslplo mv f depslibprocess_laprocesstpo depslibprocess_laprocessplo make3  allrecursive error 1 make2  allrecursive error 1 make1  all error 2 make  allrecursive error 1,2
use of expect in test and relying on the checked condition afterwards in docker_containerizer_test we have the following pattern code expect_ne0u offersgetsize const offer offer  offersget0 code as we rely on the value afterwards we should use assert_ne instead in that case the test will fail immediately,1
authorizer module interface design h4motivation design an interface covering authorizer modules while staying minimally invasive in regards to changes to the existing localauthorizer implementation,2
authorizer module implementation integration  tests h4motivation provide an example authorizer module based on the localauthorizer implementation make sure that such authorizer module can be fully unit and integration tested within the mesos test suite,8
draft design for generalized authorizer interface as mentioned in mesos2948 the current mesosauthorizer interface is rather inflexible if new _actions_ or _objects_ need to be added a new api needs to be designed in a way that allows for arbitrary _actions_ and _objects_ to be added to the authorization mechanism without having to recompile mesos,3
implement current mesos authorizer in terms of generalized authorizer interface in order to maintain compatibility with existent versions of mesos as well as to prove the flexibility of the generalized mesosauthorizer design the current authorization mechanism through acl definitions needs to run under the updated interface without any changes being noticeable by the current authorization users,8
inefficient container usage collection docker containerizer currently collects usage statistics by calling oss process statistics eg ps  there is scope for making this efficient say by querying cgroups file system,3
stack trace in isolator tests on linux vm perfeventisolatortest fails with stack trace when run in linux vm  1 test from perfeventisolatortest  run  perfeventisolatortestroot_cgroups_sample f0629 113817088412 14114 isolator_testscpp837 check_someisolator failed to create perfevent isolator invalid events  cycles taskclock   check failure stack trace   0x2ab5e5aeeb1a googlelogmessagefail  0x2ab5e5aeea66 googlelogmessagesendtolog  0x2ab5e5aee468 googlelogmessageflush  0x2ab5e5af137c googlelogmessagefatallogmessagefatal  0x864b0c _checkfatal_checkfatal  0xc458ed mesosinternaltestsperfeventisolatortest_root_cgroups_sample_testtestbody  0x119fb17 testinginternalhandlesehexceptionsinmethodifsupported  0x119ac9e testinginternalhandleexceptionsinmethodifsupported  0x118305f testingtestrun  0x1183782 testingtestinforun  0x1183d0a testingtestcaserun  0x11889d4 testinginternalunittestimplrunalltests  0x11a09ae testinginternalhandlesehexceptionsinmethodifsupported  0x119b9c3 testinginternalhandleexceptionsinmethodifsupported  0x11878e0 testingunittestrun  0xcdc8c7 main  0x2ab5e7fdbec5 unknown  0x861a89 unknown make3  checklocal aborted core dumped  run  usercgroupisolatortest2root_cgroups_usercgroup f0629 114938763434 18836 isolator_testscpp1200 check_someisolator failed to create perfevent isolator invalid events  cpucycles   check failure stack trace   0x2ba40eb2db1a googlelogmessagefail  0x2ba40eb2da66 googlelogmessagesendtolog  0x2ba40eb2d468 googlelogmessageflush  0x2ba40eb3037c googlelogmessagefatallogmessagefatal  0x864b0c _checkfatal_checkfatal  0xc5ddb1 mesosinternaltestsusercgroupisolatortest_root_cgroups_usercgroup_testtestbody  0x119fc43 testinginternalhandlesehexceptionsinmethodifsupported  0x119adca testinginternalhandleexceptionsinmethodifsupported  0x118318b testingtestrun  0x11838ae testingtestinforun  0x1183e36 testingtestcaserun  0x1188b00 testinginternalunittestimplrunalltests  0x11a0ada testinginternalhandlesehexceptionsinmethodifsupported  0x119baef testinginternalhandleexceptionsinmethodifsupported  0x1187a0c testingunittestrun  0xcdc9f3 main  0x2ba41101aec5 unknown  0x861a89 unknown make3  checklocal aborted core dumped,1
add version to masterinfo this will help schedulers figure out the version of the master that they are interacting with see mesos2736 for additional context,1
update call protobuf to move top level frameworkinfo inside subscribe it is better for frameworkinfo to be only included in subscribe message that needs to be added instead of for every call instead the top level call should contain a frameworkid to identify the framework making the call,3
add cpuacct subsystem utils to cgroups current cgroups implementation does not have a cpuacct subsystem implementation this subsystem reports important metrics like user and system cpu ticks spent by a process cgroups namespace has subsystem specific utilities for cpu memory etc it could use other subsystems specific utils eg cpuacct in the future we could also view cgroups as a mesossubsystem with features like event notifications although refactoring cgroups would be a different epic listing the possible tasks  have hierarchies subsystems abstracted to represent the domain  create cgroups service  cgroups service listen to update events from the os on files like stats this would be an interrupt based systemmaybe use linux fsnotify  cgroups service services events to mesos containers for example,2
slave fails with abort stacktrace when dns cannot resolve hostname if the dns cannot resolve the hostnametoip for a slave node we correctly return an error object but we then fail with a segfault this code adds a more userfriendly message and exits normally with an exit_failure code for example forcing netgetip to always return an error now causes the slave to exit like this noformat  binmesosslavesh master101011215405 warning logging before initgooglelogging is written to stderr e0630 113145777465 1944417024 processcpp899 could not obtain the ip address for stratoslocal the dns service may not be able to resolve it  marco was here  echo  1 noformat,1
configure jenkins to build ssl,5
libprocess io does not support peek finally i so wish we could just do code iopeekrequestsocket 6 thenrequestconst string data   comment about the rules  if datalength  2   rule 1  else if    rule 2  else if    rule 3  if ssl  accept_ssl_callbackrequest  else     code from httpsreviewsapacheorgr31207,3
add implicit cast to string operator to path for example codeinline trynothing rmconst stdstring pathcode does not have an overload for codeinline trynothing rmconst path pathcode the implementation should be something like code inline trynothing rmconst path path  rmpathvalue  code,2
socketpeer and socketaddress might fail with ssl enabled libevent ssl currently uses a secondary fd so we need to virtualize the get function on socket interface,5
missing doxygen documentation for libprocess socket interface convert existing comments to doxygen format,5
implement shared copy based provisioner backend currently appc and docker both implemented its own copy backend but most of the logic is the same where the input is just a image name with its dependencies we can refactor both so that we just have one implementation that is shared between both provisioners so appc and docker can reuse the shared copy backend,3
implement overlayfs based provisioner backend part of the image provisioning process is to call a backend to create a root filesystem based on the image on disk layout the problem with the copy backend is that its both waste of io and space and bind only can deal with one layer overlayfs backend allows us to utilize the filesystem to merge multiple filesystems into one efficiently,5
serialize docker image spec as protobuf the docker image specification defines a schema for the metadata json that it puts into each image currently the docker image provisioner needs to be able to parse and understand this metadata json and we should create a protobuf equivelent schema so we can utilize the json to protobuf conversion to read and validate the metadata,3
ssl tests dont work with gtest_repeat commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f author joris van remoortere jorisvanremoorteregmailcom date wed jul 1 161652 2015 0700 mesos2973 allow ssl tests to run using gtest_repeat the ssl ctx object carried some settings between reinitialize calls reconstruct the object to avoid this state transition review httpsreviewsapacheorgr36074,3
stout flags cant have their defaults reset stout flags dont remember their default values and so cant have their defaults reset this makes it hard to reset flags to their defaults between tests,5
ssl tests dont work with gtest_shuffle,3
allow runtime configuration to be returned from provisioner image specs also includes execution configuration eg env user ports etc we should support passing those information from the image provisioner back to the containerizer,5
deprecating json extension in slave endpoints url remove the json extension on endpoints such as slavestatejson so it become slavestate,1
deprecating json extension in files endpoints url remove the json extension on endpoints such as filesbrowsejson so it become filesbrowse,1
docker version output is not compatible with mesos we currently use docker version to get docker version in docker master branch and soon in docker 18 1 the output for this command changes the solution for now will be to use the unchanged docker version output in the long term we should consider stop using the cli and use the api instead 1 httpsgithubcomdockerdockerpull14047,1
compilation error on mac os 10104 with clang 350 compiling 0230 rc1 produces compilation errors on mac os 10104 with g based on llvm 35 it looks like the issue was introduced in a5640ad813e6256b548fca068f04fd9fa3a03eda httpsreviewsapacheorgr32838 in contrast to the commit message compiling the rc with gcc44 on centos worked fine for me according to 023 release notes and mesos2604 we should support clang 35 code 3rdpartylibprocess3rdpartystouttestsos_testscpp54325 error conversion from void  to const optionvoid  is ambiguous forkdosetsid  greatgreatgranchild  3rdpartylibprocess3rdpartystoutincludestoutoptionhpp403 note candidate constructor optionconst t _t  statesome t_t   3rdpartylibprocess3rdpartystoutincludestoutoptionhpp423 note candidate constructor optiont _t  statesome tstdmove_t   3rdpartylibprocess3rdpartystoutincludestoutoptionhpp453 note candidate constructor with u  void  optionconst u u  statesome tu   code compiler version code  g version configured with prefixapplicationsxcodeappcontentsdeveloperusr withgxxincludedirusrincludec421 apple llvm version 60 clang600054 based on llvm 35svn target x86_64appledarwin1440 thread model posix code,1
document per container unique egress flow and network queueing statistics document new network isolation capabilities in 023,3
design doc for creating user namespaces inside containers,5
standardize use of path as per the discussion in mesos2965 the use of the path object should be standardized  functions which effectively use paths as strings should instead take paths  functions which modify and return paths as strings should instead return paths  extraneous uses of pathvalue should be removed,3
ssl connection failure causes failed check code  run  ssltestbasicsameprocess f0706 183228465451 238583808 libevent_ssl_socketcpp507 check failed selfbev must be non null code,3
create a demo http api client we want to create a simple demo http api client in java python or go that can serve as an example framework for people who will want to use the new api for their frameworks the scope should be fairly limited eg launching a simple container task but sufficient to exercise most of the new api endpoint messagescapabilities scope tbd nongoals  create a bestofbreed framework to deliver any specific functionality  create an integration test for the http api,8
rename optiontgetconst t _t to getorelse broke network isolator change to option from get to getorelse breaks network isolator building with configure withnetworkisolator generates the following error srcslavecontainerizerisolatorsnetworkport_mappingcpp in static member function static trymesosslaveisolator mesosinternalslaveportmappingisolatorprocesscreateconst mesosinternalslaveflags srcslavecontainerizerisolatorsnetworkport_mappingcpp110329 error no matching function for call to optionstdbasic_stringchar getconst char 1 const flagsresourcesget  srcslavecontainerizerisolatorsnetworkport_mappingcpp110329 note candidates are in file included from 3rdpartylibprocess3rdpartystoutincludestoutcheckhpp260 from 3rdpartylibprocessincludeprocesscheckhpp19 from 3rdpartylibprocessincludeprocesscollecthpp7 from srcslavecontainerizerisolatorsnetworkport_mappingcpp30 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp13012 note const t optiontget const with t  stdbasic_stringchar const t get const  assertissome return t   3rdpartylibprocess3rdpartystoutincludestoutoptionhpp13012 note candidate expects 0 arguments 1 provided 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp1316 note t optiontget with t  stdbasic_stringchar t get  assertissome return t   3rdpartylibprocess3rdpartystoutincludestoutoptionhpp1316 note candidate expects 0 arguments 1 provided make2  slavecontainerizerisolatorsnetworklibmesos_no_3rdparty_laport_mappinglo error 1 make2 leaving directory homepbrettsandboxmesosmasterbuildsrc make1  check error 2 make1 leaving directory homepbrettsandboxmesosmasterbuildsrc make  checkrecursive error 1,1
design support running the command executor with provisioned image for running a task in a container mesos containerizer uses the command executor to actually launch the user defined command and the command executor then can communicate with the slave about the process lifecycle when we provision a new container with the user specified image we also need to be able to run the command executor in the container to support the same semantics one approach is to dynamically mount in a static binary of the command executor with all its dependencies in a special directory so it doesnt interfere with the provisioned root filesystem and configure the mesos containerizer to run the command executor in that directory,5
ssl tests can fail depending on hostname configuration depending on how etchosts is configured the ssl tests can fail with a bad hostname match for the certificate we can avoid this by explicitly matching the hostname for the certificate to the ip that will be used during the test,3
add cgroups memory stats api cgroups api current does expose stats from the memory namespace having this api would enable isolators to use its various fieldseg rss rss_huge writeback etc in use cases like usage metrics,2
libevent ssl doesnt use epoll we currently disable to epoll in libevent to allow ssl to work it would be more scalable if we didnt have to do that,8
reproduce systemd cgroup behavior it has been noticed before that systemd reorganizes cgroup hierarchy created by mesos slave because of this mesos is no longer able to find the cgroup and there is also a chance of undoing the isolation that mesos slave puts in place,5
support existing message passing optimization with eventcall see the thread here httpmarkmailorgthreadwvapc7vkbv7z6gbx the scheduler driver currently sends framework messages directly to the slave when possible noformat through master scheduler  master  slave  executor driver  driver skip master noformat the slave always sends messages directly to the scheduler driver noformat scheduler master slave  executor driver  driver skip master noformat in order for the scheduler driver to receive events from the master it needs enough information to continue directly sending messages to slaves this was previously accomplished by sending the slaves pid inside the offer messagehttpsgithubcomapachemesosblob0230rc1srcmessagesmessagesprotol168 code message resourceoffersmessage  repeated offer offers  1 repeated string pids  2  code we could add an address to the offer protobuf to provide the scheduler driver with the same information code message address  required string ip required string hostname required uint32_t port  all http requests to this address must begin with this prefix required string path_prefix  message offer  required offerid id  1 required frameworkid framework_id  2 required slaveid slave_id  3 required string hostname  4  deprecated in favor of address optional address address  8  obviates hostname   code the path prefix is required for testing purposes where we can have multiple slaves within a process eg localhost5051slave1statejson vs localhost5051slave2statejson this provides enough information to allow the scheduler driver to continue to directly send messages to the slaves which unblocks mesos2910,1
extend containerinfo to include networkinfo message as per the design dochttpsdocsgooglecomdocumentd17mxtamdaxcnbwp_jfrxmzcqrs7eo6ancsbejrqjlq0g we need to enable frameworks to specify network requirements the proposed message could be along the lines of code   collection of network request  todokapil add a highlevel explanationmotivation  message networkinfo   specify ipaddress requirement enum protocol  ipv4  0 ipv6  1   todo document how to use this field to request an  1 ipv4 address  2 ipv6 address  3 any of the above optional protocol protocol  1  statically assigned ips provided by the framework optional string ip_address  2  a group is the name given to a set of logicallyrelated ips that are  allowed to communicate within themselves for example one might want  to create separate groups for dev testing qa and prod deployment  environments repeated string groups  3  to tag certain metadata to be used by isolatoripam eg rack pop etc optional labels labels  4  message containerinfo   repeated networkinfo network_infos   message containerstatus  repeated networkinfo network_infos  message taskstatus    todo comment on the fact that this is resolved during container setup optional containerstatus container   code,2
add hooks for slave exits the hook will be triggered on slave exits a master hook module can use this to do slavespecific cleanups in our particular use case the hook would trigger cleanup of ips assigned to the given slave see the design doc  httpsdocsgooglecomdocumentd17mxtamdaxcnbwp_jfrxmzcqrs7eo6ancsbejrqjlq0gedit,2
add task status update hooks for masterslave the task termination hooks are needed for doing taskspecific cleanup in masterslave,3
make containerip available via master endpoint,5
a mechanism for messages between master modules and slave modules a slave module should be able to send a message to a master module and viceversa to allow outofband communication between masterslave modules,8
expose major minor and patch components from stout version stout version class does not expose version components preventing computations manipulation of version information solution is to make major minor and patch public,1
implement docker image provisioner reference store create a comprehensive store to look up an image and tags associated image layer id implement add remove save and update images and their associated tags,3
factoring out the pattern for url generation fetcher_testcpp uses the following code for generating urls string url  http  netgethostnameprocessselfaddressipget    stringifyprocessselfaddressport    processselfid it would be good to isolate that code in a function and replace the code above with something like string url  http  endpoint_urlprocess uri_test,1
http endpoint authn is enabled merely by specifying credentials if i set credentials on the master framework and slave authentication are allowed but not required on the other hand http authentication is now required for authenticated endpoints currently only shutdown that means that i cannot enable framework or slave authentication without also enabling http endpoint authentication this is undesirable framework and slave authentication have separate flags authenticate and authenticate_slaves to require authentication for each it would be great if there was also such a flag for http authentication or maybe we get rid of these flags altogether and rely on acls to determine which unauthenticated principals are even allowed to authenticate for each endpointaction,8
022x scheduler driver drops 023x reconciliation status updates due to missing statusupdateuuid in the process of fixing mesos2940 we accidentally introduced a nonbackwards compatible change  statusupdateuuid was required in 022x and was always set  statusupdateuuid is optional in 023x and the master is not setting it for mastergenerated updates in 022x the scheduler driver ignores the uuid for masterdriver generated updates already id suggest the following fix  in 023x rather than not setting statusupdateuuid set it to an empty string  in 023x ensure the scheduler driver also ignores empty statusupdateuuids  in 024x stop setting statusupdateuuid,3
processtestcache fails and hangs code  run  processtestcache 3rdpartylibprocesssrctestsprocess_testscpp1726 failure value of responsegetstatus actual 200 ok expected 304 not modified  failed  processtestcache 1 ms code the tests then finish running but the gtest framework fails to terminate and uses 100 cpu,5
document containerizer launch we currently dont have enough documentation for the containerizer component this task adds documentation for containerizer launch sequence the mail goals are  have diagrams state sequence class etc depicting the containerizer launch process  make the documentation newbie friendly  usable for future design discussions,3
as a developer i would like a standard way to run a subprocess in libprocess as part of mesos2830 and mesos2902 i have been researching the ability to run a subprocess and capture the stdout  stderr along with the exit status code processsubprocess offers much of the functionality but in a way that still requires a lot of handiwork on the developers part we would like to further abstract away the ability to just pass a string an optional set of commandline arguments and then collect the output of the command bonus without blocking,3
add a suppress call to the scheduler suppress call is the complement to the current revive call ie it will inform mesos to stop sending offers to the framework for the scheduler driver to send only call messages mesos2913 deactivateframeworkmessage needs to be converted to calls we can implement this by having the driver send a suppress call followed by a decline call for outstanding offers,3
resource offers do not contain unavailability given a maintenance schedule given a schedule defined elsewhere any resource offers to affected slaves must include an unavailability field the maintenance schedule for a single slave should be held in persistent storagemesos2075 and locally by the master ie in srcmastermasterhpp code struct slave    existing fields  new field that the masterallocator can access maintenances pendingdowntime  code the new field should be populated via an api call see mesos2067 the unavailability field can be added to masteroffer srcmastermastercpp code offermutable_unavailabilitymergefromslavependingdowntime code possible tests  pendingunavailibilitytest  start master slave  check unavailability of offer  none  set unavailability to the future  check offer has unavailability,8
allow executors binding ip to be different than slave binding ip currently the slave will bind either to the loopback ip 127001 or to the ip passed via the ip flag when it launches a containerized executor eg via mesos containerizer the executor inherits the binding ip of the slave this is due to the fact that the ip flags sets the environment variable libprocess_ip to the passed ip the executor then inherits this environment variable and is forced to bind to the slave ip if an executor is running in its own containerized environment with a separate ip than that of the slave currently there is no way of forcing it to bind to its own ip a potential solution is to use the executor environment decorator hooks to update libprocess_ip environment variable for the executor,2
decline call does not include an optional reason in the eventcall api in the eventcall api the decline call is currently used by frameworks to reject resource offers in the case of inverseoffers the framework could give additional information to the operators andor allocator as to why the inverseoffer is declined ie suppose a cluster running some consensus algorithm is given an inverseoffer on one of its nodes it may decline saying too few nodes or more verbosely specified inverseoffer would lower the number of active nodes below quorum this change requires the following changes  includemesosschedulerschedulerproto code message call   message decline  repeated offerid offer_ids  1 optional filters filters  2  add this extra string for each offerid  ie reasonsi is for offer_idsi repeated string reasons  3    code  srcmastermastercpp change masterdecline to either store the reason or log it  add a declineoffer overload in the mesosschedulerdriver with an optional reason  extend the interface in includemesosschedulerhpp  addchange the declineoffer method in srcschedschedcpp,3
masterallocator does not send inverseoffers to resources to be maintained offers are currently sent from masterallocator to framework via resourceoffersmessages inverseoffers which are roughly equivalent to negative offers can be sent in the same package in srcmessagesmessagesproto code message resourceoffersmessage  repeated offer offers  1 repeated string pids  2  new field with inverseoffers repeated inverseoffer inverseoffers  3  code sent inverseoffers can be tracked in the masters local state ie in srcmastermasterhpp code struct slave    existing fields  active inverseoffers on this slave  similar pattern to the offers field hashsetinverseoffer inverseoffers  code one actor master or allocator should populate the new inverseoffers field  in master srcmastermastercpp  masteroffer is where the resourceoffersmessage and offer object is constructed  the same method could also check for maintenance and send inverseoffers  in the allocator srcmasterallocatormesoshierarchicalhpp  hierarchicalallocatorprocessallocate is where slave resources are aggregated an sent off to the frameworks  inverseoffers ie negative resources allocation could be calculated in this method  a change to masteroffer ie the offercallback may be necessary to account for the negative resources possible tests  inverseoffertest  start master slave framework  accept resource offer start task  set maintenance schedule to the future  check that inverseoffers are sent to the framework  decline inverseoffer  check that more inverseoffers are sent  accept inverseoffer  check that more inverseoffers are sent,8
master does not handle inverseoffers in the accept call eventcall api inverseoffers are similar to offers in that they are accepted or declined based on their offerid some additional logic may be neccesary in masteraccept srcmastermastercpp to gracefully handle the acceptance of inverseoffers  the inverseoffer needs to be removed from the set of pending inverseoffers  the inverseoffer should not result any errorswarnings note accepted inverseoffers do not preclude further inverseoffers from being sent to the framework instead an accepted inverseoffer merely signifies that the framework is _currently_ fine with the expected downtime,3
slaves are not deactivated upon reaching a maintenance window after a maintenance window is reached the slave should be deactivated to prevent further tasks from utilizing it  for slaves that have completely drained simply deactivate the slave see masterdeactivateslave  for tasks which have not explicitly declined the inverseoffers ie theyve accepted them or do not understand inverseoffers send kill signals see masterkilltask  if a slave has tasks that have declined the inverseoffers do not deactivate the slave possible tests  slavedrainedtest  start master slave  set maintenance to now  check that slave gets deactivated  inverseofferagnostictest  start master slave framework  have a task run on the slave ignores inverseoffers  set maintenance to now  check that task gets killed  check that slave gets deactivated  inverseofferacceptancetest  start master slave framework  run a task on the slave  set maintenance to future  have task accept inverseoffer  check task gets killed slave gets deactivated  inverseofferdeclinedtest  start master slave framework  run task on slave  set maintenance to future  have task decline maintenance with reason  check task lives slave still active,8
maintenance information is not populated in case of failover when a master starts up or after a master has failed it must repopulate maintenance information ie from the registry to the local state particularly masterrecover in srcmastermastercpp should be changed to process maintenance information,3
stouts uuid reseeds a new random generator during each call to uuidrandom per stephanerb and kevintss observations on mesos2940 stouts uuid abstraction is reseeding the random generator during each call to uuidrandom which is really expensive this is confirmed in the perf graph from mesos2940,3
failing root_ tests on centos 71 running sudo make check on centos 71 for mesos 0230rc3 causes several several failureserrors code  run  dockertestroot_docker_checkportresource srctestsdocker_testscpp303 failure runfailure container exited on error exited with status 1  failed  dockertestroot_docker_checkportresource 709 ms code  code  run  perfeventisolatortestroot_cgroups_sample srctestsisolator_testscpp837 failure isolator failed to create perfevent isolator invalid events  cycles taskclock   failed  perfeventisolatortestroot_cgroups_sample 9 ms  1 test from perfeventisolatortest 9 ms total  2 tests from sharedfilesystemisolatortest  run  sharedfilesystemisolatortestroot_relativevolume  mount n bind tmpsharedfilesystemisolatortest_root_relativevolume_4yteacvartmp vartmp  touch vartmp492407e15dec4b348f2f130430f41aac srctestsisolator_testscpp1001 failure value of osexistsfile actual true expected false  failed  sharedfilesystemisolatortestroot_relativevolume 92 ms  run  sharedfilesystemisolatortestroot_absolutevolume  mount n bind tmpsharedfilesystemisolatortest_root_absolutevolume_owyrxk vartmp  touch vartmp7de712aa52eb4976b0f932b6a006418d srctestsisolator_testscpp1086 failure value of osexistspathjoincontainerpath filename actual true expected false  failed  sharedfilesystemisolatortestroot_absolutevolume 100 ms code  code  1 test from usercgroupisolatortest0 where typeparam  mesosinternalslavecgroupsmemisolatorprocess userdel user mesostestunprivilegeduser does not exist  run  usercgroupisolatortest0root_cgroups_usercgroup bash sysfscgroupblkiouserslicecgroupprocs permission denied mkdir cannot create directory sysfscgroupblkiousersliceuser permission denied srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupblkiousersliceusercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0 bash sysfscgroupmemorymesosbbf8c8f03d6740dfa269b3dc6a9597aacgroupprocs permission denied bash sysfscgroupcpuacctcpuuserslicecgroupprocs no such file or directory mkdir cannot create directory sysfscgroupcpuacctcpuusersliceuser no such file or directory srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupcpuacctcpuusersliceusercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0 bash sysfscgroupnamesystemdusersliceuser2004slicesession3865scopecgroupprocs no such file or directory mkdir cannot create directory sysfscgroupnamesystemdusersliceuser2004slicesession3865scopeuser no such file or directory srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupnamesystemdusersliceuser2004slicesession3865scopeusercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0  failed  usercgroupisolatortest0root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupsmemisolatorprocess 1034 ms  1 test from usercgroupisolatortest0 1034 ms total  1 test from usercgroupisolatortest1 where typeparam  mesosinternalslavecgroupscpushareisolatorprocess userdel user mesostestunprivilegeduser does not exist  run  usercgroupisolatortest1root_cgroups_usercgroup bash sysfscgroupblkiouserslicecgroupprocs permission denied mkdir cannot create directory sysfscgroupblkiousersliceuser permission denied srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupblkiousersliceusercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0 bash sysfscgroupcpuacctcpumesoseeeb99f07c5c4185869d635d51dcc6e1cgroupprocs no such file or directory mkdir cannot create directory sysfscgroupcpuacctcpumesoseeeb99f07c5c4185869d635d51dcc6e1user no such file or directory srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupcpuacctcpumesoseeeb99f07c5c4185869d635d51dcc6e1usercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0 bash sysfscgroupnamesystemdusersliceuser2004slicesession3865scopecgroupprocs no such file or directory mkdir cannot create directory sysfscgroupnamesystemdusersliceuser2004slicesession3865scopeuser no such file or directory srctestsisolator_testscpp1274 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupnamesystemdusersliceuser2004slicesession3865scopeusercgroupprocs no such file or directory srctestsisolator_testscpp1283 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess 763 ms  1 test from usercgroupisolatortest1 763 ms total  1 test from usercgroupisolatortest2 where typeparam  mesosinternalslavecgroupsperfeventisolatorprocess userdel user mesostestunprivilegeduser does not exist  run  usercgroupisolatortest2root_cgroups_usercgroup srctestsisolator_testscpp1200 failure isolator failed to create perfevent isolator invalid events  cpucycles   failed  usercgroupisolatortest2root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupsperfeventisolatorprocess 6 ms  1 test from usercgroupisolatortest2 6 ms total code,5
performance issues with port ranges comparison testing in an environment with lots of frameworks 200 where the frameworks permanently decline resources they dont need the allocator ends up spending a lot of time figuring out whether offers are refused the code path through hierarchicalallocatorprocessisfiltered in profiling a synthetic benchmark it turns out that comparing port ranges is very expensive involving many temporary allocations 61 of resourcescontains run time is in operator  resource 35 of resourcescontains run time is in resources_contains the heaviest call chain through resources_contains is code running time self ms symbol name 72370ms 355 40 mesosresources_containsmesosresource const const 72000ms 353 10 mesoscontainsmesosresource const mesosresource const 71330ms 350 1210 mesosoperatormesosvalue_ranges const mesosvalue_ranges const 63190ms 310 70 mesoscoalescemesosvalue_ranges mesosvalue_ranges const 62400ms 306 1610 mesoscoalescemesosvalue_ranges mesosvalue_range const 18670ms 91 250 mesosvalue_rangesadd_range 16940ms 83 40 mesosvalue_rangesvalue_ranges 14950ms 73 160 mesosvalue_rangesoperatormesosvalue_ranges const 4450ms 21 940 mesosvalue_rangemergefrommesosvalue_range const 1540ms 07 240 mesosvalue_rangesrangeint const 1030ms 05 240 mesosvalue_rangesrange_size const 950ms 04 20 mesosvalue_rangevalue_rangemesosvalue_range const 590ms 02 40 mesosvalue_rangesvalue_ranges 500ms 02 500 mesosvalue_rangebegin const 280ms 01 280 mesosvalue_rangeend const 260ms 01 00 mesosvalue_rangevalue_range code mesoscoalescevalue_ranges gets done a lot and ends up being really expensive the heaviest parts of the inverted call chain are code running time self ms symbol name 32090ms 157 32090 mesosvalue_rangevalue_range 32090ms 157 00 googleprotobufinternalgenerictypehandlermesosvalue_rangedeletemesosvalue_range 32090ms 157 00 void googleprotobufinternalrepeatedptrfieldbasedestroygoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandler 32090ms 157 00 googleprotobufrepeatedptrfieldmesosvalue_rangerepeatedptrfield 32090ms 157 00 googleprotobufrepeatedptrfieldmesosvalue_rangerepeatedptrfield 32090ms 157 00 mesosvalue_rangesvalue_ranges 32090ms 157 00 mesosvalue_rangesvalue_ranges 24410ms 119 00 mesoscoalescemesosvalue_ranges mesosvalue_range const 4520ms 22 00 mesosremovemesosvalue_ranges mesosvalue_range const 1690ms 08 00 mesosoperatormesosvalue_ranges const mesosvalue_ranges const 820ms 04 00 mesosoperatormesosvalue_ranges mesosvalue_ranges const 650ms 03 00 mesosvalue_rangesvalue_ranges 25410ms 124 25410 googleprotobufinternalgenerictypehandlermesosvalue_rangenew 25410ms 124 00 googleprotobufrepeatedptrfieldmesosvalue_rangetypehandlertype googleprotobufinternalrepeatedptrfieldbaseaddgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandler 23050ms 113 00 googleprotobufrepeatedptrfieldmesosvalue_rangeadd 23050ms 113 00 mesosvalue_rangesadd_range 19620ms 96 00 mesoscoalescemesosvalue_ranges mesosvalue_range const 3430ms 16 00 mesosrangesaddmesosvalue_ranges long long long long 2360ms 11 00 void googleprotobufinternalrepeatedptrfieldbasemergefromgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandlergoogleprotobufinternalrepeatedptrfieldbase const 14710ms 72 14710 googleprotobufinternalrepeatedptrfieldbasereserveint 13330ms 65 00 googleprotobufrepeatedptrfieldmesosvalue_rangetypehandlertype googleprotobufinternalrepeatedptrfieldbaseaddgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandler 13330ms 65 00 googleprotobufrepeatedptrfieldmesosvalue_rangeadd 13330ms 65 00 mesosvalue_rangesadd_range 10860ms 53 00 mesoscoalescemesosvalue_ranges mesosvalue_range const 2470ms 12 00 mesosrangesaddmesosvalue_ranges long long long long 1070ms 05 00 void googleprotobufinternalrepeatedptrfieldbasemergefromgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandlergoogleprotobufinternalrepeatedptrfieldbase const 1070ms 05 00 googleprotobufrepeatedptrfieldmesosvalue_rangemergefromgoogleprotobufrepeatedptrfieldmesosvalue_range const 1070ms 05 00 mesosvalue_rangesmergefrommesosvalue_ranges const 1050ms 05 00 mesosvalue_rangescopyfrommesosvalue_ranges const 1050ms 05 00 mesosvalue_rangesoperatormesosvalue_ranges const 1040ms 05 00 mesoscoalescemesosvalue_ranges mesosvalue_range const 10ms 00 00 mesosremovemesosvalue_ranges mesosvalue_range const 20ms 00 00 mesosresourcemergefrommesosresource const 20ms 00 00 googleprotobufinternalgenerictypehandlermesosresourcemergemesosresource const mesosresource 20ms 00 00 void googleprotobufinternalrepeatedptrfieldbasemergefromgoogleprotobufrepeatedptrfieldmesosresourcetypehandlergoogleprotobufinternalrepeatedptrfieldbase const 290ms 01 00 void googleprotobufinternalrepeatedptrfieldbasemergefromgoogleprotobufrepeatedptrfieldmesosresourcetypehandlergoogleprotobufinternalrepeatedptrfieldbase const 8980ms 44 8980 googleprotobufrepeatedptrfieldmesosvalue_rangetypehandlertype googleprotobufinternalrepeatedptrfieldbaseaddgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandler 5170ms 25 00 googleprotobufrepeatedptrfieldmesosvalue_rangeadd 5170ms 25 00 mesosvalue_rangesadd_range 4290ms 21 00 mesoscoalescemesosvalue_ranges mesosvalue_range const 880ms 04 00 mesosrangesaddmesosvalue_ranges long long long long 3790ms 18 00 void googleprotobufinternalrepeatedptrfieldbasemergefromgoogleprotobufrepeatedptrfieldmesosvalue_rangetypehandlergoogleprotobufinternalrepeatedptrfieldbase const code,8
master doesnt properly handle subscribe call mastersubscribe incorrectly handles reregistration it handles it as a registration request not reregistration because of a bug in the if loop should have been frameworkinfohas_id code void mastersubscribe const upid from const schedulercallsubscribe subscribe  const frameworkinfo frameworkinfo  subscribeframework_info  todovinod instead of calling reregisterframework from  here refactor those methods to call subscribe if frameworkinfohas_id  frameworkinfoid    registerframeworkfrom frameworkinfo  else  reregisterframeworkfrom frameworkinfo subscribeforce   code,2
ftp response code for success not recognized by fetcher the response code for successful http requests is 200 the response code for successful ftp file transfers is 226 the fetcher currently only checks for a response code of 200 even for ftp uris this results in failed fetching even though the resource gets downloaded successfully this has been found by a dedicated external test using an ftp server,1
expose docker container ip in masters statejson we want to expose docker container ip to mesosdns one potential solution is to make it available via masters statejson we can set a label dockernetworksettingsipaddress in taskstatus message when it is sent the first time with task_running status,2
add authorization for dynamic reservation dynamic reservations should be authorized with the principal of the reserving entity framework or master the idea is to introduce reserve and unreserve into the acl code message reserve   subjects required entity principals  1  objects mvp only possible values  any none required entity resources  1  message unreserve   subjects required entity principals  1  objects required entity reserver_principals  2  code when a frameworkoperator reserves resources reserve acls are checked to see if the framework frameworkinfoprincipal or the operator credentialuser is authorized to reserve the specified resources if not authorized the reserve operation is rejected when a frameworkoperator unreserves resources unreserve acls are checked to see if the framework frameworkinfoprincipal or the operator credentialuser is authorized to unreserve the resources reserved by a framework or operator resourcereservationinfoprincipal if not authorized the unreserve operation is rejected,2
add principal field to resourcediskinfopersistence in order to support authorization for persistent volumes we should add the principal to resourcediskinfo analogous to resourcereservationinfoprincipal,1
add framework authorization for persistent volume this is the third in a series of tickets that adds authorization support to persistent volumes when a framework creates a persistent volume create acls are checked to see if the framework frameworkinfoprincipal or the operator credentialuser is authorized to create persistent volumes if not authorized the create operation is rejected when a framework destroys a persistent volume destroy acls are checked to see if the framework frameworkinfoprincipal or the operator credentialuser is authorized to destroy the persistent volume created by a framework or operator resourcediskinfoprincipal if not authorized the destroy operation is rejected a separate ticket will use the structures created here to enable authorization of the create and destroy http endpoints httpsissuesapacheorgjirabrowsemesos3903,5
replicated registry needs a representation of maintenance schedules in order to persist maintenance schedules across failovers of the master the schedule information must be kept in the replicated registry this means adding an additional message in the registry protobuf in srcmasterregistryproto the status of each individual slaves maintenance will also be persisted in this way code message maintenance  message hoststatus  required string hostname  1  true if the slave is deactivated for maintenance  false if the slave is draining in preparation for maintenance required bool is_down  2  or an enum  message schedule   the set of affected slaves repeated hoststatus hosts  1  interval in which this set of slaves is expected to be down for optional unavailability interval  2  message schedules  repeated schedule schedules  optional schedules schedules  1  code note there can be multiple slaveids attached to a single hostname,3
implement a streaming response decoder for events stream we need a streaming response decoder to deserialize chunks sent from the master on the events stream from the http api design doc master encodes each event in recordio format ie a string representation of length of the event in bytes followed by json or binary protobuf possibly compressed encoded event as of now for getting the basic features right  this is being done in the testcases code auto reader  responsegetreader assert_somereader futurestdstring eventfuture  readergetread await_readyeventfuture event event eventparsefromstringeventfutureget code two things need to happen  we need master to emit events in recordio format ie event size followed by the serialized event instead of just the serialized events as is the case now  the decoder class should then abstract away the logic of reading the response and deserializing events from the stream ideally the decoder should work with both json and protobuf responses,3
registry operations are hardcoded for a single key registry object this is primarily a refactoring the prototype for modifying the registry is currently code trybool operator   registry registry hashsetslaveid slaveids bool strict code in order to support maintenance schedules possibly quotas as well there should be an alternate prototype for maintenance something like code trybool operation   maintenance maintenance bool strict code the existing registrarprocessupdate srcmasterregistrarcpp should be refactored to allow for more than one key if necessary refactor existing operations defined in srcmastermasterhpp adminslave readminslave removeslave,5
registry operations do not exist for manipulating maintanence schedules in order to modify the maintenance schedule in the replicated registry we will need operations srcmasterregistrarhpp the operations will likely correspond to the http api  updatemaintenanceschedule given a blob representing a maintenance schedule perform some verification on the blob write the blob to the registry  startmaintenance given a set of machines verify then transition machines from draining to deactivated  stopmaintenance given a set of machines verify then transition machines from deactivated to normal remove affected machines from the schedules,8
unify initialization of modularized components h1introduction as it stands right now default implementations of modularized components are required to have a non parametrized create static method this allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way for example with the interface foo code class foo  public virtual foo  virtual futureint hello  0 protected foo   code with a default implementation code class localfoo  public tryfoo create  return new foo  virtual futureint hello  return 1   code this allows to create typed tests which look as following code typedef testingtypeslocalfoo testsmodulefoo testlocalfoo footesttypes typed_test_casefootest footesttypes typed_testfootest atest  tryfoo foo  typeparamcreate assert_somefoo await_check_equalfoogethello 1  code the test will be applied to each of types in the template parameters of footesttypes this allows to test different implementation of an interface in our code it tests default implementations and a module which uses the same default implementation the class testsmoduletypename t moduleid n needs a little explanation it is a wrapper around modulemanager which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method the wrapper around create the real important method looks as follows code templatetypename t moduleid n static tryt testmodulet ncreate  trystdstring modulename  getmodulenamen if modulenameiserror  return errormodulenameerror  return mesosmodulesmodulemanagercreatetmodulenameget  code h1the problem consider the following implementation of foo code class parameterfoo  public tryfoo createint i  return new parameterfooi  parameterfooint i  i_i  virtual futureint hello  return i  private int i_  code as it can be seen this implementation cannot be used as a default implementation since its create api does not match the one of testmodule create has a different signature for both types it is still a common situation to require initialization parameters for objects however this constraint keeping both interfaces alike forces default implementations of modularized components to have default constructors therefore the tests are forcing the design of the interfaces implementations which are supposed to be used as modules only ie non default implementations are allowed to have constructor parameters since the actual signature of their factory method is this factory methods function is to decode the parameters and call the appropriate constructor code templatetypename t t moduletcreateconst parameters params code where parameters is just an array of keyvalue string pairs whose interpretation is left to the specific module sadly this call is wrapped by modulemanager which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules h1the ugly workaround with the requirement of a default constructor and parameters devoid create factory function a common pattern see authenticatorhttpsgithubcomapachemesosblob9d4ac11ed757aa5869da440dfe5343a61b07199aincludemesosauthenticationauthenticatorhpp has been introduced to feed construction parameters into default implementation this leads to adding an initialize call to the public interface which will have foo become code class foo  public virtual foo  virtual trynothing initializeoptionint i  0 virtual futureint hello  0 protected foo   code parameterfoo will thus look as follows code class parameterfoo  public tryfoo create  return new parameterfoo  parameterfoo  i_none  virtual trynothing initializeoptionint i  if iisnone  return errorneed value to initialize  i_  i return nothing  virtual futureint hello  if i_isnone  return futureintfailurenot initialized  return i_get  private optionint i_  code look that this initialize method now has to be implemented by all descendants of foo even if theres a databasefoo which takes is return value for hello from a db it will need to support int as an initialization parameter the problem is more severe the more specific the parameter to initialize is for example if there is a very complex structure implementing acls all implementations of an authorizer will need to import this structure even if they can completely ignore it in the foo example if parameterfoo were to become the default implementation of foo the tests would look as follows code typedef testingtypesparameterfoo testsmodulefoo testparameterfoo footesttypes typed_test_casefootest footesttypes typed_testfootest atest  tryfoo foo  typeparamcreate assert_somefoo int foovalue  1 foogetinitializefoovalue await_check_equalfoogethello foovalue  code,3
introduce http endpoints for quota we need to implement the http endpoints for quota as outlined in the design doc httpsdocsgooglecomdocumentd16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq9i,3
add capacity heuristic for quota requests in master we need to to validate quota requests in the mesos master as outlined in the design doc httpsdocsgooglecomdocumentd16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq9i this ticket aims to validate satisfiability in terms of available resources of a quota request using a heuristic algorithm in the mesos master rather than validating the syntax of the request,3
add labels to taskstatus and expose them via statejson this would allow the executors and slave modules to expose some metadata to frameworks and mesosdns via statejson a typical use case is to allow the containers to expose their ip to frameworkmesosdns,2
registry recovery does not recover the maintenance object persisted info is fetched from the registry when a master is elected or after failover currently this process involves 3 steps  fetch the registry  start an operation to add the new master to the fetched registry  check the success of the operation and finish recovering these methods can be found in srcmasterregistrarcpp coderegistrarprocessrecover _recover __recovercode since the maintenance schedule is stored in a separate key the recover process must also fetch a new maintenance object this object needs to be passed along to the master along with the existing registry object possible tests  srctestsregistrar_testscpp  change the recovery test to include checks for the new object,5
recovered resources are not reallocated until the next allocation delay currently when resources are recovered we do not perform an allocation for that slave rather we wait until the next allocation interval for small task high throughput frameworks this can have a significant impact on overall throughput see the following thread httpmarkmailorgthready6mzfwzlurv6nik3 we should consider immediately performing a reallocation for the slave upon resource recovery,5
sudo make distcheck fails on ubuntu 1404 and possibly other oses too running tests as root causes a large number of failures noformat  lsb_release a lsb version core20amd64core20noarchcore30amd64core30noarchcore31amd64core31noarchcore32amd64core32noarchcore40amd64core40noarchcore41amd64core41noarchcxx30amd64cxx30noarchcxx31amd64cxx31noarchcxx32amd64cxx32noarchcxx40amd64cxx40noarchcxx41amd64cxx41noarchdesktop31amd64desktop31noarchdesktop32amd64desktop32noarchdesktop40amd64desktop40noarchdesktop41amd64desktop41noarchgraphics20amd64graphics20noarchgraphics30amd64graphics30noarchgraphics31amd64graphics31noarchgraphics32amd64graphics32noarchgraphics40amd64graphics40noarchgraphics41amd64graphics41noarchlanguages32amd64languages32noarchlanguages40amd64languages40noarchlanguages41amd64languages41noarchmultimedia32amd64multimedia32noarchmultimedia40amd64multimedia40noarchmultimedia41amd64multimedia41noarchprinting32amd64printing32noarchprinting40amd64printing40noarchprinting41amd64printing41noarchqt431amd64qt431noarchsecurity40amd64security40noarchsecurity41amd64security41noarch distributor id ubuntu description ubuntu 14042 lts release 1404 codename trusty  sudo make j12 v0 check  712 tests from 116 test cases ran 318672 ms total  passed  676 tests  failed  36 tests listed below  failed  perfeventisolatortestroot_cgroups_sample  failed  usercgroupisolatortest2root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupsperfeventisolatorprocess  failed  slaverecoverytest0recoverslavestate where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0recoverstatusupdatemanager where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0reconnectexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0recoverunregisteredexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0recoverterminatedexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0recovercompletedexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0cleanupexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0removenoncheckpointingframework where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0noncheckpointingframework where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0killtask where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0reboot where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0gcexecutor where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0shutdownslave where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0shutdownslavesigusr1 where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0registerdisconnectedslave where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0reconcilekilltask where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0reconcileshutdownframework where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0reconciletasksmissingfromslave where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0schedulerfailover where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0partitionedslave where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0masterfailover where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0multipleframeworks where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0multipleslaves where typeparam  mesosinternalslavemesoscontainerizer  failed  slaverecoverytest0restartbeforecontainerizerlaunch where typeparam  mesosinternalslavemesoscontainerizer  failed  mesoscontainerizerslaverecoverytestresourcestatistics  failed  mesoscontainerizerslaverecoverytestcgroups_root_perfrollforward  failed  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespaceforward  failed  mesoscontainerizerslaverecoverytestcgroups_root_pidnamespacebackward  failed  cgroupsanyhierarchywithperfeventtestroot_cgroups_perf  failed  memorypressuremesostestcgroups_root_statistics  failed  memorypressuremesostestcgroups_root_slaverecovery  failed  nstestroot_setns  failed  perftestroot_events  failed  perftestroot_samplepid 36 failed tests noformat full log attached,2
perf related tests rely on cycles which might not always be present when running the tests on ubuntu 1404 the cycles value collected by perf is always 0 meaning certain tests always fail these lines in the test have been commented out for now and a todo has been attached which links to this jira issue since the solution is unclear in particular cycles might not properly be counted because it is a hardware counter and this particular machine was a virtual machine either way we should determine the best events to collect from perf in either vm or physical settings,5
doing clone on linux with the clone_newuser namespace type can drop root privileges the namespace tests attempt to clone a process with all namespaces that are available from the kernel which includes the user namespace in ubuntu 1404 which causes the child process to be user nobody instead of user root after invoking clone which is bad because the test requires that the child process is root and so things fail because of insufficient permissions for now we explicitly ignore the user namespace in the tests but this issue is to track exactly how we might want to manage this going forward,5
create cgroups taskskiller for non freeze subsystems we have a number of test issues when we cannot remove cgroups in case there are still related tasks running in cases where the freezer subsystem is not available in the current code httpsgithubcomapachemesosblob0221srclinuxcgroupscppl1728 we will fallback to a very simple mechnism of recursivly trying to remove the cgroups which fails if there are still tasks running therefore we need an additional nonfreezetaskskiller which doesnt rely on the freezer subsystem this problem caused issues when running sudo make check during 023 release testing where benh provided already a better error message with b1a23d6a52c31b8c5c840ab01902dbe00cb1feef  httpsreviewsapacheorgr36604,4
typos in oversubscription doc  in docsoversubscriptionmd there are three cases where revocable is written as recovable including the name of a json field noformat  grep nir recovable  docsoversubscriptionmd51with revocable resources further more recovable resources cannot be docsoversubscriptionmd95launching tasks using recovable resources is done through the existing docsoversubscriptionmd96launchtasks api revocable resources will have the recovable field set see noformat  also in docsoversubscriptionmd the last sentence doesnt make sense noformat to select custom a resource estimator and qos controller please refer to the modules documentationmodulesmd noformat maybe should say to select a custom or to install a custom,1
update scheduler driver to send subscribe call see mesos2913 for context,2
update scheduler library to send request call see mesos2913 for context from the dev list it looks like users depend on this call for their custom allocator so we need to support it going forward,2
configure jenkins to run docker tests add a jenkin job to run the docker tests,2
support https requests in libprocess in order to pull images from docker registries https calls are needed to securely communicate with the registry hosts currently only http requests are supported through libprocess now that ssl sockets are available through libprocess support for https can be added,3
poc running command executor with image provisioner this is to implement a poc of the alternative design choices with mesos3004,3
authentication for communicating with docker registry in order to pull docker images from docker hub and private docker registries the provisioner must support two primary authentication frameworks to authenticate with the registries basic authentication and the oauth20 authorization framework as per the docker registry spec a docker registry can also operate in standalone mode and may not require authentication,5
osspecific code touched by the containerizer tests is not windows compatible in the process of adding the cmake build system hausdorff noted and stubbed out all osspecific code that sweep mostly of libprocess and stout is here httpsgithubcomhausdorffmesoscommitb862f66c6ff58c115a009513621e5128cb734d52 instead of having inline if defined the osspecific code will be separated into directories the windows code will be stubbed out,13
implement windowscontainerizer and windowsdockercontainerizer the mvp for windows support is a containerizer that 1 runs on windows and 2 runs and passes all the tests that are relevant to the windows platform _eg_ not the tests that involve cgroups to do this we require at least a windowscontainerizer to be implemented alongside the mesoscontainerizer which provides no meaningful _eg_ process namespacing much like the default unix containerizer in the long term hopefully before mesoscon we want to support also the windows container api this will require implementing a separate containerizer maybe called windowsdockercontainerizer since the windows container api is actually officially supported through the docker interface _ie_ msft actually ported the docker engine to windows and that is the official api the interfaces like the fetcher shouldnt change much the tests probably will have to change as we dont have access to any isolation primitives like cgroups for those tests outstanding todohausdorff flesh out this description when more details are available regarding  the container api for windows when we know them  the nuances of windows vs linux when we know them  etc,13
validation of docker image manifests from docker registry docker image manifests pulled from remote docker registries should be verified against their signature digest before they are used,3
validation of docker layers pulled from docker registry docker layers should be verified against their checksum digests before they are stored to ensure the integrity of the docker layer content this includes supporting sha256 sha384 sha512 hash algorithms,3
standardize separation of windowslinuxspecific os code there are 50 files that must be touched to separate osspecific code first we will standardize the changes by using stoutaborthpp as an example the reviewdiscussion can be found here httpsreviewsapacheorgr36625,3
separate osspecific code in the stout library this issue tracks changes for all files under 3rdpartylibprocess3rdpartystout the changes will be based on this commit httpsgithubcomhausdorffmesoscommitb862f66c6ff58c115a009513621e5128cb734d52diffa6d038bad64b154996452bec020cfa7c,5
separate osspecific code in the libprocess library this issue tracks changes for all files under 3rdpartylibprocessinclude and 3rdpartylibprocesssrc the changes will be based on this commit httpsgithubcomhausdorffmesoscommitb862f66c6ff58c115a009513621e5128cb734d52diffa6d038bad64b154996452bec020cfa7c,5
port flag generation logic from the autotools solution to cmake one major barrier to widespread adoption of the cmakebased build system other than the fact that we havent implemented it yet is that most of our institutional knowledge of the quirks of how to build mesos across many platforms is tied up in files like configureac therefore a good cmakebased build system will require us to go through these files systematically and manually port this logic to cmake as well as testing it,3
extend cmake build system to support building against thirdparty libraries from either the system or the local mesos rebundling currently mesos has thirdparty dependencies of two types 1 those that are expected to be on the system such as apr libsvn _etc_ and 2 those that have been historically bundled as tarballs inside the mesos repository and are not expected to be on the system when mesos is installed these are located in the 3rdparty directory and includes things like boost and glog for type 2 the mvp of the cmakebased build system will always pull down a fresh tarball from an external source instead of using the bundled tarballs in the 3rdparty folder however many ci systems do not have internet access so in the long term we need to provide many options for getting these dependencies,5
define cmake style guide the short story is that it is important to be principled about how the cmake build system is maintained because there cmake language makes it difficult to statically verify that a configuration is correct it is not unique in this regard but make is arguably even worse but it is something thats important to make sure we get right the longer story is cmakes language is dynamically scoped and often has somewhat odd defaults for variable values _eg_ iirc target names passed to externalproject_add default to prefix instead of erroring out this means that it is rare to get a configurationtime error _ie_ cmake usually doesnt say something like hey this variable isnt defined and in large projects this can make it very difficult to know where definitions come from or whether its important that one config routine runs before another dynamic scoping also makes it particularly easy to write spaghetti code which is clearly undesirable for something as important as a build system thus it is particularly important that we lay down our expectations for how the cmake system is to be structured this might include  function naming _eg_ making it easy to tell whether a function was defined by us and where it was defined so we might say that we want our functions to have an underscore to start and start with the package the come from like libprocess so that we know where to look for the definition  what assertions we want to check variable values against so that we can replace subtle errors _eg_ a library is accidentally named something silly like prefix001 with an obvious ones _eg_ you have failed to define your target name so cmake has defaulted to prefix please check your configuration routines  decisions of what goes where _eg_ the most complex parts of the cmake mvps is in the configuration routines like mesosconfigurecmake to curb this we should have strict rules about what goes in that file vs other files and how we know what is to be run before what part of this should probably be prominent comments explaining the structure of the project so that people arent confused  and so on,3
add autotoolsstyle mesos distributions to the cmake build system in the autoconfbased build system we there is a notion of building a distribution of mesos essentially it is a version of mesos that is configured for a specific platform ubuntu say so if a consumer knows their platform and there is a mesos distribution they need only run make all and mesos builds this allows the consumer to skip the configure step in cmake it should be possible to do this should be and we should explore making it work after we complete the mvp,3
expand cmake build system to support building the containerizer and associated components in other tasks in epic mesos898 we implement a cmakebased build system that allows us to build process library the process tests and the stout tests for the cmake build system mvp its important that we expand this to build the containerizer associated modules and all related tests,3
harden the cmake systemdependencylocating routines currently the mesos project has two flavors of dependency 1 the dependencies we expect are already on the system _eg_ apr libsvn and 2 the dependencies that are historically bundled with mesos _eg_ glog dependency type 1 requires solid modules that will locate them on any system linux bsd or windows this would come for free if we were using cmake 30 but were using cmake 28 so that ubuntu users can install it out of the box instead of upgrading cmake first this is additionally useful for dependency type 2 where we will expect to have to use these routines when we support both the rebundled dependencies in the 3rdparty folder and system installations of those dependencies,3
fetcher should perform cache eviction based on cache file usage patterns currently the fetcher uses a trivial strategy to select eviction victims it picks the first cache file it finds in linear iteration this means that potentially a file that has just been used gets evicted the next moment this performance loss can be avoided by even the simplest enhancement of the selection procedure proposed approach determine an effective yet relatively uncomplex and quick algorithm and implement it in fetcherprocesscacheselectvictimsconst bytes requiredspace suggestion approximate mruretention somehow unittest what actually happens,8
add resource usage section to containerizer documentation currently the containerizer documentation doesnt touch upon the usage api and how to interpret the collected statistics,3
simplify json by providing jsonify along the lines of stringify we want to be able to do things like code jsonvalue number1  25 jsonnumber number2  26 expect_nenumber1 number2 expect_eqjsonify12 number1 expect_eqjsonify12 number2 code,3
convert mesosslavelimitationexecutorrunstate into protobufs published rr httpsreviewsapacheorgr36718,1
pass executorinfo argument into isolatorisolate some isolators need to lookup the executor environment variables to customize their isolation needs currently one has to use the prepare call to cache the executorinfo to use it later during isolate call,2
pass containerid into slaveexecutorenvironmentdecorator hook,1
remove pthread specific code from stout,3
remove pthread specific code from libprocess,3
remove pthread specific code from mesos,3
always disable sslv2 the ssl protocol mismatch tests are failing on centos7 when matching sslv2 with sslv2 since this version of the protocol is highly discouraged anyway lets disable it completely unless requested otherwise,2
add configurable unimplemented macro to stout during the transition to support for windows it would be great if we had the ability to use a macro that marks functions as unimplemented to support being able to find all the unimplemented functions easily at compile time while also being able to run the tests at the same time we can add a configuration flag that controls whether this macro aborts or expands to a static assertion,2
updating persistent volumes after slave restart is problematic just realize that while reviewing httpsreviewsapacheorgr34135 since we dont checkpoint resources in mesos containerizer when slave restarts and recovers the resources in container struct will be empty but there are symlinks exists in the sandbox well end up with trying to create already exist symlinks and fail i think we should ignore the creation if it already exists,3
docker_host env variable stopped working for executors with httpsreviewsapacheorgr36282 no environment variables are available anymore in the docker executors hence setting docker_host outside of mesos stopped working setups which use a remote docker daemon or tools like powerstrip stopped working,2
improve task reconciliation documentation include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary,1
move all mesoscontainerizer related files under srcslavecontainerizermesos currently some mesoscontainerizer specific files are not in the correct location for example noformat srcslavecontainerizerisolators srcslavecontainerizerprovisionerhppcpp noformat they should be put under srcslavecontainerizermesos,2
custom isolators should implement isolator instead of isolatorprocess similar to mesos2213 we should not restrict custom isolators to use libprocess process we should do a similar refactor as we did for mesos2213,3
master should send heartbeats on the subscription connection in order to deal with network partitions and ensuring network intermediately do not close the persistent subscription connection master must periodically send heartbeats the expectation with schedulers is that they resubscribe when they do not receive heartbeats for some time,3
allow slave to forward messages through the master for http schedulers the master currently has no install handler for executortoframework messages and the slave directly sends these messages to the scheduler driver bypassing the master entirely we need to preserve this behavior for the driver but http schedulers will not have a libprocess pid well have to ensure that the runtaskmessage and updateframeworkmessage have an optional pid for now the master will continue to set the pid but 0240 slaves will know to send messages through the master when the pid is not available,5
isolatorprepare should return executor environment vars as well sometimes the isolators need to pass on some environment variables for the executor that is being launched for example to successfully launch an executor inside a network namespace one needs to set libprocess_ip to point to the container ip otherwise the executor tries to bind to the slave ip which may be invalid inside the namespace another example is where the file system isolator should be able to specify the work_dir depending on if a new rootfs is used,2
port bootstrap to cmake bootstrap does a lot of significant things like setting up the git commit hooks we will want something like bootstrap to run also on systems that dont have bash  ideally this should just run in cmake itself,5
publish masterinfo to zk using json following from mesos2340 which now allows master to correctly decode json information masterinfo published to zookeeper we can now enable the master leader contender to serialize it too in json,2
persistentvolumetestslaverecovery test fails on osx with a clean build make clean running this tests fails code gtest_filterpersistentvolumetest make check code this is the log noformat  running 7 tests from 1 test case  global test environment setup  7 tests from persistentvolumetest  run  persistentvolumetestsendingcheckpointresourcesmessage  ok  persistentvolumetestsendingcheckpointresourcesmessage 189 ms  run  persistentvolumetestresourcescheckpointing  ok  persistentvolumetestresourcescheckpointing 86 ms  run  persistentvolumetestpreparepersistentvolume  ok  persistentvolumetestpreparepersistentvolume 82 ms  run  persistentvolumetestmasterfailover  ok  persistentvolumetestmasterfailover 130 ms  run  persistentvolumetestincompatiblecheckpointedresources  ok  persistentvolumetestincompatiblecheckpointedresources 74 ms  run  persistentvolumetestaccesspersistentvolume i0723 112140265787 1955922688 execcpp132 version 0240 i0723 112140268676 174858240 execcpp206 executor registered on slave 2015072311214016777343618582866s0 e0723 112140268697 178077696 sockethpp173 shutdown failed on fd18 socket is not connected 57 e0723 112140273510 178077696 sockethpp173 shutdown failed on fd18 socket is not connected 57 registered executor on localhost starting task 39e32f2b475e47549e3d39fd56fb787b forked command at 2911 sh c echo abc  path1file e0723 112140281900 178077696 sockethpp173 shutdown failed on fd18 socket is not connected 57 command exited with status 0 pid 2911 e0723 112140389068 178077696 sockethpp173 shutdown failed on fd18 socket is not connected 57  ok  persistentvolumetestaccesspersistentvolume 421 ms  run  persistentvolumetestslaverecovery i0723 112140639749 1955922688 execcpp132 version 0240 i0723 112140641904 187400192 execcpp206 executor registered on slave 2015072311214016777343618582866s0 e0723 112140641943 191156224 sockethpp173 shutdown failed on fd18 socket is not connected 57 e0723 112140646507 191156224 sockethpp173 shutdown failed on fd18 socket is not connected 57 registered executor on localhost starting task 809fa50fbee04c9ba770434183a9650b sh c while true do test d path1 done forked command at 2941 e0723 112140655097 191156224 sockethpp173 shutdown failed on fd18 socket is not connected 57 i0723 112140671840 186863616 execcpp252 received reconnect request from slave 2015072311214016777343618582866s0 e0723 112140671953 191156224 sockethpp173 shutdown failed on fd18 socket is not connected 57 i0723 112140672744 187400192 execcpp229 executor reregistered on slave 2015072311214016777343618582866s0 e0723 112140672839 191156224 sockethpp173 shutdown failed on fd18 socket is not connected 57 reregistered executor on localhost srctestspersistent_volume_testscpp709 failure value of status2getstate actual task_failed expected task_killed  failed  persistentvolumetestslaverecovery 286 ms  7 tests from persistentvolumetest 1268 ms total  global test environment teardown  7 tests from 1 test case ran 1289 ms total  passed  6 tests  failed  1 test listed below  failed  persistentvolumetestslaverecovery 1 failed test you have 8 disabled tests noformat,2
incorporate cmake into standard documentation right now its anyones guess how to build with cmake if we want people to use it we should put up documentation the central challenge is that the cmake instructions will be slightly different for different platforms for example on linux the gist of the build is basically the same as autotools you pull down the system dependencies like apr _etc_ and then  bootstrap mkdir buildcmake  cd buildcmake cmake  make  but on windows it will be somewhat more complicated there is no bootstrap step for example because windows doesnt have bash natively and even when we put that in youll still have to build the glog stuff outofband because cmake has no way of booting up visual studio and calling build so practically we need to figure out  what our build story is for different platforms  write specific instructions for our core target platforms,13
implement docker remote puller given a docker image name and registry host url fetches the image if necessary it will download the manifest and layers from the registry host it will place the layers and image manifest into persistent store done when a docker image can be successfully stored and retrieved using put and get methods,5
compiler warning when mocking function type has an enum return type the purpose of this ticket is to document a very cryptic error message actually a warning that gets propagated by werror that gets generated by clang35 from gmock source code when trying to mock a perfectly innocentlooking function h3 problem the following code is attempting to mock a mesosexecutordriver code class mockmesosexecutordriver  public mesosexecutordriver  public mockmesosexecutordrivermesosexecutor executor  mesosexecutordriverexecutor  mock_method1sendstatusupdate statusconst taskstatus  code the above code generates the following error message noformat in file included from 3rdpartylibprocess3rdpartygmock160includegmockgmockh58 in file included from 3rdpartylibprocess3rdpartygmock160includegmockgmockactionsh46 3rdpartylibprocess3rdpartygmock160includegmockinternalgmockinternalutilsh35510 error indirection of nonvolatile null pointer will be deleted not trap werrorwnulldereference return static_casttypename remove_referencettype__null  3rdpartylibprocess3rdpartygmock160includegmockgmockactionsh7822 note in instantiation of function template specialization testinginternalinvalidmesosstatus requested here return internalinvalidt  3rdpartylibprocess3rdpartygmock160includegmockgmockactionsh19043 note in instantiation of member function testinginternalbuiltindefaultvaluemesosstatusget requested here internalbuiltindefaultvaluetget  value_  3rdpartylibprocess3rdpartygmock160includegmockgmockspecbuildersh143534 note in instantiation of member function testingdefaultvaluemesosstatusget requested here return defaultvalueresultget  3rdpartylibprocess3rdpartygmock160includegmockgmockspecbuildersh133422 note in instantiation of member function testinginternalfunctionmockerbasemesosstatus const mesostaskstatus performdefaultaction requested here func_mockerperformdefaultactionargs call_description  3rdpartylibprocess3rdpartygmock160includegmockgmockspecbuildersh144826 note in instantiation of function template specialization testinginternalactionresultholdermesosstatusperformdefaultactionmesosstatus const mesostaskstatus  requested here return resultholderperformdefaultactionthis args call_description  3rdpartylibprocess3rdpartygmock160includegmockgmockgeneratedfunctionmockersh817 note in instantiation of member function testinginternalfunctionmockerbasemesosstatus const mesostaskstatus untypedperformdefaultaction requested here class functionmockerra1  public  3rdpartylibprocess3rdpartygmock160includegmockinternalgmockinternalutilsh35510 note consider using __builtin_trap or qualifying pointer with volatile return static_casttypename remove_referencettype__null  noformat the source of the issue here is that status is an enum in gmock160includegmockinternalgmockinternalutilsh you can find the following function code template typename t t invalid  return static_casttypename remove_referencettypenull  code this function gets called with the return type of a mocked function in our case the return type of the mocked function is status attempting to compile the following minimal example with clang35 reproduces the error message code include type_traits template typename t t invalid  return static_casttypename stdremove_referencettype nullptr  enum e  a b  int main  invalide  code  see it online on gcc explorerhttpsgooglt1fepz note that if the type is not an enum the warning is not generated this is why existing mocked functions that return nonenum types such as futurevoid does not encounter this issue h3 solutions the simplest solution is to add wnonulldeference to mesos_tests_cppflags in srcmakefileam code mesos_tests_cppflags  mesos_cppflags wnonulldereference code another solution is to upgrade gmock from 16 to 17 because this problem is solved in the newer versions in gmock 17 code template typename t inline t invalid  return const_casttypename remove_referencettype static_castvolatile typename remove_referencettypenull  code add volatile could avoid this warning httpsgooglopcilc,3
as a developer i want a better way to run shell commands when reviewing the code in r36425httpsreviewsapacheorgr36425 benjaminhindman noticed that there is a better abstraction that is possible to introduce for osshell that will simplify the callers life instead of having to handle all possible outcomes we propose to refactor osshell as follows code   returns the output from running the specified command with the shell  trystdstring shellconst string command   actually handle the wifexited wifsignaled here  code where the returned string is stdout and should the program be signaled or exit with a nonzero exit code we will simply return a failure with an error message that will encapsulate both the returnedsignaled state and possibly stderr and some test driven development code expect_errorosshellfalse expect_someosshelltrue expect_some_eqhello world osshellecho hello world code alternatively the caller can ask to have stderr conflated with stdout code trystring outanderr  osshellmycmd foo 21 code however stderr will be ignored by default code  we dont read standard error by default expect_some_eq osshellecho hello world 12  we dont even read stderr if something fails to return in tryerror trystring output  osshellecho hello world 12  false expect_erroroutput expect_falsestringscontainsoutputerror hello world code an analysis of existing usage shows that in almost all cases the caller only cares if not error in fact the actual exit code is read only once and even then in a test case we believe this will simplify the api to the caller and will significantly reduce the length and complexity at the calling sites 6 loc against the current 20,2
disable endpoints rule fails to recognize http path delegates in mesos one can use the flag firewall_rules to disable endpoints disabled endpoints will return a _403 forbidden_ response whenever someone tries to access endpoints libprocess support adding one default delegate for endpoints which is the default process id which handles endpoints if no process id was given for example the default id of the master libprocess process is master which is also set as the delegate for the master system process so a request to the endpoint httpmasteraddress5050statejson will effectively be resolved by httpmasteraddress5050masterstatejson but if one disables statejson because of how delegates work it can still access masterstatejson the only workaround is to disabled both enpoints,2
update homebrew formula for mesos mac osx we have pushed a pull requesthttpsgithubcomhomebrewhomebrewpull42099 to homebrew for the new 023 formula once accepted we must verify that this works on a mac osx device this would also be a great time to ensure our documentation is uptodate currently the homebrew check fails as they have deprecated sha1 checksums noformat error message failed brew audit mesos stacktrace error 7 problems in 1 formula mesos  stable resource protobuf sha1 checksums are deprecated please use sha256  stable resource pythongflags sha1 checksums are deprecated please use sha256  stable resource six sha1 checksums are deprecated please use sha256  stable resource googleapputils sha1 checksums are deprecated please use sha256  stable resource pythondateutil sha1 checksums are deprecated please use sha256  stable resource boto sha1 checksums are deprecated please use sha256  stable resource pytz sha1 checksums are deprecated please use sha256 noformat dont know enough about homebrew to really figure out what is going on here nor how to fix this the mesos sha256 has been correctly entered and computed via the online shamd5 calculatorhttpsmd5filecomcalculator i guess we should go download the packages and compute their sha256 andor research from the respective download sites whether they publish the sha,1
using a unresolvable hostname crashes the framework on registration the following commands trigger the crash noformat  sudo hostname foo  an unresolvable hostname  sudo binmesosmastersh ip127001 work_dirvarlibmesos  libprocess_ip127001 srcmesosexecute master1270015050 namebar commandwhile true do sleep 100 done noformat the crash output noformat warning logging before initgooglelogging is written to stderr w0724 142039960733 1925993216 schedcpp1487  scheduler driver bound to loopback interface cannot communicate with remote masters you might want to set libprocess_ip environment variable to use a routable ip address  abort 3rdpartylibprocess3rdpartystoutincludestouttryhpp85 tryget but state  error nodename nor servname provided or not known1 24560 abort libprocess_ip127001 srcmesosexecute master1270015050 noformat,1
add a new api call to the allocator to update available resources this ticket is to track the updateavailable api call being added to the allocator which updates the available resources in the allocator its used for master endpoints for dynamic reservation and persistent volumes updateavailable is similar to updateslave except that updateavailable never leaves the allocator in an overallocated state,8
resolve issue with hanging tests with zookeeper see mesos2736 for the original issue the submitted reviewhttpsreviewsapacheorgr36663 currently has no tests the one posted in the subsequent r3687httpsreviewsapacheorgr36807 currently hangs when ran after the other test_fmasterzookeepertest lostzookeepercluster the issue is around the await in startmaster clusterhpp 430 that waits indefinitely for the master recovery,1
need for http delete requests as we decided to create a more restful api for managing quota request therefore we also want to use the http delete request and hence need to enable the libprocesshttp to send delete request besides get and post requests,1
add tests for https ssl socket communication unit tests are lacking for the following cases 1 https post with none payload 2 verification of https payload on the ssl socketmaybe decode to a request object 3 http  ssl socket 4 https  raw socket,3
enable mesos agent node to use arbitrary script  module to figure out ip hostname following from mesos2902 we want to enable the same functionality in the mesos agents too this is probably best done once we implement the new osshell semantics as described in mesos3142,1
libprocess process join runqueue workers during finalization the lack of synchronization between processmanager destruction and the thread pool threads running the queued processes means that the shared state that is part of the processmanager gets destroyed prematurely synchronizing the processmanager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction,3
document using the gold linker for faster development on linux the gold linkerhttpsenwikipediaorgwikigold_linker seems to provide a decent speedup about 20 on a parallel build from a quick test noformat titletimings for make check j24 gtest_filter w 24 hyperthreaded cores gold real 7m18526s user 81m21213s sys 5m17224s default ld real 9m7908s user 85m13466s sys 5m52199s noformat on centos 5 w devtoolset2 noformat sudo usrsbinalternatives altdir optrhdevtoolset2rootetcalternatives admindir optrhdevtoolset2rootvarlibalternatives set ld optrhdevtoolset2rootusrbinldgold noformat on ubuntu noformat sudo updatealternatives install usrbinld ld usrbingold 1 noformat ideally we could this out on the website with instructions for each os,3
provide a means to check http connection equality for streaming connections if one uses an httppipewriter to stream a response one cannot compare the writer with another to see if the connection has changed this is useful for example in the masters http api when there is asynchronous disconnection logic when we handle the disconnection its possible for the scheduler to have resubscribed and so the master needs to tell if the disconnection event is relevant for the current connection before taking action,3
proper handling of query andor fragment out of path in http handler the libprocess httpcpp postget handlers currently do not consider query and fragments parts of the path correctly eg code if pathissome   todobenh get query andor fragment out of path urlpath  stringsjoin urlpath pathget  code,1
introduce quotainfo message a quotainfo protobuf message is internal representation for quota related information eg for persisting quota the protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals it may also be used to pass quota information to allocators,3
persist and recover quota tofrom registry to persist quotas across failovers the master should save them in the registry to support this we shall  introduce a quota state variable in registryproto  extend the operation interface so that it supports a quota accumulator see srcmasterregistrarhpp  introduce addquota  removequota operations  recover quotas from the registry on failover to the masters internalmasterrole struct  extend registrartest with quotaspecific tests note registry variable can be rather big for production clusters see mesos2075 while it should be fine for mvp to add quota information to registry we should consider storing quota separately as this does not need to be in sync with slaves update however currently adding more variable is not supported by the registrar while the agents are reregistering note they may fail to do so the information about what part of the quota is allocated is only partially available to the master in other words the state of the quota allocation is reconstructed as agents reregister during this period some roles may be under quota from the perspective of the newly elected master the same problem exists on the allocator side it may think the cluster is under quota and may eagerly try to satisfy quotas before enough agents reregister which may result in resources being allocated to frameworks beyond their quota to address this issue and also to avoid panicking and generating under quota alerts the master should give a certain amount of time for the majority eg 80 of the agents to reregister before reporting any quota status and notifying the allocator about granted quotas,5
design doc for docker image registry client create design document for the docker registry authenticator component so that we have a baseline for the implementation,3
design doc for versioning the http api in concert with the release of the http api we would also like to come up with a versioning strategy this enables to do a meaningful 10 release,3
mesoszookeepertest fixture can have side effects across tests mesoszookeepertest fixture doesnt restart the zookeeper server for each test this means if a test shuts down the zookeeper server the next test using the same fixture might fail for an example see httpsreviewsapacheorgr36807,2
frameworkinfo should only be updated if the reregistration is valid see ben mahlers comment in httpsreviewsapacheorgr32961 frameworkinfo should not be updated if the reregistration is invalid this can happen in a few cases under the branching logic so this requires some refactoring notice that a codeframeworkerrormessagecode can be generated both inside codeelse if from  frameworkpidcode as well as from inside codefailoverframeworkframework fromcode,2
fetcher tests use expect while subsequent logic relies on the outcome the fetcher tests use expect validation for critical measures eg nonempty results and the subsequent logic releis on this ie by accessing the first element in such cases we should use assertcheck,1
mark pathbasename pathdirname as const functions the functions pathbasename and pathdirname in stoutpathhpp are not marked const although they could marking them const would remove some ambiguities in the usage of these functions,1
fetcher logs erroneous message when successfully extracting an archive when fetching an asset while not using the cache the fetcher may erroneously report this copying instead of extracting resource from uri with extract flag because it does not seem to be an archive  this message appears in the stderr log in the sandbox no matter whether extraction succeeded or not it should be absent after successful extraction,1
perform a self bind mount of rootfs itself in fschrootenter syscall pivot_root requires that the old and the new root are not in the same filesystem otherwise the user will receive a device or resource busy error currently we rely on the provisioner to prepare the rootfs and do proper bind mount if needed so that pivot_root can succeed the drawback of this approach is that it potentially pollutes the host mount table which requires cleanup logics for instance in the test we create a test rootfs by copying the host files we need to do a self bind mount so that we can pivot_root on it that pollute the host mount table and it might leak mounts if test crashes before we do the lazy umount httpsgithubcomapachemesosblobmastersrctestscontainerizerlaunch_testscppl96l102 what i propose is that we always perform a recursive self bind mount of rootfs itself in fschrootenter after enter the new mount namespace seems that this is also done in libcontainer httpsgithubcomopencontainersruncblobmasterlibcontainerrootfs_linuxgol402,2
create a test abstraction for preparing test rootfs several tests need this abstraction so its better to unify them for example srctestscontainerizerlaunch_testscpp needs to create a test rootfs we also need that to test filesystem isolators the test rootfs can be created by copying filesdirectories from host file system,3
make masterregisterframework and masterreregisterframework call into mastersubscribe currently mastersubscribe calls into masterregisterframework and masterreregisterframework we should do it the other way around to be consistent with how we did all the other calls,3
documentation images do not load any images which are referenced from the generated docs docsmd do not show up on the website for example  architecturehttpmesosapacheorgdocumentationlatestarchitecture  external containerizerhttpmesosapacheorgdocumentationlatestexternalcontainerizer  fetcher cache internalshttpmesosapacheorgdocumentationlatestfetchercacheinternals  maintenancehttpmesosapacheorgdocumentationlatestmaintenance  oversubscriptionhttpmesosapacheorgdocumentationlatestoversubscription,3
refactor subprocess logic in linuxperfcpp to use common subroutine mesos2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions in order to achieve this it requires to execute the perf version command we should decompose the existing subcommand processing in perf so that we can share the implementation between the multiple uses of perf,3
timetestnow fails with enablelibevent  run  timetestnow 3rdpartylibprocesssrcteststime_testscpp50 failure expected microseconds10  clocknow  t1 actual 8byte object 1027 0000 0000 0000 vs 0ns  failed  timetestnow 0 ms,2
implement a utility for computing hash it is useful for both appc and docker to compute and verify image hash,2
containerinfoimageappcid should be optional as i commented here httpsreviewsapacheorgr34136 currently containerinfoimageappc is defined as the following noformattitle message appc  required string name  1 required string id  2 optional labels labels  3  noformat in which the id is a required field when users specify the image in tasks they likely will not use an image id much like when you use docker or rkt to launch containers you often use ubuntu or ubuntulatest and seldom a sha512 id and we should change it to be optional the motivating scenario is that if the frameworks in the mesos use something like imageubuntu1404 to run a task and imageubuntu defaults to imageubuntulatest the operator can swap the latest version for all new tasks requesting imageubuntu if they allow users to specify imageubuntulive they can swap the live version under the covers as well this allows the operator to release important image updates eg security patches and have it picked up by new tasks in the cluster without asking the users to update their jobtask configs,1
implement appc image discovery appc spec specifies two image discovery mechanisms simple and meta discovery we need to have an abstraction for image discovery in appcstore for mvp we can implement the simple discovery first httpsreviewsapacheorgr34139,2
implement a readonly appc image store its going to be derived from this httpsreviewsapacheorgr34140 and other related patches but in the initial readonly version the stores content is prepared by outofband mechanisms so the store component in mesos only needs to provide access to images already in it and recover images upon slave restart this greatly simplifies the initial versions responsibility and test cases features that fetch the images into the store will be added later and they will take into consideration its impact on task start latency and slave restart responsiveness etc,5
fix master metrics for scheduler calls currently the master increments metrics for old style messages from the driver but not when it receives calls since the driver is now sending calls master should update metrics correctly,3
always set taskstatusexecutor_id when sending a status update message from executor currently the executor doesnt always set taskstatusexecutor_id this prevents the slave taskstatus label decorator hook from knowing the executor id an appropriate place to automatically fill in the executor_id is executorprocesssendstatusupdate since we are already filling in some other information here,1
memisolatortest01memusage fails on os x looks like this is due to mlockall being unimplemented on os x noformat  1 test from memisolatortest0 where typeparam  n5mesos8internal5slave23posixmemisolatorprocesse  run  memisolatortest0memusage failed to allocate rss memory failed to make pages to be mapped unevictable function not implementedsrctestscontainerizerisolator_testscpp812 failure helperincreaserssallocation failed to sync with the subprocess srctestscontainerizerisolator_testscpp815 failure usagefailure failed to get usage no process found at 40558  failed  memisolatortest0memusage where typeparam  n5mesos8internal5slave23posixmemisolatorprocesse 56 ms  1 test from memisolatortest0 57 ms total  1 test from memisolatortest1 where typeparam  n5mesos8internal5tests6moduleins_5slave8isolatorelns1_8moduleide0eee  run  memisolatortest1memusage failed to allocate rss memory failed to make pages to be mapped unevictable function not implementedsrctestscontainerizerisolator_testscpp812 failure helperincreaserssallocation failed to sync with the subprocess srctestscontainerizerisolator_testscpp815 failure usagefailure failed to get usage no process found at 40572  failed  memisolatortest1memusage where typeparam  n5mesos8internal5tests6moduleins_5slave8isolatorelns1_8moduleide0eee 50 ms  1 test from memisolatortest1 50 ms total noformat,2
validate quota requests we need to validate quota requests in terms of syntactical and semantical correctness,3
remove unused fatal and fatalerror macros there exist fatal and fatalerror macros in both libprocess and stout none of them are currently used as we favor glogs logfatal and therefore should be removed,1
libev handle_async can deadlock with run_in_event_loop due to the arbitrary nature of the functions that are executed in handle_async invoking them under the a watchers_mutex can lead to deadlocks if b is acquired before calling run_in_event_loop and b is also acquired within the arbitrary function code 82679 thread 10 lock order 0x60774f8 before 0x60768c0 violated 82679 82679 observed incorrect order is acquisition of lock at 0x60768c0 82679 at 0x4c32145 pthread_mutex_lock in usrlibvalgrindvgpreload_helgrindamd64linuxso 82679 by 0x692c9b __gthread_mutex_lockpthread_mutex_t gthrdefaulth748 82679 by 0x6950bf stdmutexlock mutex134 82679 by 0x696219 synchronizedstdmutex synchronizestdmutexstdmutexlambdastdmutex1operatorstdmutex const synchronizedhpp58 82679 by 0x696238 synchronizedstdmutex synchronizestdmutexstdmutexlambdastdmutex1_funstdmutex synchronizedhpp58 82679 by 0x6984cf synchronizedstdmutexsynchronizedstdmutex void stdmutex void stdmutex synchronizedhpp35 82679 by 0x6962de synchronizedstdmutex synchronizestdmutexstdmutex synchronizedhpp60 82679 by 0x728fe1 processhandle_asyncev_loop ev_async int libevcpp48 82679 by 0x761384 ev_invoke_pending evc2994 82679 by 0x7643c4 ev_run evc3394 82679 by 0x728e37 ev_loop evh826 82679 by 0x729469 processeventlooprun libevcpp135 82679 82679 followed by a later acquisition of lock at 0x60774f8 82679 at 0x4c32145 pthread_mutex_lock in usrlibvalgrindvgpreload_helgrindamd64linuxso 82679 by 0x4c6f9d __gthread_mutex_lockpthread_mutex_t gthrdefaulth748 82679 by 0x4c6fed __gthread_recursive_mutex_lockpthread_mutex_t gthrdefaulth810 82679 by 0x4f5d3d stdrecursive_mutexlock mutex175 82679 by 0x516513 synchronizedstdrecursive_mutex synchronizestdrecursive_mutexstdrecursive_mutexlambdastdrecursive_mutex1operatorstdrecursive_mutex const synchronizedhpp58 82679 by 0x516532 synchronizedstdrecursive_mutex synchronizestdrecursive_mutexstdrecursive_mutexlambdastdrecursive_mutex1_funstdrecursive_mutex synchronizedhpp58 82679 by 0x52e619 synchronizedstdrecursive_mutexsynchronizedstdrecursive_mutex void stdrecursive_mutex void stdrecursive_mutex synchronizedhpp35 82679 by 0x5165d4 synchronizedstdrecursive_mutex synchronizestdrecursive_mutexstdrecursive_mutex synchronizedhpp60 82679 by 0x6bf4e1 processprocessmanageruseprocessupid const processcpp2127 82679 by 0x6c2b8c processprocessmanagerterminateprocessupid const bool processprocessbase processcpp2604 82679 by 0x6c6c3c processterminateprocessupid const bool processcpp3107 82679 by 0x692b65 processlatchtrigger latchcpp53 code this was introduced in httpsgithubcomapachemesoscommit849fc4d361e40062073324153ba97e98e294fdf2,3
masterauthorizationtestduplicateregistration test is flaky  run  masterauthorizationtestduplicateregistration using temporary directory tmpmasterauthorizationtest_duplicateregistration_nkt3f7 i0804 221601578500 26185 leveldbcpp176 opened db in 2188338ms i0804 221601579172 26185 leveldbcpp183 compacted db in 645075ns i0804 221601579211 26185 leveldbcpp198 created db iterator in 15766ns i0804 221601579227 26185 leveldbcpp204 seeked to beginning of db in 1658ns i0804 221601579238 26185 leveldbcpp273 iterated through 0 keys in the db in 313ns i0804 221601579282 26185 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0804 221601579787 26212 recovercpp449 starting replica recovery i0804 221601580075 26212 recovercpp475 replica is in empty status i0804 221601581014 26205 replicacpp641 replica in empty status received a broadcasted recover request i0804 221601581357 26211 recovercpp195 received a recover response from a replica in empty status i0804 221601581761 26207 recovercpp566 updating replica status to starting i0804 221601582334 26218 mastercpp377 master 2015080422160125501413565930226185 d6d349cd895b started on 17217015259302 i0804 221601582355 26218 mastercpp379 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 credentialstmpmasterauthorizationtest_duplicateregistration_nkt3f7credentials framework_sorterdrf helpfalse initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0240_instsharemesoswebui work_dirtmpmasterauthorizationtest_duplicateregistration_nkt3f7master zk_session_timeout10secs i0804 221601582711 26218 mastercpp424 master only allowing authenticated frameworks to register i0804 221601582722 26218 mastercpp429 master only allowing authenticated slaves to register i0804 221601582728 26218 credentialshpp37 loading credentials for authentication from tmpmasterauthorizationtest_duplicateregistration_nkt3f7credentials i0804 221601582929 26204 leveldbcpp306 persisting metadata 8 bytes to leveldb took 421543ns i0804 221601582950 26204 replicacpp323 persisted replica status to starting i0804 221601583032 26218 mastercpp468 using default crammd5 authenticator i0804 221601583132 26211 recovercpp475 replica is in starting status i0804 221601583154 26218 mastercpp505 authorization enabled i0804 221601583356 26214 whitelist_watchercpp79 no whitelist given i0804 221601583411 26217 hierarchicalhpp346 initialized hierarchical allocator process i0804 221601583976 26213 replicacpp641 replica in starting status received a broadcasted recover request i0804 221601584187 26209 recovercpp195 received a recover response from a replica in starting status i0804 221601584581 26213 mastercpp1495 the newly elected leader is master17217015259302 with id 2015080422160125501413565930226185 i0804 221601584609 26213 mastercpp1508 elected as the leading master i0804 221601584627 26213 mastercpp1278 recovering from registrar i0804 221601584656 26204 recovercpp566 updating replica status to voting i0804 221601584770 26212 registrarcpp313 recovering registrar i0804 221601585261 26218 leveldbcpp306 persisting metadata 8 bytes to leveldb took 370526ns i0804 221601585285 26218 replicacpp323 persisted replica status to voting i0804 221601585412 26216 recovercpp580 successfully joined the paxos group i0804 221601585667 26216 recovercpp464 recover process terminated i0804 221601586047 26213 logcpp661 attempting to start the writer i0804 221601587164 26211 replicacpp477 replica received implicit promise request with proposal 1 i0804 221601587549 26211 leveldbcpp306 persisting metadata 8 bytes to leveldb took 358261ns i0804 221601587568 26211 replicacpp345 persisted promised to 1 i0804 221601588173 26209 coordinatorcpp230 coordinator attemping to fill missing position i0804 221601589316 26208 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0804 221601589700 26208 leveldbcpp343 persisting action 8 bytes to leveldb took 351778ns i0804 221601589721 26208 replicacpp679 persisted action at 0 i0804 221601590698 26213 replicacpp511 replica received write request for position 0 i0804 221601590754 26213 leveldbcpp438 reading position from leveldb took 31557ns i0804 221601591147 26213 leveldbcpp343 persisting action 14 bytes to leveldb took 321842ns i0804 221601591167 26213 replicacpp679 persisted action at 0 i0804 221601591790 26217 replicacpp658 replica received learned notice for position 0 i0804 221601592133 26217 leveldbcpp343 persisting action 16 bytes to leveldb took 315281ns i0804 221601592155 26217 replicacpp679 persisted action at 0 i0804 221601592180 26217 replicacpp664 replica learned nop action at position 0 i0804 221601592686 26211 logcpp677 writer started with ending position 0 i0804 221601593729 26205 leveldbcpp438 reading position from leveldb took 26394ns i0804 221601596165 26209 registrarcpp346 successfully fetched the registry 0b in 11343104ms i0804 221601596281 26209 registrarcpp445 applied 1 operations in 26242ns attempting to update the registry i0804 221601598415 26212 logcpp685 attempting to append 178 bytes to the log i0804 221601598563 26215 coordinatorcpp340 coordinator attempting to write append action at position 1 i0804 221601599324 26215 replicacpp511 replica received write request for position 1 i0804 221601599778 26215 leveldbcpp343 persisting action 197 bytes to leveldb took 420523ns i0804 221601599800 26215 replicacpp679 persisted action at 1 i0804 221601600349 26204 replicacpp658 replica received learned notice for position 1 i0804 221601600684 26204 leveldbcpp343 persisting action 199 bytes to leveldb took 310315ns i0804 221601600706 26204 replicacpp679 persisted action at 1 i0804 221601600723 26204 replicacpp664 replica learned append action at position 1 i0804 221601601632 26213 registrarcpp490 successfully updated the registry in 5287936ms i0804 221601601747 26213 registrarcpp376 successfully recovered registrar i0804 221601601826 26215 logcpp704 attempting to truncate the log to 1 i0804 221601601948 26210 coordinatorcpp340 coordinator attempting to write truncate action at position 2 i0804 221601602145 26208 mastercpp1305 recovered 0 slaves from the registry 139b  allowing 10mins for slaves to reregister i0804 221601602859 26219 replicacpp511 replica received write request for position 2 i0804 221601603181 26219 leveldbcpp343 persisting action 16 bytes to leveldb took 284713ns i0804 221601603209 26219 replicacpp679 persisted action at 2 i0804 221601603984 26211 replicacpp658 replica received learned notice for position 2 i0804 221601604313 26211 leveldbcpp343 persisting action 18 bytes to leveldb took 302445ns i0804 221601604365 26211 leveldbcpp401 deleting 1 keys from leveldb took 29354ns i0804 221601604387 26211 replicacpp679 persisted action at 2 i0804 221601604408 26211 replicacpp664 replica learned truncate action at position 2 i0804 221601616402 26185 schedcpp164 version 0240 i0804 221601616902 26209 schedcpp262 new master detected at master17217015259302 i0804 221601617000 26209 schedcpp318 authenticating with master master17217015259302 i0804 221601617019 26209 schedcpp325 using default crammd5 authenticatee i0804 221601617324 26212 authenticateecpp115 creating new client sasl connection i0804 221601617550 26209 mastercpp4405 authenticating schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601617641 26212 authenticatorcpp406 starting authentication session for crammd5_authenticatee25917217015259302 i0804 221601617858 26208 authenticatorcpp92 creating new server sasl connection i0804 221601618140 26216 authenticateecpp206 received sasl authentication mechanisms crammd5 i0804 221601618191 26216 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0804 221601618324 26213 authenticatorcpp197 received sasl authentication start i0804 221601618413 26213 authenticatorcpp319 authentication requires more steps i0804 221601618557 26216 authenticateecpp252 received sasl authentication step i0804 221601618664 26216 authenticatorcpp225 received sasl authentication step i0804 221601618703 26216 auxpropcpp102 request to lookup properties for user testprincipal realm d6d349cd895b server fqdn d6d349cd895b sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0804 221601618719 26216 auxpropcpp174 looking up auxiliary property userpassword i0804 221601618778 26216 auxpropcpp174 looking up auxiliary property cmusaslsecretcrammd5 i0804 221601618820 26216 auxpropcpp102 request to lookup properties for user testprincipal realm d6d349cd895b server fqdn d6d349cd895b sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0804 221601618834 26216 auxpropcpp124 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0804 221601618839 26216 auxpropcpp124 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0804 221601618857 26216 authenticatorcpp311 authentication success i0804 221601618954 26219 authenticateecpp292 authentication success i0804 221601619035 26204 mastercpp4435 successfully authenticated principal testprincipal at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601619083 26219 authenticatorcpp424 authentication session cleanup for crammd5_authenticatee25917217015259302 i0804 221601619309 26208 schedcpp407 successfully authenticated with master master17217015259302 i0804 221601619335 26208 schedcpp713 sending subscribe call to master17217015259302 i0804 221601619494 26208 schedcpp746 will retry registration in 439203ns if necessary i0804 221601619627 26217 mastercpp1812 received subscribe call for framework default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601619695 26217 mastercpp1534 authorizing framework principal testprincipal to receive offers for role  i0804 221601620848 26217 schedcpp713 sending subscribe call to master17217015259302 i0804 221601620929 26217 schedcpp746 will retry registration in 2099193326secs if necessary i0804 221601621036 26210 mastercpp1812 received subscribe call for framework default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601621083 26210 mastercpp1534 authorizing framework principal testprincipal to receive offers for role  i0804 221601621727 26217 mastercpp1876 subscribing framework default with checkpointing disabled and capabilities   i0804 221601621981 26208 schedcpp262 new master detected at master17217015259302 i0804 221601622131 26208 schedcpp318 authenticating with master master17217015259302 i0804 221601622153 26208 schedcpp325 using default crammd5 authenticatee i0804 221601622323 26212 authenticateecpp115 creating new client sasl connection i0804 221601622324 26210 hierarchicalhpp391 added framework 20150804221601255014135659302261850000 i0804 221601622369 26210 hierarchicalhpp1008 no resources available to allocate i0804 221601622386 26210 hierarchicalhpp908 performed allocation for 0 slaves in 28592ns i0804 221601622511 26210 schedcpp640 framework registered with 20150804221601255014135659302261850000 i0804 221601622586 26210 schedcpp654 schedulerregistered took 48005ns i0804 221601622592 26208 mastercpp4405 authenticating schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601622673 26212 authenticatorcpp406 starting authentication session for crammd5_authenticatee26017217015259302 i0804 221601622923 26205 authenticatorcpp92 creating new server sasl connection i0804 221601623112 26204 authenticateecpp206 received sasl authentication mechanisms crammd5 i0804 221601623133 26216 mastercpp1870 dropping subscribe call for framework default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 reauthentication in progress i0804 221601623144 26204 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0804 221601623258 26215 authenticatorcpp197 received sasl authentication start i0804 221601623313 26215 authenticatorcpp319 authentication requires more steps i0804 221601623394 26215 authenticateecpp252 received sasl authentication step i0804 221601623512 26212 authenticatorcpp225 received sasl authentication step i0804 221601623546 26212 auxpropcpp102 request to lookup properties for user testprincipal realm d6d349cd895b server fqdn d6d349cd895b sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0804 221601623564 26212 auxpropcpp174 looking up auxiliary property userpassword i0804 221601623603 26212 auxpropcpp174 looking up auxiliary property cmusaslsecretcrammd5 i0804 221601623622 26212 auxpropcpp102 request to lookup properties for user testprincipal realm d6d349cd895b server fqdn d6d349cd895b sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0804 221601623631 26212 auxpropcpp124 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0804 221601623636 26212 auxpropcpp124 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0804 221601623649 26212 authenticatorcpp311 authentication success i0804 221601623777 26212 authenticateecpp292 authentication success i0804 221601623846 26212 mastercpp4435 successfully authenticated principal testprincipal at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221601623913 26212 authenticatorcpp424 authentication session cleanup for crammd5_authenticatee26017217015259302 i0804 221601624130 26212 schedcpp407 successfully authenticated with master master17217015259302 i0804 221602583772 26218 hierarchicalhpp1008 no resources available to allocate i0804 221602583818 26218 hierarchicalhpp908 performed allocation for 0 slaves in 80538ns i0804 221603585110 26211 hierarchicalhpp1008 no resources available to allocate i0804 221603585156 26211 hierarchicalhpp908 performed allocation for 0 slaves in 69272ns i0804 221604586539 26214 hierarchicalhpp1008 no resources available to allocate i0804 221604586586 26214 hierarchicalhpp908 performed allocation for 0 slaves in 79232ns i0804 221605587239 26209 hierarchicalhpp1008 no resources available to allocate i0804 221605587293 26209 hierarchicalhpp908 performed allocation for 0 slaves in 85128ns i0804 221606587935 26212 hierarchicalhpp1008 no resources available to allocate i0804 221606587985 26212 hierarchicalhpp908 performed allocation for 0 slaves in 78141ns i0804 221607588817 26214 hierarchicalhpp1008 no resources available to allocate i0804 221607588865 26214 hierarchicalhpp908 performed allocation for 0 slaves in 81433ns i0804 221608589857 26214 hierarchicalhpp1008 no resources available to allocate i0804 221608589906 26214 hierarchicalhpp908 performed allocation for 0 slaves in 71929ns i0804 221609591085 26207 hierarchicalhpp1008 no resources available to allocate i0804 221609591133 26207 hierarchicalhpp908 performed allocation for 0 slaves in 78223ns i0804 221610591737 26207 hierarchicalhpp1008 no resources available to allocate i0804 221610591785 26207 hierarchicalhpp908 performed allocation for 0 slaves in 71894ns i0804 221611593166 26210 hierarchicalhpp1008 no resources available to allocate i0804 221611593221 26210 hierarchicalhpp908 performed allocation for 0 slaves in 89782ns i0804 221612593647 26212 hierarchicalhpp1008 no resources available to allocate i0804 221612593689 26212 hierarchicalhpp908 performed allocation for 0 slaves in 69426ns i0804 221613594154 26210 hierarchicalhpp1008 no resources available to allocate i0804 221613594202 26210 hierarchicalhpp908 performed allocation for 0 slaves in 70581ns i0804 221614594712 26207 hierarchicalhpp1008 no resources available to allocate i0804 221614594758 26207 hierarchicalhpp908 performed allocation for 0 slaves in 71201ns i0804 221615595412 26219 hierarchicalhpp1008 no resources available to allocate i0804 221615595464 26219 hierarchicalhpp908 performed allocation for 0 slaves in 85183ns i0804 221616596201 26217 hierarchicalhpp1008 no resources available to allocate i0804 221616596247 26217 hierarchicalhpp908 performed allocation for 0 slaves in 95132ns srctestsmaster_authorization_testscpp794 failure failed to wait 15secs for frameworkregisteredmessage i0804 221616624354 26212 mastercpp966 framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 disconnected i0804 221616624398 26212 mastercpp2092 disconnecting framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221616624445 26212 mastercpp2116 deactivating framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221616624686 26212 mastercpp988 giving framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 0ns to failover i0804 221616625641 26219 hierarchicalhpp474 deactivated framework 20150804221601255014135659302261850000 i0804 221616626688 26218 mastercpp4180 framework failover timeout removing framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221616626734 26218 mastercpp4759 removing framework 20150804221601255014135659302261850000 default at schedulerac5e7b68e2d2441ca5f560c1ff8cf00c17217015259302 i0804 221616627074 26218 mastercpp858 master terminating i0804 221616627218 26215 hierarchicalhpp428 removed framework 20150804221601255014135659302261850000 3rdpartylibprocessincludeprocessgmockhpp365 failure actual function call count doesnt match expect_callfiltermock filtertestingaconst messageevent expected args message matcher 8byte object 9898 02ac 542b 0000 1byte object 97 1byte object d2 expected to be call,1
no need to checkpoint container root filesystem path given the design discussed in mesos3004httpsissuesapacheorgjirabrowsemesos3004 one container might have multiple provisioned root filesystems only checkpointing the root filesystem for containerinfoimage does not make sense also we realized that checkpointing container root filesystem path is not necessary because each provisioner should be able to destroy root filesystems for a given container based on a canonical directory layout eg appc_rootfs_dircontainer_idxxx,3
c style guide is not rendered correctly code section syntax disregarded some paragraphs at the bottom of docsmesoscstyleguidemd containing code sections are not rendered correctly by the web site generator it looks fine in a github gist and apparently the syntax used is correct,1
fetch checksum files to inform fetcher cache use this is the first part of phase 1 as described in the comments for mesos2073 we add a field to commandinfouri that contains the uri of a checksum file when this file has new content then the contents of the associated value uri needs to be refreshed in the fetcher cache in this implementation step we just add the above basic functionality download checksum comparison in later steps we will add more control flow to cover corner cases and thus make this feature more useful,3
as a python developer i want a simple way to obtain information about master from zookeeper with the new json masterinfo published to zk we want to provide a simple library class for python developers to retrieve info about the masters and the leader,2
as a java developer i want a simple way to obtain information about master from zookeeper with the new json masterinfo published to zk we want to provide a simple library class for java framework developers to retrieve info about the masters and the leader,2
design doc for docker registry token manager create design document for describing the component and interaction between docker registry client and remote docker registry for token based authorization,2
cgroupsanyhierarchywithperfeventtest failing on ubuntu 1404  run  cgroupsanyhierarchywithperfeventtestroot_cgroups_perf srctestscontainerizercgroups_testscpp172 failure cgroupsdestroyhierarchy cgroupfailure failed to remove cgroup sysfscgroupperf_eventmesos_test device or resource busy srctestscontainerizercgroups_testscpp190 failure cgroupsdestroyhierarchy cgroupfailure failed to remove cgroup sysfscgroupperf_eventmesos_test device or resource busy  failed  cgroupsanyhierarchywithperfeventtestroot_cgroups_perf 9 ms  1 test from cgroupsanyhierarchywithperfeventtest 9 ms total,3
implement docker registry client implement the following functionality  fetch manifest from remote registry based on authorization method dictated by the registry  fetch image layers from remote registry based on authorization method dictated by the registry,5
implement token manager for docker registry implement the following  a component that fetches json web authorization token from a given registry  caches the token keyed on registry service and scope  validates the cache for expiry date nice to have  cache gets pruned as tokens are aged beyond expiration time,4
some variables in versionhpp use type var instead of type var some variables in 3rdpartylibprocess3rdpartystoutincludestoutversionhpp violate mesos code style of biding  and  to the type name as opposed to binding to the variable name,1
introduce an either type we currently dont have an abstraction in stout to capture the notion of having a container with many types and a single value for example in our abstractions like try rather than being able to say eithererror value t we must encode two options optionerror optiont with the implicit invariant that exactly one will be set this also comes in handy in many other places in the code note that we have the ability to 1 use c11 unions now as well as 2 use boosts variant directly instead of introducing either however creating a named union every time this is needed is verbose and unions require that we externally track which member is set for variant we already use this eg jsonhpp but we can benefit from the better naming as either many languages expose either as having only two values left and right id propose making this two or more as is the case with variant,5
implement image chroot support into command executor,3
fetchercachehttptesthttpcachedserialized and fetchercachehttptesthttpcachedconcurrent are flaky on osx make clean  make j8 v0 check code  3 tests from fetchercachehttptest  run  fetchercachehttptesthttpcachedserialized http11 200 ok date fri 07 aug 2015 172305 gmt contentlength 30 i0807 102305673596 2085372672 execcpp133 version 0240 e0807 102305675884 184373248 sockethpp173 shutdown failed on fd18 socket is not connected 57 i0807 102305675897 182226944 execcpp207 executor registered on slave 201508071023051393950825233852313s0 e0807 102305683980 184373248 sockethpp173 shutdown failed on fd18 socket is not connected 57 registered executor on 100798 starting task 0 forked command at 54363 sh c mesosfetchertestcmd 0 e0807 102305694953 184373248 sockethpp173 shutdown failed on fd18 socket is not connected 57 command exited with status 0 pid 54363 e0807 102305793927 184373248 sockethpp173 shutdown failed on fd18 socket is not connected 57 i0807 102306590008 2085372672 execcpp133 version 0240 e0807 102306592244 355938304 sockethpp173 shutdown failed on fd18 socket is not connected 57 i0807 102306592243 353255424 execcpp207 executor registered on slave 201508071023051393950825233852313s0 e0807 102306597995 355938304 sockethpp173 shutdown failed on fd18 socket is not connected 57 registered executor on 100798 starting task 1 forked command at 54411 sh c mesosfetchertestcmd 1 e0807 102306608708 355938304 sockethpp173 shutdown failed on fd18 socket is not connected 57 command exited with status 0 pid 54411 e0807 102306707649 355938304 sockethpp173 shutdown failed on fd18 socket is not connected 57 srctestsfetcher_cache_testscpp860 failure failed to wait 15secs for awaitfinishedtaskget  aborted at 1438968214 unix time try date d 1438968214 if you are using gnu date   failed  fetchercachehttptesthttpcachedserialized 28685 ms  run  fetchercachehttptesthttpcachedconcurrent pc  0x113723618 processownedget  sigsegv 0x0 received by pid 52313 tid 0x118d59000 stack trace   0x7fff8fcacf1a _sigtramp  0x7f9bc3109710 unknown  0x1136f07e2 mesosinternalslavefetcherfetch  0x113862f9d mesosinternalslavemesoscontainerizerprocessfetch  0x1138f1b5d _zzn7process8dispatchi7nothingn5mesos8internal5slave25mesoscontainerizerprocesserkns2_11containeriderkns2_11commandinfoerknst3__112basic_stringicnsc_11char_traitsiceensc_9allocatoriceeeerk6optionisi_erkns2_7slaveides6_s9_si_sm_sp_eens_6futureit_eerkns_3pidit0_eemsw_fsu_t1_t2_t3_t4_t5_et6_t7_t8_t9_t10_enkulpns_11processbaseee_cles1d_  0x1138f18cf _znst3__110__function6__funcizn7process8dispatchi7nothingn5mesos8internal5slave25mesoscontainerizerprocesserkns5_11containeriderkns5_11commandinfoerkns_12basic_stringicns_11char_traitsiceens_9allocatoriceeeerk6optionisk_erkns5_7slaveides9_sc_sk_so_sr_eens2_6futureit_eerkns2_3pidit0_eemsy_fsw_t1_t2_t3_t4_t5_et6_t7_t8_t9_t10_eulpns2_11processbaseee_nsi_is1g_eefvs1f_eecleos1f_  0x1143768cf std__1functionoperator  0x11435ca7f processprocessbasevisit  0x1143ed6fe processdispatcheventvisit  0x1127aaaa1 processprocessbaseserve  0x114343b4e processprocessmanagerresume  0x1143431ca processinternalschedule  0x1143da646 _znst3__114__thread_proxyins_5tupleijpfvveeeeeepvs5_  0x7fff95090268 _pthread_body  0x7fff950901e5 _pthread_start  0x7fff9508e41d thread_start failed to synchronize with slave its probably exited make3  checklocal segmentation fault 11 make2  checkam error 2 make1  check error 2 make  checkrecursive error 1 code this was encountered just once out of 3 make checks,2
updated slave task label decorator hook to pass in executorinfo if that task being launched has a command executor there is no way for the hook to determine the executorid for that task the executorid is sometimes required by the label decorators for accounting purposes and for preparing ground for executorenvironmentdecorator which is not passed the taskinfo,1
http requests with nested path are not properly handled by libprocess for example if master adds a route apiv1scheduler a handler named apiv1scheduler is added to master libprocess but when a request is posted to the above path processvisit looks for a http handler named api instead of apiv1scheduler ideally libprocess should look for handlers in the following preference order apiv1scheduler  apiv1  api,2
httpget api evaluates host wrongly currently libprocess http api sets the host header field from the peer socket address ipport the problem is that socket address might not be right http server and might be just a proxy,1
ignore no statistics condition for containers with no qdisc in portmappingstatisticsexecute we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container code failed to get the network statistics for the htb qdisc on eth0 failed to get the network statistics for the fq_codel qdisc on eth0 code this can occur because of an error reading the qdisc statistics function return an error or because the qdisc does not exist function returns none we should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting we do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created,2
cgroup check fails test harness check in clean up of containerizertest causes test harness to abort rather than fail or skip only perf related tests  run  slaverecoverytest0restartbeforecontainerizerlaunch  ok  slaverecoverytest0restartbeforecontainerizerlaunch 628 ms  24 tests from slaverecoverytest0 38986 ms total  4 tests from mesoscontainerizerslaverecoverytest  run  mesoscontainerizerslaverecoverytestresourcestatistics srctestsmesoscpp720 failure cgroupsmounthierarchy subsystem perf_event is already attached to another hierarchy  we cannot run any cgroups tests that require a hierarchy with subsystem perf_event because we failed to find an existing hierarchy or create a new one tried tmpmesos_test_cgroupperf_event you can either remove all existing hierarchies or disable this test case ie gtest_filtermesoscontainerizerslaverecoverytest  f0811 172343874696 12955 mesoscpp774 check_somecgroups tmpmesos_test_cgroupperf_event is not a valid hierarchy  check failure stack trace   0x7fb2fb4835fd googlelogmessagefail  0x7fb2fb48543d googlelogmessagesendtolog  0x7fb2fb4831ec googlelogmessageflush  0x7fb2fb485d39 googlelogmessagefatallogmessagefatal  0x4e3f98 _checkfatal_checkfatal  0x82f25a mesosinternaltestscontainerizertestteardown  0xc030e3 testinginternalhandleexceptionsinmethodifsupported  0xbf9050 testingtestrun  0xbf912e testingtestinforun  0xbf9235 testingtestcaserun  0xbf94e8 testinginternalunittestimplrunalltests  0xbf97a4 testingunittestrun  0x4a9df3 main  0x7fb2f9371ec5 unknown  0x4b63ee unknown build step execute shell marked build as failure,2
httptestnestedget is flaky  run  httptestnestedget 3rdpartylibprocesssrctestshttp_testscpp459 failure value of responsegetstatus actual 202 accepted expected httpstatuses200 which is 200 ok  aborted at 1439569965 unix time try date d 1439569965 if you are using gnu date  pc  0x63abe8 testingunittestaddtestpartresult  sigsegv 0x0 received by pid 25766 tid 0x7f499415c780 from pid 0 stack trace   0x7f499224dca0 unknown  0x63abe8 testingunittestaddtestpartresult  0x62f6af testinginternalasserthelperoperator  0x43cd78 httptest_nestedget_testtestbody  0x65935e testinginternalhandlesehexceptionsinmethodifsupported  0x653c5e testinginternalhandleexceptionsinmethodifsupported  0x6349a3 testingtestrun  0x635128 testingtestinforun  0x635778 testingtestcaserun  0x63c0e2 testinginternalunittestimplrunalltests  0x65a11d testinginternalhandlesehexceptionsinmethodifsupported  0x654958 testinginternalhandleexceptionsinmethodifsupported  0x63ae08 testingunittestrun  0x4877f9 run_all_tests  0x487613 main  0x7f49915739f4 __libc_start_main,2
starting maintenance needs to deactivate agents and kill tasks after using the maintenancestart endpoint to begin maintenance on a machine agents running on said machine should  be deactivated such that no offers are sent from that agent investigate if masterdeactivateslave can be used or modified for this purpose  kill all tasks still running on the agent see mesos1475  prevent other agents on that machine from registering or sending out offers this will likely involve some modifications to masterregister and masterreregister,8
stoppingcompleting maintenance needs to reactivate agents after using the maintenancestop endpoint to end maintenance on a machine any deactivated agents must be reactivated and allowed to register with the master,5
json serializationdeserialization of bytes is incorrect currently we use our own serialization of bytes in jsonhpp but we use picojson for deserialization weve observed that for some bytes the serialization results in a string that is incorrectly decoded by picojson example string  bfnrtx00x19 x7fxff result of our own encoding bfnrtu0000u0019 u007fxff picojsons encoding bfnrtu0000u0019 u007fu00ff fix we just use picojson to serialize bytes for consistency,2
eventcall test framework is flaky observed this on asf ci ht haosdentgmailcom looks like the http scheduler never sent a subscribe request to the master code  run  examplestesteventcallframework using temporary directory tmpexamplestest_eventcallframework_k4vxkx i0813 195515643579 26085 execcpp443 ignoring exited event because the driver is aborted shutting down sending sigterm to process tree at pid 26061 killing the following process trees   shutting down sending sigterm to process tree at pid 26062 shutting down killing the following process trees   sending sigterm to process tree at pid 26063 killing the following process trees   shutting down sending sigterm to process tree at pid 26098 killing the following process trees   shutting down sending sigterm to process tree at pid 26099 killing the following process trees   warning logging before initgooglelogging is written to stderr i0813 195517161726 26100 processcpp1012 libprocess is initialized on 1721721060249 for 16 cpus i0813 195517161888 26100 loggingcpp177 logging to stderr i0813 195517163625 26100 schedulercpp157 version 0240 i0813 195517175302 26100 leveldbcpp176 opened db in 3167446ms i0813 195517176393 26100 leveldbcpp183 compacted db in 1047996ms i0813 195517176496 26100 leveldbcpp198 created db iterator in 77155ns i0813 195517176518 26100 leveldbcpp204 seeked to beginning of db in 8429ns i0813 195517176527 26100 leveldbcpp273 iterated through 0 keys in the db in 4219ns i0813 195517176708 26100 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0813 195517178951 26136 recovercpp449 starting replica recovery i0813 195517179934 26136 recovercpp475 replica is in empty status i0813 195517181970 26126 mastercpp378 master 201508131955171679077566024926100 297daca2d01a started on 1721721060249 i0813 195517182317 26126 mastercpp380 flags at startup aclspermissive false register_frameworks  principals  type some values testprincipal  roles  type some values    run_tasks  principals  type some values testprincipal  users  type some values mesos    allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_slavesfalse authenticatorscrammd5 credentialstmpexamplestest_eventcallframework_k4vxkxcredentials framework_sorterdrf helpfalse initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout5secs registry_strictfalse root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0240srcwebui work_dirtmpmesosii8gua zk_session_timeout10secs i0813 195517183475 26126 mastercpp427 master allowing unauthenticated frameworks to register i0813 195517183536 26126 mastercpp432 master allowing unauthenticated slaves to register i0813 195517183615 26126 credentialshpp37 loading credentials for authentication from tmpexamplestest_eventcallframework_k4vxkxcredentials w0813 195517183859 26126 credentialshpp52 permissions on credentials file tmpexamplestest_eventcallframework_k4vxkxcredentials are too open it is recommended that your credentials file is not accessible by others i0813 195517183969 26123 replicacpp641 replica in empty status received a broadcasted recover request i0813 195517184306 26126 mastercpp469 using default crammd5 authenticator i0813 195517184661 26126 authenticatorcpp512 initializing server sasl i0813 195517185104 26138 recovercpp195 received a recover response from a replica in empty status i0813 195517185972 26100 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i0813 195517186058 26135 recovercpp566 updating replica status to starting i0813 195517187001 26138 leveldbcpp306 persisting metadata 8 bytes to leveldb took 654586ns i0813 195517187037 26138 replicacpp323 persisted replica status to starting i0813 195517187499 26134 recovercpp475 replica is in starting status i0813 195517187605 26126 auxpropcpp66 initialized inmemory auxiliary property plugin i0813 195517187710 26126 mastercpp506 authorization enabled i0813 195517188657 26138 replicacpp641 replica in starting status received a broadcasted recover request i0813 195517188853 26131 hierarchicalhpp346 initialized hierarchical allocator process i0813 195517189252 26132 whitelist_watchercpp79 no whitelist given i0813 195517189321 26134 recovercpp195 received a recover response from a replica in starting status i0813 195517190001 26125 recovercpp566 updating replica status to voting i0813 195517190696 26124 leveldbcpp306 persisting metadata 8 bytes to leveldb took 357331ns i0813 195517190775 26124 replicacpp323 persisted replica status to voting i0813 195517190970 26133 recovercpp580 successfully joined the paxos group i0813 195517192183 26129 recovercpp464 recover process terminated i0813 195517192699 26123 slavecpp190 slave started on 11721721060249 i0813 195517192741 26123 slavecpp191 flags at startup authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0240_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resource_monitoring_interval1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirtmpmesosii8gua0 i0813 195517194514 26100 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i0813 195517194658 26123 slavecpp354 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i0813 195517194854 26123 slavecpp384 slave hostname 297daca2d01a i0813 195517194877 26123 slavecpp389 slave checkpoint true i0813 195517196751 26132 mastercpp1524 the newly elected leader is master1721721060249 with id 201508131955171679077566024926100 i0813 195517196797 26132 mastercpp1537 elected as the leading master i0813 195517196815 26132 mastercpp1307 recovering from registrar i0813 195517197032 26138 registrarcpp311 recovering registrar i0813 195517197845 26132 slavecpp190 slave started on 21721721060249 i0813 195517198420 26125 logcpp661 attempting to start the writer i0813 195517197948 26132 slavecpp191 flags at startup authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0240_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resource_monitoring_interval1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirtmpmesosii8gua1 i0813 195517199121 26132 slavecpp354 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i0813 195517199235 26138 statecpp54 recovering state from tmpmesosii8gua0meta i0813 195517199322 26132 slavecpp384 slave hostname 297daca2d01a i0813 195517199345 26132 slavecpp389 slave checkpoint true i0813 195517199676 26100 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i0813 195517200085 26135 statecpp54 recovering state from tmpmesosii8gua1meta i0813 195517200317 26132 status_update_managercpp202 recovering status update manager i0813 195517200371 26129 status_update_managercpp202 recovering status update manager i0813 195517202003 26129 replicacpp477 replica received implicit promise request with proposal 1 i0813 195517202585 26131 slavecpp190 slave started on 31721721060249 i0813 195517202596 26129 leveldbcpp306 persisting metadata 8 bytes to leveldb took 523191ns i0813 195517202756 26129 replicacpp345 persisted promised to 1 i0813 195517202770 26132 containerizercpp379 recovering containerizer i0813 195517203061 26135 containerizercpp379 recovering containerizer i0813 195517202663 26131 slavecpp191 flags at startup authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0240_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resource_monitoring_interval1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirtmpmesosii8gua2 i0813 195517203819 26131 slavecpp354 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i0813 195517203930 26131 slavecpp384 slave hostname 297daca2d01a i0813 195517203948 26131 slavecpp389 slave checkpoint true i0813 195517204674 26137 statecpp54 recovering state from tmpmesosii8gua2meta i0813 195517205178 26135 status_update_managercpp202 recovering status update manager i0813 195517205323 26135 containerizercpp379 recovering containerizer i0813 195517205521 26136 slavecpp4069 finished recovery i0813 195517206074 26136 slavecpp4226 querying resource estimator for oversubscribable resources i0813 195517206424 26128 slavecpp4069 finished recovery i0813 195517206722 26137 status_update_managercpp176 pausing sending status updates i0813 195517206858 26136 slavecpp684 new master detected at master1721721060249 i0813 195517206902 26138 slavecpp4069 finished recovery i0813 195517206962 26128 slavecpp4226 querying resource estimator for oversubscribable resources i0813 195517208312 26134 schedulercpp272 new master detected at master1721721060249 i0813 195517208364 26136 slavecpp709 no credentials provided attempting to register without authentication i0813 195517208608 26136 slavecpp720 detecting new master i0813 195517208839 26138 slavecpp4226 querying resource estimator for oversubscribable resources i0813 195517209216 26123 coordinatorcpp231 coordinator attemping to fill missing position i0813 195517209247 26127 status_update_managercpp176 pausing sending status updates i0813 195517209259 26128 slavecpp684 new master detected at master1721721060249 i0813 195517209322 26127 status_update_managercpp176 pausing sending status updates i0813 195517209364 26128 slavecpp709 no credentials provided attempting to register without authentication i0813 195517209344 26138 slavecpp684 new master detected at master1721721060249 i0813 195517209455 26128 slavecpp720 detecting new master i0813 195517209492 26138 slavecpp709 no credentials provided attempting to register without authentication i0813 195517209573 26128 slavecpp4240 received oversubscribable resources from the resource estimator i0813 195517209601 26138 slavecpp720 detecting new master i0813 195517209730 26138 slavecpp4240 received oversubscribable resources from the resource estimator i0813 195517209883 26136 slavecpp4240 received oversubscribable resources from the resource estimator i0813 195517211266 26136 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0813 195517211771 26136 leveldbcpp343 persisting action 8 bytes to leveldb took 462128ns i0813 195517211797 26136 replicacpp679 persisted action at 0 i0813 195517212980 26130 replicacpp511 replica received write request for position 0 i0813 195517213124 26130 leveldbcpp438 reading position from leveldb took 67075ns i0813 195517213580 26130 leveldbcpp343 persisting action 14 bytes to leveldb took 301649ns i0813 195517213603 26130 replicacpp679 persisted action at 0 i0813 195517214284 26123 replicacpp658 replica received learned notice for position 0 i0813 195517214622 26123 leveldbcpp343 persisting action 16 bytes to leveldb took 284547ns i0813 195517214648 26123 replicacpp679 persisted action at 0 i0813 195517214675 26123 replicacpp664 replica learned nop action at position 0 i0813 195517215420 26136 logcpp677 writer started with ending position 0 i0813 195517217463 26133 leveldbcpp438 reading position from leveldb took 47943ns i0813 195517220762 26125 registrarcpp344 successfully fetched the registry 0b in 23649024ms i0813 195517221081 26125 registrarcpp443 applied 1 operations in 136902ns attempting to update the registry i0813 195517223667 26133 logcpp685 attempting to append 174 bytes to the log i0813 195517223778 26125 coordinatorcpp341 coordinator attempting to write append action at position 1 i0813 195517224516 26127 replicacpp511 replica received write request for position 1 i0813 195517225009 26127 leveldbcpp343 persisting action 193 bytes to leveldb took 466230ns i0813 195517225042 26127 replicacpp679 persisted action at 1 i0813 195517225653 26126 replicacpp658 replica received learned notice for position 1 i0813 195517225953 26126 leveldbcpp343 persisting action 195 bytes to leveldb took 286966ns i0813 195517225975 26126 replicacpp679 persisted action at 1 i0813 195517226013 26126 replicacpp664 replica learned append action at position 1 i0813 195517227545 26137 registrarcpp488 successfully updated the registry in 6328064ms i0813 195517227722 26137 registrarcpp374 successfully recovered registrar i0813 195517227918 26124 logcpp704 attempting to truncate the log to 1 i0813 195517228024 26133 coordinatorcpp341 coordinator attempting to write truncate action at position 2 i0813 195517228193 26131 mastercpp1334 recovered 0 slaves from the registry 135b  allowing 10mins for slaves to reregister i0813 195517228659 26127 replicacpp511 replica received write request for position 2 i0813 195517228972 26127 leveldbcpp343 persisting action 16 bytes to leveldb took 297903ns i0813 195517229004 26127 replicacpp679 persisted action at 2 i0813 195517229565 26127 replicacpp658 replica received learned notice for position 2 i0813 195517229837 26127 leveldbcpp343 persisting action 18 bytes to leveldb took 260326ns i0813 195517229899 26127 leveldbcpp401 deleting 1 keys from leveldb took 48697ns i0813 195517229923 26127 replicacpp679 persisted action at 2 i0813 195517229956 26127 replicacpp664 replica learned truncate action at position 2 i0813 195517325634 26138 slavecpp1209 will retry registration in 445955946ms if necessary i0813 195517326088 26124 mastercpp3635 registering slave at slave21721721060249 297daca2d01a with id 201508131955171679077566024926100s0 i0813 195517327446 26124 registrarcpp443 applied 1 operations in 231072ns attempting to update the registry i0813 195517330252 26136 logcpp685 attempting to append 344 bytes to the log i0813 195517330407 26132 coordinatorcpp341 coordinator attempting to write append action at position 3 i0813 195517331418 26128 replicacpp511 replica received write request for position 3 i0813 195517331753 26128 leveldbcpp343 persisting action 363 bytes to leveldb took 264140ns i0813 195517331778 26128 replicacpp679 persisted action at 3 i0813 195517332324 26133 replicacpp658 replica received learned notice for position 3 i0813 195517332809 26133 leveldbcpp343 persisting action 365 bytes to leveldb took 313064ns i0813 195517332834 26133 replicacpp679 persisted action at 3 i0813 195517332865 26133 replicacpp664 replica learned append action at position 3 i0813 195517334211 26132 registrarcpp488 successfully updated the registry in 6668032ms i0813 195517334430 26127 logcpp704 attempting to truncate the log to 3 i0813 195517334566 26132 coordinatorcpp341 coordinator attempting to write truncate action at position 4 i0813 195517335283 26129 replicacpp511 replica received write request for position 4 i0813 195517335615 26127 slavecpp3058 received ping from slaveobserver11721721060249 i0813 195517335816 26129 leveldbcpp343 persisting action 16 bytes to leveldb took 458268ns i0813 195517335908 26137 mastercpp3698 registered slave 201508131955171679077566024926100s0 at slave21721721060249 297daca2d01a with cpus2 mem10240 disk370122e0,5
master fails to access replicated log after network partition in a 5 node cluster with 3 masters and 2 slaves and zk on each node when a network partition is forced all the masters apparently lose access to their replicated log the leading master halts unknown reasons but presumably related to replicated log access the others fail to recover from the replicated log unknown reasons this could have to do with zk setup but it might also be a mesos bug this was observed in a chronos test drive scenario described in detail here httpsgithubcommesoschronosissues511 with setup instructions here httpsgithubcommesoschronosissues508,8
create a user doc for scheduler http api we need to convert the design doc into user doc that we can add to our docs folder,3
json representation of protobuf should use base64 encoding for bytes fields currently we encode bytes fields as utf8 strings which is lossy for binary data due to invalid byte sequences in order to encode binary data in a lossless fashion we can encode bytes fields in base64 note that this is also how proto3 does its encoding see herehttpsdevelopersgooglecomprotocolbuffersdocsproto3hlenjson so this would make migration easier as well,3
downloadwithhadoop tries to access error for a valid trybool this was reported while trying to install hadoop  mesos integration noformat i0818 053635058688 24428 fetchercpp409 fetcher info cache_directorytmpmesosfetchslaves201507060752181611773194505028439s473hadoopitemsactionbypass_cacheuriextracttruevaluehdfshdfsprod54310userashwanthhadoopwithmesos260cdh544targzsandbox_directoryvarlibmesosslaves201507060752181611773194505028439s473frameworks2015070607521816117731945050284394532executorsexecutor_task_tracker_4129runsc26f52d4405546fab99911d73f2096dduserhadoop i0818 053635059806 24428 fetchercpp364 fetching uri hdfshdfsprod54310userashwanthhadoopwithmesos260cdh544targz i0818 053635059821 24428 fetchercpp238 fetching directly into the sandbox directory i0818 053635059835 24428 fetchercpp176 fetching uri hdfshdfsprod54310userashwanthhadoopwithmesos260cdh544targz mesosfetcher tmpmesosbuildmesosrepo3rdpartylibprocess3rdpartystoutincludestouttryhpp90 const string tryterror const with t  bool stdstring  stdbasic_stringchar assertion dataisnone failed noformat this is however a genuine bug in srclauncherfetchercppl99 code trybool available  hdfsavailable if availableiserror  availableget  return errorskipping fetch with hadoop client as  hadoop client not available   availableerror  code the root cause is that probably the hdfs client is not available on the slave however we do not error but rather return a false result the bug is exposed in the return line where we try to retrieve availableerror which is not there  its just false this was a latent bug that has been exposed by my recent refactoring of osshell which is used by hdfsavailable under the covers,1
implement docker registry client implement the docker registry client as per design document httpsdocsgooglecomdocumentd1kehxpql4lqgampiiad4ytdrn4heqc4fne93whr4x4edit,5
add dockerregistry unit tests add unit tests suite for docker registry implementation this could include  creating mock docker registry server  using openssl library for digest functions,5
master should drop http calls when its recovering much like what we do with pid based frameworks master should drop http calls if its not the leader andor still recovering,3
failing root_ tests on centos 71  limitedcpuisolatortest h2 limitedcpuisolatortestroot_cgroups_pids_and_tids this is one of several root failing tests we want to track them individually and for each of them decide whether to  fix  remove or  redesign full verbose logs attached h2 steps to reproduce completely cleaned the build removed directory clean pull from master sha fb93d93  same results 9 failed tests noformat  751 tests from 114 test cases ran 231218 ms total  passed  742 tests  failed  9 tests listed below  failed  limitedcpuisolatortestroot_cgroups_pids_and_tids  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess  failed  containerizertestroot_cgroups_balloonframework  failed  linuxfilesystemisolatortestroot_changerootfilesystem  failed  linuxfilesystemisolatortestroot_volumefromsandbox  failed  linuxfilesystemisolatortestroot_volumefromhost  failed  linuxfilesystemisolatortestroot_volumefromhostsandboxmountpoint  failed  linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem  failed  mesoscontainerizerlaunchtestroot_changerootfs 9 failed tests you have 10 disabled tests noformat,5
failing root_ tests on centos 71  usercgroupisolatortest h2 usercgroupisolatortest this is one of several root failing tests we want to track them individually and for each of them decide whether to  fix  remove or  redesign full verbose logs attached h2 steps to reproduce completely cleaned the build removed directory clean pull from master sha fb93d93  same results 9 failed tests noformat  751 tests from 114 test cases ran 231218 ms total  passed  742 tests  failed  9 tests listed below  failed  limitedcpuisolatortestroot_cgroups_pids_and_tids  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess  failed  containerizertestroot_cgroups_balloonframework  failed  linuxfilesystemisolatortestroot_changerootfilesystem  failed  linuxfilesystemisolatortestroot_volumefromsandbox  failed  linuxfilesystemisolatortestroot_volumefromhost  failed  linuxfilesystemisolatortestroot_volumefromhostsandboxmountpoint  failed  linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem  failed  mesoscontainerizerlaunchtestroot_changerootfs 9 failed tests you have 10 disabled tests noformat,5
failing root_ tests on centos 71  containerizertest h2 containerizertestroot_cgroups_balloonframework this is one of several root failing tests we want to track them individually and for each of them decide whether to  fix  remove or  redesign full verbose logs attached h2 steps to reproduce completely cleaned the build removed directory clean pull from master sha fb93d93  same results 9 failed tests noformat  751 tests from 114 test cases ran 231218 ms total  passed  742 tests  failed  9 tests listed below  failed  limitedcpuisolatortestroot_cgroups_pids_and_tids  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess  failed  containerizertestroot_cgroups_balloonframework  failed  linuxfilesystemisolatortestroot_changerootfilesystem  failed  linuxfilesystemisolatortestroot_volumefromsandbox  failed  linuxfilesystemisolatortestroot_volumefromhost  failed  linuxfilesystemisolatortestroot_volumefromhostsandboxmountpoint  failed  linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem  failed  mesoscontainerizerlaunchtestroot_changerootfs 9 failed tests you have 10 disabled tests noformat,5
failing root_ tests on centos 71  linuxfilesystemisolatortest h2 linuxfilesystemisolatortest this is one of several root failing tests we want to track them individually and for each of them decide whether to  fix  remove or  redesign full verbose logs attached h2 steps to reproduce completely cleaned the build removed directory clean pull from master sha fb93d93  same results 9 failed tests noformat  751 tests from 114 test cases ran 231218 ms total  passed  742 tests  failed  9 tests listed below  failed  limitedcpuisolatortestroot_cgroups_pids_and_tids  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess  failed  containerizertestroot_cgroups_balloonframework  failed  linuxfilesystemisolatortestroot_changerootfilesystem  failed  linuxfilesystemisolatortestroot_volumefromsandbox  failed  linuxfilesystemisolatortestroot_volumefromhost  failed  linuxfilesystemisolatortestroot_volumefromhostsandboxmountpoint  failed  linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem  failed  mesoscontainerizerlaunchtestroot_changerootfs 9 failed tests you have 10 disabled tests noformat,5
failing root_ tests on centos 71  mesoscontainerizerlaunchtest h2 mesoscontainerizerlaunchtest this is one of several root failing tests we want to track them individually and for each of them decide whether to  fix  remove or  redesign full verbose logs attached h2 steps to reproduce completely cleaned the build removed directory clean pull from master sha fb93d93  same results 9 failed tests noformat  751 tests from 114 test cases ran 231218 ms total  passed  742 tests  failed  9 tests listed below  failed  limitedcpuisolatortestroot_cgroups_pids_and_tids  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess  failed  containerizertestroot_cgroups_balloonframework  failed  linuxfilesystemisolatortestroot_changerootfilesystem  failed  linuxfilesystemisolatortestroot_volumefromsandbox  failed  linuxfilesystemisolatortestroot_volumefromhost  failed  linuxfilesystemisolatortestroot_volumefromhostsandboxmountpoint  failed  linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem  failed  mesoscontainerizerlaunchtestroot_changerootfs 9 failed tests you have 10 disabled tests noformat,5
add a protobuf to represent time with integer precision existing timestamps in the protobufs use double to encode time generally the field represents seconds with the decimal component to represent smaller denominations of time this is less than ideal instead we should use integers so as to not lose data and to be able to compare value reliably something like code message time  int64 seconds int32 nanoseconds  code,1
remove remnants of libprocess_statistics_window as seen in mesos1283 libprocess_statistics_window is no longer needed since metrics now require specification of a window size and default to no history if not provided some commentedout code remnants associated with this environment variable still remain and should be removed,1
configurable size of completed task  framework history we try to make mesos work with multiple frameworks and mesosdns at the same time the goal is to have set of frameworks per team  project on a single mesos cluster at this point our mesos statejson is at 4mb and it takes a while to assembly 5 mesosdns instances hit statejson every 5 seconds effectively pushing mesosmaster cpu usage through the roof its at 100 all the time heres the problem noformat mesos λ curl s httpmesosmaster5050masterstatejson  jq frameworkscompleted_tasksframework_id  sort  uniq c  sort n 1 20150606001827252388362505059820003 16 20150606001827252388362505059820005 18 20150606001827252388362505059820029 73 20150606001827252388362505059820007 141 20150606001827252388362505059820009 154 201508201548173027200105050153200000 289 20150606001827252388362505059820004 510 20150606001827252388362505059820012 666 20150606001827252388362505059820028 923 201501160026122691655785050322040003 1000 20150606001827252388362505059820001 1000 20150606001827252388362505059820006 1000 20150606001827252388362505059820010 1000 20150606001827252388362505059820011 1000 20150606001827252388362505059820027 mesos λ fgrep 1000 r srcmaster srcmasterconstantscppconst size_t max_removed_slaves  100000 srcmasterconstantscppconst uint32_t max_completed_tasks_per_framework  1000 noformat active tasks are just 6 of statejson response noformat mesos λ cat tempmesosstatejson  jq c   wc 1 14796 4138942 mesos λ cat tempmesosstatejson  jq frameworkstasks  jq c   wc 16 37 252774 noformat i see four options that can improve the situation 1 add query string param to exclude completed tasks from statejson and use it in mesosdns and similar tools there is no need for mesosdns to know about completed tasks its just extra load on master and mesosdns 2 make history size configurable 3 make json serialization faster with 10000s of tasks even without history it would take a lot of time to serialize tasks for mesosdns doing it every 60 seconds instead of every 5 seconds isnt really an option 4 create event bus for mesos master marathon has it and itd be nice to have it in mesos this way mesosdns could avoid polling master state and switch to listening for events all can be done independently note to mesosphere folks please start distributing debug symbols with your distribution i was asking for it for a while and it is really helpful httpsgithubcommesospheremarathonissues1497issuecomment104182501 perf report for leading master httpiimgurcomiz7c3o0png im on 0230,3
define the container rootfs directories within the slave work_dir a few motivations 1 given the design in mesos3004 it became apparent that we need to support multiple images in a container and these images can be of different image types there are no sufficient reasons or major obstacles that force us not to allow it and it obviously gives the users more flexibility 2 also even though we currently allow only one backend for each provisioner when we update a running slave there can be multiple backends left in each container that we need to launch tasks with or at least recover we should evaluate in the future whether to support multiple backends and choose among them dynamically based on image characteristics 3 since the rootfs lifecycle tie with the running containers and should be cleaned up after containers die it fits into the pattern of word_dir and we can manage them inside the work dir without needing to ask the operator to specify more flags,2
support provisioning images specified in volumes this is related to mesos3095 and mesos3227 the idea is that we should allow command executor to run under host filesystem and provision the filesystem for the user the command line executor will then chroot into users root filesystem this solves the issue that the command executor is not launchable in the user specified root filesystem the design doc is here httpsdocsgooglecomdocumentd16hylvrl0nzkbts1j5stgyxzpnifpbpbs7rzrqvch4edituspsharing,3
slavetesthttpschedulerslaverestart observed on asf ci code  run  slavetesthttpschedulerslaverestart using temporary directory tmpslavetest_httpschedulerslaverestart_cxydra i0825 220736809872 27610 leveldbcpp176 opened db in 3751801ms i0825 220736811115 27610 leveldbcpp183 compacted db in 12194ms i0825 220736811175 27610 leveldbcpp198 created db iterator in 30669ns i0825 220736811197 27610 leveldbcpp204 seeked to beginning of db in 7829ns i0825 220736811208 27610 leveldbcpp273 iterated through 0 keys in the db in 6017ns i0825 220736811245 27610 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0825 220736811722 27638 recovercpp449 starting replica recovery i0825 220736811980 27638 recovercpp475 replica is in empty status i0825 220736813033 27641 replicacpp641 replica in empty status received a broadcasted recover request i0825 220736813355 27635 recovercpp195 received a recover response from a replica in empty status i0825 220736813756 27628 recovercpp566 updating replica status to starting i0825 220736814434 27636 leveldbcpp306 persisting metadata 8 bytes to leveldb took 570160ns i0825 220736814471 27636 replicacpp323 persisted replica status to starting i0825 220736814743 27642 recovercpp475 replica is in starting status i0825 220736814965 27638 mastercpp378 master 201508252207362348855485121927610 09c6504e3a31 started on 1721701451219 i0825 220736814999 27638 mastercpp380 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpslavetest_httpschedulerslaverestart_cxydracredentials framework_sorterdrf helpfalse initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0250_instsharemesoswebui work_dirtmpslavetest_httpschedulerslaverestart_cxydramaster zk_session_timeout10secs i0825 220736815347 27638 mastercpp425 master only allowing authenticated frameworks to register i0825 220736815371 27638 mastercpp430 master only allowing authenticated slaves to register i0825 220736815402 27638 credentialshpp37 loading credentials for authentication from tmpslavetest_httpschedulerslaverestart_cxydracredentials i0825 220736815634 27632 replicacpp641 replica in starting status received a broadcasted recover request i0825 220736815752 27638 mastercpp469 using default crammd5 authenticator i0825 220736815904 27638 mastercpp506 authorization enabled i0825 220736815979 27643 recovercpp195 received a recover response from a replica in starting status i0825 220736816185 27637 whitelist_watchercpp79 no whitelist given i0825 220736816186 27641 hierarchicalhpp346 initialized hierarchical allocator process i0825 220736816519 27630 recovercpp566 updating replica status to voting i0825 220736817258 27639 leveldbcpp306 persisting metadata 8 bytes to leveldb took 475231ns i0825 220736817296 27639 replicacpp323 persisted replica status to voting i0825 220736817420 27637 mastercpp1525 the newly elected leader is master1721701451219 with id 201508252207362348855485121927610 i0825 220736817467 27637 mastercpp1538 elected as the leading master i0825 220736817483 27637 mastercpp1308 recovering from registrar i0825 220736817509 27635 recovercpp580 successfully joined the paxos group i0825 220736817708 27633 registrarcpp311 recovering registrar i0825 220736817844 27635 recovercpp464 recover process terminated i0825 220736818439 27631 logcpp661 attempting to start the writer i0825 220736819694 27636 replicacpp477 replica received implicit promise request with proposal 1 i0825 220736820133 27636 leveldbcpp306 persisting metadata 8 bytes to leveldb took 421255ns i0825 220736820168 27636 replicacpp345 persisted promised to 1 i0825 220736820804 27630 coordinatorcpp231 coordinator attemping to fill missing position i0825 220736822105 27638 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0825 220736822597 27638 leveldbcpp343 persisting action 8 bytes to leveldb took 468065ns i0825 220736822625 27638 replicacpp679 persisted action at 0 i0825 220736823737 27637 replicacpp511 replica received write request for position 0 i0825 220736823796 27637 leveldbcpp438 reading position from leveldb took 39603ns i0825 220736824267 27637 leveldbcpp343 persisting action 14 bytes to leveldb took 446655ns i0825 220736824296 27637 replicacpp679 persisted action at 0 i0825 220736824961 27634 replicacpp658 replica received learned notice for position 0 i0825 220736825340 27634 leveldbcpp343 persisting action 16 bytes to leveldb took 362236ns i0825 220736825369 27634 replicacpp679 persisted action at 0 i0825 220736825388 27634 replicacpp664 replica learned nop action at position 0 i0825 220736825975 27642 logcpp677 writer started with ending position 0 i0825 220736826997 27628 leveldbcpp438 reading position from leveldb took 56us i0825 220736829946 27639 registrarcpp344 successfully fetched the registry 0b in 12187136ms i0825 220736830077 27639 registrarcpp443 applied 1 operations in 40874ns attempting to update the registry i0825 220736832870 27635 logcpp685 attempting to append 174 bytes to the log i0825 220736833088 27641 coordinatorcpp341 coordinator attempting to write append action at position 1 i0825 220736833845 27636 replicacpp511 replica received write request for position 1 i0825 220736834293 27636 leveldbcpp343 persisting action 193 bytes to leveldb took 425175ns i0825 220736834324 27636 replicacpp679 persisted action at 1 i0825 220736835077 27643 replicacpp658 replica received learned notice for position 1 i0825 220736835500 27643 leveldbcpp343 persisting action 195 bytes to leveldb took 404831ns i0825 220736835532 27643 replicacpp679 persisted action at 1 i0825 220736835574 27643 replicacpp664 replica learned append action at position 1 i0825 220736836545 27643 registrarcpp488 successfully updated the registry in 6393088ms i0825 220736836707 27643 registrarcpp374 successfully recovered registrar i0825 220736836874 27639 logcpp704 attempting to truncate the log to 1 i0825 220736837174 27632 mastercpp1335 recovered 0 slaves from the registry 135b  allowing 10mins for slaves to reregister i0825 220736837291 27634 coordinatorcpp341 coordinator attempting to write truncate action at position 2 i0825 220736838249 27639 replicacpp511 replica received write request for position 2 i0825 220736838685 27639 leveldbcpp343 persisting action 16 bytes to leveldb took 412214ns i0825 220736838716 27639 replicacpp679 persisted action at 2 i0825 220736839735 27628 replicacpp658 replica received learned notice for position 2 i0825 220736840304 27628 leveldbcpp343 persisting action 18 bytes to leveldb took 547841ns i0825 220736840375 27628 leveldbcpp401 deleting 1 keys from leveldb took 51256ns i0825 220736840401 27628 replicacpp679 persisted action at 2 i0825 220736840428 27628 replicacpp664 replica learned truncate action at position 2 i0825 220736849371 27610 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i0825 220736856500 27633 slavecpp190 slave started on 2861721701451219 i0825 220736856541 27633 slavecpp191 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpslavetest_httpschedulerslaverestart_ukka8lcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpslavetest_httpschedulerslaverestart_ukka8lfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0250_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resource_monitoring_interval1secs resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirtmpslavetest_httpschedulerslaverestart_ukka8l i0825 220736857074 27633 credentialshpp85 loading credential for authentication from tmpslavetest_httpschedulerslaverestart_ukka8lcredential i0825 220736857275 27633 slavecpp321 slave using credential for testprincipal i0825 220736857822 27633 slavecpp354 slave resources cpus2 mem1024 disk1024 ports3100032000 i0825 220736857936 27633 slavecpp384 slave hostname 09c6504e3a31 i0825 220736857959 27633 slavecpp389 slave checkpoint true i0825 220736858886 27637 statecpp54 recovering state from tmpslavetest_httpschedulerslaverestart_ukka8lmeta i0825 220736859130 27638 status_update_managercpp202 recovering status update manager i0825 220736859465 27636 containerizercpp379 recovering containerizer i0825 220736860631 27634 slavecpp4069 finished recovery i0825 220736861034 27634 slavecpp4226 querying resource estimator for oversubscribable resources i0825 220736861239 27643 status_update_managercpp176 pausing sending status updates i0825 220736861240 27634 slavecpp684 new master detected at master1721701451219 i0825 220736861322 27634 slavecpp747 authenticating with master master1721701451219 i0825 220736861343 27634 slavecpp752 using default crammd5 authenticatee i0825 220736861450 27634 slavecpp720 detecting new master i0825 220736861495 27628 authenticateecpp115 creating new client sasl connection i0825 220736861569 27634 slavecpp4240 received oversubscribable resources from the resource estimator i0825 220736861716 27632 mastercpp4694 authenticating slave2861721701451219 i0825 220736861799 27629 authenticatorcpp407 starting authentication session for crammd5_authenticatee6651721701451219 i0825 220736862045 27642 authenticatorcpp92 creating new server sasl connection i0825 220736862308 27635 authenticateecpp206 received sasl authentication mechanisms crammd5 i0825 220736862337 27635 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0825 220736862421 27629 authenticatorcpp197 received sasl authentication start i0825 220736862478 27629 authenticatorcpp319 authentication requires more steps i0825 220736862579 27633 authenticateecpp252 received sasl authentication step i0825 220736862679 27628 authenticatorcpp225 received sasl authentication step i0825 220736862707 27628 auxpropcpp102 request to lookup properties for user testprincipal realm 09c6504e3a31 server fqdn 09c6504e3a31 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0825 220736862717 27628 auxpropcpp174 looking up auxiliary property userpassword i0825 220736862754 27628 auxpropcpp174 looking up auxiliary property cmusaslsecretcrammd5 i0825 220736862785 27628 auxpropcpp102 request to lookup properties for user testprincipal realm 09c6504e3a31 server fqdn 09c6504e3a31 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0825 220736862797 27628 auxpropcpp124 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0825 220736862802 27628 auxpropcpp124 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0825 220736862817 27628 authenticatorcpp311 authentication success i0825 220736862884 27629 authenticateecpp292 authentication success i0825 220736862921 27630 mastercpp4724 successfully authenticated principal testprincipal at slave2861721701451219 i0825 220736862969 27642 authenticatorcpp425 authentication session cleanup for crammd5_authenticatee6651721701451219 i0825 220736863139 27639 slavecpp815 successfully authenticated with master master1721701451219 i0825 220736863256 27639 slavecpp1209 will retry registration in 15028678ms if necessary i0825 220736863382 27643 mastercpp3636 registering slave at slave2861721701451219 09c6504e3a31 with id 201508252207362348855485121927610s0 i0825 220736863899 27610 schedcpp164 version 0250 i0825 220736863940 27636 registrarcpp443 applied 1 operations in 94492ns attempting to update the registry i0825 220736864670 27632 schedcpp262 new master detected at master1721701451219 i0825 220736864790 27632 schedcpp318 authenticating with master master1721701451219 i0825 220736864821 27632 schedcpp325 using default crammd5 authenticatee i0825 220736865095 27637 authenticateecpp115 creating new client sasl connection i0825 220736865453 27643 mastercpp4694 authenticating scheduler6c5ddcdb9dd14b38b0515f714d3c1c551721701451219 i0825 220736865603 27629 authenticatorcpp407 starting authentication session for crammd5_authenticatee6661721701451219 i0825 220736865840 27638 authenticatorcpp92 creating new server sasl connection i0825 220736866217 27630 authenticateecpp206 received sasl authentication mechanisms crammd5 i0825 220736866260 27630 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0825 220736866433 27639 authenticatorcpp197 received sasl authentication start i0825 220736866513 27639 authenticatorcpp319 authentication requires more steps i0825 220736866710 27630 authenticateecpp252 received sasl authentication step i0825 220736866999 27638 authenticatorcpp225 received sasl authentication step i0825 220736867051 27638 auxpropcpp102 request to lookup properties for user testprincipal realm 09c6504e3a31 server fqdn 09c6504e3a31 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0825 220736867077 27638 auxpropcpp174 looking up auxiliary property userpassword i0825 220736867130 27638 auxpropcpp174 looking up auxiliary property cmusaslsecretcrammd5 i0825 220736867162 27638 auxpropcpp102 request to lookup properties for user testprincipal realm 09c6504e3a31 server fqdn 09c6504e3a31 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0825 220736867175 27638 auxpropcpp124 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0825 220736867183 27638 auxpropcpp124 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0825 220736867202 27638 authenticatorcpp311 authentication success i0825 220736867426 27636 authenticateecpp292 authentication success i0825 220736867434 27633 authenticatorcpp425 authentication session cleanup for crammd5_authenticatee6661721701451219 i0825 220736867627 27630 mastercpp4724 successfully authenticated principal testprincipal at scheduler6c5ddcdb9dd14b38b0515f714d3c1c551721701451219 i0825 220736867951 27641 schedcpp407 successfully authenticated with master master1721701451219 i0825 220736867986 27641 schedcpp713 sending subscribe call to master1721701451219 i0825 220736868114 27641 schedcpp746 will retry registration in 1352726078secs if necessary i0825 220736868233 27634 logcpp685 attempting to append 344 bytes to the log i0825 220736868268 27638 mastercpp2094 received subscribe call for framework default at scheduler6c5ddcdb9dd14b38b0515f714d3c1c551721701451219 i0825 220736868305 27638 mastercpp1564 authorizing framework principal testprincipal to receive offers for role  i0825 220736868373 27631 coordinatorcpp341 coordinator attempting to write append action at position 3 i0825 220736868614 27642 mastercpp2164 subscribing framework default with checkpointing enabled and capabilities   i0825 220736868999 27643 hierarchicalhpp391 added framework 2015082522073623488554851219276100000 i0825 220736869030 27643 hierarchicalhpp1010 no resources available to allocate i0825 220736869046 27643 hierarchicalhpp910 performed allocation for 0 slaves in 34654ns i0825 220736869215 27631 schedcpp640 framework registered with 2015082522073623488554851219276100000 i0825 220736869215 27643 replicacpp511 replica received write request for position 3 i0825 220736869268 27631 schedcpp654 schedulerregistered took 29976ns i0825 220736869453 27643 leveldbcpp343 persisting action 363 bytes to leveldb took 181689ns i0825 220736869477 27643 replicacpp679 persisted action at 3 i0825 220736870075 27629 replicacpp658 replica received learned notice for position 3 i0825 220736870542 27629 leveldbcpp343 persisting action 365 bytes to leveldb took 469081ns i0825 220736870589 27629 replicacpp679 persisted action at 3 i0825 220736870622 27629 replicacpp664 replica learned append action at position 3 i0825 220736872133 27632 registrarcpp488 successfully updated the registry in 8113152ms i0825 220736872354 27639 logcpp704 attempting to truncate the log to 3 i0825 220736872470 27632 coordinatorcpp341 coordinator attempting to write truncate action at position 4 i0825 220736872879 27637 slavecpp3058 received ping from slaveobserver2741721701451219 i0825 220736873015 27636 mastercpp3699 registered slave 201508252207362348855485121927610s0 at slave2861721701451219 09c6504e3a31 with cpus2 mem1024 disk1024 ports3100032000 i0825 220736873180 27637 slavecpp859 registered with master master1721701451219 given slave id 201508252207362348855485121927610s0 i0825 220736873219 27637 fetchercpp77 clearing fetcher cache i0825 220736873410 27634 status_update_managercpp183 resuming sending status updates i0825 220736873379 27628 hierarchicalhpp542 added slave 201508252207362348855485121927610s0 09c6504e3a31 with cpus2 mem1024 disk1024 ports3100032000 allocated  i0825 220736873482 27642 replicacpp511 replica received write request for position 4 i0825 220,2
factor out json to repeated protobuf conversion in general we have the collection of protobuf messages as another protobuf message which makes json  protobuf conversion straightforward this is not always the case for example resources class is not a protobuf though protobufconvertible to facilitate conversions like json  resources and avoid writing code for each particular case we propose to introduce jsonarray  repeated protobuf conversion with this in place jsonarray  resources boils down to jsonarray  repeated resource  extra ctor call  resources,2
rework jenkins build script mesos jenkins build script needs to be reworked to support the following  wider test coverage libevent libssl root tests docker tests  more oscompiler docker images for testing mesos  excluding tests on perimage basis  reproducing the test image locally,3
mesos will not build when configured with gperftools enabled mesos configured with enableperftools currently will not build on osx 10104 or ubuntu 1404 possibly because the bundled gperftools20 is not current the stable release is now 24 which builds successfully on both of these platforms this issue is resolved when mesos will build successfully out of the box with gperftools enabled after this ticket is resolved the libprocess profiler should be tested to confirm that it still works and if not it should be fixed,2
spurious fetcher message about extracting an archive the fetcher emits a spurious log message about not extracting an archive with tgz extension even though the tarball is extracted correctly code i0826 190208304914 2109 loggingcpp172 info level logging started i0826 190208305253 2109 fetchercpp413 fetcher info cache_directorytmpmesosfetchslaves2015082618571625166276450501s0rootitemsactionbypass_cacheuriextracttruevaluefilemesossampleflaskapptgzsandbox_directorytmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402eduserroot i0826 190208306834 2109 fetchercpp368 fetching uri filemesossampleflaskapptgz i0826 190208306864 2109 fetchercpp242 fetching directly into the sandbox directory i0826 190208306884 2109 fetchercpp179 fetching uri filemesossampleflaskapptgz i0826 190208306900 2109 fetchercpp159 copying resource with commandcp mesossampleflaskapptgz tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402edsampleflaskapptgz i0826 190208309063 2109 fetchercpp76 extracting with command tar c tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402ed xf tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402edsampleflaskapptgz i0826 190208315313 2109 fetchercpp84 extracted tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402edsampleflaskapptgz into tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402ed w0826 190208315381 2109 fetchercpp264 copying instead of extracting resource from uri with extract flag because it does not seem to be an archive filemesossampleflaskapptgz i0826 190208315604 2109 fetchercpp445 fetched filemesossampleflaskapptgz to tmpmesosslaves2015082618571625166276450501s0frameworks20150826185716251662764505010000executorssampleflaskappf222d2024c2411e5a6280242ac110011runse71f50b8816d46d5bcc6f9850a0402edsampleflaskapptgz code,1
autogenerate protos for stout tests stout protobufs afaik right now its just a single file protobuf_testsproto are not generated automatically including proto generation step would be cleaner and more convenient,2
make use of c11 atomics now that we require c11 we can make use of stdatomic for example  libprocessprocesscpp uses a bare int  __sync_synchronize for running  __sync_synchronize is used in logginghpp in libprocess and forkhpp in stout  schedschedcpp uses a volatile int for running  this is wrong volatile is not sufficient to ensure safe concurrent access  volatile is used in a few other places  most are probably dubious but i havent looked closely,2
support http pipelining in libprocess httppost currently  httppost in libprocess does not support http pipelining each call as of know sends in the connection close header thereby signaling to the server to close the tcp socket after the response we either need to create a new interface for supporting http pipelining  or modify the existing httppost to do so this is needed for the schedulerexecutor library implementations to make sure calls are sent in order to the master currently in order to do so we send in the next request only after we have received a response for an earlier call that results in degraded performance,8
refactored libprocess ssl tests refactor ssl test fixture to be available for reuse by other projects currently the fixture class and its the symbols it depends on are not present in libprocess include files,3
dynamic reservations are not counted as used resources in the master dynamically reserved resources should be considered used or allocated and hence reflected in mesos bookkeeping structures and statejson i expanded the reservationtestreservethenunreserve test with the following section code  check that the master counts the reservation as a used resource  futureprocesshttpresponse response  processhttpgetmasterget statejson await_readyresponse tryjsonobject parse  jsonparsejsonobjectresponsegetbody assert_someparse resultjsonnumber cpus  parsegetfindjsonnumberslaves0used_resourcescpus assert_some_eqjsonnumber1 cpus  code and got noformat srctestsreservation_testscpp168 failure value of cpusget actual 0 expected jsonnumber1 which is 1 noformat idea for new resources states httpsdocsgooglecomdrawingsd1aquviqpy8d_mrcqjzuwz5nnn3cyp3jxqeguhlkzcedit,3
implement filtering mechanism for scheduler api events testing currently our testing infrastructure does not have a mechanism of filteringdropping http events of a particular type from the scheduler api response stream we need a drop_http_calls abstraction that can help us to filter a particular event type code  enqueues all received events into a libprocess queue action_penqueue queue  stdqueueevent events  arg0 while eventsempty   note that we currently drop heartbeats because most of these tests  are not designed to deal with heartbeats  todovinod implement drop_http_calls that can filter heartbeats if eventsfronttype  eventheartbeat  vlog1  ignoring heartbeat event  else  queueputeventsfront  eventspop   code this helper code is duplicated in at least two places currently scheduler librarymaintenance primitives tests  the solution can be as trivial as moving this helper function to a common testheader  implement a drop_http_calls similar to what we do for other protobufs via drop_calls,3
commandline flags should take precedence over os env variables currently it appears that redefining a flag on the commandline that was already defined via a os env var mesos_ causes the master to fail with a not very helpful message for example if one has mesos_quorum defined this happens noformat  mesosmaster zkzk19216814mesos quorum1 hostname19216814 ip19216814 duplicate flag quorum on command line noformat which is not very helpful ideally we would parse the flags with a wellknown priority commandline first environment last  but at the very least the error message should be more helpful in explaining what the issue is,2
rate limiting functionality for http frameworks we need to build rate limiting functionality for frameworks connecting via the scheduler http api similar to the pid based frameworks link to the ratelimiting section from design doc httpsdocsgooglecomdocumentd1pniy_hckimknvpqhkrhbc9esitwnftprixh_urrt0editheadinghkzgdk4d5fmba  this ticket deals with refactoring the existing pid based framework functionality and extend it for http frameworks  the second part of notifying the framework when ratelimiting is active ie returning a status of 429 can be undertook as part of mesos1664,5
expand the range of integer precision when converting intoout of json for mesos3299 we added some protobufs to represent time with integer precision however this precision is not maintained through protobuf  json conversion because of how our json encodersdecoders convert numbers to floating point to maintain precision we can try one of the following  try using a long double to represent a number  add logic to stringifyparse numbers without loss when possible  try representing int64_t as a string and parse it as such  update picojson and add a compiler flag ie dpicojson_use_int64 in all cases well need to make sure that  integers are properly stringified without loss  the json decoder parses the integer without loss  we have some unit tests for big close to int32_maxint64_max and small integers,5
add filter support for inverse offers a filter attached to the inverse offer can be used by the framework to control when it wants to be contacted again with the inverse offer since future circumstances may change the viability of the maintenance schedule the filter for inverseoffers is identical to the existing mechanism for reoffering offers to frameworks,5
add either log rotation or cappedsize logging for tasks tasks currently log their output ie stdoutstderr to files the sandbox on an agents disk in some cases the accumulation of these logs can completely fill up the agents disk and thereby kill the task or machine to prevent this we should either implement a log rotation mechanism or cappedsize logging this would be used by executors to control the amount of logs they keep masteragent logs will not be affected we will first scope out several possible approaches for log rotationcapping in a design document see mesos3356 once an approach is chosen this story will be broken down into some corresponding issues,13
removing mount point fails with ebusy in linuxfilesystemisolator when running the tests as root we found persistentvolumetestaccesspersistentvolume fails consistently on some platforms noformat  run  persistentvolumetestaccesspersistentvolume i0901 021726435140 39432 execcpp133 version 0250 i0901 021726442129 39461 execcpp207 executor registered on slave 2015090102172618286599785210232604s0 registered executor on hostname starting task d8ff1f00e7204a61b440e111009dfdc3 sh c echo abc  path1file forked command at 39484 command exited with status 0 pid 39484 srctestspersistent_volume_testscpp579 failure value of osexistspathjoindirectory path1 actual true expected false  failed  persistentvolumetestaccesspersistentvolume 777 ms noformat turns out that the rmdir after the umount fails with ebusy because theres still some references to the mount fyi jieyu mcypark,5
problem statement summary for systemd cgroup launcher there have been many reports of cgroups related issues when running mesos on systemd many of these issues are rooted in the manual manipulation of the cgroups filesystem by mesos this task is to describe the problem in a 1page summary and elaborate on the suggested 2 part solution 1 using the delegatetrue flag for the slave 2 implementing a systemd launcher to run executors with tighter systemd integration,5
scope out approaches to deal with logging to finite disks ie log rotationcappedsize logging for the background see the parent story mesos3348 for the workdesigndiscussion see the linked design document below,5
update quota design doc based on user comments and offline syncs we got plenty of feedback from different parties which we would like to persist in the design doc for posterity,3
export per container snmp statistics we need to export the per container snmp statistics too from its procnetsnmp,5
allow resourcesattributes discovery in heterogeneous clusters tasks sometimes have strong constraints on the type of hardware they need to execute on the current solution is to use custom resources and attributes on the agents detecting nonstandard resourcesattributes requires wrapping the mesosslave binary behind a script and use custom code to probe the agent unfortunately this approach doesnt allow composition the solution would be to provide a hookmodule mechanism to allow users to use custom code performing resourcesattributes discovery please review the detailed document below httpsdocsgooglecomdocumentd15okebdezfxzeylsyqou0upb0eovecalzekeg0hqax9w feel free to express commentsconcerns by annotating the document or by replying to this issue,3
add device support in cgroups abstraction add support for device cgroupshttpswwwkernelorgdocdocumentationcgroupv1devicestxt to aid isolators controlling access to devices in the future we could think about how to numerate and control access to devices as resource or taskcontainer policy,3
add executor protobuf to v1 a new protobuf for executor was introduced in mesos for the http api it needs to be added to v1 so it reflects changes made on v1mesosproto this protobuf is ought to be changed as the executor http api design evolves,1
document a test pattern for expediting event firing we use clockadvance extensively in tests to expedite event firing and minimize overall make check time document this pattern for posterity,3
remove unused executor protobuf the executor protobuf definition living outside the v1 directory is unused it should be removed to avoid confusion,1
rewrite perf events code our current code base invokes and parses perf stat which sucks because cmdline output is not a stable abi at all it can break our code at any time for example mesos2834 we should use the stable api perf_event_open2 with this patch httpsreviewsapacheorgr37540 we already have the infrastructure for the implementation so it should not be hard to rewrite all the perf events code,5
mesosexecute does not support credentials mesosexecute does not appear to support passing credentials this makes it impossible to use on a cluster where framework authentication is required,2
refactor the plain json parsing in the docker containerizer two functions in the dockerrelated code take a string and parse it to json  dockercontainercreate in srcdockerdockercpp  tokencreate in srcslavecontainerizerprovisionersdockertoken_managercpp this json is then validated lots of ifelses and used via the jsonvalue accessors we could instead use a protobuf and the related stout jsonprotobuf conversion function,3
docker containerizer does not symlink persistent volumes into sandbox for the arangodb framework i am trying to use the persistent primitives nearly all is working but i am missing a crucial piece at the end i have successfully created a persistent disk resource and have set the persistence and volume information in the diskinfo message however i do not see any way to find out what directory on the host the mesos slave has reserved for us i know it is mesos_slave_workdirvolumesrolesmyrolename_uuid but we have no way to query this information anywhere the docker containerizer does not automatically mount this directory into our docker container or symlinks it into our sandbox therefore i have essentially no access to it note that the mesos containerizer which i cannot use for other reasons seems to create a symlink in the sandbox to the actual path for the persistent volume with that i could mount the volume into our docker container and all would be well,5
publish egg for 0240 to pypi 0240 was released but the python egg has not been published,1
log source address replicated log recieved broadcasts currently mesos doesnt log what machine a replicated log status broadcast was recieved from code sep 11 214114 master01 mesosmaster15625 i0911 214114320164 15637 replicacpp641 replica in empty status received a broadcasted recover request sep 11 214114 master01 mesosdns15583 i0911 214114321097 15583 detectgo118 ignoring childrenchanged event leader has not changed mesos sep 11 214114 master01 mesosmaster15625 i0911 214114353914 15639 replicacpp641 replica in empty status received a broadcasted recover request sep 11 214114 master01 mesosmaster15625 i0911 214114479132 15639 replicacpp641 replica in empty status received a broadcasted recover request code it would be really useful for debugging replicated log startup issues to have info about where the message came from libprocess address ip or hostname the message came from,2
factor out v1 api test helper functions we currently have some helper functionality for v1 api tests this is copied in a few test files factor this out into a common place once the api is stabilized code  helper class for using expect_call since the mesos scheduler api  is callback based class callbacks  public mock_method0connected voidvoid mock_method0disconnected voidvoid mock_method1received voidconst stdqueueevent  code code  enqueues all received events into a libprocess queue  todojmlvanre factor this common code out of tests into v1  helper action_penqueue queue  stdqueueevent events  arg0 while eventsempty   note that we currently drop heartbeats because most of these tests  are not designed to deal with heartbeats  todovinod implement drop_http_calls that can filter heartbeats if eventsfronttype  eventheartbeat  vlog1  ignoring heartbeat event  else  queueputeventsfront  eventspop   code we can also update the helpers in testsmesoshpp to support the v1 api this would let us get ride of lines like code v1taskinfo taskinfo  evolvecreatetaskdevolveoffer  default_executor_id code in favor of code v1taskinfo taskinfo  createtaskoffer  default_executor_id code,2
perf event isolator stops performing sampling if a single timeout occurs currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses code duration timeout  flagsperf_duration  seconds2 code this should be based on the reap interval maximum also the code stops sampling altogether when a single timeout occurs weve observed time outs during normal operation so it would be better for the isolator to continue performing perf sampling in the case of timeouts it may also make sense to continue sampling in the case of errors since these may be transient,3
support fetching appc images into the store so far appc store is read only and depends on out of band mechanisms to get the images we need to design a way to support fetching in a native way as commented on mesos2824 its unacceptable to have either have  the slave to be blocked for extended period of time minutes which delays the communication between the executor and scheduler or  the first task that uses this image to be blocked for a long time to wait for the container image to be ready the solution needs to enable the operator to prefetch a list of preferred images without introducing the above problems,5
modify linuxlauncher to support systemd implement the solution described in mesos3352 in the linuxlauncher in order to avoid the migration of cgroup pids by systemd we can use the delegatetrue flag this guards systemd from migrating the pids that are descendants of the process launched by a systemd unit in order for this strategy to work the delegate flag must be supported by the systemd version support for this was introduced in systemd v218 however it has also been backported to v208 for rhel7 and centos7 herehttpcentoserratanagaternetitemceba20150037centos7i386x86_64html with the package systemd20820httpsrhnredhatcomerratarhba20151155html it is highly recommended to upgrade to this package if running those operating systems once the delegatetrue flag has been set the cgroups that are manually manipulated by the agent will no longer be migrated during the lifetime of the agent this still leaves the problem of tasks being migrated _after the agent has stopped running_ voluntarily or not in order to deal with the problem we propose the following solution if an agent is running on a systemd initialized machine then the agent will create a systemd slice with a lifetime that is independent of the agent and delegatetrue the linux launcher used when cgroups isolators are enabled will then assign the cgroup name for any executor that is launched to this separate slice the consequence of this is that when the agent unit is terminated the separate slice will continue to delegate the cgroups preventing systemd from migrating the pids a side benefit of this is that we can maintain the killmodecontrolgroup flag on the agent and terminate all agent specific services such as the fetcher without terminating the tasks this provides for a nice cleanup this solution will still require that the agent unit be launched with the delegatetrue flag such that there is no race during the transition of the pids from the agent to the separate slice the agent will be responsible for verifying the slice is still available upon recovery and warning the operator if it notices that the tasks it is recovering are no longer associated with this separate slice as this can cause silent loss of isolation of existing tasks,8
processcollect and processawait do not perform discard propagation when aggregating futures with collect one may discard the outer future code promiseint p1 promisestring p2 futureint string collect  processcollectp1future p2future collectdiscard  collect will transition to discarded  however p12futurehasdiscard remains false  as there is no discard propagation code discard requests should propagate down into the inner futures being collected,3
support running filesystem isolation with command executor in mesoscontainerizer,4
linuxfilesystemisolatortestroot_persistentvolumewithoutrootfilesystem fails on centos 71 just ran root tests on centos 71 and had the following failure clean build just pulled from master noformat  run  linuxfilesystemisolatortestroot_persistentvolumewithoutrootfilesystem srctestscontainerizerfilesystem_isolator_testscpp498 failure waitfailure failed to clean up an isolator when destroying container 366b6d37b3264ed18a5f43d483dbbace failed to unmount volume tmplinuxfilesystemisolatortest_root_persistentvolumewithoutrootfilesystem_kxgvohsandboxvolume failed to unmount tmplinuxfilesystemisolatortest_root_persistentvolumewithoutrootfilesystem_kxgvohsandboxvolume invalid argument srctestsutilscpp75 failure osrmdirsandboxget device or resource busy  failed  linuxfilesystemisolatortestroot_persistentvolumewithoutrootfilesystem 1943 ms  1 test from linuxfilesystemisolatortest 1943 ms total  global test environment teardown  1 test from 1 test case ran 1951 ms total  passed  0 tests  failed  1 test listed below  failed  linuxfilesystemisolatortestroot_persistentvolumewithoutrootfilesystem noformat,2
unify the implementations of the image provisioners the current design uses separate provisioner implementation for each type of image eg appc docker this creates a lot of code duplications since we already have a unified provisioner backend eg copy bind overlayfs we should be able to unify the implementations of image provisioners and hide the image specific logics in the corresponding store implementation,5
unmount irrelevant host mounts in the new containers mount namespace as described in this todohttpsgithubcomapachemesosblobe601e469c64594dd8339352af405cbf26a574ea8srcslavecontainerizerisolatorsfilesystemlinuxcppl418 noformattitle  todojieyu try to unmount work directory mounts and persistent  volume mounts for other containers to release the extra  references to those mounts noformat this will a best effort attempt to alleviate the race condition between provisioners container cleanup and new containers copying host mount table,3
windows port protobuf_testshpp we have ported stoutprotobufhpp but to make the protobuf_testscpp file to work we need to port stoutuuidhpp,2
expand the range of integer precision in json  protobuf conversions to include unsigned integers the previous changes mesos3345 to support integer precision when converting json  protobuf did not support precision for unsigned integers between int64_max and uint64_max theres some loss but the conversion is still as goodbad as it was with doubles this problem is due to a limitation in the json parsing library we use picojson which parses integers as int64_t some possible solutions or things to investigate  we can patch picojson to parse some large values as uint64_t  we can investigate using another parsing library  if we want extra precision beyond 64 or 80 bits per double one possibility is the gmp libraryhttpsgmpliborg wed still need to change the parsing library though,5
higher level construct for expressing process dispatch since mesos code is based on the actor model and dispatching an interface asynchronously is a large part of the code base generalizing the concept of asynchronously dispatching an interface would eliminate the need to manual programming of the dispatch boilerplate an example usage for a simple interface like code class interface  virtual futuresize_t writetofileconst char data  0 virtual interface  code today the developer has to do the following a write a wrapper class that implements the same interface to add the dispatching boilerplate b spend precious time in reviews c risk introducing bugs none of the above steps add any value to the executable binary the wrapper class would look like code   hpp file class interfaceprocess class interfaceimpl  public interface  public tryownedinterfaceimpl createconst flags flags virtual futuresize_t writetofileconst char data interfaceimpl private ownedinterfaceprocess process    cpp file tryownedinterfaceimpl createconst flags flags   code to create the interfaceprocess class  future futuresize_t interfaceimplwritetofileconst char data  processdispatch interfaceprocesswritetofile data  interfaceimplinterfaceimpl   code to spawn the process  interfaceimplinterfaceimpl   code to stop the process  code at the callerclient site the code would look like code tryownedinterface in  interfaceimplcreateflags futuresize_t result  inwritetofiledata code proposal we should use cs rich language semnatics to express the intent and avoid the boilerplate we write manually the basic intent of the code that leads to all the boilerplate above is a an interface that provides a set of functionality b an implementation of the interface c ability to dispatch that interface asynchronously using actor c has a rich set of generics that can be used to express above components processdispatcher this component will dispatch an interface implementation asychronously using the process framework this component can be expressed as code processdispatcherinterface interfaceimplmentation code dispatchinterface any interface that provides an implementation that can be dispatched can be expressed using this component this component can be expressed as code dispatchableinterface code usage simple usage code tryowneddispatchableinterface dispatcher  processdispatcherinterface interfaceimplcreateflags futuresize_t result  dispatcherdispatch interfacewritetofile data code collecting the interface in a container code vectorowneddispatchableinterface dispatchcollection tryowneddispatchableinterface dispatcher1  processdispatcherinterface interfaceimpl1createflags tryowneddispatchableinterface dispatcher2  processdispatcherinterface interfaceimpl2createtest dispatchcollectionpush_backdispatcher1 dispatchcollectionpush_backdispatcher2 code the advantages of using the generic dispatcher saves time by avoiding to write all the boilerplate and going through review cycles less bugs focus on real problem and not boilerplate,6
add flag to disable hostname lookup in testing  buildinging dcos weve found that we need to set hostname explicitly on the masters for our uses ip and hostname must always be the same thing more in general under certain circumstances dynamic lookup of hostname while successful provides undesirable results we would also like in those circumstances be able to just set the hostname to the chosen ip address possibly set via the  ip_discovery_command method we suggest adding a nohostnamelookup note that we can introduce this flag as hostnamelookup with a default to true which is the current semantics and that way someone can do nohostnamelookup or hostnamelookupfalse,3
segfault when accepting or declining inverse offers discovered while writing a test for filters in regards to inverse offers fix here httpsreviewsapacheorgr38470,1
change machineup and machinedown endpoints to take an array with mesos3312 committed the machineup and machinedown endpoints should also take an input as an array it is important to change this before maintenance primitives are released httpsreviewsapacheorgr38011 also a minor change to the error message from these endpoints httpsreviewsapacheorgr37969,1
add metrics for filesystem isolation and image provisioning we need to know about 1 errors encountered while provisioning root filesystems 2 errors encountered while cleaning up root filesystems 3 number of containers changing root filesystem,2
provide the users with a fully writable filesystem in the first phase of filesystem provisioning and isolation we are disallowing or at least should especially in the case of copybackend users to write outside the sandbox without explicitly mounting specific volumes into the container we do this even when overlaybackend can potentially support a empty writable top layer however in the real world use of containers and for people coming from the vm world users and applications often are used to being able to write to the full filesystem restricted by plain file system permissions with reasons ranging from applications being nonportable filesystemwise to the need to do custom installs at run time to system directories inside its container in general its a good practice to restrict the application to write to confined locations and software dependencies can be managed through prepackaged layers but these often introduce a high entry barrier for users we should discuss a solution that gives the users the option to write to a full filesystem with a filesystem layer on top of provisioned images and optionally enable persistence of that layer through persistent volumes this has implication in the management of user namespaces and resource reservations and requires a thorough design,13
improve apply_reviewssh script to apply chain of reviews currently the supportapplyreviewsh script allows an user typically committer to apply a single review on top the head since mesos contributors typically submit a chain of reviews for a given issue it makes sense for the script to apply the whole chain recursively,8
usercgroupisolatortest failed on centos 66 usercgroupisolatortest use sysfscgroup as cgroups_hierarchy but centos 66 cgroups_hierarchy is cgroup need change to follow the way in containerizertest,1
registrytokentestexpiredtoken test is flaky registrytokentestexpiredtoken test is flaky here is the error i got on osx after running it for several times noformat  run  registrytokentestexpiredtoken srctestscontainerizerprovisioner_docker_testscpp167 failure value of tokeniserror actual false expected true libcabidylib terminating with uncaught exception of type testinginternalgoogletestfailureexception srctestscontainerizerprovisioner_docker_testscpp167 failure value of tokeniserror actual false expected true  aborted at 1442708631 unix time try date d 1442708631 if you are using gnu date  pc  0x7fff925fd286 __pthread_kill  sigabrt 0x7fff925fd286 received by pid 7082 tid 0x7fff7d7ad300 stack trace   0x7fff9041af1a _sigtramp  0x7fff59759968 unknown  0x7fff9bb429b3 abort  0x7fff90ce1a21 abort_message  0x7fff90d099b9 default_terminate_handler  0x7fff994767eb _objc_terminate  0x7fff90d070a1 std__terminate  0x7fff90d06d48 __cxa_rethrow  0x10781bb16 testinginternalhandleexceptionsinmethodifsupported  0x1077e9d30 testingunittestrun  0x106d59a91 run_all_tests  0x106d55d47 main  0x7fff8fc395c9 start  0x3 unknown abort trap 6 srcmesosbuild 3ee82e3  noformat,3
refactor status update method on agent to handle http based executors currently receiving a status update sent from slave to itself  runtask  killtask and status updates from executors are handled by the slavestatusupdate method on slave the signature of the method is void slavestatusupdatestatusupdate update const upid pid we need to create another overload of it that can also handle http based executors which the previous pid based function can also call into the signature of the new function could be void slavestatusupdatestatusupdate update executor executor the http executor would also call into this new function via srcslavehttpcpp,8
refactor executor struct in slave to handle http based executors currently the struct executor in slave only supports executors connected via message passing driver we should refactor it to add support for http based executors similar to what was done for the scheduler api struct framework in srcmastermasterhpp,3
add const accessor to master flags it would make sense to have an accessor to the masters flags especially for tests for example see this testhttpsgithubcomapachemesosblob2876b8c918814347dd56f6f87d461e414a90650asrctestsmaster_maintenance_testscppl1231l1235,2
linuxfilesystemisolator should make the slaves work_dir a shared mount so that when a user task is forked it does not hold extra references to the sandbox mount and provisioner bind backend mounts if we dont do that we could get the following error message when cleaning up bind backend mount points and sandbox mount points noformat e0921 173557268159 47010 bindcpp182 failed to remove rootfs mount point varlibmesosprovisionercontainers07eb666025ff4e838b2f06955567e04abackendsbindrootfses30f7e5e255d04d4da662f8aad0d56b33 device or resource busy e0921 173557268349 47010 provisionercpp403 failed to remove the provisioned container directory at varlibmesosprovisionercontainers07eb666025ff4e838b2f06955567e04a device or resource busy noformat,3
make hook execution order deterministic currently when using multiple hooks of the same type the execution order is implementationdefined this is because in srchookmanagercpp the list of available hooks is stored in a hashmapstring hook a hashmap is probably unnecessary for this task since the number of hooks should remain reasonable a data structure preserving ordering should be used instead to allow the user to predict the execution order of the hooks i suggest that the execution order should be the order in which hooks are specified with hooks when starting an agentmaster this will be useful when combining multiple hooks after mesos3366 is done,3
add support for exposing acceptdecline responses for inverse offers current implementation of maintenance primitives does not support exposing acceptdecline responses of frameworks to the cluster operators this functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule,2
mesos ui fails to represent json entities the mesos ui is broken it seems to fail to represent json from state this may have been introduced with httpsreviewsapacheorgr38028,1
enable ubuntu builds in asf ci ive disabled ubuntu1404 builds on asf ci because the job randomly fails on fetching packages code get406 httparchiveubuntucomubuntu trustyupdatesmain gdisk amd64 0881ubuntu01 185 kb err httparchiveubuntucomubuntu trustysecuritymain libldap242 amd64 24311nmu2ubuntu81 404 not found ip 911899115 80 err httparchiveubuntucomubuntu trustysecuritymain libfreetype6 amd64 2521ubuntu24 404 not found ip 911899115 80 err httparchiveubuntucomubuntu trustysecuritymain libicu52 amd64 5213ubuntu03 404 not found ip 911899115 80 fetched 213 mb in 1min 57s 1812 kbs 91me 0m 91m failed to fetch httparchiveubuntucomubuntupoolmainoopenldaplibldap242_24311nmu2ubuntu81_amd64deb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainffreetypelibfreetype6_2521ubuntu24_amd64deb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainiiculibicu52_5213ubuntu03_amd64deb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainggvfsgvfscommon_12030ubuntu11_alldeb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainggvfsgvfslibs_12030ubuntu11_amd64deb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainggvfsgvfsdaemons_12030ubuntu11_amd64deb 404 not found ip 911899115 80 e failed to fetch httparchiveubuntucomubuntupoolmainggvfsgvfs_12030ubuntu11_amd64deb 404 not found ip 911899115 80 e unable to fetch some archives maybe run aptget update or try with fixmissing 0mthe command binsh c aptget y install buildessential clang git maven autoconf libtool returned a nonzero code 100 code we need to figure out what the problem is and fix it before enabling testing on ubuntu,1
expose maintenance user doc via the documentation home page the committed docs can be found here httpmesosapacheorgdocumentationlatestmaintenance we need to add a link to docshomemd also the doc needs some minor formatting tweaks,1
create interface for digest verifier add interface for digest verifier so that we can add implementations for digest types like sha256 sha512 etc,2
add implementation for sha256 based file content verification httpsreviewsapacheorgr38747,3
add a test for osrealpath,1
configure cannot find libevent headers in centos 6 if libevent is installed via sudo yum install libeventheaders running configure enablelibevent will fail to discover the libevent headers code checking event2eventh usability no checking event2eventh presence no checking for event2eventh no configure error cannot find libevent headers  libevent is required for libprocess to build  code,2
introduce mesos_sandbox environment variable in mesos containerizer similar to docker containerizer if a container changes rootfs well have two environment variables mesos_directory the path in the host filesystem mesos_sandbox the path in the container filesystem,3
build instructions for centos 66 should include sudo yum update neglecting to run sudo yum update on centos 66 currently causes the build to break when building mesos0250jar the build instructions for this platform on the getting started page should be changed accordingly,1
synchronize v1 helper functions with prev1,5
dont retry close on eintr on linux retrying close on eintr is dangerous because the fd is already released and we may accidentally close a newly opened fd from another thread see httpewontfixcom4 httplwnnetarticles576478 httplwnnetarticles576591 it appears that other oses like hpux require a retry of close on eintr the austin group recently proposed changes to posix to require that the eintr case need a retry but einprogress be used for when a retry should not occur httpaustingroupbugsnetviewphpid529 however linux does not follow this and so we need to remove our eintr retries some more links for posterity httpsgithubcomwaherncqueuesissues56issuecomment108656004 httpscodegooglecompchromiumissuesdetailid269623 httpscodereviewchromiumorg23455051,1
cgroups test filters aborts tests on centos 66 running make check on centos 66 causes all tests to abort due to check_some test in cgroupsfilter code build directory homejenkinsworkspacemesosconfigcentos6build f0923 230049748896 27362 environmentcpp132 check_somehierarchies_ failed to determine canonical path of sysfscgroupfreezer no such file or directory  check failure stack trace   0x7fb786ca0c4d googlelogmessagefail  0x7fb786ca298c googlelogmessagesendtolog  0x7fb786ca083c googlelogmessageflush  0x7fb786ca3289 googlelogmessagefatallogmessagefatal  0x58e66c mesosinternaltestscgroupsfiltercgroupsfilter  0x58712f mesosinternaltestsenvironmentenvironment  0x4c882f main  0x7fb782767d5d __libc_start_main  0x4d6331 unknown make3  checklocal aborted code,1
support subscribe call for http based executors we need to add a subscribe method in srcslaveslavecpp to introduce the ability for http based executors to subscribe and then receive events on the persistent http connection most of the functionality needed would be similar to mastersubscribe in srcmastermastercpp,5
add user doc for networking support in mesos 0250,2
fix file descriptor leakage  double close in the code base,3
add an abstraction to manage the life cycle of file descriptors in order to avoid missing close calls on file descriptors or doubleclosing file descriptors it would be nice to add a reference counted filedescriptor in a similar way to what weve done for socket this will be closed automatically when the last reference goes away and double closes can be prevented via internal state,5
figure out how to enforce 64bit builds on windows we need to make sure people dont try to compile mesos on 32bit architectures we dont want a windows repeat of something like this httpsissuesapacheorgjirabrowsemesos267,3
validate that slaves work_dir is a shared mount in its own peer group when linuxfilesystemisolator is used to address this todo in the code noformat srcslavecontainerizerisolatorsfilesystemlinuxcpp 122  todojieyu currently we dont check if the slaves work_dir  mount is a shared mount or not we just assume it is we cannot  simply mark the slave as shared again because that will create a  new peer group for the mounts this is a temporary workaround for  now while we are thinking about fixes noformat,3
libevent termination triggers broken pipe when the libevent loop terminates and we unblock the sigpipe signal the pending sigpipe instantly triggers and causes a broken pipe when the test binary stops running code program received signal sigpipe broken pipe switching to thread 0x7ffff18b4700 lwp 16270 pthread_sigmask how1 newmaskoptimized out oldmask0x7ffff18b3d80 at sysdepsunixsysvlinuxpthread_sigmaskc53 53 sysdepsunixsysvlinuxpthread_sigmaskc no such file or directory gdb bt 0 pthread_sigmask how1 newmaskoptimized out oldmask0x7ffff18b3d80 at sysdepsunixsysvlinuxpthread_sigmaskc53 1 0x00000000006fd9a4 in unblock  at 3rdpartylibprocess3rdpartystoutincludestoutosposixsignalshpp90 2 0x00000000007d7915 in run  at 3rdpartylibprocesssrclibeventcpp125 3 0x00000000007950cb in _m_invokevoid  at usrincludec49functional1700 4 0x0000000000795000 in operator  at usrincludec49functional1688 5 0x0000000000794f6e in _m_run  at usrincludec49thread115 6 0x00007ffff668de30 in   from usrlibx86_64linuxgnulibstdcso6 7 0x00007ffff79a16aa in start_thread arg0x7ffff18b4700 at pthread_createc333 8 0x00007ffff5df1eed in clone  at sysdepsunixsysvlinuxx86_64clones109 code,2
create a executor library based on the new executor http api similar to the scheduler library srcschedulerschedulercpp  we would need a executor library that speaks the new executor http api,5
replace use of strerror with threadsafe alternatives strerror_r  strerror_l strerror is not required to be thread safe by posix and is listed as unsafe on linux httppubsopengrouporgonlinepubs9699919799 httpman7orglinuxmanpagesman3strerror3html i dont believe weve seen any issues reported due to this we should replace occurrences of strerror accordingly possibly offering a wrapper in stout to simplify callsites,3
check failure due to floating point precision on reservation request resultcpus  cpus check is failing due to  double  double  comparison problem root cause  framework requested 01 cpu reservation for the first task so far so good next reserve operation  lead to double operations resulting in following double values  resultscpus  239999999999999964472863211995 cpus  24 and the check  resultcpus  cpus  failed the double arithmetic operations caused resultscpus value to be  239999999999999964472863211995 and hence  239999999999999964472863211995  24  failed,3
libprocess_ip not passed when executors environment is specified when the executors environment is specified explicitly via executor_environment_variables libprocess_ip will not be passed leading to errors in some cases  for example when no dns is available,2
allocator changes trigger large recompiles due to the templatized nature of the allocator even small changes trigger large recompiles of the codebase this make iterating on changes expensive for developers,3
mesoscli broken in 024x the issue was initially reported on the mailing list httpwwwmailarchivecomusermesosapacheorgmsg04670html the format of the master data stored in zookeeper has changed but the mesoscli does not reflect these changes causing tools like mesostail and mesosps to fail example error from mesostail noformat mesosmaster  mesos tail f n 50 service traceback most recent call last file usrlocalbinmesostail line 11 in module sysexitmain file usrlocallibpython27distpackagesmesoscliclipy line 61 in wrapper return fnargs kwargs file usrlocallibpython27distpackagesmesosclicmdstailpy line 55 in main argstask argsfile failnot argsfollow file usrlocallibpython27distpackagesmesoscliclusterpy line 27 in files tlist  mastertasksfltr file usrlocallibpython27distpackagesmesosclimasterpy line 174 in tasks self_task_listactive_only file usrlocallibpython27distpackagesmesosclimasterpy line 153 in _task_list utilmergex keys for x in selfframeworksactive_only file usrlocallibpython27distpackagesmesosclimasterpy line 185 in frameworks return utilmergeselfstate keys file usrlocallibpython27distpackagesmesoscliutilpy line 58 in __get__ value  selffgetinst file usrlocallibpython27distpackagesmesosclimasterpy line 123 in state return selffetchmasterstatejsonjson file usrlocallibpython27distpackagesmesosclimasterpy line 64 in fetch return requestsgeturlparseurljoinselfhost url kwargs file usrlocallibpython27distpackagesrequestsapipy line 69 in get return requestget url paramsparams kwargs file usrlocallibpython27distpackagesrequestsapipy line 50 in request response  sessionrequestmethodmethod urlurl kwargs file usrlocallibpython27distpackagesrequestssessionspy line 451 in request prep  selfprepare_requestreq file usrlocallibpython27distpackagesrequestssessionspy line 382 in prepare_request hooksmerge_hooksrequesthooks selfhooks file usrlocallibpython27distpackagesrequestsmodelspy line 304 in prepare selfprepare_urlurl params file usrlocallibpython27distpackagesrequestsmodelspy line 357 in prepare_url raise invalidurleargs requestsexceptionsinvalidurl failed to parse 1010011005050port5050version0241 noformat the problem exists in httpsgithubcommesospheremesoscliblobmastermesosclimasterpyl107 the code should be along the lines of noformat try parsed  jsonloadsval return parsedaddressip    strparsedaddressport except exception return valsplit1 noformat this causes the master address to come back correctly,1
implement httpcommandexecutor that uses the executor library instead of using the mesosexecutordriver  we should make the commandexecutor in srclauncherexecutorcpp use the new executor http library that we create in mesos3550 this would act as a good validation of the http api implementation,13
make the command scheduler use the http scheduler library we should make the command scheduler in srccliexecutorcpp use the scheduler library srcschedulerschedulercpp instead of the scheduler driver,3
jsonbased credential files do not work correctly specifying the following credentials file code  credentials   principal user secret password    code then hitting a master endpoint with code curl i u userpassword  code does not work this is contrary to the textbased credentials file which works code user password code currently the password in a jsonbased credentials file needs to be base64encoded in order for it to work code  credentials   principal user secret cgfzc3dvcmq    code,1
revocable task cpu shows as zero in statejson the slaves statejson reports revocable task resources as zero noformat resources  cpus 0 disk 3071 mem 1248 ports 3171531715  noformat also there is no indication that a task uses revocable cpu it would be great to have this type of info,2
support tcp checks in mesos health check program in marathon we have the ability to specify health checks for  command mesos supports this  http see progress in mesos2533  tcp missing see here for reference httpsmesospheregithubiomarathondocshealthcheckshtml since we made good experiences with those 3 options in marathon i see a lot of value if mesos would also support them,8
make scheduler library use http pipelining abstraction in libprocess currently the scheduler library sends calls in order by chaining them and sending them only when it has received a response for the earlier call this was done because there was no http pipelining abstraction in libprocess processpost however once mesos3332 is resolved we should be now able to use the new abstraction,8
refactor registry_client refactor registry client component to  make methods shorter for readability  pull out structs not related to registry client into common namespace,5
mesos does not kill orphaned docker containers after upgrade to 0240 we noticed hanging containers appearing looks like there were changes between 0230 and 0240 that broke cleanup heres how to trigger this bug 1 deploy app in docker container 2 kill corresponding mesosdockerexecutor process 3 observe hanging container here are the logs after kill noformat slave_1  i1002 121259362002 7791 dockercpp1576 executor for container f083aaa2d5c343c1b6ba342de8829fa8 has exited slave_1  i1002 121259362284 7791 dockercpp1374 destroying container f083aaa2d5c343c1b6ba342de8829fa8 slave_1  i1002 121259363404 7791 dockercpp1478 running docker stop on container f083aaa2d5c343c1b6ba342de8829fa8 slave_1  i1002 121259363876 7791 slavecpp3399 executor sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 terminated with signal terminated slave_1  i1002 121259367570 7791 slavecpp2696 handling status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 from 00000 slave_1  i1002 121259367842 7791 slavecpp5094 terminating task sleepy87eb619168fe11e594448eb895523b9c slave_1  w1002 121259368484 7791 dockercpp986 ignoring updating unknown container f083aaa2d5c343c1b6ba342de8829fa8 slave_1  i1002 121259368671 7791 status_update_managercpp322 received status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 slave_1  i1002 121259368741 7791 status_update_managercpp826 checkpointing update for status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 slave_1  i1002 121259370636 7791 status_update_managercpp376 forwarding update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 to the slave slave_1  i1002 121259371335 7791 slavecpp2975 forwarding the update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 to master17216911285050 slave_1  i1002 121259371908 7791 slavecpp2899 status update manager successfully handled status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 master_1  i1002 121259372047 11 mastercpp4069 status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 from slave 20151002120829215345169250501s0 at slave117216911285051 1721691128 master_1  i1002 121259372534 11 mastercpp4108 forwarding status update task_failed uuid 4a1b2387a4694f01bfcb0d1cccbde550 for task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 master_1  i1002 121259373018 11 mastercpp5576 updating the latest state of task sleepy87eb619168fe11e594448eb895523b9c of framework 201509231221302153451692505010000 to task_failed master_1  i1002 121259373447 11 hierarchicalhpp814 recovered cpus01 mem16 ports3168531685 total cpus4 mem1001 disk52869 ports3100032000 allocated cpus832667e17 on slave 20151002120829215345169250501s0 from framework 201509231221302153451692505010000 noformat another issue if you restart mesosslave on the host with orphaned docker containers they are not getting killed this was the case before and i hoped for this trick to kill hanging containers but it doesnt work now marking this as critical because it hoards cluster resources and blocks scheduling,5
v1 api javapython protos are not generated the javapython protos for the v1 api should be generated according to the makefile however they do not show up in the generated build directory,2
fetchercachetestlocaluncachedextract is flaky from asf ci httpsbuildsapacheorgjobmesos866compilerclangconfigurationverboseosubuntu1404label_expdocker7c7chadoopconsole code  run  fetchercachetestlocaluncachedextract using temporary directory tmpfetchercachetest_localuncachedextract_jhbfea i0925 191539541198 27410 leveldbcpp176 opened db in 343934ms i0925 191539542362 27410 leveldbcpp183 compacted db in 1136184ms i0925 191539542428 27410 leveldbcpp198 created db iterator in 35866ns i0925 191539542448 27410 leveldbcpp204 seeked to beginning of db in 8807ns i0925 191539542459 27410 leveldbcpp273 iterated through 0 keys in the db in 6325ns i0925 191539542505 27410 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0925 191539543143 27438 recovercpp449 starting replica recovery i0925 191539543393 27438 recovercpp475 replica is in empty status i0925 191539544373 27436 replicacpp641 replica in empty status received a broadcasted recover request i0925 191539544791 27433 recovercpp195 received a recover response from a replica in empty status i0925 191539545284 27433 recovercpp566 updating replica status to starting i0925 191539546155 27436 mastercpp376 master c8bf1c9550f44832a570c560f0b466ae f57fd4291168 started on 17217119541781 i0925 191539546257 27433 leveldbcpp306 persisting metadata 8 bytes to leveldb took 747249ns i0925 191539546288 27433 replicacpp323 persisted replica status to starting i0925 191539546483 27434 recovercpp475 replica is in starting status i0925 191539546187 27436 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpfetchercachetest_localuncachedextract_jhbfeacredentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0260_instsharemesoswebui work_dirtmpfetchercachetest_localuncachedextract_jhbfeamaster zk_session_timeout10secs i0925 191539546567 27436 mastercpp423 master only allowing authenticated frameworks to register i0925 191539546617 27436 mastercpp428 master only allowing authenticated slaves to register i0925 191539546632 27436 credentialshpp37 loading credentials for authentication from tmpfetchercachetest_localuncachedextract_jhbfeacredentials i0925 191539546931 27436 mastercpp467 using default crammd5 authenticator i0925 191539547044 27436 mastercpp504 authorization enabled i0925 191539547276 27441 whitelist_watchercpp79 no whitelist given i0925 191539547320 27434 hierarchicalhpp468 initialized hierarchical allocator process i0925 191539547471 27438 replicacpp641 replica in starting status received a broadcasted recover request i0925 191539548318 27443 recovercpp195 received a recover response from a replica in starting status i0925 191539549067 27435 recovercpp566 updating replica status to voting i0925 191539549115 27440 mastercpp1603 the newly elected leader is master17217119541781 with id c8bf1c9550f44832a570c560f0b466ae i0925 191539549162 27440 mastercpp1616 elected as the leading master i0925 191539549190 27440 mastercpp1376 recovering from registrar i0925 191539549342 27434 registrarcpp309 recovering registrar i0925 191539549666 27430 leveldbcpp306 persisting metadata 8 bytes to leveldb took 418187ns i0925 191539549753 27430 replicacpp323 persisted replica status to voting i0925 191539550089 27442 recovercpp580 successfully joined the paxos group i0925 191539550320 27442 recovercpp464 recover process terminated i0925 191539550904 27430 logcpp661 attempting to start the writer i0925 191539551955 27434 replicacpp477 replica received implicit promise request with proposal 1 i0925 191539552351 27434 leveldbcpp306 persisting metadata 8 bytes to leveldb took 380746ns i0925 191539552372 27434 replicacpp345 persisted promised to 1 i0925 191539552896 27436 coordinatorcpp231 coordinator attemping to fill missing position i0925 191539554003 27432 replicacpp378 replica received explicit promise request for position 0 with proposal 2 i0925 191539554534 27432 leveldbcpp343 persisting action 8 bytes to leveldb took 510572ns i0925 191539554558 27432 replicacpp679 persisted action at 0 i0925 191539555516 27443 replicacpp511 replica received write request for position 0 i0925 191539555595 27443 leveldbcpp438 reading position from leveldb took 65355ns i0925 191539556177 27443 leveldbcpp343 persisting action 14 bytes to leveldb took 542757ns i0925 191539556200 27443 replicacpp679 persisted action at 0 i0925 191539556813 27429 replicacpp658 replica received learned notice for position 0 i0925 191539557251 27429 leveldbcpp343 persisting action 16 bytes to leveldb took 422272ns i0925 191539557281 27429 replicacpp679 persisted action at 0 i0925 191539557315 27429 replicacpp664 replica learned nop action at position 0 i0925 191539558061 27442 logcpp677 writer started with ending position 0 i0925 191539559294 27434 leveldbcpp438 reading position from leveldb took 56536ns i0925 191539560333 27432 registrarcpp342 successfully fetched the registry 0b in 10936064ms i0925 191539560469 27432 registrarcpp441 applied 1 operations in 41340ns attempting to update the registry i0925 191539561244 27441 logcpp685 attempting to append 176 bytes to the log i0925 191539561378 27436 coordinatorcpp341 coordinator attempting to write append action at position 1 i0925 191539562126 27439 replicacpp511 replica received write request for position 1 i0925 191539562515 27439 leveldbcpp343 persisting action 195 bytes to leveldb took 364968ns i0925 191539562539 27439 replicacpp679 persisted action at 1 i0925 191539563160 27438 replicacpp658 replica received learned notice for position 1 i0925 191539563699 27438 leveldbcpp343 persisting action 197 bytes to leveldb took 455933ns i0925 191539563730 27438 replicacpp679 persisted action at 1 i0925 191539563753 27438 replicacpp664 replica learned append action at position 1 i0925 191539564749 27434 registrarcpp486 successfully updated the registry in 4214016ms i0925 191539564893 27434 registrarcpp372 successfully recovered registrar i0925 191539564950 27442 logcpp704 attempting to truncate the log to 1 i0925 191539565039 27429 coordinatorcpp341 coordinator attempting to write truncate action at position 2 i0925 191539565172 27430 mastercpp1413 recovered 0 slaves from the registry 137b  allowing 10mins for slaves to reregister i0925 191539565946 27429 replicacpp511 replica received write request for position 2 i0925 191539566349 27429 leveldbcpp343 persisting action 16 bytes to leveldb took 375473ns i0925 191539566371 27429 replicacpp679 persisted action at 2 i0925 191539566994 27431 replicacpp658 replica received learned notice for position 2 i0925 191539567440 27431 leveldbcpp343 persisting action 18 bytes to leveldb took 437095ns i0925 191539567483 27431 leveldbcpp401 deleting 1 keys from leveldb took 31954ns i0925 191539567498 27431 replicacpp679 persisted action at 2 i0925 191539567514 27431 replicacpp664 replica learned truncate action at position 2 i0925 191539576660 27410 containerizercpp143 using isolation posixcpuposixmemfilesystemposix w0925 191539577055 27410 backendcpp50 failed to create bind backend bindbackend requires root privileges i0925 191539583020 27443 slavecpp190 slave started on 4617217119541781 i0925 191539583062 27443 slavecpp191 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpfetchercachetest_localuncachedextract_lwfzk4credential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpfetchercachetest_localuncachedextract_lwfzk4fetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0260_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resource_monitoring_interval1secs resourcescpus1000 mem1000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpfetchercachetest_localuncachedextract_lwfzk4 i0925 191539583472 27443 credentialshpp85 loading credential for authentication from tmpfetchercachetest_localuncachedextract_lwfzk4credential i0925 191539583752 27443 slavecpp321 slave using credential for testprincipal i0925 191539584249 27443 slavecpp354 slave resources cpus1000 mem1000 disk370122e06 ports3100032000 i0925 191539584344 27443 slavecpp390 slave hostname f57fd4291168 i0925 191539584362 27443 slavecpp395 slave checkpoint true i0925 191539585180 27428 statecpp54 recovering state from tmpfetchercachetest_localuncachedextract_lwfzk4meta i0925 191539585383 27440 status_update_managercpp202 recovering status update manager i0925 191539585636 27435 containerizercpp386 recovering containerizer i0925 191539586380 27438 slavecpp4110 finished recovery i0925 191539586845 27438 slavecpp4267 querying resource estimator for oversubscribable resources i0925 191539587059 27430 status_update_managercpp176 pausing sending status updates i0925 191539587064 27438 slavecpp705 new master detected at master17217119541781 i0925 191539587139 27438 slavecpp768 authenticating with master master17217119541781 i0925 191539587163 27438 slavecpp773 using default crammd5 authenticatee i0925 191539587321 27438 slavecpp741 detecting new master i0925 191539587357 27434 authenticateecpp115 creating new client sasl connection i0925 191539587574 27438 slavecpp4281 received oversubscribable resources from the resource estimator i0925 191539587739 27442 mastercpp5138 authenticating slave4617217119541781 i0925 191539587853 27441 authenticatorcpp407 starting authentication session for crammd5_authenticatee13917217119541781 i0925 191539588052 27439 authenticatorcpp92 creating new server sasl connection i0925 191539588248 27431 authenticateecpp206 received sasl authentication mechanisms crammd5 i0925 191539588297 27431 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0925 191539588443 27437 authenticatorcpp197 received sasl authentication start i0925 191539588506 27437 authenticatorcpp319 authentication requires more steps i0925 191539588677 27443 authenticateecpp252 received sasl authentication step i0925 191539588814 27436 authenticatorcpp225 received sasl authentication step i0925 191539588855 27436 auxpropcpp102 request to lookup properties for user testprincipal realm f57fd4291168 server fqdn f57fd4291168 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0925 191539588876 27436 auxpropcpp174 looking up auxiliary property userpassword i0925 191539588937 27436 auxpropcpp174 looking up auxiliary property cmusaslsecretcrammd5 i0925 191539588979 27436 auxpropcpp102 request to lookup properties for user testprincipal realm f57fd4291168 server fqdn f57fd4291168 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0925 191539588997 27436 auxpropcpp124 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0925 191539589011 27436 auxpropcpp124 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0925 191539589036 27436 authenticatorcpp311 authentication success i0925 191539589126 27443 authenticateecpp292 authentication success i0925 191539589192 27437 mastercpp5168 successfully authenticated principal testprincipal at slave4617217119541781 i0925 191539589238 27433 authenticatorcpp425 authentication session cleanup for crammd5_authenticatee13917217119541781 i0925 191539589412 27440 slavecpp836 successfully authenticated with master master17217119541781 i0925 191539589540 27440 slavecpp1230 will retry registration in 13562027ms if necessary i0925 191539589745 27436 mastercpp3862 registering slave at slave4617217119541781 f57fd4291168 with id c8bf1c9550f44832a570c560f0b466aes0 i0925 191539590121 27438 registrarcpp441 applied 1 operations in 70627ns attempting to update the registry i0925 191539590831 27430 logcpp685 attempting to append 345 bytes to the log i0925 191539590927 27439 coordinatorcpp341 coordinator attempting to write append action at position 3 i0925 191539591809 27430 replicacpp511 replica received write request for position 3 i0925 191539592072 27430 leveldbcpp343 persisting action 364 bytes to leveldb took 221734ns i0925 191539592099 27430 replicacpp679 persisted action at 3 i0925 191539592643 27442 replicacpp658 replica received learned notice for position 3 i0925 191539593215 27442 leveldbcpp343 persisting action 366 bytes to leveldb took 560946ns i0925 191539593237 27442 replicacpp679 persisted action at 3 i0925 191539593255 27442 replicacpp664 replica learned append action at position 3 i0925 191539594663 27433 registrarcpp486 successfully updated the registry in 4472832ms i0925 191539594874 27431 logcpp704 attempting to truncate the log to 3 i0925 191539595407 27429 slavecpp3138 received ping from slaveobserver4517217119541781 i0925 191539595450 27433 coordinatorcpp341 coordinator attempting to write truncate action at position 4 i0925 191539596017 27442 replicacpp511 replica received write request for position 4 i0925 191539596029 27429 hierarchicalhpp675 added slave c8bf1c9550f44832a570c560f0b466aes0 f57fd4291168 with cpus1000 mem1000 disk370122e06 ports3100032000 allocated  i0925 191539595952 27441 mastercpp3930 registered slave c8bf1c9550f44832a570c560f0b466aes0 at slave4617217119541781 f57fd4291168 with cpus1000 mem1000 disk370122e06 ports3100032000 i0925 191539596240 27429 hierarchicalhpp1326 no resources available to allocate i0925 191539596263 27439 slavecpp880 registered with master master17217119541781 given slave id c8bf1c9550f44832a570c560f0b466aes0 i0925 191539596341 27439 fetchercpp77 clearing fetcher cache i0925 191539596345 27429 hierarchicalhpp1421 no inverse offers to send out i0925 191539596367 27429 hierarchicalhpp1239 performed allocation for slave c8bf1c9550f44832a570c560f0b466aes0 in 299337ns i0925 191539596524 27434 status_update_managercpp183 resuming sending status updates i0925 191539596571 27442 leveldbcpp343 persisting action 16 bytes to leveldb took 575374ns i0925 191539596662 27442 replicacpp679 persisted action at 4 i0925 191539596984 27439 slavecpp903 checkpointing slaveinfo to tmpfetchercachetest_localuncachedextract_lwfzk4metaslavesc8bf1c9550f44832a570c560f0b466aes0slaveinfo i0925 191539597522 27434 replicacpp658 replica received learned notice for position 4 i0925 191539597553 27410 schedcpp164 version 0260 i0925 191539597746 27439 slavecpp939 forwarding total oversubscribed resources i0925 191539598021 27429 mastercpp4272 received update of slave c8bf1c9550f44832a570c560f0b466aes0 at slave4617217119541781 f57fd4291168 with total oversubscribed resources i0925 191539598070 27434 leveldbcpp343 persisting action 18 bytes to leveldb took 531503ns i0925 191539598162 27434 leveldbcpp401 deleting 2 keys from leveldb took 79081ns i0925 191539598170 27428 schedcpp262 new master detected at master17217119541781 i0925 191539598206 27434 replicacpp679 persisted action at 4 i0925 191539598238 27434 replicacpp664 replica learned truncate action at position 4 i0925 191539598276 27428 schedcpp318 authenticating with master master17217119541781 i0925 191539598296 27428 schedcpp325 using default crammd5 authenticatee i0925 191539598950 27430 hierarchicalhpp735 slave c8bf1c9550f44832a570c560f0b466aes0 f57fd4291168 updated with oversubscribed resources total cpus1000 mem1000 disk370122e06 ports3100032000 allocated  i0925 191539599242 27430 hierarchicalhpp1326 no resources available to allocate i0925 191539599282 27430 hierarchicalhpp1421 no inverse offers to send out i0925 191539599341 27430 hierarchicalhpp1239 performed allocation for slave c8bf1c9550f44832a570c560f0b466aes0 in 327742ns i0925 191539599632 27437 authenticateecpp115 creating new client sasl connection i0925 191539600005 27428 mastercpp5138 authenticating schedulerdda30e8e47b74b1d9a9632364754be6317217119541781 i0925 191539600170 27435 authenticatorcpp407 starting authentication session for crammd5_authenticatee14017217119541781 i0925 191539600518 27433 authenticatorcpp92 creating new server sasl connection i0925 191539600788 27436 authenticateecpp206 received sasl authentication mechanisms crammd5 i0925 191539600831 27436 authenticateecpp232 attempting to authenticate with mechanism crammd5 i0925 191539600944 27433 authenticatorcpp197 received sasl authentication start i0925 191539601019 27433 authenticatorcpp319 authentication requires more steps i0925 191539601150 27436 authenticateecpp252 received sasl authentication step i0925 191539601284 27436 authenticatorcpp225 received sasl authentication step i0925 191539601326 27436 auxpropcpp102 request to lookup properties for user testprincipal realm f57fd4291168 server fqdn f,2
license headers show up all over doxygen documentation currently license headers are commented in something resembling javadoc style code   licensed  code since we use javadocstyle comment blocks for doxygen documentation all license headers appear in the generated documentation potentially and likely hiding the actual documentation using  to start the comment blocks would be enough to hide them from doxygen but would likely also result in a largish though mostly uninteresting patch,2
introduce stream ids in http scheduler api currently the http scheduler api has no concept of sessions aka sessionid or a tokenid this is useful in some failure scenarios as of now if a framework fails over and then subscribes again with the same frameworkid with the force option set the mesos master would subscribe it if the previous instance of the frameworkscheduler tries to send a call  eg callkill with the same previous frameworkid set it would be still accepted by the master leading to erroneously killing a task this is possible because we do not have a way currently of distinguishing connections it used to work in the previous driver implementation due to the master also performing a upid check to verify if they matched and only then allowing the call following the design process we will implemented stream ids for mesos http schedulers each id will be associated with a single subscription connection and the scheduler must include it as a header in all nonsubscribe calls sent to the master,5
rename libprocess tests to libprocesstests stout tests are in a binary named stouttests mesos tests are in mesostests but libprocess tests are just tests it would be helpful to name them libprocesstests,1
add a test module for ippercontainer support with the addition of networkinfo to allow frameworks to request ippercontainer for their tasks we should add a simple module that mimics the behavior of a real networkisolation module for testing purposes we can then add this module in srcexamples and write some tests against it this module can also serve as a template module for thirdparty network isolation provides for building their own network isolator modules,3
memorypressuremesostestcgroups_root_statistics and cgroups_root_slaverecovery are flaky i am install mesos 0240 on 4 servers which have very similar hardware and software configurations after performing configure make and make check some servers have completed successfully and other failed on test  run  memorypressuremesostestcgroups_root_statistics is there something i should check in this test code performed make check node001  run  memorypressuremesostestcgroups_root_statistics i1005 143735585067 38479 execcpp133 version 0240 i1005 143735593789 38497 execcpp207 executor registered on slave 2015100514373523937682023510627900s0 registered executor on svdidac038techlabsaccenturecom starting task 010b2fe94eac41368a8a6ce7665488b0 forked command at 38510 sh c while true do dd count512 bs1m ifdevzero oftemp done performed make check node002  run  memorypressuremesostestcgroups_root_statistics i1005 143858794112 36997 execcpp133 version 0240 i1005 143858802851 37022 execcpp207 executor registered on slave 2015100514385723602137705042726325s0 registered executor on svdidac039techlabsaccenturecom starting task 9bb317ba41cb44a4b507d1c85ceabc28 sh c while true do dd count512 bs1m ifdevzero oftemp done forked command at 37028 srctestscontainerizermemory_pressure_testscpp145 failure expected usagegetmem_medium_pressure_counter  usagegetmem_critical_pressure_counter actual 5 vs 6 20151005 143900130263250x2af08cc78700zoo_errorhandle_socket_error_msg1697 socket 12700137198 zk retcode4 errno111connection refused server refused to accept the client  failed  memorypressuremesostestcgroups_root_statistics 4303 ms code,1
framework failover when framework is active does not trigger allocation fwict this is just a consequence of some technical debt in the master code when an active framework fails over we do not go through the deactivationactivation code paths and so 1 the frameworks filters in the allocator remain after the failover 2 the failed over framework does not receive an immediate allocation it has to wait for the next allocation interval if the framework had disconnected first then the failover goes through the deactivationactivation code paths this also means that some tests take longer to run than necessary,5
propagate isolatorprepare failures to the framework currently if isolatorprepare fails for some isolators we simply return a generic message about container being destroyed during launch it would be especially helpful if a thirdparty isolator modules could report the error back to the framework,2
framework process hangs after master failover when number frameworks  libprocess thread pool size when running multi framework instances per process if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock eg on a machine with 24 cpus if the framework instance count exceeds 24  per process then when the master fails over all the libprocess threads block updating the cache  groupprocess leading to deadlock below is the stack trace of one the libprocess thread  code thread 101 thread 0x7f42821f1700 lwp 5974 0 0x000000314100b5bc in pthread_cond_waitglibc_232  from lib64libpthreadso0 1 0x00007f42870d1637 in gatearrivelong  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 2 0x00007f42870be87c in processprocessmanagerwaitprocessupid const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eg gmesosnative_mesosso 3 0x00007f42870c25f7 in processwaitprocessupid const duration const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64e ggmesosnative_mesosso 4 0x00007f428708e294 in processlatchawaitduration const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnativ e_mesosso 5 0x00007f4286b67dea in processfutureintawaitduration const const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64egg mesosnative_mesosso 6 0x00007f4286b5a0df in processfutureintget const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_me sosso 7 0x00007f4286ff0508 in zookeepergetchildrenstdbasic_stringchar stdchar_traitschar stdallocatorchar  const bool stdvectorstdbasic_stringchar stdcha r_traitschar stdallocatorchar  stdallocatorstdbasic_stringchar stdchar_traitschar stdallocatorchar     from usersmchadhavenvlibpython27site packagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 8 0x00007f4286cb394e in zookeepergroupprocesscache  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mes osso 9 0x00007f4286cb1e63 in zookeepergroupprocessupdatedlong stdbasic_stringchar stdchar_traitschar stdallocatorchar  const  from usersmchadhavenvlibpy thon27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 10 0x00007f4286ce027a in stdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stringchar stdchar_traitschar stdallocatorchar  constoperatorzo okeepergroupprocess long stdbasic_stringchar stdchar_traitschar stdallocatorchar  const const  from usersmchadhavenvlibpython27sitepackagesmesosn ative0221003py27linuxx86_64eggmesosnative_mesosso 11 0x00007f4286ce0067 in stdtr1result_ofstdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stringchar stdchar_traitschar stdallocatorchar  con st stdtr1result_ofstdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocesstype stdtr1res ult_ofstdtr1_mulong false false long stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocess type stdtr1result_ofstdtr1_mustdbasic_stringchar stdchar_traitschar stdallocatorchar  false false stdbasic_stringchar stdchar_traitschar  stdallocatorchar  stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocesstypetype stdtr1 _bindstdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stringchar stdchar_traitschar stdallocatorchar  const stdtr1_placeholder1 lo ng stdbasic_stringchar stdchar_traitschar stdallocatorchar __callzookeepergroupprocess 0 1 2stdtr1_mustdtr1_placeholder1 false true  c onststdtr1_placeholder1 stdtr1tuplezookeepergroupprocess stdtr1_index_tuple0 1 2  from usersmchadhavenvlibpython27sitepackagesmesosnati ve0221003py27linuxx86_64eggmesosnative_mesosso 12 0x00007f4286cdfd16 in stdtr1result_ofstdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stringchar stdchar_traitschar stdallocatorchar  con st stdtr1result_ofstdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocesstype stdtr1resu lt_ofstdtr1_mulong false false long stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocess type stdtr1result_ofstdtr1_mustdbasic_stringchar stdchar_traitschar stdallocatorchar  false false stdbasic_stringchar stdchar_traitschar stdallocatorchar  stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplezookeepergroupprocesstypetype stdtr1_ bindstdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stringchar stdchar_traitschar stdallocatorchar  const stdtr1_placeholder1 long stdbasic_stringchar stdchar_traitschar stdallocatorchar operatorzookeepergroupprocesszookeepergroupprocess  from usersmchadhavenvlibpython2 7sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 13 0x00007f4286cdf8be in stdtr1_function_handlervoid zookeepergroupprocess stdtr1_bindstdtr1_mem_fnvoid zookeepergroupprocesslong stdbasic_stri ngchar stdchar_traitschar stdallocatorchar  const stdtr1_placeholder1 long stdbasic_stringchar stdchar_traitschar stdallocatorchar  _ m_invokestdtr1_any_data const zookeepergroupprocess  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative _mesosso 14 0x00007f4286cc2394 in stdtr1functionvoid zookeepergroupprocessoperatorzookeepergroupprocess const  from usersmchadhavenvlibpython27sitepackage smesosnative0221003py27linuxx86_64eggmesosnative_mesosso 15 0x00007f4286cbc3a2 in void processinternalvdispatcherzookeepergroupprocessprocessprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupproc ess   from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 16 0x00007f4286ccdca5 in stdtr1result_ofvoid stdtr1result_ofstdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplepr ocessprocessbasetype stdtr1result_ofstdtr1_mustdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess  false false stdtr1shared_p trstdtr1functionvoid zookeepergroupprocess  stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tupleprocessprocessba setypeprocessprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess type stdtr1_bindvoid stdtr1_placeholder1 stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess processprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess  __callprocessprocessbase 0 1stdtr1_mustdtr1_placeholder1 false true  conststdtr1_placeholder1 stdtr1tupleprocessprocessbase std tr1_index_tuple0 1  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 17 0x00007f4286cc7a5a in stdtr1result_ofvoid stdtr1result_ofstdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tuplepr ocessprocessbasetype stdtr1result_ofstdtr1_mustdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess  false false stdtr1shared_pt rstdtr1functionvoid zookeepergroupprocess  stdtr1_mustdtr1_placeholder1 false true stdtr1_placeholder1 stdtr1tupleprocessprocessbas etypeprocessprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess type stdtr1_bindvoid stdtr1_placeholder1 st dtr1shared_ptrstdtr1functionvoid zookeepergroupprocess processprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess  operatorprocessprocessbaseprocessprocessbase  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_me sosso 18 0x00007f4286cc2480 in stdtr1_function_handlervoid processprocessbase stdtr1_bindvoid stdtr1_placeholder1 stdtr1shared_ptrstdtr1function void zookeepergroupprocess processprocessbase stdtr1shared_ptrstdtr1functionvoid zookeepergroupprocess  _m_invokestdtr1_any_data con st processprocessbase  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 19 0x00007f42870db546 in stdtr1functionvoid processprocessbaseoperatorprocessprocessbase const  from usersmchadhavenvlibpython27sitepackagesmeso snative0221003py27linuxx86_64eggmesosnative_mesosso 20 0x00007f42870c1013 in processprocessbasevisitprocessdispatchevent const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx8 6_64eggmesosnative_mesosso 21 0x00007f42870c5582 in processdispatcheventvisitprocesseventvisitor const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx 86_64eggmesosnative_mesosso 22 0x00007f428666680e in processprocessbaseserveprocessevent const  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64egg mesosnative_mesosso 23 0x00007f42870bd88f in processprocessmanagerresumeprocessprocessbase  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64 eggmesosnative_mesosso 24 0x00007f42870b1cb9 in processschedulevoid  from usersmchadhavenvlibpython27sitepackagesmesosnative0221003py27linuxx86_64eggmesosnative_mesosso 25 0x00000031410079d1 in start_thread  from lib64libpthreadso0 26 0x00000031408e88fd in clone  from lib64libcso6 code solution create master detector per url instead of per framework will send the review request,3
test build failure due to comparison between signed and unsigned integers compilation fails on opensuse tumbleweed linux 416 gcc 511 glibc 222 with the following errors code in file included from srctestsvalues_testscpp220 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth in instantiatio n of testingassertionresult testinginternalcmphelpereqconst char const char const t1 const t2 with t1  int t2  long unsigned int 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth148423 requi red from static testingassertionresult testinginternaleqhelperlhs_is_null_litera lcompareconst char const char const t1 const t2 with t1  int t2  long un signed int bool lhs_is_null_literal  false srctestsvalues_testscpp2873 required from here 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth144816 error comparison between signed and unsigned integer expressions werrorsigncompare if expected  actual   cxx testscontainerizermesos_testsprovisioner_docker_testso cmakefile6779 recipe for target testsmesos_testsvalues_testso failed make3  testsmesos_testsvalues_testso interrupt code,1
examplestestpersistentvolumeframework does not work in os x el capitan the example persistent volume framework test does not pass in os x el capitan it seems to be executing the build_dirsrclibsmesosexecutor directly while it should be executing the wrapper script at build_dirsrcmesosexecutor instead the noexecutor framework passes however which seem to have a very similar configuration with the persistent volume framework the following is the output that shows the dyld load error noformat i1008 012252280140 4284416 launchercpp132 forked child with pid 1706 for contain er b6d3bd962ebd47b1a16aa22ffba992aa i1008 012252280300 4284416 containerizercpp873 checkpointing executors forked pid 1706 to varfoldersp6nfxknpz52dzfc6zqnz23tq180000gntmesosxxxxxx5oz3locb0meta slaves34d6329e69cb4a72aee4fe892bf1c70bs2frameworks34d6329e69cb4a72aee4fe892b f1c70b0000executorsdec188d4d2dc40c5ac4d881adc3d81c0runsb6d3bd962ebd47b1a16a a22ffba992aapidsforkedpid dyld library not loaded usrlocalliblibmesos0260dylib referenced from usersmparkprojectsmesosbuildsrclibsmesosexecutor reason image not found dyld library not loaded usrlocalliblibmesos0260dylib referenced from usersmparkprojectsmesosbuildsrclibsmesosexecutor reason image not found dyld library not loaded usrlocalliblibmesos0260dylib referenced from usersmparkprojectsmesosbuildsrclibsmesosexecutor reason image not found i1008 012252365397 3211264 containerizercpp1284 executor for container 06b649be88c840478fb5e89bdd096b66 has exited i1008 012252365433 3211264 containerizercpp1097 destroying container 06b649be88c840478fb5e89bdd096b66 noformat,3
port slavepathscpp to windows important subset of dependency tree of changes necessary slavepathscpp os path,1
port slavestatecpp important subset of changes this depends on slavestatecpp pid os path protobuf paths state pidhpp addresshpp iphpp addresshpp iphpp nethpp nethpp ip networking stuff state type_utils pid os path protobuf uuid type_utilshpp uuidhpp,3
port slavecontainerizerfetchercpp important subset of the dependency tree follows slavecontainerizerfetchercpp slave fetcher collect dispatch net collect future defer process fetcher type_utils future process subprocess dispatchhpp processhpp nethpp ip networking stuff futurehpp pidhpp deferhpp deferredhpp dispatchhpp deferredhpp dispatchhpp pidhpp type_utilshpp uuidhpp subprocess os future,3
port slavecontainerizerisolatorcpp to windows important subset of the dependency tree follows isolatorhpp dispatchhpp pathhpp isolator process dispatchhpp processhpp,3
create slavecontainerizerisolatorsfilesystemwindowscpp should look a lot like the posixcpp flavor important subset of the dependency tree follows for the posix flavor slavecontainerizerisolatorsfilesystemposixcpp filesystemposix fs os path filesystemposix flags isolator,3
port slavecontainerizermesoscontainerizercpp to windows important subset of the dependency tree follows slavecontainerizermesoscontainerizercpp isolator collect defer io metrics reap subprocess fs os path protobuf_utils paths slave containerizer fetcher launcher posix disk containerizer launch provisioner,3
port slavecontainerizermesoslaunchcpp to windows important subset of the dependency tree follows slavecontainerizermesoslaunchcpp os protobuf launch launch subcommand subcommand flags flagshpp oshpp pathhpp fetchhpp,3
add support for github and variable base urls to applyreviewspy from adams email on dev list i have used the g feature for github prs in the past and we should continue to support that model so that new mesos contributors dont have to create new rb accounts and learn a new process just for quick documentation changes etc as a side note now that the myriad incubator project has migrated to apache git and we can no longer merge prs directly we were hoping to take advantage of a tool like applyreviews to apply our pr patches it looks like applyreviewssh only specifies mesos in the github_urlapi_url would applyreviewspy be just as easy to reuse for another project ie myriad,3
implement stoutoswindowskilltreehpp killtree is implemented using windows job objects the processes created by the executor are associated with a job object using create_job killtree is simply terminating the job object helper functions create_job function creates a job object whose name is derived from the pid and associates the pid process with the job object every process started by the process which is part of the job object becomes part of the job object the job name should match the name used in kill_job the jobs should be create with job_object_limit_kill_on_job_close and allow the caller to decide how to handle the returned handle kill_job function assumes the process identified by pid is associated with a job object whose name is derive from it every process started by the process which is part of the job object becomes part of the job object destroying the task will close all such processes,5
implement stoutoswindowslshpp,3
implement stoutoswindowsreadhpp and writehpp,2
implement stoutoswindowsstathpp,8
port slavecontainerizerisolatorhpp to windows,3
get container name information when launching a container task we want to get the docker name or docker id or both when launching a container task with mesos the container name is generated by mesos itself ie mesos77e5fde683e74618a2ddd5b10f2b4d25 obtained with docker ps and it would be nice to expose this information to frameworks so that this information can be used for example by marathon to give this information to users via a rest api to go a bit in depth with our use case we have files created by fluentd logdriver that are named with docker name or docker id full or short and we need a mapping for the users of the rest api and thus the first step is to make this information available from mesos,3
clarify error message could not chown work directory when deploying a framework i encountered the error message could not chown work directory it took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the docker container and the agent was running as root i suggest to clarify this message by pointing out to either set switchuser to false or to run the framework as the same user as the agent,1
enable building mesosapacheorg locally in a docker container we should make it easy for everyone to modify the website and be able to generate it locally before pushing to upstream,3
json parsing allows nonwhitespace trailing characters picojson supports a streaming mode in which a stream containing a series of json values can be repeatedly parsed for this reason it does not return an error when passed a string containing a valid json value followed by nonwhitespace trailing characters however in addition to the fourargument picojsonparse that were using picojson contains a twoargument parse function httpsgithubcomkazuhopicojsonblobmasterpicojsonhl938l942 which accepts a stdstring and should probably validate its input to ensure it doesnt contain trailing characters a pull request has been filed for this change at httpsgithubcomkazuhopicojsonpull70 and if its merged we can switch to the twoargument function call in the meantime we should provide such input validation ourselves in jsonparse,1
deprecate resource_monitoring_interval flag this parameter should be deprecated after 0230 release as it has no use now,1
allow easier detection when hook signature changes currently if the signature of a hook function changes we dont get any compile time errors if the hook implementation is not updated this results in a hook that is never called,2
http pipelining doesnt keep order of requests http 11 pipelininghttpsenwikipediaorgwikihttp_pipelining describes a mechanism by which multiple http request can be performed over a single socket the requirement here is that responses should be send in the same order as requests are being made libprocess has some mechanisms built in to deal with pipelining when multiple http requests are made it is still however possible to create a situation in which responses are scrambled respected to the requests arrival consider the situation in which there are two libprocess processes processa and processb each running in a different thread thread2 and thread3 respectively the processmanagerhttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl374 runs in thread1 processa is of type processa which looks roughly as follows code class processa  public processbaseprocessa  public processa  futurehttpresponse fooconst httprequest    do something  return httpok  protected virtual void initialize  routefoo none processafoo   code processb is from type processb which is just like processa but routes bar instead of foo the situation in which the bug arises is the following  two requests one for httpserver_uri1foo and one for httpserver_uri2bar are made over the same socket  the first request arrives to processmanagerhandlehttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl2202 which is still running in thread1 this one creates an httpevent and delivers to the handler in this case processa  processmanagerdeliverhttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl2361 enqueues the http event in to the processa queue this happens in thread1  the second request arrives to processmanagerhandlehttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl2202 which is still running in thread1 another httpevent is created and delivered to the handler in this case processb  processmanagerdeliverhttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl2361 enqueues the http event in to the processb queue this happens in thread1  thread2 is blocked so processa cannot handle the first request it is stuck in the queue  thread3 is idle so it picks up the request to processb immediately  processbasevisithttpeventhttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl3073 is called in thread3 this one in turn dispatcheshttpsgithubcomapachemesosblob1d68eed9089659b06a1e710f707818dbcafeec523rdpartylibprocesssrcprocesscppl3106 the responses future to the httpproxy associated with the socket where the request came at the last point the bug is evident the request to processb will be send before the request to processa even if the handler takes a long time and the processabar actually finishes before the responses are not send in the order the requests are done h1 reproducer the following is a test which successfully reproduces the issue codetitle3rdpartylibprocesssrctestshttp_testscpp include processlatchhpp using processlatch using testinginvokewithoutargs  this tests tries to force a situation in which http pipelining is scrambled  it does so by having two actors to which three requests are made the first  two requests to the first actor and a third request to the second actor  the first request will block the first actor long enough to allow the second  actor to process the third request since the first actor will not be able to  handle any event until it is done processing the first request the third  request is finished before the second even starts  the ultimate goal of the test is to alter the order in which  processbasevisithttpevent is executed for the different events  respect to the order in which the requests arrived testhttpconnectiontest complexpipelining  http server1 server2 futurehttprequest get1 get2 get3 latch latch expect_callserver1process get_ willoncedoallfuturearg0get1 invokewithoutargslatch  latchawait  returnhttpok1 willoncedoallfuturearg0get2 returnhttpok2 expect_callserver2process get_ willoncedoallfuturearg0get3 returnhttpok3 auto url1  httpurl http server1processselfaddressip server1processselfaddressport server1processselfid  get auto url2  httpurl http server1processselfaddressip server1processselfaddressport server2processselfid  get  create a connection to the server for http pipelining futurehttpconnection connect  httpconnecturl1 await_readyconnect httpconnection connection  connectget httprequest request1 request1method  get request1url  url1 request1keepalive  true request1body  1 futurehttpresponse response1  connectionsendrequest1 httprequest request2  request1 request2body  2 futurehttpresponse response2  connectionsendrequest2 httprequest request3 request3method  get request3url  url2 request3keepalive  true request3body  3 futurehttpresponse response3  connectionsendrequest3  verify that request1 arrived at server1 and it is the right request  now server1 is blocked processing request1 and cannot pick up more events  in the queue await_readyget1 expect_eqrequest1body get1body  verify that request3 arrived at server2 and it is the right request await_readyget3 expect_eqrequest3body get3body  request2 hasnt been picked up since server1 is still blocked serving  request1 expect_trueget2ispending  free server1 so it can serve request2 latchtrigger  verify that request2 arrived at server1 and it is the right request await_readyget2 expect_eqrequest2body get2body  wait for all responses await_readyresponse1 await_readyresponse2 await_readyresponse3  if pipelining works as expected even though server2 finished processing  its request before server1 even began with request2 the responses should  arrive in the order they were made expect_eqrequest1body response1body expect_eqrequest2body response2body expect_eqrequest3body response3body await_readyconnectiondisconnect await_readyconnectiondisconnected  code,3
update allocator interface to support quota an allocator should be notified when a quota is being setupdated or removed also to support master failover in presence of quota allocator should be notified about the reregistering agents and allocations towards quota,3
master recovery in presence of quota quota complicates master failover in several ways the new master should determine if it is possible to satisfy the total quota and notify an operator in case its not imagine simultaneous failovers of multiple agents the new master should hint the allocator how many agents might reconnect in the future to help it decide how to satisfy quota before the majority of agents reconnect,5
implement quota support in allocator the builtin hierarchical drf allocator should support quota this includes but not limited to adding updating removing and satisfying quota avoiding both overcomitting resources and handing them to nonquotaed roles in presence of master failover a design doc for quota support in allocatorhttpsissuesapacheorgjirabrowsemesos2937 provides an overview of a feature set required to be implemented,5
tests for quota support in master allocatoragnostic tests for quota support in the master they can be divided into several groups  heuristic check  master failover  functionality and quota guarantees,5
prototype quota request authentication quota requests need to be authenticated this ticket will authenticate quota requests using credentials provided by the authorization field of the http request this is similar to how authentication is implemented in masterhttp,5
prototype quota request authorization when quotas are requested they should authorize their roles this ticket will authorize quota requests with acls the existing authorization support that has been implemented in mesos1342 will be extended to add a request_quotas acl,5
speed up faulttolerancetestframeworkreregister test faulttolerancetestframeworkreregister test takes more than one second to complete code  run  faulttolerancetestframeworkreregister  ok  faulttolerancetestframeworkreregister 1056 ms code there must be a 1s timeout somewhere which we should mitigate via clockadvance,1
incorrect sed syntax for mac osx the build currently fails on osx noformat 3rdpartylibprocess3rdpartyprotobuf250srcprotoc imesosincludemesoscontainerizer  imesosinclude imesossrc  python_outpythoninterfacesrcmesosinterface mesosincludemesoscontainerizercontainerizerproto mesosinstallsh c d pythoninterfacesrcmesosv1interface sed i smesosmesos_pb2mesos_pb2 pythoninterfacesrcmesosinterfacecontainerizer_pb2py sed 1 pythoninterfacesrcme  extra characters at the end of p command make1  pythoninterfacesrcmesosinterfacecontainerizer_pb2py error 1 noformat this is because the sed command uses the wrong syntax for osx you need codesed i code to instruct sed to not use a backup file,2
support docker local store pull same image simultaneously the current local store implements get using the local puller for all requests of pulling same docker image at the same time the local puller just untar the image tarball as many times as those requests are and cp all of them to the same directory which wastes time and bear high demand of computation we should be able to support the local storepuller only do these for the first time and the simultaneous pulling request should wait for the promised future and get it once the first pulling finishes,3
mesos does not set contenttype for 400 bad request while integrating with the http scheduler api i encountered the following scenario the message below was serialized to protobuf and sent as the post body codetitlemessage call  type acknowledge acknowledge  uuid bytes agentid  value 201510121827341677734350508978s2  taskid  value task1    code codetitlerequest headers post apiv1scheduler http11 contenttype applicationxprotobuf accept applicationxprotobuf contentlength 73 host localhost5050 useragent rxnetty client code i received the following response codetitleresponse headers http11 400 bad request date wed 14 oct 2015 232136 gmt contentlength 74 failed to validate schedulercall expecting framework_id to be present code even though my accept header made no mention of textplain the message body returned to me is textplain additionally there is no contenttype header set on the response so i cant even do anything intelligently in my response handler,2
libprocess_ip not passed to docker containers docker containers arent currently passed all the same environment variables that mesos containerizer tasks are see httpsgithubcomapachemesosblobmastersrcslavecontainerizercontainerizercppl254 for all the environment variables explicitly set for mesos containers while some of them dont necessarily make sense for docker containers when the docker has inside of it a libprocess process a mesos framework scheduler and is using nethost the task needs to have libprocess_ip set otherwise the same sort of problems that happen because of mesos3553 can happen libprocess will try to guess the machines ip address with likely bad results in a number of operating environment,3
provide diagnostic output in agent log when fetching fails when fetching fails the fetcher has written log output to stderr in the task sandbox but it is not easy to get to it may even be impossible to get to if one only has the agent log available and no more access to the sandbox this is for instance the case when looking at output from a ci run the fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not when it exits with an error code we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log this is similar to this patch httpsreviewsapacheorgr37813 the difference is that the output of the latter is triggered by test failures outside the fetcher whereas what is proposed here is triggering upon failures inside the fetcher,2
http scheduler library does not gracefully parse invalid resource identifiers if you pass a nonsense string for master into a framework using the c http scheduler library the framework segfaults for example using the example frameworks codetitlescheduler driver buildsrctestframework masterasdf1270015050 code results in code failed to create a master detector for asdf1270015050 failed to parse asdf1270015050 code codetitlehttp scheduler library export default_principalroot buildsrceventcallframework masterasdf1270015050 code results in code i1015 161845432075 2062201600 schedulercpp157 version 0260 segmentation fault 11 code codetitlestack trace  thread 2 tid  0x28b6bb 0x0000000100ad03ca libmesos0260dylibmesosv1schedulermesosprocessinitializethis0x00000001076031a0  42 at schedulercpp213 stop reason  exc_bad_access code1 address0x0  frame 0 0x0000000100ad03ca libmesos0260dylibmesosv1schedulermesosprocessinitializethis0x00000001076031a0  42 at schedulercpp213 frame 1 0x0000000100ad05f2 libmesos0260dylibvirtual thunk to mesosv1schedulermesosprocessinitializethis0x00000001076031a0  34 at schedulercpp210 frame 2 0x00000001022b60f3 libmesos0260dylibresume  931 at processcpp2449 frame 3 0x00000001022c131c libmesos0260dyliboperator  268 at processcpp2174 frame 4 0x00000001022c0fa2 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool     inlined __invokelambda at 3rdpartylibprocesssrcprocesscpp215835  const std__1atomicbool   27 at __functional_base415 frame 5 0x00000001022c0f87 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool     inlined __apply_functorlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1tuplestd__1reference_wrapperconst std__1atomicbool   0 std__1tuple   55 at functional2060 frame 6 0x00000001022c0f50 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool     inlined operator  41 at functional2123 frame 7 0x00000001022c0f27 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool     inlined __invokestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool    14 at __functional_base415 frame 8 0x00000001022c0f19 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool     inlined __thread_executestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool    25 at thread337 frame 9 0x00000001022c0f00 libmesos0260dylib__thread_proxystd__1tuplestd__1__bindlambda at 3rdpartylibprocesssrcprocesscpp215835 std__1reference_wrapperconst std__1atomicbool      368 at thread347 frame 10 0x00007fff964c705a libsystem_pthreaddylib_pthread_body  131 frame 11 0x00007fff964c6fd7 libsystem_pthreaddylib_pthread_start  176 frame 12 0x00007fff964c43ed libsystem_pthreaddylibthread_start  13 code,1
configuration docs are missing enablelibevent and enablessl the enablelibevent and enablessl config flags are currently not documented in the configuration docs with the rest of the flags they should be added,1
mesos_native_java_library not set on mesoscontainerize tasks with executor_environmnent_variables when using executor_environment_variables and having mesos_native_java_library in the environment of mesosslave the mesos containerizer does not set mesos_native_java_library itself relevant code httpsgithubcomapachemesosblob14f7967ef307f3d98e3a4b93d92d6b3a56399b20srcslavecontainerizercontainerizercppl281 it sees that the variable is in the mesosslaves environment osgetenv rather than checking if it is set in the environment variable set,2
centos 6 dependency install fails at maven it seems the apache maven dependencies have changed such that following the getting started docs for centos 66 will fail at maven installation code  package apachemavennoarch 03332el6 will be installed  processing dependency javadevel  1170 for package apachemaven3332el6noarch  finished dependency resolution error package apachemaven3332el6noarch epelapachemaven requires javadevel  1170 available java150gcjdevel1500291el6x86_64 base javadevel  150 available 1java160openjdkdevel1603511371el6_6x86_64 base javadevel  1160 available 1java160openjdkdevel1603611381el6_7x86_64 updates javadevel  1160 you could try using skipbroken to work around the problem you could try running rpm va nofiles nodigest code,1
test the http scheduler library with ssl enabled currently the http scheduler library does not support sslenabled mesos you can manually test this by spinning up an sslenabled master and attempt to run the eventcall framework example against it we need to add tests that check the http scheduler library against sslenabled mesos  with downgrade support  with required frameworkclientside certifications  withwithout verification of certificates masterside  withwithout verification of certificates frameworkside  with a custom certificate authority ca these options should be controlled by the same environment variables found on the ssl user dochttpmesosapacheorgdocumentationlatestssl note this issue will be broken down into smaller subissues as bugsproblems are discovered,13
generalized http authentication modules libprocess is going to factor out an authentication interface mesos3231 here we propose that mesos can provide implementations for this interface as mesos modules,13
0260 release manage the release of apache mesos version 0260 the mesos 0260 release will aim at being timely and at improving robustness it will not be gated by new features however there may be blockers when it comes to bugs or incompleteness of existing features once these blockers are resolved we will start deferring unresolved issues by priority and status until we are ready to make the first cut here is how you can stay informed and help out h3 users  note the is blocked by links in this ticket for major targeted features  check out the 0260 dashboardhttpsissuesapacheorgjirasecuredashboardjspaselectpageid12327111 for status indicators  see the inprogress release noteshttpsissuesapacheorgjirasecurereleasenotejspaversion12333528stylenamehtmlprojectid12311242createcreateatl_tokena5kq2qavt4jafded7c675829c365428965ebec16702c62d3637db57d847clin to see whats committed so far  add comments to issues describing your problems or use cases h3 issue reporters  set target version to 0260 if appropriate  set priority for fixing in 0260  ask around on irc or dev for a shepherd h3 developers  newbies check out accepted unassigned newbie issues  looking for something meatier to work on accepted unassigned for 026  for shepherdless issues find a shepherd before diving too deep  update target version and priority as needed  discuss your intended design on the jira perhaps sharing a design doc  update the status to in progress and reviewable as you go  assign yourself if you are working on it unassign yourself in case you stop before finishing h3 committers  accept and shepherd all relevant shepherdless issues  update target version and priority as needed  add newbie label to any easy ones h3 important jira fields  target version set to 0260 if you want the issue to be addressed in 026  priority indicates how important it is for the issue to be fixed in the next release 0260 in this case if you want to update a priority please add a comment explaining your reason and only change the priority updown one level  blockedby links major features and critical tickets can be linked as blockers to this ticket to give a highlevel overview of what we plan to land in 026 noncritical issues should just set the target version,5
document messagesproto the messages we pass between mesos components are largely undocumented see this todohttpsgithubcomapachemesosblob19f14d06bac269b635657960d8ea8b2928b7830csrcmessagesmessagesprotol23,3
refactor ssltest fixture such that mesostest can use the same helpers in order to write tests that exercise ssl with other components of mesos such as the http scheduler library we need to use the setupteardown logic found in the ssltest fixture currently the test fixtures have separate inheritance structures like this code ssltest  testingtest mesostest  temporarydirectorytest  testingtest code where testingtest is a gtest class the plan is the following  change ssltest to inherit from temporarydirectorytest this will require moving the setup generation of keys and certs from setuptestcase to setup at the same time some of the cleanup logic in the ssltest will not be needed  move the logic of generating keyscerts into helpers so that individual tests can call them when needed much like mesostest  write a child class of ssltest which has the same functionality as the existing ssltest for use by the existing tests that rely on ssltest or the registryclienttest  have mesostest inherit from ssltest which might be renamed during the refactor if mesos is not compiled with enablessl then ssltest could be ifdefd into any empty class the resulting structure should be like code mesostest  ssltest  temporarydirectorytest  testingtest childofssltest  code,3
need for httpput request method as we decided to create a more restful api for managing quota request therefore we also want to use the http put request and hence need to enable the libprocesshttp to send put request besides get and post requests,1
mesos json api creates invalid json due to lack of binary data  nonascii handling spark encodes some binary data into the executorinfodata field this field is sent as a bytes protobuf value which can have arbitrary nonutf8 data if you have such a field it seems that it is splatted out into json without any regards to proper character encoding code 0006b0b0 2e 73 70 61 72 6b 2e 65 78 65 63 75 74 6f 72 2e sparkexecutor 0006b0c0 4d 65 73 6f 73 45 78 65 63 75 74 6f 72 42 61 63 mesosexecutorbac 0006b0d0 6b 65 6e 64 22 7d 2c 22 64 61 74 61 22 3a 22 ac kenddata 0006b0e0 ed 5c 75 30 30 30 30 5c 75 30 30 30 35 75 72 5c u0000u0005ur 0006b0f0 75 30 30 30 30 5c 75 30 30 30 66 5b 4c 73 63 61 u0000u000flsca 0006b100 6c 61 2e 54 75 70 6c 65 32 3b 2e cc 5c 75 30 30 latuple2u00 code i suspect this is because the http api emits the executorinfodata directly code jsonobject modelconst executorinfo executorinfo  jsonobject object objectvaluesexecutor_id  executorinfoexecutor_idvalue objectvaluesname  executorinfoname objectvaluesdata  executorinfodata objectvaluesframework_id  executorinfoframework_idvalue objectvaluescommand  modelexecutorinfocommand objectvaluesresources  modelexecutorinforesources return object  code i think this may be because the custom json processing library in stout seems to not have any idea of what a byte array is im guessing that some implicit conversion makes it get written as a string instead but code inline stdostream operatorstdostream out const string string   todobenh this escaping does not handle unicode it encodes as ascii  see rfc4627 for the json string specificiation return out  picojsonvaluestringvalueserialize  code thank you for any assistance here our cluster is currently entirely down  the frameworks cannot handle parsing the invalid json produced it is not even valid utf8,2
consistency of quoted strings in error messages example log output quote i1020 185602933956 1790 slavecpp1270 got assigned task 13 for framework 496620b943684a71b74168216f3d909f0000 i1020 185602934185 1790 slavecpp1386 launching task 13 for framework 496620b943684a71b74168216f3d909f0000 i1020 185602934408 1790 slavecpp1618 queuing task 13 for executor default of framework 496620b943684a71b74168216f3d909f0000 i1020 185602935417 1790 slavecpp1760 sending queued task 13 to executor default of framework 496620b943684a71b74168216f3d909f0000 quote aside from the typo unmatched quote in the third line these log messages using quoting inconsistently sometimes task executor and framework ids are quoted other times they are not we should probably adopt a general rule a la httpwwwpostgresqlorgdocs94staticerrorstyleguidehtml  my proposal when interpolating a variable only use quotes if it is possible that the value might contain whitespace or punctuation in the latter case the punctuation should probably be escaped,3
registryclienttestsimplegetblob is flaky registryclienttestsimplegetblob fails about 15 times this was encountered on osx codetitlerepro binmesostestssh gtest_filterregistryclienttestsimplegetblob gtest_repeat10 gtest_break_on_failure code codetitleexample failure  run  registryclienttestsimplegetblob srctestscontainerizerprovisioner_docker_testscpp946 failure value of blobresponse actual 20151020 2058595793930240000 expected blobget which is x15x3x300pxcaxc6x4x16xexb2xffb1axb9zxe0x80xdaxbctx5rx81x6xf8 x8bxa8xa9x4xabxb6 exe6xdexcfxd9xccxc2x15 20151020 2058595793930240000  aborted at 1445374739 unix time try date d 1445374739 if you are using gnu date  pc  0x103144ddc testingunittestaddtestpartresult  sigsegv 0x0 received by pid 49008 tid 0x7fff73ca3300 stack trace   0x7fff8c58af1a _sigtramp  0x7fff8386e187 malloc  0x1031445b7 testinginternalasserthelperoperator  0x1030d32e0 mesosinternaltestsregistryclienttest_simplegetblob_testtestbody  0x1030d3562 mesosinternaltestsregistryclienttest_simplegetblob_testtestbody  0x1031ac8f3 testinginternalhandlesehexceptionsinmethodifsupported  0x103192f87 testinginternalhandleexceptionsinmethodifsupported  0x1031533f5 testingtestrun  0x10315493b testingtestinforun  0x1031555f7 testingtestcaserun  0x103163df3 testinginternalunittestimplrunalltests  0x1031af8c3 testinginternalhandlesehexceptionsinmethodifsupported  0x103195397 testinginternalhandleexceptionsinmethodifsupported  0x1031639f2 testingunittestrun  0x1025abd41 run_all_tests  0x1025a8089 main  0x7fff86b155c9 start code codetitleless common failure  run  registryclienttestsimplegetblob srctestscontainerizerprovisioner_docker_testscpp926 failure socketfailure failed accept connection error error00000000lib0func0reason0 code,4
masterallocatortestslavelost is slow the masterallocatortestslavelost takes more that 5s to complete a brief look into the code hints that the stopped agent does not quit immediately and hence its resources are not released by the allocator because it waits for the executor to terminatehttpsgithubcomapachemesosblobmastersrctestsmaster_allocator_testscppl717 5s timeout comes from executor_shutdown_grace_period agent constant possible solutions  do not wait until the stopped agent quits can be flaky needs deeper analysis  decrease the agents executor_shutdown_grace_period flag  terminate the executor faster this may require some refactoring since the executor driver is created in the testcontainerizer and we do not have direct access to it,1
replace masterslave terminology phase i  rename flag names and deprecate old ones,3
use uri content modification time to trigger fetcher cache updates instead of using checksums to trigger fetcher cache updates we can for starters use the content modification time mtime which is available for a number of download protocols eg http and hdfs proposal instead of just fetching the content size we fetch both size and mtime together as before if there is no size then caching fails and we fall back on direct downloading to the sandbox assuming a size is given we compare the mtime from the fetch uri with the mtime known to the cache if it differs we update the cache as a defensive measure a difference in size should also trigger an update not having an mtime available at the fetch uri is simply treated as a unique valid mtime value that differs from all others this means that when initially there is no mtime cache content remains valid until there is one thereafter anew lack of an mtime invalidates the cache once in other words any change from no mtime to having one or back is the same as encountering a new mtime note that this scheme does not require any new protobuf fields,5
backticks are not mentioned in mesos c style guide as far as i can tell current practice is to quote code excerpts and object names with backticks when writing comments for example code  you know sadpanda seems extra sad lately stdstring sadpanda sadpanda     code however i dont see this documented in our c style guide at all it should be added,1
cannot start mesos local on a debian gnulinux 8 docker machine we updated the mesos version to 0250 in our marathon docker image that runs our integration tests we use mesos local for those tests this fails with this message noformat roota06e4b4eb776marathon mesos local i1022 184226852485 136 leveldbcpp176 opened db in 6103258ms i1022 184226853302 136 leveldbcpp183 compacted db in 765740ns i1022 184226853343 136 leveldbcpp198 created db iterator in 9001ns i1022 184226853355 136 leveldbcpp204 seeked to beginning of db in 1287ns i1022 184226853366 136 leveldbcpp273 iterated through 0 keys in the db in 1111ns i1022 184226853406 136 replicacpp744 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1022 184226853775 141 recovercpp449 starting replica recovery i1022 184226853862 141 recovercpp475 replica is in empty status i1022 184226854751 138 replicacpp641 replica in empty status received a broadcasted recover request i1022 184226854856 140 recovercpp195 received a recover response from a replica in empty status i1022 184226855002 140 recovercpp566 updating replica status to starting i1022 184226855655 138 mastercpp376 master a3f398181bda4710b96b2a60ed4d12b8 a06e4b4eb776 started on 172170145050 i1022 184226855680 138 mastercpp378 flags at startup allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_slavesfalse authenticatorscrammd5 authorizerslocal framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout5secs registry_strictfalse root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrsharemesoswebui work_dirtmpmesoslocalak0xpg zk_session_timeout10secs i1022 184226855790 138 mastercpp425 master allowing unauthenticated frameworks to register i1022 184226855803 138 mastercpp430 master allowing unauthenticated slaves to register i1022 184226855815 138 mastercpp467 using default crammd5 authenticator w1022 184226855829 138 authenticatorcpp505 no credentials provided authentication requests will be refused i1022 184226855840 138 authenticatorcpp512 initializing server sasl i1022 184226856442 136 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i1022 184226856943 140 leveldbcpp306 persisting metadata 8 bytes to leveldb took 1888185ms i1022 184226856987 140 replicacpp323 persisted replica status to starting i1022 184226857115 140 recovercpp475 replica is in starting status i1022 184226857270 140 replicacpp641 replica in starting status received a broadcasted recover request i1022 184226857312 140 recovercpp195 received a recover response from a replica in starting status i1022 184226857368 140 recovercpp566 updating replica status to voting i1022 184226857781 140 leveldbcpp306 persisting metadata 8 bytes to leveldb took 371121ns i1022 184226857841 140 replicacpp323 persisted replica status to voting i1022 184226857895 140 recovercpp580 successfully joined the paxos group i1022 184226857928 140 recovercpp464 recover process terminated i1022 184226862455 137 mastercpp1603 the newly elected leader is master172170145050 with id a3f398181bda4710b96b2a60ed4d12b8 i1022 184226862498 137 mastercpp1616 elected as the leading master i1022 184226862511 137 mastercpp1376 recovering from registrar i1022 184226862560 137 registrarcpp309 recovering registrar failed to create a containerizer could not create mesoscontainerizer failed to create launcher failed to create linux launcher failed to mount cgroups hierarchy at sysfscgroupfreezer freezer is already attached to another hierarchy noformat the setup worked with mesos 0240 the dockerfile is here httpsgithubcommesospheremarathonblobmvmesos_025dockerfile noformat roota06e4b4eb776marathon ls sysfscgroup roota06e4b4eb776marathon noformat noformat roota06e4b4eb776marathon cat procmounts none  aufs rwrelatimesi6e7ac87f36042e03diodirperm1 0 0 proc proc proc rwnosuidnodevnoexecrelatime 0 0 tmpfs dev tmpfs rwnosuidmode755 0 0 devpts devpts devpts rwnosuidnoexecrelatimegid5mode620ptmxmode666 0 0 shm devshm tmpfs rwnosuidnodevnoexecrelatimesize65536k 0 0 mqueue devmqueue mqueue rwnosuidnodevnoexecrelatime 0 0 sysfs sys sysfs ronosuidnodevnoexecrelatime 0 0 devsda1 etcresolvconf ext4 rwrelatimedataordered 0 0 devsda1 etchostname ext4 rwrelatimedataordered 0 0 devsda1 etchosts ext4 rwrelatimedataordered 0 0 devpts devconsole devpts rwrelatimemode600ptmxmode000 0 0 proc procbus proc ronosuidnodevnoexecrelatime 0 0 proc procfs proc ronosuidnodevnoexecrelatime 0 0 proc procirq proc ronosuidnodevnoexecrelatime 0 0 proc procsys proc ronosuidnodevnoexecrelatime 0 0 proc procsysrqtrigger proc ronosuidnodevnoexecrelatime 0 0 tmpfs prockcore tmpfs rwnosuidmode755 0 0 tmpfs proctimer_stats tmpfs rwnosuidmode755 0 0 noformat berndmesos can you please assign to the correct person,3
master should not store arbitrarily sized data in executorinfo from a comment in mesos3771 master should not be storing the data fields from executorinfo we currently store the entire objecthttpsgithubcomapachemesosblobmastersrcmastermasterhppl262l271 which means master would be at high risk of ooming if a bunch of executors were started with big data blobs  master should scrub out unneeded bloat from executorinfo before storing it  we can use an alternate internal object like we do for taskinfo vs task see thishttpsgithubcomapachemesosblobmastersrcmessagesmessagesprotol39l41,3
containerizer attempts to create linux launcher by default mesos containerizer attempts to create a linux launcher by default without verifying whether the necessary prerequisites such as availability of cgroups are met,3
add checks to make sure isolators and the launcher are compatible theres a recent change regarding the picking of which launcher linux or posix to use httpsreviewsapacheorgr39604 in our environment cgroups are not automounted after reboot we rely on mesos itself to mount all relevant cgroups hierachies after the reboot the above patch detects that freezer hierarchy is not mounted therefore decided to use the posix launcher if launcher is not specified explictly port mapping isolator requires network namespace to be created for each container thus requires linux launcher but we dont have a check to verify that launcher and isolators are compatible slave thus starts fine and task failed with weird error like noformat collect failed failed to create the ingress qdisc on mesos61099 link mesos61099 is not found noformat it does take us quite a few time to figure out the root cause,2
add documentation explaining roles docs currently talk about resources staticdynamic reservations but dont explain what a role concept is to begin with,2
testonly libprocess reinitialization background libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests some properties of the server socket are configured via environment variables such as the ip and port or the ssl configuration in the case of tests libprocess is initialized once per test binary this means that testing different configurations ssl in particular is cumbersome as a separate process would be needed for every test case proposal  add some optional code between some tests like code  cleanup all of libprocesss state as if were starting anew processfinalize  for tests that need to test ssl connections with the master opensslreinitialize processinitialize code see mesos3863 for more on processfinalize,3
enable mesosreviewbot project on jenkins to use ssl currently mesosreviewbot project does not support parameterized configuration this limits the project from building using enablessl and others configuration arguments for building mesos,3
document operator http endpoints these are not exhaustively documented they probably should be some endpoints have docs eg reserve and unreserve are described in the reservation doc page but it would be good to have a single page that lists all the endpoints and their semantics,3
help endpoints do not work for nested paths mesos displays the list of all supported endpoints starting at a given path prefix using the help suffix eg master5050help it seems that the help functionality is broken for urls having nested paths eg master5050helpmastermachinedown the response returned is quote malformed url expecting helpidname quote,2
rootfs in provisioner test doesnt handle symlink directories properly currently rootfs doesnt fully copy the directory structure over and also doesnt create the symlinks in the new rootfs and will cause shell and other binaries that rely on the symlinks to no longer function,4
update documentation for fetchercache mtimerelated changes,1
root tests for linuxfilesystemisolatortest are broken the refactor in mesos3762 ended up exposing some differences in the temporarydirectorytest classes one in stout one in mesosproper the tests that broke during tear down code linuxfilesystemisolatortestroot_persistentvolumewithrootfilesystem linuxfilesystemisolatortestroot_persistentvolumewithoutrootfilesystem linuxfilesystemisolatortestroot_multiplecontainers code as per an offline discussion between jvanremoortere and jieyu the solution is to merge the two temporarydirectorytest classes and to fix the tear down of linuxfilesystemisolatortest,2
refactor environmentmkdtemp into temporarydirectorytest as part of mesos3762 many tests were changed from one temporarydirectorytest to another temporarydirectorytest one subtle difference is that the name of the temporary directory no longer contains the name of the test in mesos3847 the duplicate temporarydirectorytest was removed the original temporarydirectorytest called environmentmkdtemphttpsgithubcomapachemesosblobmastersrctestsenvironmentcppl494 we would like the naming which is valuable for debugging to be available for a majority of tests a majority of tests inherit from temporarydirectorytest in some way note  any additional directories created via environmentmkdtemp are cleaned up after the test  we dont want mesosspecific logic in stout like the umount shell command in environmentteardown proposed change move the temporary directory logic from environmentmkdtemp to temporarydirectorytest tests that need to change  log_testscpp  logzookeepertest  we can change zookeepertest to inherit from temporarydirectorytest to get rid of code duplication   testsmesoscpp  mesostestcreateslaveflags  mesostest already inherits from temporarydirectorytest   testsscripthpp  test_script  this is used for the exampletests we can define a test class that inherits appropriately   docker_testscpp    already inherits from mesostest,3
corrected style in makefiles order of files in makefiles is not strictly alphabetic,1
investigate recent crashes in command executor post httpsreviewsapacheorgr38900 ie updating commandexecutor to support rootfs there seem to be some tests showing frequent crashes due to assert violations fetchercachetestsimpleeviction failed due to the following log code i1107 193646360908 30657 slavecpp1793 sending queued task 3 to executor 3 of framework 7d94c7fb89504bcf80c146112292dcd60000 at executor117217520033871 i1107 193646363682 1236 execcpp297 i1107 193646373569 1245 execcpp210 executor registered on slave 7d94c7fb89504bcf80c146112292dcd6s0  0x7f9f5a7db3fa googlelogmessagefail i1107 193646394081 1245 execcpp222 executorregistered took 395411ns  0x7f9f5a7db359 googlelogmessagesendtolog  0x7f9f5a7dad6a googlelogmessageflush  0x7f9f5a7dda9e googlelogmessagefatallogmessagefatal  0x48d00a _checkfatal_checkfatal  0x49c99d mesosinternalcommandexecutorprocesslaunchtask  0x4b3dd7 _zzn7process8dispatchin5mesos8internal22commandexecutorprocessepns1_14executordrivererkns1_8taskinfoes5_s6_eevrkns_3pidit_eemsa_fvt0_t1_et2_t3_enkulpns_11processbaseee_clesl_  0x4c470c _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchin5mesos8internal22commandexecutorprocessepns5_14executordrivererkns5_8taskinfoes9_sa_eevrkns0_3pidit_eemse_fvt0_t1_et2_t3_euls2_e_e9_m_invokeerkst9_any_datas2_  0x7f9f5a761b1b stdfunctionoperator  0x7f9f5a749935 processprocessbasevisit  0x7f9f5a74d700 processdispatcheventvisit  0x48e004 processprocessbaseserve  0x7f9f5a745d21 processprocessmanagerresume  0x7f9f5a742f52 _zzn7process14processmanager12init_threadsevenkulrkst11atomic_boole_cles3_  0x7f9f5a74cf2c _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eee6__callivieilm0eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_eee  0x7f9f5a74cedc _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eeecliieveet0_dpot_  0x7f9f5a74ce6e _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeevee9_m_invokeiieeevst12_index_tupleiixspt_eee  0x7f9f5a74cdc5 _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeeveeclev  0x7f9f5a74cd5e _znst6thread5_implist12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis6_eeeveee6_m_runev  0x7f9f5624f1e0 unknown  0x7f9f564a8df5 start_thread  0x7f9f559b71ad __clone i1107 193646551370 30656 containerizercpp1257 executor for container 6553a6176b4a418d97595681f45ff854 has exited i1107 193646551429 30656 containerizercpp1074 destroying container 6553a6176b4a418d97595681f45ff854 i1107 193646553869 30656 containerizercpp1257 executor for container d2c1f924c92a453e82b1c294d09c4873 has exited code the reason seems to be a race between the executor receiving a runtaskmessage before executorregisteredmessage leading to the check_someexecutorinfo failure link to complete log httpsissuesapacheorgjirabrowsemesos2831focusedcommentid14995535pagecomatlassianjirapluginsystemissuetabpanelscommenttabpanelcomment14995535 another related failure from examplestestpersistentvolumeframework code  0x7f4f71529cbd googlelogmessagesendtolog i1107 131509949987 31573 slavecpp2337 status update manager successfully handled status update acknowledgement uuid 721c731655804636a83a098e3bd4ed1f for task ad90531fd3d843f696f2c81c4548a12d of framework ac4ea54a7d194e419ee31a761f8e5b0f0000  0x7f4f715296ce googlelogmessageflush  0x7f4f7152c402 googlelogmessagefatallogmessagefatal  0x48d00a _checkfatal_checkfatal  0x49c99d mesosinternalcommandexecutorprocesslaunchtask  0x4b3dd7 _zzn7process8dispatchin5mesos8internal22commandexecutorprocessepns1_14executordrivererkns1_8taskinfoes5_s6_eevrkns_3pidit_eemsa_fvt0_t1_et2_t3_enkulpns_11processbaseee_clesl_  0x4c470c _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchin5mesos8internal22commandexecutorprocessepns5_14executordrivererkns5_8taskinfoes9_sa_eevrkns0_3pidit_eemse_fvt0_t1_et2_t3_euls2_e_e9_m_invokeerkst9_any_datas2_  0x7f4f714b047f stdfunctionoperator  0x7f4f71498299 processprocessbasevisit  0x7f4f7149c064 processdispatcheventvisit  0x48e004 processprocessbaseserve  0x7f4f71494685 processprocessmanagerresume code full logs at httpsbuildsapacheorgjobmesos1191compilergccconfigurationverboseoscentos7label_expdocker7c7chadoopconsolefull,2
finalize design for generalized authorizer interface finalize the structure of acls and achieve consensus on the design doc proposed in mesos2949,2
add mtimerelated fetcher tests,2
draft design doc for first step external volume mvp as part of the overall design doc for global resources we would like to introduce improvements for docker volume driver isolator module httpsgithubcomemccodemesosmoduledvdi currently the isolator module is controlled by setting environment variables as follows code env  dvdi_volume_name testing dvdi_volume_driver platform1 dvdi_volume_opts size5iops150volumetypeio1newfstypeext4overwritefsfalse dvdi_volume_name1 testing2 dvdi_volume_driver1 platform2 dvdi_volume_opts1 size6volumetypegp2newfstypexfsoverwritefstrue  code we should develop a more structured way for passing these settings to the isolator module which is in line with the overall goal of global resources,3
draft quota limits design document in the design documents for quota httpsdocsgooglecomdocumentd16irnmziasejvoblyp5bbkebz7pnjnlaizpqqmthq9iedit the proposed mvp does not include quota limits quota limits represent an upper bound of resources that a role is allowed to use the task of this ticket is to outline a design document on how to implement quota limits when the quota mvp is implemented,5
add github support to applyreviewspy,3
authenticate quota requests quota requests need to be authenticated this ticket will authenticate quota requests using credentials provided by the authorization field of the http request this is similar to how authentication is implemented in masterhttp,3
authorize set quota requests when quotas are requested they should authorize their roles this ticket will authorize quota requests with acls the existing authorization support that has been implemented in mesos1342 will be extended to add a request_quotas acl,5
investigate the requirements of programmatically reinitializing libprocess this issue is for investigating what needs to be addedchanged in processfinalize such that processinitialize will start on a clean slate additional issues will be created once done also see the parent issuemesos3820 processfinalize should cover the following components  __s__ the server socket  delete should be sufficient this closes the socket and thereby prevents any further interaction from it  process_manager  related prior work mesos3158  cleans up the garbage collector help logging profiler statistics route processes including this onehttpsgithubcomapachemesosblob3bda55da1d0b580a1b7de43babfdc0d30fbc87ea3rdpartylibprocesssrcprocesscppl963 which currently leaks a pointer  cleans up any other spawn d process  manages the eventloop  clock  the goal here is to clear any timers so that nothing can deference process_manager while were finalizingfinalized its probably not important to execute any remaining timers since were shutting down libprocess this means  the clock should be paused and settled before the clean up of process_manager  processes which might interact with the clock should be cleaned up next  a new clockfinalize method would then clear timers processspecific clocks and tick s and then resume the clock  __address__ the advertised ip and port  needs to be cleared after process_manager has been cleaned up processes use this to communicate events if cleared prematurely terminateevents will not be sent correctly leading to infinite waits  socket_manager  the idea here is to close all sockets and deallocate any existing httpproxy or encoder objects  all sockets are created via __s__ so cleaning up the server socket prior will prevent any new activity  mime  this is effectively a static map  it should be possible to statically initialize it  synchronization atomics initialized  initializing  once cleanup is done these should be reset summary  implement clockfinalize mesos3882  implement socketmanager mesos3910  make sure the metricsprocess and reaperprocess are reinitialized mesos3934  optional clean up mime  wrap everything up in processfinalize,2
simplify andor document the libprocess initialization synchronization logic tracks this todohttpsgithubcomapachemesosblob3bda55da1d0b580a1b7de43babfdc0d30fbc87ea3rdpartylibprocesssrcprocesscppl749 the synchronization logic of libprocesshttpsgithubcomapachemesoscommitcd757cf75637c92c438bf4cd22f21ba1b5be702fdiff128d3b56fc8c9ec0176fdbadcfd11fc2 predates abstractionshttpsgithubcomapachemesoscommit6c3b107e4e02d5ba0673eb3145d71ec9d256a639diff0eebc8689450916990abe080d86c2acb like processonce which is used in almost all other onetime initialization blocks the logic should be documented it can also be simplified see the review descriptionhttpsreviewsapacheorgr39949 or it can be replaced with processonce,1
make resourcediskinfopersistenceprincipal a required field a principal field is being added to the resourcediskinfopersistence message to facilitate authorization of persistent volume creationdeletion in the longrun it should be a required field but its being initially introduced as optional to avoid breaking existing frameworks the field should be changed to required at the end of a deprecation cycle,1
make applyreviewsh use applyreviewspy,1
enhance allocator interface with the recovery method there are some scenarios eg quota is set for some roles when it makes sense to notify an allocator about the recovery introduce a method into the allocator interface that allows for this,3
investigate recovery for the hierarchical allocator the builtin hierarchical allocator should implement the recovery in the presence of quota,3
account dynamic reservations towards quota dynamic reservationswhether allocated or notshould be accounted towards roles quota this requires update in at least two places  the builtin allocator which actually satisfies quota  the sanity check in the master,3
draft operator documentation for quota draft an operator guide for quota which describes basic usage of the endpoints and few basic and advanced usage cases,5
incorrect and inconsistent include order for gmockgmockh and gtestgtesth we currently have an inconsistent and mostly incorrect include order for gmockgmockh and gtestgtesth see below some files include them incorrectly between the c and cpp standard header while other correclt include them afterwards according to the google styleguide httpsgooglegithubiostyleguidecppguidehtmlnames_and_order_of_includes the second include order is correct codetitleexternal_containerizer_testcpp include unistdh include gmockgmockh include string code codetitlelauncherhpp include vector include gmockgmockh code,1
propose a guideline for log messages we are rather inconsistent in the way we write log messages it would be helpful to come up with a style and document various aspects of logs including but not limited to  usage of backticks andor single quotes to quote interpolated variables  usage of backticks andor single quotes to quote types and other names  usage of tenses and other grammatical forms  proper way of nesting error messages,5
implement stoutospstreehpp on windows,2
libprocess implement processclockfinalize tracks this todohttpsgithubcomapachemesosblobaa0cd7ed4edf1184cbc592b5caa2429a8373e8133rdpartylibprocesssrcprocesscppl974l975 the clock is initialized with a callback that among other things will dereference the global process_manager object when libprocess is shutting down the process_manager is cleaned up between cleanup and termination of libprocess there is some chance that a timer will time out and result in dereferencing process_manager proposal  implement clockfinalize this would clear  existing timers  processspecific clocks  ticks  change processfinalize  resume the clock the clock is only paused during some tests when the clock is not paused the callback does not dereference process_manager  clean up process_manager this terminates all the processes that would potentially interact with clock  call clockfinalize,3
add support to applyreviewspy to update svn when necessary quote that said this can be automated as a step in applyreviews script for example the script can check if something in site or docs  is being committed and if yes also do an svn update artem do you want to take this on as you revamp the applyreviews script on tue nov 10 2015 at 123 am adam bordelon adammesosphereio wrote  since its still a manual process the website is usually only updated a  when we have a new release to announce or b when some other blogworthy  content arises eg mesoscon quote httpsmailarchivesapacheorgmod_mboxmesosdev201511mbox3ccaakwvazqjq9kmdpcaq_f2bh1bnnzbrrknqzxkwjwztrihuf66fg40mailgmailcom3e,3
corrected style in hierarchical allocator the builtin allocator code has some style issues namespaces in the cpp file unfortunate formatting which should be corrected for readability,1
add a flag to master to enable optimistic offers,3
support distinguishing revocable resources in the resource protobuf add enum type into revocableinfo  framework need to assign revocableinfo when launching task if its not assign use reserved resources framework need to identify which resources its using  oversubscription resources need to assign the type by agent mesos3930  update oversubscription document that oo has oversubscribe the allocation slack and recommend qos to handle the usage slack only mesos3889 code message resource   message revocableinfo  enum type   underutilized allocated resources controlled by  oversubscription qoscontroller  resourceestimator usage_slack  1  unallocated reserved resources  controlled by optimistic offers allocator allocation_slack  2  optional type type  1   optional revocableinfo revocable  9  code,2
modify oversubscription documentation to explicitly forbid the qos controller from killing executors running on optimistically offered resources,2
add notion of evictable task to runtaskmessage code message runtaskmessage    this list can be nonempty when a task is launched on reserved  resources if the reserved resources are in use as revocable  resources this list contains the executors that can be evicted  to make room to run this task repeated executorid evictable_executors  5   code,2
add a helper function to the agent to check available resources before launching a task launching a task using revocable resources should be funnelled through an accounting system  if a task is launched using revocable resources the resources must not be in use when launching the task if they are in use then the task should fail to start  if a task is launched using reserved resources the resources must be made available this means potentially evicting tasks which are using revocable resources both cases could be implemented by adding a check in slaveruntask like a new helper method noformat class slave    checks if the given resources are available ie not utilized  for starting a task if not the task should either fail to  start or result in the eviction of revocable resources virtual processfuturebool checkavailableresources const resources resources   noformat,5
add a helper function to the agent to retrieve the list of executors that are using optimistically offered revocable resources in the agent add a helper function to get the list of the exeuctor using allocation_slack its short term solution which is different the design document because master did not have executor for command line executor send evicatble executors from master to slave will addess in postmvp after mesos1718 noformat class slave    if the executor used revocable resources add it into evictableexecutors  list void addevictableexecutorexecutor executor  if the executor used revocable resources remove it from  evictableexecutors list void removeevictableexecutorexecutor executor  get evictable executor id list by request resources the return value is resultlistexecutor   if iserror theres not enough resources to launch tasks   if isnone no evictable exectuors need to be terminated   if isnone the list of executors that need to be evicted for resources resultstdlistexecutor getevictableexecutorsconst resources request   the map of evictable executor list if theres not enough resources  the evictable executor will be terminated by slave to release resources hashmapframeworkid stdsetexecutorid evictableexecutors   noformat,5
implement tests for verifying allocator resource math write a test to ensure that the allocator performs the reservation slack calculations correctly,8
rebuild reservation slack allocator state during master failover,13
update reservation slack allocator state during agent failover,13
add accounting for reservation slack in the allocator mesosxxx optimsistic accounter code class hierarchicalallocatorprocess  struct slave   struct optimistic  resources total  the total allocation slack resources resources allocated  the allocated allocation slack resources  optimistic optimistic   code mesos4146 flatten  allocationslack for optimistic offer code class resources   returns a resources object with the same amount of each resource  type as these resources but with all resource objects marked as  the specified revocableinfotype the other attribute is not  affected resources flattenresourcerevocableinfotype type  return a resources object that   if role is given the resources did not include roles reserved  resources   the resourcess revocable type is allocation_slack   the role of resources is set to  resources allocationslackoptionstring role  none  code mesosxxx allocate the allocation_slack resources to framework code void hierarchicalallocatorprocessallocate const hashsetslaveid slaveids_  foreach slave foreach role foreach framework  resource optimistic if frameworkrevocable  resources total  slavesslaveidoptimistictotalallocationslackrole optimistic  total  slavesslaveidoptimisticallocated   offerableframeworkidslaveid  resources  optimistic  slavesslaveidoptimisticallocated  optimistic   code heres some consideration about allocation_slack 1 old resources availabletotal did not include allocation_slack 2 after quota remainingclusterresourcescontains should not check allocation_slack if there no enough resources master can still offer allocation_salck resources 3 in sorter itll not include allocation_slack as those resources are borrowed from other roleframework 4 if either normal resources or allocation_slack resources are allocablefiltered it can be offered to framework 5 currently allocator will assign all allocation_salck resources in slave to one framework mesosxxx update allocation_slack for dynamic reservation updateallocation code void hierarchicalallocatorprocessupdateallocation const frameworkid frameworkid const slaveid slaveid const vectorofferoperation operations   tryresources updatedoptimistic  slavesslaveidoptimistictotalapplyoperations check_someupdatedtotal slavesslaveidoptimistictotal  updatedoptimisticgetstatelessreservedflattenallocation_slack   code mesosxxx add allocation_slack when slaver registerreregister addslave code void hierarchicalallocatorprocessaddslave const slaveid slaveid const slaveinfo slaveinfo const optionunavailability unavailability const resources total const hashmapframeworkid resources used   slavesslaveidoptimistictotal  totalstatelessreservedflattenallocation_slack   code no need to handle removeslave itll all related info from slaves including optimistic mesosxxx return resources to allocator recoverresources code void hierarchicalallocatorprocessrecoverresources const frameworkid frameworkid const slaveid slaveid const resources resources const optionfilters filters  if slavescontainsslaveid   slavesslaveidoptimisticallocated  resourcesallocationslack    code,13
identify and implement test cases for verifying eviction logic in the agent,13
identify and implement test cases for handling a race between optimistic lender and tenant offers an example is the when lender launches the task on an agent followed by a borrower launching a task on the same agent before the optimistic offer is rescinded,13
wrong syntax and inconsistent formatting of json examples in flag documentation the json examples in the documentation of the commandline flags mesosmastersh help and mesosslavesh help dont have a consistent formatting furthermore some examples arent even compliant json because they have trailing commas were they shouldnt,1
enable mesosreviewbot project on jenkins to use docker as a first step to adding capability for building multiple configurations on reviewbot we need to change the build scripts to use docker,3
add authorization for createvolume and destroyvolume http endpoints this is the fourth in a series of tickets that adds authorization support for persistent volumes we need to add acl authorization for the createvolume and destroyvolume http endpoints in other complementary work authorization for frameworks performing create and destroy operations is being added by mesos3065 this will consist of adding authorization calls into the http endpoint code in srcmasterhttpcpp as well as tests for both failed  successful calls to createvolumes and destroyvolumes with authorization we also must ensure that the principal field of resourcediskinfopersistence is being populated correctly,2
five new dockerrelated slave flags are not covered by the configuration documentation these flags were added to slaveflagscpp but are not mentioned in docsconfigurationmd addflagsdocker_auth_server docker_auth_server docker authentication server authdockerio addflagsdocker_auth_server_port docker_auth_server_port docker authentication server port 443 addflagsdocker_puller_timeout_secs docker_puller_timeout timeout value in seconds for pulling images from docker registry 60 addflagsdocker_registry docker_registry default docker image registry server host registry1dockerio addflagsdocker_registry_port docker_registry_port default docker registry server port 443,1
isolator module headers depend on picojson headers when trying to build an isolator module stout headers end up depending on picojsonhpp which is not installed code in file included from optmesosincludemesosmoduleisolatorhpp25 in file included from optmesosincludemesosslaveisolatorhpp30 in file included from optmesosincludeprocessdispatchhpp22 in file included from optmesosincludeprocessprocesshpp26 in file included from optmesosincludeprocesseventhpp21 in file included from optmesosincludeprocesshttphpp39 optmesosincludestoutjsonhpp2310 fatal error picojsonh file not found include picojsonh  8 warnings and 1 error generated code,3
libprocess implement cleanup of the socketmanager in processfinalize the socket_manager and process_manager are intricately tied together currently only the process_manager is cleaned up by processfinalize to clean up the socket_manager we must close all sockets and deallocate any existing httpproxy or encoder objects and we should prevent further objects from being createdtracked by the socket_manager proposal  clean up all processes other than gc this will clear all links and delete all httpproxy s while socket_manager still exists  close all sockets via socketmanagerclose all of socket_manager s state is cleaned up via socketmanagerclose including termination of httpproxy termination is idempotent meaning that killing httpproxy s via process_manager is safe  at this point socket_manager should be empty and only the gc process should be running since were finalizing assume there are no threads trying to spawn processes socket_manager can be deleted  gc can be deleted this is currently a leaked pointer so well also need to track and delete that  process_manager should be devoid of processes so we can proceed with cleanup join threads stop the eventloop etc,5
add a force flag to disable sanity check in quota there are use cases when an operator may want to disable the sanity check for quota endpoints mesos3074 even if this renders the cluster under quota for example an operator sets quota before adding more agents in order to make sure that no nonquota allocations from new agents are made,1
rescind offers in order to satisfy quota when a quota request comes in we may need to rescind a certain amount of outstanding offers in order to satisfy it because resources are allocated in the allocator there can be a race between rescinding and allocating this race makes it hard to determine the exact amount of offers that should be rescinded in the master,3
disallow empty string roles having an empty role empty string looks like a terrible idea but we do not prohibit it i think we should add corresponding checks and update the docs to officially disallow empty roles,3
mastermaintenancetestinverseoffersfilters is flaky verbose logs code  run  mastermaintenancetestinverseoffersfilters i1113 164358486469 8728 leveldbcpp176 opened db in 2360405ms i1113 164358486935 8728 leveldbcpp183 compacted db in 407105ns i1113 164358486995 8728 leveldbcpp198 created db iterator in 16221ns i1113 164358487030 8728 leveldbcpp204 seeked to beginning of db in 10935ns i1113 164358487046 8728 leveldbcpp273 iterated through 0 keys in the db in 999ns i1113 164358487090 8728 replicacpp780 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1113 164358487735 8747 recovercpp449 starting replica recovery i1113 164358488047 8747 recovercpp475 replica is in empty status i1113 164358488977 8745 replicacpp676 replica in empty status received a broadcasted recover request from 5810021545384 i1113 164358489452 8746 recovercpp195 received a recover response from a replica in empty status i1113 164358489712 8747 recovercpp566 updating replica status to starting i1113 164358490706 8742 leveldbcpp306 persisting metadata 8 bytes to leveldb took 745443ns i1113 164358490739 8742 replicacpp323 persisted replica status to starting i1113 164358490859 8742 recovercpp475 replica is in starting status i1113 164358491786 8747 replicacpp676 replica in starting status received a broadcasted recover request from 5910021545384 i1113 164358492542 8749 recovercpp195 received a recover response from a replica in starting status i1113 164358493221 8743 recovercpp566 updating replica status to voting i1113 164358493710 8743 leveldbcpp306 persisting metadata 8 bytes to leveldb took 331874ns i1113 164358493767 8743 replicacpp323 persisted replica status to voting i1113 164358493868 8743 recovercpp580 successfully joined the paxos group i1113 164358494119 8743 recovercpp464 recover process terminated i1113 164358504369 8749 mastercpp367 master d59449fc546243c5b935e05563fdd4b6 vagrantubuntuwily64 started on 10021545384 i1113 164358504438 8749 mastercpp369 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpzb7csscredentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpzb7cssmaster zk_session_timeout10secs i1113 164358504717 8749 mastercpp416 master allowing unauthenticated frameworks to register i1113 164358504889 8749 mastercpp419 master only allowing authenticated slaves to register i1113 164358504922 8749 credentialshpp37 loading credentials for authentication from tmpzb7csscredentials i1113 164358505497 8749 mastercpp458 using default crammd5 authenticator i1113 164358505759 8749 mastercpp495 authorization enabled i1113 164358507638 8746 mastercpp1606 the newly elected leader is master10021545384 with id d59449fc546243c5b935e05563fdd4b6 i1113 164358507693 8746 mastercpp1619 elected as the leading master i1113 164358507720 8746 mastercpp1379 recovering from registrar i1113 164358507946 8749 registrarcpp309 recovering registrar i1113 164358508561 8749 logcpp661 attempting to start the writer i1113 164358510282 8747 replicacpp496 replica received implicit promise request from 6010021545384 with proposal 1 i1113 164358510867 8747 leveldbcpp306 persisting metadata 8 bytes to leveldb took 475696ns i1113 164358510946 8747 replicacpp345 persisted promised to 1 i1113 164358511912 8745 coordinatorcpp240 coordinator attempting to fill missing positions i1113 164358513030 8749 replicacpp391 replica received explicit promise request from 6110021545384 for position 0 with proposal 2 i1113 164358513819 8749 leveldbcpp343 persisting action 8 bytes to leveldb took 739171ns i1113 164358513867 8749 replicacpp715 persisted action at 0 i1113 164358522002 8745 replicacpp540 replica received write request for position 0 from 6210021545384 i1113 164358522114 8745 leveldbcpp438 reading position from leveldb took 33549ns i1113 164358522599 8745 leveldbcpp343 persisting action 14 bytes to leveldb took 435729ns i1113 164358522652 8745 replicacpp715 persisted action at 0 i1113 164358523291 8746 replicacpp694 replica received learned notice for position 0 from 00000 i1113 164358523901 8746 leveldbcpp343 persisting action 16 bytes to leveldb took 538894ns i1113 164358523983 8746 replicacpp715 persisted action at 0 i1113 164358524060 8746 replicacpp700 replica learned nop action at position 0 i1113 164358524775 8747 logcpp677 writer started with ending position 0 i1113 164358525902 8745 leveldbcpp438 reading position from leveldb took 39685ns i1113 164358526852 8745 registrarcpp342 successfully fetched the registry 0b in 18832896ms i1113 164358527084 8745 registrarcpp441 applied 1 operations in 24930ns attempting to update the registry i1113 164358528020 8745 logcpp685 attempting to append 189 bytes to the log i1113 164358528323 8748 coordinatorcpp350 coordinator attempting to write append action at position 1 i1113 164358529465 8744 replicacpp540 replica received write request for position 1 from 6310021545384 i1113 164358530081 8744 leveldbcpp343 persisting action 208 bytes to leveldb took 552812ns i1113 164358530128 8744 replicacpp715 persisted action at 1 i1113 164358530781 8745 replicacpp694 replica received learned notice for position 1 from 00000 i1113 164358531121 8745 leveldbcpp343 persisting action 210 bytes to leveldb took 271774ns i1113 164358531162 8745 replicacpp715 persisted action at 1 i1113 164358531188 8745 replicacpp700 replica learned append action at position 1 i1113 164358532064 8743 registrarcpp486 successfully updated the registry in 49152ms i1113 164358532402 8743 registrarcpp372 successfully recovered registrar i1113 164358532768 8742 logcpp704 attempting to truncate the log to 1 i1113 164358532891 8743 mastercpp1416 recovered 0 slaves from the registry 150b  allowing 10mins for slaves to reregister i1113 164358532968 8742 coordinatorcpp350 coordinator attempting to write truncate action at position 2 i1113 164358534010 8742 replicacpp540 replica received write request for position 2 from 6410021545384 i1113 164358534488 8742 leveldbcpp343 persisting action 16 bytes to leveldb took 420186ns i1113 164358534533 8742 replicacpp715 persisted action at 2 i1113 164358535081 8748 replicacpp694 replica received learned notice for position 2 from 00000 i1113 164358535482 8748 leveldbcpp343 persisting action 18 bytes to leveldb took 360618ns i1113 164358535550 8748 leveldbcpp401 deleting 1 keys from leveldb took 23693ns i1113 164358535575 8748 replicacpp715 persisted action at 2 i1113 164358535611 8748 replicacpp700 replica learned truncate action at position 2 i1113 164358550834 8746 slavecpp191 slave started on 510021545384 i1113 164358550834 8746 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpmastermaintenancetest_inverseoffersfilters_2zc09gcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmastermaintenancetest_inverseoffersfilters_2zc09gfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostnamemaintenancehost hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirhomevagrantbuildmesossrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmastermaintenancetest_inverseoffersfilters_2zc09g i1113 164358551501 8746 credentialshpp85 loading credential for authentication from tmpmastermaintenancetest_inverseoffersfilters_2zc09gcredential i1113 164358551703 8746 slavecpp322 slave using credential for testprincipal i1113 164358552422 8746 slavecpp392 slave resources cpus2 mem1024 disk1024 ports3100032000 i1113 164358552510 8746 slavecpp400 slave attributes   i1113 164358552532 8746 slavecpp405 slave hostname maintenancehost i1113 164358552547 8746 slavecpp410 slave checkpoint true i1113 164358553520 8746 statecpp54 recovering state from tmpmastermaintenancetest_inverseoffersfilters_2zc09gmeta i1113 164358553938 8746 status_update_managercpp202 recovering status update manager i1113 164358554251 8746 slavecpp4230 finished recovery i1113 164358555016 8746 slavecpp729 new master detected at master10021545384 i1113 164358555166 8746 slavecpp792 authenticating with master master10021545384 i1113 164358555207 8746 slavecpp797 using default crammd5 authenticatee i1113 164358555589 8746 slavecpp765 detecting new master i1113 164358555076 8749 status_update_managercpp176 pausing sending status updates i1113 164358555719 8742 authenticateecpp123 creating new client sasl connection i1113 164358560645 8744 mastercpp5150 authenticating slave510021545384 i1113 164358561305 8744 authenticatorcpp100 creating new server sasl connection i1113 164358566682 8744 authenticateecpp214 received sasl authentication mechanisms crammd5 i1113 164358566779 8744 authenticateecpp240 attempting to authenticate with mechanism crammd5 i1113 164358566872 8744 authenticatorcpp205 received sasl authentication start i1113 164358566936 8744 authenticatorcpp327 authentication requires more steps i1113 164358567602 8744 authenticateecpp260 received sasl authentication step i1113 164358567775 8744 authenticatorcpp233 received sasl authentication step i1113 164358568128 8744 authenticatorcpp319 authentication success i1113 164358568282 8742 authenticateecpp300 authentication success i1113 164358568320 8749 mastercpp5180 successfully authenticated principal testprincipal at slave510021545384 i1113 164358568701 8742 slavecpp860 successfully authenticated with master master10021545384 i1113 164358569272 8747 mastercpp3859 registering slave at slave510021545384 maintenancehost with id d59449fc546243c5b935e05563fdd4b6s0 i1113 164358570096 8747 registrarcpp441 applied 1 operations in 59195ns attempting to update the registry i1113 164358570772 8748 logcpp685 attempting to append 362 bytes to the log i1113 164358570772 8749 coordinatorcpp350 coordinator attempting to write append action at position 3 i1113 164358572155 8745 replicacpp540 replica received write request for position 3 from 6910021545384 i1113 164358572801 8745 leveldbcpp343 persisting action 381 bytes to leveldb took 563073ns i1113 164358572854 8745 replicacpp715 persisted action at 3 i1113 164358573707 8745 replicacpp694 replica received learned notice for position 3 from 00000 i1113 164358574255 8745 leveldbcpp343 persisting action 383 bytes to leveldb took 485234ns i1113 164358574311 8745 replicacpp715 persisted action at 3 i1113 164358574342 8745 replicacpp700 replica learned append action at position 3 i1113 164358575857 8747 mastercpp3847 ignoring register slave message from slave510021545384 maintenancehost as admission is already in progress i1113 164358576217 8744 logcpp704 attempting to truncate the log to 3 i1113 164358575887 8748 registrarcpp486 successfully updated the registry in 5682176ms i1113 164358576400 8744 coordinatorcpp350 coordinator attempting to write truncate action at position 4 i1113 164358577169 8746 mastercpp3927 registered slave d59449fc546243c5b935e05563fdd4b6s0 at slave510021545384 maintenancehost with cpus2 mem1024 disk1024 ports3100032000 i1113 164358577287 8745 hierarchicalcpp344 added slave d59449fc546243c5b935e05563fdd4b6s0 maintenancehost with cpus2 mem1024 disk1024 ports3100032000 allocated  i1113 164358577472 8744 slavecpp904 registered with master master10021545384 given slave id d59449fc546243c5b935e05563fdd4b6s0 i1113 164358577999 8745 status_update_managercpp183 resuming sending status updates i1113 164358578279 8748 replicacpp540 replica received write request for position 4 from 7010021545384 i1113 164358578346 8744 slavecpp963 forwarding total oversubscribed resources i1113 164358578734 8744 mastercpp4269 received update of slave d59449fc546243c5b935e05563fdd4b6s0 at slave510021545384 maintenancehost with total oversubscribed resources i1113 164358578846 8748 leveldbcpp343 persisting action 16 bytes to leveldb took 304993ns i1113 164358578889 8748 replicacpp715 persisted action at 4 i1113 164358578897 8744 hierarchicalcpp400 slave d59449fc546243c5b935e05563fdd4b6s0 maintenancehost updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i1113 164358579463 8744 replicacpp694 replica received learned notice for position 4 from 00000 i1113 164358579888 8744 leveldbcpp343 persisting action 18 bytes to leveldb took 384596ns i1113 164358579952 8744 leveldbcpp401 deleting 2 keys from leveldb took 27011ns i1113 164358579977 8744 replicacpp715 persisted action at 4 i1113 164358580001 8744 replicacpp700 replica learned truncate action at position 4 i1113 164358584300 8743 slavecpp191 slave started on 610021545384 i1113 164358584398 8743 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpmastermaintenancetest_inverseoffersfilters_cdfgvtcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmastermaintenancetest_inverseoffersfilters_cdfgvtfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostnamemaintenancehost2 hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirhomevagrantbuildmesossrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmastermaintenancetest_inverseoffersfilters_cdfgvt i1113 164358584731 8743 credentialshpp85 loading credential for authentication from tmpmastermaintenancetest_inverseoffersfilters_cdfgvtcredential i1113 164358584915 8743 slavecpp322 slave using credential for testprincipal i1113 164358585309 8743 slavecpp392 slave resources cpus2 mem1024 disk1024 ports3100032000 i1113 164358585482 8743 slavecpp400 slave attributes   i1113 164358585566 8743 slavecpp405 slave hostname maintenancehost2 i1113 164358585619 8743 slavecpp410 slave checkpoint true i1113 164358586431 8743 statecpp54 recovering state from tmpmastermaintenancetest_inverseoffersfilters_cdfgvtmeta i1113 164358586890 8745 status_update_managercpp202 recovering status update manager i1113 164358587136 8745 slavecpp4230 finished recovery i1113 164358587817 8745 slavecpp729 new master detected at master10021545384 i1113 164358587836 8747 status_update_managercpp176 pausing sending status updates i1113 164358587908 8745 slavecpp792 authenticating with master master10021545384 i1113 164358587934 8745 slavecpp797 using default crammd5 authenticatee i1113 164358588043 8745 slavecpp765 detecting new master i1113 164358588170 8745 authenticateecpp123 creating new client sasl connection i1113 164358592891 8745 mastercpp5150 authenticating slave610021545384 i1113 164358594146 8745 authenticatorcpp100 creating new server sasl connection i1113 164358599606 8749 authenticateecpp214 received sasl authentication mechanisms crammd5 i1113 164358599684 8749 authenticateecpp240 attempting to authenticate with mechanism crammd5 i1113 164358599774 8749 authenticatorcpp205 received sasl authentication start i1113 164358599830 8749 authenticatorcpp327 authentication requires more steps i1113,3
implement authn handling in master for the scheduler endpoint if authenticationauthn is enabled on a master frameworks attempting to use the http scheduler api cant register code  cat tmpsubscribe943257503176798091bin  http printhhbb stream prettycolors auth verificationpassword1 post 5050apiv1scheduler acceptapplicationxprotobuf contenttypeapplicationxprotobuf post apiv1scheduler http11 connection keepalive contenttype applicationxprotobuf acceptencoding gzip deflate accept applicationxprotobuf contentlength 126 useragent httpie090 host localhost5050 authorization basic dmvyawzpy2f0aw9uonbhc3n3b3jkmq   note binary data not shown in terminal   http11 401 unauthorized date fri 13 nov 2015 200045 gmt wwwauthenticate basic realmmesos master contentlength 65 http schedulers are not supported when authentication is required code authorizationauthz is already supported for http based frameworks,5
add hdfs based uri fetcher plugin this plugin uses hdfs client to fetch artifacts it can support schemes like hdfshftps3s3n itll shell out the hadoop command to do the actual fetching,3
modularize uri fetcher plugin interface so that we can add custom uri fetcher plugins using modules,3
root tests fail on mesos 026 on ubuntucentos running 0260rc1 on both centos 71 and ubuntu 1404 with sudo privileges causes segfaults when running docker tests logs attached,2
automate the process of landing commits for committers this script should do the following things 1 apply a chain of reviews to a local branch 2 push the commits upstream 3 mark the reviews as submitted 4 optionally close any attached jira tickets,3
libprocess unify the initialization of the metricsprocess and reaperprocess related to this todohttpsgithubcomapachemesosblobaa0cd7ed4edf1184cbc592b5caa2429a8373e8133rdpartylibprocesssrcprocesscppl949l950 the metricsprocess and reaperprocess are global processes singletons which are initialized upon first use the two processes could be initialized alongside the gc help logging profiler and system statistics processes inside processinitialize this is also necessary for libprocess reinitialization,3
document possible task state transitions for framework authors we should document the possible ways in which the state of a task can evolve over time what happens when an agent is partitioned from the master and more generally how we recommend that framework authors develop faulttolerant schedulers and do task state reconciliation,5
test dockercontainerizertestroot_docker_launch_executor fails noformat configure make check sudo binmesostestssh gtest_filterdockercontainerizertestroot_docker_launch_executor verbose noformat noformat  running 1 test from 1 test case  global test environment setup  1 test from dockercontainerizertest i1117 150809265943 26380 leveldbcpp176 opened db in 3199666ms i1117 150809267761 26380 leveldbcpp183 compacted db in 1684873ms i1117 150809267902 26380 leveldbcpp198 created db iterator in 58313ns i1117 150809267966 26380 leveldbcpp204 seeked to beginning of db in 4927ns i1117 150809267997 26380 leveldbcpp273 iterated through 0 keys in the db in 1605ns i1117 150809268156 26380 replicacpp780 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1117 150809270148 26396 recovercpp449 starting replica recovery i1117 150809272105 26396 recovercpp475 replica is in empty status i1117 150809275640 26396 replicacpp676 replica in empty status received a broadcasted recover request from 410021550088 i1117 150809276578 26399 recovercpp195 received a recover response from a replica in empty status i1117 150809277600 26397 recovercpp566 updating replica status to starting i1117 150809279613 26396 leveldbcpp306 persisting metadata 8 bytes to leveldb took 1016098ms i1117 150809279731 26396 replicacpp323 persisted replica status to starting i1117 150809280306 26399 recovercpp475 replica is in starting status i1117 150809282181 26400 replicacpp676 replica in starting status received a broadcasted recover request from 510021550088 i1117 150809282552 26400 mastercpp367 master 59c600f192ff49269c84073d9b81f68a vagrantubuntutrusty64 started on 10021550088 i1117 150809283021 26400 mastercpp369 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmp40alt8credentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmp40alt8master zk_session_timeout10secs i1117 150809283920 26400 mastercpp414 master only allowing authenticated frameworks to register i1117 150809283972 26400 mastercpp419 master only allowing authenticated slaves to register i1117 150809284032 26400 credentialshpp37 loading credentials for authentication from tmp40alt8credentials i1117 150809282944 26401 recovercpp195 received a recover response from a replica in starting status i1117 150809284639 26401 recovercpp566 updating replica status to voting i1117 150809285539 26400 mastercpp458 using default crammd5 authenticator i1117 150809285995 26401 leveldbcpp306 persisting metadata 8 bytes to leveldb took 1075466ms i1117 150809286062 26401 replicacpp323 persisted replica status to voting i1117 150809286200 26401 recovercpp580 successfully joined the paxos group i1117 150809286471 26401 recovercpp464 recover process terminated i1117 150809287303 26400 authenticatorcpp520 initializing server sasl i1117 150809289371 26400 mastercpp495 authorization enabled i1117 150809296018 26399 mastercpp1606 the newly elected leader is master10021550088 with id 59c600f192ff49269c84073d9b81f68a i1117 150809296115 26399 mastercpp1619 elected as the leading master i1117 150809296187 26399 mastercpp1379 recovering from registrar i1117 150809296717 26397 registrarcpp309 recovering registrar i1117 150809298842 26396 logcpp661 attempting to start the writer i1117 150809301563 26394 replicacpp496 replica received implicit promise request from 610021550088 with proposal 1 i1117 150809302561 26394 leveldbcpp306 persisting metadata 8 bytes to leveldb took 922719ns i1117 150809302635 26394 replicacpp345 persisted promised to 1 i1117 150809303755 26394 coordinatorcpp240 coordinator attempting to fill missing positions i1117 150809306161 26394 replicacpp391 replica received explicit promise request from 710021550088 for position 0 with proposal 2 i1117 150809306972 26394 leveldbcpp343 persisting action 8 bytes to leveldb took 711749ns i1117 150809307034 26394 replicacpp715 persisted action at 0 i1117 150809308732 26401 replicacpp540 replica received write request for position 0 from 810021550088 i1117 150809308830 26401 leveldbcpp438 reading position from leveldb took 46444ns i1117 150809309710 26401 leveldbcpp343 persisting action 14 bytes to leveldb took 779098ns i1117 150809309754 26401 replicacpp715 persisted action at 0 i1117 150809311007 26397 replicacpp694 replica received learned notice for position 0 from 00000 i1117 150809311652 26397 leveldbcpp343 persisting action 16 bytes to leveldb took 567289ns i1117 150809311731 26397 replicacpp715 persisted action at 0 i1117 150809311771 26397 replicacpp700 replica learned nop action at position 0 i1117 150809313212 26397 logcpp677 writer started with ending position 0 i1117 150809315682 26399 leveldbcpp438 reading position from leveldb took 27974ns i1117 150809318694 26395 registrarcpp342 successfully fetched the registry 0b in 21862144ms i1117 150809319007 26395 registrarcpp441 applied 1 operations in 91867ns attempting to update the registry i1117 150809321730 26395 logcpp685 attempting to append 193 bytes to the log i1117 150809321935 26397 coordinatorcpp350 coordinator attempting to write append action at position 1 i1117 150809323103 26399 replicacpp540 replica received write request for position 1 from 910021550088 i1117 150809323917 26399 leveldbcpp343 persisting action 212 bytes to leveldb took 735223ns i1117 150809323983 26399 replicacpp715 persisted action at 1 i1117 150809324975 26398 replicacpp694 replica received learned notice for position 1 from 00000 i1117 150809325695 26398 leveldbcpp343 persisting action 214 bytes to leveldb took 668268ns i1117 150809325741 26398 replicacpp715 persisted action at 1 i1117 150809325778 26398 replicacpp700 replica learned append action at position 1 i1117 150809327258 26396 registrarcpp486 successfully updated the registry in 8090112ms i1117 150809327525 26396 registrarcpp372 successfully recovered registrar i1117 150809328083 26400 logcpp704 attempting to truncate the log to 1 i1117 150809328251 26394 mastercpp1416 recovered 0 slaves from the registry 154b  allowing 10mins for slaves to reregister i1117 150809328814 26396 coordinatorcpp350 coordinator attempting to write truncate action at position 2 i1117 150809330158 26401 replicacpp540 replica received write request for position 2 from 1010021550088 i1117 150809330994 26401 leveldbcpp343 persisting action 16 bytes to leveldb took 760471ns i1117 150809331055 26401 replicacpp715 persisted action at 2 i1117 150809331583 26401 replicacpp694 replica received learned notice for position 2 from 00000 i1117 150809332172 26401 leveldbcpp343 persisting action 18 bytes to leveldb took 497457ns i1117 150809332500 26401 leveldbcpp401 deleting 1 keys from leveldb took 49327ns i1117 150809332715 26401 replicacpp715 persisted action at 2 i1117 150809332964 26401 replicacpp700 replica learned truncate action at position 2 i1117 150809354073 26401 slavecpp191 slave started on 110021550088 i1117 150809354316 26401 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpdockercontainerizertest_root_docker_launch_executor_hakhaqcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpdockercontainerizertest_root_docker_launch_executor_hakhaqfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirhomevagrantmesosbuildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpdockercontainerizertest_root_docker_launch_executor_hakhaq i1117 150809355077 26401 credentialshpp85 loading credential for authentication from tmpdockercontainerizertest_root_docker_launch_executor_hakhaqcredential i1117 150809355587 26401 slavecpp322 slave using credential for testprincipal i1117 150809357144 26401 slavecpp392 slave resources cpus2 mem1024 disk1024 ports3100032000 i1117 150809357477 26401 slavecpp400 slave attributes   i1117 150809357719 26401 slavecpp405 slave hostname vagrantubuntutrusty64 i1117 150809357936 26380 schedcpp166 version 0260 i1117 150809358010 26401 slavecpp410 slave checkpoint true i1117 150809359058 26400 schedcpp264 new master detected at master10021550088 i1117 150809359216 26400 schedcpp320 authenticating with master master10021550088 i1117 150809359277 26400 schedcpp327 using default crammd5 authenticatee i1117 150809359856 26400 authenticateecpp99 initializing client sasl i1117 150809360539 26400 authenticateecpp123 creating new client sasl connection i1117 150809361399 26398 statecpp54 recovering state from tmpdockercontainerizertest_root_docker_launch_executor_hakhaqmeta i1117 150809361994 26398 status_update_managercpp202 recovering status update manager i1117 150809362191 26395 mastercpp5150 authenticating scheduler38aa807a672a4e1eb82371f119980e8610021550088 i1117 150809362565 26401 dockercpp536 recovering docker containers i1117 150809362908 26395 authenticatorcpp100 creating new server sasl connection i1117 150809363533 26401 slavecpp4230 finished recovery i1117 150809363675 26394 authenticateecpp214 received sasl authentication mechanisms crammd5 i1117 150809363950 26394 authenticateecpp240 attempting to authenticate with mechanism crammd5 i1117 150809364137 26394 authenticatorcpp205 received sasl authentication start i1117 150809364241 26394 authenticatorcpp327 authentication requires more steps i1117 150809364481 26394 authenticateecpp260 received sasl authentication step i1117 150809364667 26394 authenticatorcpp233 received sasl authentication step i1117 150809364828 26394 authenticatorcpp319 authentication success i1117 150809365039 26398 authenticateecpp300 authentication success i1117 150809365170 26398 mastercpp5180 successfully authenticated principal testprincipal at scheduler38aa807a672a4e1eb82371f119980e8610021550088 i1117 150809365656 26398 schedcpp409 successfully authenticated with master master10021550088 i1117 150809366044 26401 slavecpp729 new master detected at master10021550088 i1117 150809366283 26398 mastercpp2176 received subscribe call for framework default at scheduler38aa807a672a4e1eb82371f119980e8610021550088 i1117 150809366317 26401 slavecpp792 authenticating with master master10021550088 i1117 150809366688 26401 slavecpp797 using default crammd5 authenticatee i1117 150809366525 26395 status_update_managercpp176 pausing sending status updates i1117 150809366442 26398 mastercpp1645 authorizing framework principal testprincipal to receive offers for role  i1117 150809367207 26401 slavecpp765 detecting new master i1117 150809367496 26395 mastercpp2247 subscribing framework default with checkpointing disabled and capabilities   i1117 150809368417 26396 hierarchicalcpp195 added framework 59c600f192ff49269c84073d9b81f68a0000 i1117 150809367250 26398 authenticateecpp123 creating new client sasl connection i1117 150809368506 26395 schedcpp643 framework registered with 59c600f192ff49269c84073d9b81f68a0000 i1117 150809369287 26398 mastercpp5150 authenticating slave110021550088 i1117 150809370213 26401 authenticatorcpp100 creating new server sasl connection i1117 150809370846 26396 authenticateecpp214 received sasl authentication mechanisms crammd5 i1117 150809370964 26396 authenticateecpp240 attempting to authenticate with mechanism crammd5 i1117 150809371233 26396 authenticatorcpp205 received sasl authentication start i1117 150809371387 26396 authenticatorcpp327 authentication requires more steps i1117 150809371707 26398 authenticateecpp260 received sasl authentication step i1117 150809371835 26398 authenticatorcpp233 received sasl authentication step i1117 150809371944 26398 authenticatorcpp319 authentication success i1117 150809372195 26396 authenticateecpp300 authentication success i1117 150809372248 26398 mastercpp5180 successfully authenticated principal testprincipal at slave110021550088 i1117 150809373002 26396 slavecpp860 successfully authenticated with master master10021550088 i1117 150809373566 26398 mastercpp3859 registering slave at slave110021550088 vagrantubuntutrusty64 with id 59c600f192ff49269c84073d9b81f68as0 i1117 150809374301 26401 registrarcpp441 applied 1 operations in 65094ns attempting to update the registry i1117 150809376809 26400 logcpp685 attempting to append 374 bytes to the log i1117 150809376994 26399 coordinatorcpp350 coordinator attempting to write append action at position 3 i1117 150809377960 26397 replicacpp540 replica received write request for position 3 from 1610021550088 i1117 150809378844 26397 leveldbcpp343 persisting action 393 bytes to leveldb took 805302ns i1117 150809378904 26397 replicacpp715 persisted action at 3 i1117 150809379823 26400 replicacpp694 replica received learned notice for position 3 from 00000 i1117 150809380592 26400 leveldbcpp343 persisting action 395 bytes to leveldb took 691729ns i1117 150809380666 26400 replicacpp715 persisted action at 3 i1117 150809380702 26400 replicacpp700 replica learned append action at position 3 i1117 150809382014 26398 registrarcpp486 successfully updated the registry in 7384064ms i1117 150809382184 26400 logcpp704 attempting to truncate the log to 3 i1117 150809382380 26398 coordinatorcpp350 coordinator attempting to write truncate action at position 4 i1117 150809383361 26399 mastercpp3927 registered slave 59c600f192ff49269c84073d9b81f68as0 at slave110021550088 vagrantubuntutrusty64 with cpus2 mem1024 disk1024 ports3100032000 i1117 150809383437 26396 slavecpp904 registered with master master10021550088 given slave id 59c600f192ff49269c84073d9b81f68as0 i1117 150809383741 26400 status_update_managercpp183 resuming sending status updates i1117 150809384004 26401 hierarchicalcpp344 added slave 59c600f192ff49269c84073d9b81f68as0 vagrantubuntutrusty64 with cpus2 mem1024 disk1024 ports3100032000 allocated  i1117 150809384101 26396 slavecpp963 forwarding total oversubscribed resources i1117 150809384831 26396 mastercpp4269 received update of slave 59c600f192ff49269c84073d9b81f68as0 at slave110021550088 vagrantubuntutrusty64 with total oversubscribed resources i1117 150809384466 26398 replicacpp540 replica received write request for position 4 from 1710021550088 i1117 150809385957 26397 mastercpp4979 sending 1 offers to framework 59c600f192ff49269c84073d9b81f68a0000 default at scheduler38aa807a672a4e1eb82371f119980e8610021550088 i1117 150809386066 26401 hierarchicalcpp400 slave 59c600f192ff49269c84073d9b81f68as0 vagrantubuntutrusty64 updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated cpus2 mem1024 disk1024 ports3100032000 i1117 150809386219 26398 leveldbcpp343 persisting action 16 bytes to leveldb took 605641ns i1117 150809386445 26398 replicacpp715 persisted action at 4 i1117 150809388450 26397 replicacpp694 replica received learned notice for position 4 from 00000 i1117 150809389235 26397 leveldbcpp343 persisting action 18 bytes to leveldb took 715846ns i1117 150809389345 26397 leveldbcpp401 deleting 2 keys from leveldb took 40455ns i1117 150809389402 26397 replicacpp715 persisted action at 4 i1117 150809389464 26397 replicacpp700 replica learned truncate action at position 4 i1117 150809390585 26394 mastercpp2915 processing accept call for offers  59c600f192ff49269c84073d9b81f68ao0  on slave 59c600f192ff49269c84073d9b81f68as0 at slave110021550088 vagrantubuntutrusty64 for framework 59c600f192ff49269c84073d9b81f68a0000 default at scheduler38aa807a672a4e1eb82371f119980e8610021550088 i1117 150809390805 26394 mastercpp2711 authorizing framework principal testprincipal to launch task 1 as user root w1117 150809393517 26396 validationcpp422 executor e1 for task 1 uses less cpus none than the minimum required 001 please update your executor as this will be mandatory in future releases w1117 150809393632 26396 validationcpp434 executor e1 for task 1 uses less memory none than the minimum required 32mb please update your executor as this will be mandatory in future releases i1117 150809394270 26396 masterhpp176 adding task 1 with resources cpus2 mem1024 disk1024 ports3100032000 on slave 59c600f192ff49269c84073d9b81f68as0 vagrantubuntutrusty64 i1117 150809394580 26396 mastercpp3245 launching task 1 of framework 59c600f192ff49269c840,2
allow setting quotas for the default  role investigate use cases and implications of the possibility to set quota for the  role for example having quota for  set can effectively reduce the scope of the quota capacity heuristic,3
ubsan error in netipcreatesockaddr const misaligned address running ubsan from gcc 52 on the current mesos unit tests yields this among other problems noformat mesos3rdpartylibprocess3rdpartystoutincludestoutiphpp23056 runtime error reference binding to misaligned address 0x00000199629c for type const struct sockaddr_storage which requires 8 byte alignment 0x00000199629c note pointer points here 00 00 00 00 02 00 00 00 ff ff ff 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  0 0x5950cb in netipcreatesockaddr const homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x5950cb 1 0x5970cd in netipnetworkfromlinkdevicestd__cxx11basic_stringchar stdchar_traitschar stdallocatorchar  const int homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x5970cd 2 0x58e006 in nettest_linkdevice_testtestbody homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x58e006 3 0x85abd5 in void testinginternalhandlesehexceptionsinmethodifsupportedtestingtest voidtestingtest void testingtest char const homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x85abd5 4 0x848abc in void testinginternalhandleexceptionsinmethodifsupportedtestingtest voidtestingtest void testingtest char const homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x848abc 5 0x7e2755 in testingtestrun homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x7e2755 6 0x7e44a0 in testingtestinforun homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x7e44a0 7 0x7e5ffa in testingtestcaserun homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x7e5ffa 8 0x7ffe21 in testinginternalunittestimplrunalltests homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x7ffe21 9 0x85d7a5 in bool testinginternalhandlesehexceptionsinmethodifsupportedtestinginternalunittestimpl booltestinginternalunittestimpl bool testinginternalunittestimpl char const homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x85d7a5 10 0x84b37a in bool testinginternalhandleexceptionsinmethodifsupportedtestinginternalunittestimpl booltestinginternalunittestimpl bool testinginternalunittestimpl char const homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x84b37a 11 0x7f8a4a in testingunittestrun homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x7f8a4a 12 0x608a96 in run_all_tests homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x608a96 13 0x60896b in main homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x60896b 14 0x7fd0f0c7fa3f in __libc_start_main libx86_64linuxgnulibcso60x20a3f 15 0x4145c8 in _start homevagrantbuildmesosubsan3rdpartylibprocess3rdpartystouttests0x4145c8 noformat,2
reserve and unreserve should be permissive under a master without authentication currently the reserve and unreserve endpoints do not work without authentication enabled on the master when authentication is disabled on the master these endpoints should just be permissive,1
support dynamic weight in allocator this jira will focus on update the allocator api to support weight update of a role,5
add operator documentation for weight endpoint this jira ticket will update the related doc to apply to dynamic weights and add an new operator guide for dynamic weights which describes basic usage of the weights endpoint,2
user cgroup isolation tests fail on centos 6 usercgroupisolatortest0root_cgroups_usercgroup and usercgroupisolatortest1root_cgroups_usercgroup fail on centos 66 with similar output when libevent and ssl are enabled noformat sudo binmesostestssh gtest_filterusercgroupisolatortest0root_cgroups_usercgroup verbose noformat noformat  running 1 test from 1 test case  global test environment setup  1 test from usercgroupisolatortest0 where typeparam  mesosinternalslavecgroupsmemisolatorprocess userdel user mesostestunprivilegeduser does not exist  run  usercgroupisolatortest0root_cgroups_usercgroup i1118 165335273717 30249 memcpp605 started listening for oom events for container 867a829e4a2643f586e0938bf1f47688 i1118 165335274538 30249 memcpp725 started listening on low memory pressure events for container 867a829e4a2643f586e0938bf1f47688 i1118 165335275164 30249 memcpp725 started listening on medium memory pressure events for container 867a829e4a2643f586e0938bf1f47688 i1118 165335275784 30249 memcpp725 started listening on critical memory pressure events for container 867a829e4a2643f586e0938bf1f47688 i1118 165335276448 30249 memcpp356 updated memorysoft_limit_in_bytes to 1gb for container 867a829e4a2643f586e0938bf1f47688 i1118 165335277331 30249 memcpp391 updated memorylimit_in_bytes to 1gb for container 867a829e4a2643f586e0938bf1f47688 bash sysfscgroupmemorymesos867a829e4a2643f586e0938bf1f47688cgroupprocs no such file or directory mkdir cannot create directory sysfscgroupmemorymesos867a829e4a2643f586e0938bf1f47688user no such file or directory srctestscontainerizerisolator_testscpp1307 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupmemorymesos867a829e4a2643f586e0938bf1f47688usercgroupprocs no such file or directory srctestscontainerizerisolator_testscpp1316 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0  failed  usercgroupisolatortest0root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupsmemisolatorprocess 149 ms noformat noformat sudo binmesostestssh gtest_filterusercgroupisolatortest1root_cgroups_usercgroup verbose noformat noformat  running 1 test from 1 test case  global test environment setup  1 test from usercgroupisolatortest1 where typeparam  mesosinternalslavecgroupscpushareisolatorprocess userdel user mesostestunprivilegeduser does not exist  run  usercgroupisolatortest1root_cgroups_usercgroup i1118 170100550706 30357 cpusharecpp392 updated cpushares to 1024 cpus 1 for container e57f43431a974b44b347803be47ace80 bash sysfscgroupcpuacctmesose57f43431a974b44b347803be47ace80cgroupprocs no such file or directory mkdir cannot create directory sysfscgroupcpuacctmesose57f43431a974b44b347803be47ace80user no such file or directory srctestscontainerizerisolator_testscpp1307 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupcpuacctmesose57f43431a974b44b347803be47ace80usercgroupprocs no such file or directory srctestscontainerizerisolator_testscpp1316 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0 bash sysfscgroupcpumesose57f43431a974b44b347803be47ace80cgroupprocs no such file or directory mkdir cannot create directory sysfscgroupcpumesose57f43431a974b44b347803be47ace80user no such file or directory srctestscontainerizerisolator_testscpp1307 failure value of ossystem su    unprivileged_username   c mkdir   pathjoinflagscgroups_hierarchy usercgroup   actual 256 expected 0 bash sysfscgroupcpumesose57f43431a974b44b347803be47ace80usercgroupprocs no such file or directory srctestscontainerizerisolator_testscpp1316 failure value of ossystem su    unprivileged_username   c echo    pathjoinflagscgroups_hierarchy usercgroup cgroupprocs   actual 256 expected 0  failed  usercgroupisolatortest1root_cgroups_usercgroup where typeparam  mesosinternalslavecgroupscpushareisolatorprocess 116 ms noformat,3
make hdfs tool wrappers asynchronous the existing hdfs tool wrappers srchdfshdfshpp are synchronous they use osshell to shell out the hadoop commands this makes it very hard to be reused at other locations in the code base the uri fetcher hdfs plugin will try to reuse the existing hdfs tool wrappers in order to do that we need to make it asynchronous first,5
standardize quota endpoints to be consistent with other operator endpoints require a single json object in the request as opposed to keyvalue pairs encoded in a string,3
limitedcpuisolatortestroot_cgroups_cfs and limitedcpuisolatortestroot_cgroups_cfs_big_quota fail on debian 8 sudo binmesostestsh gtest_filterlimitedcpuisolatortestroot_cgroups_cfs noformat  f1119 143452514742 30706 isolator_testscpp455 check_someisolator failed to find cpucfs_quota_us your kernel might be too old to use the cfs cgroups feature noformat,2
ensure resources in quotainfo protobuf do not contain role quotainfo protobuf currently stores perrole quotas including resource objects these resources are neither statically nor dynamically reserved hence they may not contain role field we should ensure this field is unset as well as update validation routine for quotainfo,3
add integration tests for quota these tests should verify whether quota implements declared functionality this will require the whole pipeline master harness code and an allocator implementation in contrast to to isolated master and allocator tests,8
failing make distcheck on debian 8 somehow sslrelated as nonroot make distcheck noformat binmkdir p homevagrantmesosbuildmesos0260_instbin binbash libtool modeinstall usrbininstall c mesoslocal mesoslog mesos mesosexecute mesosresolve homevagrantmesosbuildmesos0260_instbin libtool install usrbininstall c libsmesoslocal homevagrantmesosbuildmesos0260_instbinmesoslocal libtool install usrbininstall c libsmesoslog homevagrantmesosbuildmesos0260_instbinmesoslog libtool install usrbininstall c libsmesos homevagrantmesosbuildmesos0260_instbinmesos libtool install usrbininstall c libsmesosexecute homevagrantmesosbuildmesos0260_instbinmesosexecute libtool install usrbininstall c libsmesosresolve homevagrantmesosbuildmesos0260_instbinmesosresolve traceback most recent call last file string line 1 in module file homevagrantmesosbuildmesos0260build3rdpartypip156pip__init_py line 11 in module from pipvcs import git mercurial subversion bazaar  noqa file homevagrantmesosbuildmesos0260_build3rdpartypip156pipvcsmercurialpy line 9 in module from pipdownload import path_to_url file homevagrantmesosbuildmesos0260_build3rdpartypip156pipdownloadpy line 22 in module from pip_vendor import requests six file homevagrantmesosbuildmesos0260build3rdpartypip156pip_vendorrequests__init_py line 53 in module from packagesurllib3contrib import pyopenssl file homevagrantmesosbuildmesos0260_build3rdpartypip156pip_vendorrequestspackagesurllib3contribpyopensslpy line 70 in module sslprotocol_sslv3 opensslsslsslv3_method attributeerror module object has no attribute protocol_sslv3 traceback most recent call last file string line 1 in module file homevagrantmesosbuildmesos0260_build3rd noformat,3
failing make distcheck on mac os x 10105 also 1011 nonroot make distcheck noformat   global test environment teardown  826 tests from 113 test cases ran 276624 ms total  passed  826 tests you have 6 disabled tests making install in  make3 nothing to be done for installexecam installsh c d usersberndmesosmesosbuildmesos0260_instlibpkgconfig usrbininstall c m 644 mesospc usersberndmesosmesosbuildmesos0260_instlibpkgconfig making install in 3rdparty applicationsxcodeappcontentsdeveloperusrbinmake installrecursive making install in libprocess making install in 3rdparty applicationsxcodeappcontentsdeveloperusrbinmake installrecursive making install in stout making install in  make9 nothing to be done for installexecam make9 nothing to be done for installdataam making install in include make9 nothing to be done for installexecam 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instinclude 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestout usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutaborthpp 3rdpartylibprocess3rdpartystoutincludestoutattributeshpp 3rdpartylibprocess3rdpartystoutincludestoutbase64hpp 3rdpartylibprocess3rdpartystoutincludestoutbitshpp 3rdpartylibprocess3rdpartystoutincludestoutbyteshpp 3rdpartylibprocess3rdpartystoutincludestoutcachehpp 3rdpartylibprocess3rdpartystoutincludestoutcheckhpp 3rdpartylibprocess3rdpartystoutincludestoutdurationhpp 3rdpartylibprocess3rdpartystoutincludestoutdynamiclibraryhpp 3rdpartylibprocess3rdpartystoutincludestouterrorhpp 3rdpartylibprocess3rdpartystoutincludestoutexithpp 3rdpartylibprocess3rdpartystoutincludestoutflagshpp 3rdpartylibprocess3rdpartystoutincludestoutforeachhpp 3rdpartylibprocess3rdpartystoutincludestoutformathpp 3rdpartylibprocess3rdpartystoutincludestoutfshpp 3rdpartylibprocess3rdpartystoutincludestoutgtesthpp 3rdpartylibprocess3rdpartystoutincludestoutgziphpp 3rdpartylibprocess3rdpartystoutincludestouthashmaphpp 3rdpartylibprocess3rdpartystoutincludestouthashsethpp 3rdpartylibprocess3rdpartystoutincludestoutintervalhpp 3rdpartylibprocess3rdpartystoutincludestoutiphpp 3rdpartylibprocess3rdpartystoutincludestoutjsonhpp 3rdpartylibprocess3rdpartystoutincludestoutlambdahpp 3rdpartylibprocess3rdpartystoutincludestoutlinkedhashmaphpp 3rdpartylibprocess3rdpartystoutincludestoutlisthpp 3rdpartylibprocess3rdpartystoutincludestoutmachpp 3rdpartylibprocess3rdpartystoutincludestoutmultihashmaphpp 3rdpartylibprocess3rdpartystoutincludestoutmultimaphpp 3rdpartylibprocess3rdpartystoutincludestoutnethpp 3rdpartylibprocess3rdpartystoutincludestoutnonehpp 3rdpartylibprocess3rdpartystoutincludestoutnothinghpp 3rdpartylibprocess3rdpartystoutincludestoutnumifyhpp 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp 3rdpartylibprocess3rdpartystoutincludestoutoshpp 3rdpartylibprocess3rdpartystoutincludestoutpathhpp 3rdpartylibprocess3rdpartystoutincludestoutpreprocessorhpp 3rdpartylibprocess3rdpartystoutincludestoutprochpp 3rdpartylibprocess3rdpartystoutincludestoutprotobufhpp 3rdpartylibprocess3rdpartystoutincludestoutrecordiohpp 3rdpartylibprocess3rdpartystoutincludestoutresulthpp usersberndmesosmesosbuildmesos0260_instincludestout 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutos usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutosbootidhpp 3rdpartylibprocess3rdpartystoutincludestoutoschdirhpp 3rdpartylibprocess3rdpartystoutincludestoutosclosehpp 3rdpartylibprocess3rdpartystoutincludestoutosconstantshpp 3rdpartylibprocess3rdpartystoutincludestoutosenvironmenthpp 3rdpartylibprocess3rdpartystoutincludestoutosexistshpp 3rdpartylibprocess3rdpartystoutincludestoutosfcntlhpp 3rdpartylibprocess3rdpartystoutincludestoutosforkhpp 3rdpartylibprocess3rdpartystoutincludestoutosftruncatehpp 3rdpartylibprocess3rdpartystoutincludestoutosgetcwdhpp 3rdpartylibprocess3rdpartystoutincludestoutoskilltreehpp 3rdpartylibprocess3rdpartystoutincludestoutoslinuxhpp 3rdpartylibprocess3rdpartystoutincludestoutoslshpp 3rdpartylibprocess3rdpartystoutincludestoutosmkdirhpp 3rdpartylibprocess3rdpartystoutincludestoutosmktemphpp 3rdpartylibprocess3rdpartystoutincludestoutosopenhpp 3rdpartylibprocess3rdpartystoutincludestoutososhpp 3rdpartylibprocess3rdpartystoutincludestoutososxhpp 3rdpartylibprocess3rdpartystoutincludestoutospermissionshpp 3rdpartylibprocess3rdpartystoutincludestoutosprocesshpp 3rdpartylibprocess3rdpartystoutincludestoutospstreehpp 3rdpartylibprocess3rdpartystoutincludestoutosreadhpp 3rdpartylibprocess3rdpartystoutincludestoutosrealpathhpp 3rdpartylibprocess3rdpartystoutincludestoutosrenamehpp 3rdpartylibprocess3rdpartystoutincludestoutosrmhpp 3rdpartylibprocess3rdpartystoutincludestoutossendfilehpp 3rdpartylibprocess3rdpartystoutincludestoutosshellhpp 3rdpartylibprocess3rdpartystoutincludestoutossignalshpp 3rdpartylibprocess3rdpartystoutincludestoutosstathpp 3rdpartylibprocess3rdpartystoutincludestoutossysctlhpp 3rdpartylibprocess3rdpartystoutincludestoutostouchhpp 3rdpartylibprocess3rdpartystoutincludestoutosutimehpp 3rdpartylibprocess3rdpartystoutincludestoutoswritehpp usersberndmesosmesosbuildmesos0260_instincludestoutos 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutposix usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutposixgziphpp 3rdpartylibprocess3rdpartystoutincludestoutposixoshpp usersberndmesosmesosbuildmesos0260_instincludestoutposix 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutflags usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutflagsfetchhpp 3rdpartylibprocess3rdpartystoutincludestoutflagsflaghpp 3rdpartylibprocess3rdpartystoutincludestoutflagsflagshpp 3rdpartylibprocess3rdpartystoutincludestoutflagsparsehpp usersberndmesosmesosbuildmesos0260_instincludestoutflags 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestouttests usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestouttestsutilshpp usersberndmesosmesosbuildmesos0260_instincludestouttests 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutoswindows usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutoswindowsbootidhpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsexistshpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsfcntlhpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsforkhpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsftruncatehpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowskilltreehpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowslshpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsprocesshpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowspstreehpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowssendfilehpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsshellhpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowssignalshpp 3rdpartylibprocess3rdpartystoutincludestoutoswindowsstathpp usersberndmesosmesosbuildmesos0260_instincludestoutoswindows 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutosposix usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutosposixbootidhpp 3rdpartylibprocess3rdpartystoutincludestoutosposixexistshpp 3rdpartylibprocess3rdpartystoutincludestoutosposixfcntlhpp 3rdpartylibprocess3rdpartystoutincludestoutosposixforkhpp 3rdpartylibprocess3rdpartystoutincludestoutosposixftruncatehpp 3rdpartylibprocess3rdpartystoutincludestoutosposixkilltreehpp 3rdpartylibprocess3rdpartystoutincludestoutosposixlshpp 3rdpartylibprocess3rdpartystoutincludestoutosposixprocesshpp 3rdpartylibprocess3rdpartystoutincludestoutosposixpstreehpp 3rdpartylibprocess3rdpartystoutincludestoutosposixsendfilehpp 3rdpartylibprocess3rdpartystoutincludestoutosposixshellhpp 3rdpartylibprocess3rdpartystoutincludestoutosposixsignalshpp 3rdpartylibprocess3rdpartystoutincludestoutosposixstathpp usersberndmesosmesosbuildmesos0260_instincludestoutosposix 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestout usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutsethpp 3rdpartylibprocess3rdpartystoutincludestoutsomehpp 3rdpartylibprocess3rdpartystoutincludestoutstopwatchhpp 3rdpartylibprocess3rdpartystoutincludestoutstringifyhpp 3rdpartylibprocess3rdpartystoutincludestoutstringshpp 3rdpartylibprocess3rdpartystoutincludestoutsubcommandhpp 3rdpartylibprocess3rdpartystoutincludestoutsvnhpp 3rdpartylibprocess3rdpartystoutincludestoutsynchronizedhpp 3rdpartylibprocess3rdpartystoutincludestoutthread_localhpp 3rdpartylibprocess3rdpartystoutincludestouttryhpp 3rdpartylibprocess3rdpartystoutincludestoutunimplementedhpp 3rdpartylibprocess3rdpartystoutincludestoutunreachablehpp 3rdpartylibprocess3rdpartystoutincludestoututilshpp 3rdpartylibprocess3rdpartystoutincludestoutuuidhpp 3rdpartylibprocess3rdpartystoutincludestoutversionhpp 3rdpartylibprocess3rdpartystoutincludestoutwindowshpp usersberndmesosmesosbuildmesos0260_instincludestout 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutwindows usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutwindowsformathpp 3rdpartylibprocess3rdpartystoutincludestoutwindowsgziphpp 3rdpartylibprocess3rdpartystoutincludestoutwindowsoshpp usersberndmesosmesosbuildmesos0260_instincludestoutwindows 3rdpartylibprocess3rdpartystoutinstallsh c d usersberndmesosmesosbuildmesos0260_instincludestoutosraw usrbininstall c m 644 3rdpartylibprocess3rdpartystoutincludestoutosrawenvironmenthpp usersberndmesosmesosbuildmesos0260_instincludestoutosraw make8 nothing to be done for installexecam make8 nothing to be done for installdataam making install in  make6 nothing to be done for installexecam make6 nothing to be done for installdataam making install in include make6 nothing to be done for installexecam 3rdpartylibprocessinstallsh c d usersberndmesosmesosbuildmesos0260_instinclude 3rdpartylibprocessinstallsh c d usersberndmesosmesosbuildmesos0260_instincludeprocess usrbininstall c m 644 3rdpartylibprocessincludeprocessaddresshpp 3rdpartylibprocessincludeprocessasynchpp 3rdpartylibprocessincludeprocesscheckhpp 3rdpartylibprocessincludeprocessclockhpp 3rdpartylibprocessincludeprocesscollecthpp 3rdpartylibprocessincludeprocessdeferhpp 3rdpartylibprocessincludeprocessdeferredhpp 3rdpartylibprocessincludeprocessdelayhpp 3rdpartylibprocessincludeprocessdispatchhpp 3rdpartylibprocessincludeprocesseventhpp 3rdpartylibprocessincludeprocessexecutorhpp 3rdpartylibprocessincludeprocessfilterhpp 3rdpartylibprocessincludeprocessfirewallhpp 3rdpartylibprocessincludeprocessfuturehpp 3rdpartylibprocessincludeprocessgchpp 3rdpartylibprocessincludeprocessgmockhpp 3rdpartylibprocessincludeprocessgtesthpp 3rdpartylibprocessincludeprocesshelphpp 3rdpartylibprocessincludeprocesshttphpp 3rdpartylibprocessincludeprocessidhpp 3rdpartylibprocessincludeprocessiohpp 3rdpartylibprocessincludeprocesslatchhpp 3rdpartylibprocessincludeprocesslimiterhpp 3rdpartylibprocessincludeprocesslogginghpp 3rdpartylibprocessincludeprocessmessagehpp 3rdpartylibprocessincludeprocessmimehpp 3rdpartylibprocessincludeprocessmutexhpp 3rdpartylibprocessincludeprocessnetworkhpp 3rdpartylibprocessincludeprocessoncehpp 3rdpartylibprocessincludeprocessownedhpp 3rdpartylibprocessincludeprocesspidhpp 3rdpartylibprocessincludeprocessprocesshpp 3rdpartylibprocessincludeprocessprofilerhpp 3rdpartylibprocessincludeprocessprotobufhpp 3rdpartylibprocessincludeprocessqueuehpp 3rdpartylibprocessincludeprocessreaphpp 3rdpartylibprocessincludeprocessrunhpp 3rdpartylibprocessincludeprocesssequencehpp 3rdpartylibprocessincludeprocesssharedhpp 3rdpartylibprocessincludeprocesssockethpp usersberndmesosmesosbuildmesos0260_instincludeprocess 3rdpartylibprocessinstallsh c d usersberndmesosmesosbuildmesos0260_instincludeprocess usrbininstall c m 644 3rdpartylibprocessincludeprocessstatisticshpp 3rdpartylibprocessincludeprocesssystemhpp 3rdpartylibprocessincludeprocesssubprocesshpp 3rdpartylibprocessincludeprocesstimehpp 3rdpartylibprocessincludeprocesstimeouthpp 3rdpartylibprocessincludeprocesstimerhpp 3rdpartylibprocessincludeprocesstimeserieshpp usersberndmesosmesosbuildmesos0260_instincludeprocess 3rdpartylibprocessinstallsh c d usersberndmesosmesosbuildmesos0260_instincludeprocessssl usrbininstall c m 644 3rdpartylibprocessincludeprocesssslgtesthpp 3rdpartylibprocessincludeprocesssslutilitieshpp usersberndmesosmesosbuildmesos0260_instincludeprocessssl 3rdpartylibprocessinstallsh c d usersberndmesosmesosbuildmesos0260_instincludeprocessmetrics usrbininstall c m 644 3rdpartylibprocessincludeprocessmetricscounterhpp 3rdpartylibprocessincludeprocessmetricsgaugehpp 3rdpartylibprocessincludeprocessmetricsmetrichpp 3rdpartylibprocessincludeprocessmetricsmetricshpp 3rdpartylibprocessincludeprocessmetricstimerhpp usersberndmesosmesosbuildmesos0260_instincludeprocessmetrics make5 nothing to be done for installexecam make5 nothing to be done for installdataam making install in src applicationsxcodeappcontentsdeveloperusrbinmake installam test      installsh c d pythonclisrcmesos  cp pf srcpythonclisrcmesos__init__py pythonclisrcmesos__init__py test      installsh c d pythonclisrcmesos  cp pf srcpythonclisrcmesosclipy pythonclisrcmesosclipy test      installsh c d pythonclisrcmesos  cp pf srcpythonclisrcmesosfuturespy pythonclisrcmesosfuturespy test      installsh c d pythonclisrcmesos  cp pf srcpythonclisrcmesoshttppy pythonclisrcmesoshttppy test      installsh c d pythoninterfacesrcmesos  cp pf srcpythoninterfacesrcmesos__init__py pythoninterfacesrcmesos__init__py test      installsh c d pythoninterfacesrcmesosinterface  cp pf srcpythoninterfacesrcmesosinterface__init__py pythoninterfacesrcmesosinterface__init__py test      installsh c d pythoninterfacesrcmesosv1  cp pf srcpythoninterfacesrcmesosv1__init__py pythoninterfacesrcmesosv1__init__py test      installsh c d pythoninterfacesrcmesosv1interface  cp pf srcpythoninterfacesrcmesosv1interface__init__py pythoninterfacesrcmesosv1interface__init__py test,2
ssl build of mesos causes flaky testsuite when running the tests of an ssl build of mesos on centos 71 i see spurious test failures that are so far not reproducible the following tests did fail for me in complete runs but did seem fine when running them individually in repetition noformat dockertestroot_docker_checkportresource noformat noformat containerizertestroot_cgroups_balloonframework noformat noformat  run  linuxfilesystemisolatortestroot_changerootfilesystemcommandexecutor 20151120 190838826213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client  homevagrantmesosbuildsrcmesoscontainerizer mount helpfalse operationmakerslave path  grep e tmplinuxfilesystemisolatortest_root_changerootfilesystemcommandexecutor_tz7p8c procselfmountinfo  grep v 2b98025c74f141d2b35ace2cdfae347e  cut d  f5  xargs norunifempty umount l  mount n rbind tmplinuxfilesystemisolatortest_root_changerootfilesystemcommandexecutor_tz7p8cprovisionercontainers2b98025c74f141d2b35ace2cdfae347ebackendscopyrootfsesbed11080474b4c698e7f0ab85e895b0d tmplinuxfilesystemisolatortest_root_changerootfilesystemcommandexecutor_tz7p8cslaves830e842ec36a4e4cbff45b9568d7df12s0frameworks830e842ec36a4e4cbff45b9568d7df120000executorsc735be54c47f4645bfc12f4647e2cddbruns2b98025c74f141d2b35ace2cdfae347erootfs could not load cert file srctestscontainerizerfilesystem_isolator_testscpp354 failure value of statusrunninggetstate actual task_failed expected task_running 20151120 190842164213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client 20151120 190845501213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client 20151120 190848837213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client 20151120 190852174213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client srctestscontainerizerfilesystem_isolator_testscpp355 failure failed to wait 15secs for statusfinished srctestscontainerizerfilesystem_isolator_testscpp349 failure actual function call count doesnt match expect_callsched statusupdatedriver _ expected to be called twice actual called once  unsatisfied and active 20151120 190855511213800x7fa10d5f2700zoo_errorhandle_socket_error_msg1697 socket 12700153444 zk retcode4 errno111connection refused server refused to accept the client  aborted at 1448046536 unix time try date d 1448046536 if you are using gnu date  pc  0x0 unknown  sigsegv 0x0 received by pid 21380 tid 0x7fa1549e68c0 from pid 0 stack trace   0x7fa141796fbb unknown  0x7fa14179b341 unknown  0x7fa14f096130 unknown noformat vagrantfile generator noformat cat  eof  vagrantfile   mode ruby    vi set ftruby  vagrantconfigure2 do config  disable shared folder to prevent certain kernel module dependencies configvmsynced_folder  vagrant disabled true configvmhostname  centos71 configvmbox  bentocentos71 configvmprovider virtualbox do vb vbmemory  16384 vbcpus  8 end configvmprovider vmware_fusion do vb vbmemory  9216 vbcpus  4 end configvmprovision shell inline shell sudo yum y update systemd sudo yum install y tar wget sudo wget httpreposfedorapeopleorgreposdchenapachemavenepelapachemavenrepo o etcyumreposdepelapachemavenrepo sudo yum groupinstall y development tools sudo yum install y apachemaven pythondevel java170openjdkdevel zlibdevel libcurldevel openssldevel cyrussasldevel cyrussaslmd5 aprdevel subversiondevel aprutildevel sudo yum install libeventdevel sudo yum install y git sudo yum install y docker sudo service docker start sudo docker info sudo wget qo httpsgetdockercom  sh shell end eof vagrant up vagrant reload vagrant ssh c  git clone httpsgithubcomapachemesosgit mesos cd mesos git checkout b 0260rc1 0260rc1 bootstrap mkdir build cd build configure enablelibevent enablessl gtest_filter make check sudo binmesostestssh  noformat,5
c http scheduler library does not work with ssl enabled the c http scheduler library does not work against mesos when ssl is enabled without downgrade the fix should be simple  the library should detect if ssl is enabled  if ssl is enabled connections should be made with https instead of http,2
replace quotainfo with quota in allocator interface after introduction of c wrapper quota for quotainfo all allocator methods using quotainfo should be updated,3
implement recovery in the hierarchical allocator the builtin hierarchical allocator should implement the recovery in the presence of quota,3
tests for quota request validation tests should include  json validation  absence of irrelevant fields  semantic validation,3
tests for quota support in allocate function,3
tests for rescinding offers for quota,1
tests for allocator recovery,5
refactor registry clientpuller to avoid json and struct we should get rid of all json and struct for message passing as function returned type by using the methods provided by spechpp to refactor all unnecessary json message and struct in registry client and registry puller also remove all redundant check in registry client that are already checked by spec validation,3
libprocess document when why defer is necessary current rules around this are pretty confusing and undocumented as evidenced by some recent bugs in this area some example snippets in the mesos source code that were a result of this confusion and are indeed bugs 1 httpsgithubcomapachemesosblobmastersrcslavecontainerizermesosprovisionerdockerregistry_clientcppl754 code return dohttpgetbloburl none true true none thenthis bloburlpath digest filepath const httpresponse response  futuresize_t  tryint fd  osopen filepathvalue o_wronly  o_creat  o_trunc  o_cloexec s_irusr  s_iwusr  s_irgrp  s_iroth code,1
implicit roles design doc,2
reservationendpointstestunreserveavailableandofferedresources is flaky showed up on asf ci  test kept looping on and on and ultimately failing the build after 300 minutes  httpsbuildsapacheorgjobmesoscompilergccconfigurationverboseosubuntu3a1404label_expdocker7c7chadoop1269changes code  run  reservationendpointstestunreserveavailableandofferedresources i1124 010720050729 30260 leveldbcpp174 opened db in 107434842ms i1124 010720099630 30260 leveldbcpp181 compacted db in 4882312ms i1124 010720099722 30260 leveldbcpp196 created db iterator in 29905ns i1124 010720099738 30260 leveldbcpp202 seeked to beginning of db in 3145ns i1124 010720099750 30260 leveldbcpp271 iterated through 0 keys in the db in 279ns i1124 010720099804 30260 replicacpp778 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1124 010720100637 30292 recovercpp447 starting replica recovery i1124 010720100934 30292 recovercpp473 replica is in empty status i1124 010720103240 30288 replicacpp674 replica in empty status received a broadcasted recover request from 6305172171810737993 i1124 010720103672 30292 recovercpp193 received a recover response from a replica in empty status i1124 010720104142 30292 recovercpp564 updating replica status to starting i1124 010720114534 30284 mastercpp365 master ad27bc6016d142399a65235a991f9600 9f2f81738d5e started on 172171810737993 i1124 010720114558 30284 mastercpp367 flags at startup acls allocation_interval1000secs allocatorhierarchicaldrf authenticatetrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpi60i5fcredentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue rolesrole root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0260_instsharemesoswebui work_dirtmpi60i5fmaster zk_session_timeout10secs i1124 010720114809 30284 mastercpp412 master only allowing authenticated frameworks to register i1124 010720114820 30284 mastercpp417 master only allowing authenticated slaves to register i1124 010720114825 30284 credentialshpp35 loading credentials for authentication from tmpi60i5fcredentials i1124 010720115067 30284 mastercpp456 using default crammd5 authenticator i1124 010720115320 30284 mastercpp493 authorization enabled i1124 010720115792 30285 hierarchicalcpp162 initialized hierarchical allocator process i1124 010720115855 30285 whitelist_watchercpp77 no whitelist given i1124 010720118755 30285 mastercpp1625 the newly elected leader is master172171810737993 with id ad27bc6016d142399a65235a991f9600 i1124 010720118788 30285 mastercpp1638 elected as the leading master i1124 010720118809 30285 mastercpp1383 recovering from registrar i1124 010720119078 30285 registrarcpp307 recovering registrar i1124 010720143256 30292 leveldbcpp304 persisting metadata 8 bytes to leveldb took 38787419ms i1124 010720143347 30292 replicacpp321 persisted replica status to starting i1124 010720143717 30292 recovercpp473 replica is in starting status i1124 010720145454 30286 replicacpp674 replica in starting status received a broadcasted recover request from 6307172171810737993 i1124 010720145979 30292 recovercpp193 received a recover response from a replica in starting status i1124 010720146654 30292 recovercpp564 updating replica status to voting i1124 010720182672 30286 leveldbcpp304 persisting metadata 8 bytes to leveldb took 35422256ms i1124 010720182747 30286 replicacpp321 persisted replica status to voting i1124 010720182929 30286 recovercpp578 successfully joined the paxos group i1124 010720183115 30286 recovercpp462 recover process terminated i1124 010720183831 30286 logcpp659 attempting to start the writer i1124 010720185907 30285 replicacpp494 replica received implicit promise request from 6308172171810737993 with proposal 1 i1124 010720225256 30285 leveldbcpp304 persisting metadata 8 bytes to leveldb took 39291288ms i1124 010720225344 30285 replicacpp343 persisted promised to 1 i1124 010720226850 30286 coordinatorcpp238 coordinator attempting to fill missing positions i1124 010720228394 30293 replicacpp389 replica received explicit promise request from 6309172171810737993 for position 0 with proposal 2 i1124 010720266371 30293 leveldbcpp341 persisting action 8 bytes to leveldb took 37874181ms i1124 010720266456 30293 replicacpp713 persisted action at 0 i1124 010720267927 30293 replicacpp538 replica received write request for position 0 from 6310172171810737993 i1124 010720268002 30293 leveldbcpp436 reading position from leveldb took 37139ns i1124 010720308117 30293 leveldbcpp341 persisting action 14 bytes to leveldb took 39961976ms i1124 010720308205 30293 replicacpp713 persisted action at 0 i1124 010720309033 30290 replicacpp692 replica received learned notice for position 0 from 00000 i1124 010720343257 30290 leveldbcpp341 persisting action 16 bytes to leveldb took 34175337ms i1124 010720343343 30290 replicacpp713 persisted action at 0 i1124 010720343377 30290 replicacpp698 replica learned nop action at position 0 i1124 010720344446 30282 logcpp675 writer started with ending position 0 i1124 010720346143 30291 leveldbcpp436 reading position from leveldb took 56896ns i1124 010720347618 30291 registrarcpp340 successfully fetched the registry 0b in 228495104ms i1124 010720347862 30291 registrarcpp439 applied 1 operations in 41164ns attempting to update the registry i1124 010720348794 30279 logcpp683 attempting to append 178 bytes to the log i1124 010720349081 30279 coordinatorcpp348 coordinator attempting to write append action at position 1 i1124 010720350244 30294 replicacpp538 replica received write request for position 1 from 6311172171810737993 i1124 010720385246 30294 leveldbcpp341 persisting action 197 bytes to leveldb took 34872508ms i1124 010720385323 30294 replicacpp713 persisted action at 1 i1124 010720386814 30294 replicacpp692 replica received learned notice for position 1 from 00000 i1124 010720425163 30294 leveldbcpp341 persisting action 199 bytes to leveldb took 38282493ms i1124 010720425262 30294 replicacpp713 persisted action at 1 i1124 010720425298 30294 replicacpp698 replica learned append action at position 1 i1124 010720427994 30287 registrarcpp484 successfully updated the registry in 79949056ms i1124 010720428141 30283 logcpp702 attempting to truncate the log to 1 i1124 010720428738 30287 registrarcpp370 successfully recovered registrar i1124 010720429306 30290 mastercpp1435 recovered 0 slaves from the registry 139b  allowing 10mins for slaves to reregister i1124 010720429592 30290 hierarchicalcpp174 allocator recovery is not supported yet i1124 010720430083 30294 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i1124 010720431411 30294 replicacpp538 replica received write request for position 2 from 6312172171810737993 i1124 010720467258 30294 leveldbcpp341 persisting action 16 bytes to leveldb took 35661978ms i1124 010720467342 30294 replicacpp713 persisted action at 2 i1124 010720468842 30290 replicacpp692 replica received learned notice for position 2 from 00000 i1124 010720502264 30290 leveldbcpp341 persisting action 18 bytes to leveldb took 33367074ms i1124 010720502426 30290 leveldbcpp399 deleting 1 keys from leveldb took 80765ns i1124 010720502452 30290 replicacpp713 persisted action at 2 i1124 010720502488 30290 replicacpp698 replica learned truncate action at position 2 i1124 010720510509 30260 containerizercpp141 using isolation posixcpuposixmemfilesystemposix w1124 010720511119 30260 backendcpp48 failed to create bind backend bindbackend requires root privileges i1124 010720516801 30288 slavecpp189 slave started on 219172171810737993 i1124 010720516839 30288 slavecpp190 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpreservationendpointstest_unreserveavailableandofferedresources_cszecrcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpreservationendpointstest_unreserveavailableandofferedresources_cszecrfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0260_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpreservationendpointstest_unreserveavailableandofferedresources_cszecr i1124 010720517670 30288 credentialshpp83 loading credential for authentication from tmpreservationendpointstest_unreserveavailableandofferedresources_cszecrcredential i1124 010720517982 30288 slavecpp320 slave using credential for testprincipal i1124 010720518334 30288 resourcescpp472 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i1124 010720518815 30260 resourcescpp472 parsing resources as json failed cpus1mem128 trying semicolondelimited string format instead i1124 010720518975 30288 slavecpp390 slave resources cpus2 mem1024 disk1024 ports3100032000 i1124 010720519104 30288 slavecpp398 slave attributes   i1124 010720519124 30288 slavecpp403 slave hostname 9f2f81738d5e i1124 010720519136 30288 slavecpp408 slave checkpoint true i1124 010720519407 30260 resourcescpp472 parsing resources as json failed mem384 trying semicolondelimited string format instead i1124 010720522702 30288 statecpp52 recovering state from tmpreservationendpointstest_unreserveavailableandofferedresources_cszecrmeta i1124 010720523265 30288 status_update_managercpp200 recovering status update manager i1124 010720523531 30288 containerizercpp383 recovering containerizer i1124 010720524998 30288 slavecpp4258 finished recovery i1124 010720525802 30288 slavecpp4430 querying resource estimator for oversubscribable resources i1124 010720526753 30288 slavecpp727 new master detected at master172171810737993 i1124 010720527292 30288 slavecpp790 authenticating with master master172171810737993 i1124 010720528240 30288 slavecpp795 using default crammd5 authenticatee i1124 010720527003 30286 status_update_managercpp174 pausing sending status updates i1124 010720528955 30285 authenticateecpp121 creating new client sasl connection i1124 010720529469 30285 mastercpp5169 authenticating slave219172171810737993 i1124 010720529729 30283 authenticatorcpp413 starting authentication session for crammd5_authenticatee515172171810737993 i1124 010720530287 30283 authenticatorcpp98 creating new server sasl connection i1124 010720530764 30285 authenticateecpp212 received sasl authentication mechanisms crammd5 i1124 010720530903 30285 authenticateecpp238 attempting to authenticate with mechanism crammd5 i1124 010720531096 30285 authenticatorcpp203 received sasl authentication start i1124 010720531241 30285 authenticatorcpp325 authentication requires more steps i1124 010720531388 30285 authenticateecpp258 received sasl authentication step i1124 010720531616 30285 authenticatorcpp231 received sasl authentication step i1124 010720531668 30285 auxpropcpp107 request to lookup properties for user testprincipal realm 9f2f81738d5e server fqdn 9f2f81738d5e sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1124 010720531690 30285 auxpropcpp179 looking up auxiliary property userpassword i1124 010720531774 30285 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i1124 010720531834 30285 auxpropcpp107 request to lookup properties for user testprincipal realm 9f2f81738d5e server fqdn 9f2f81738d5e sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1124 010720531855 30285 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1124 010720531867 30285 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1124 010720531903 30285 authenticatorcpp317 authentication success i1124 010720532016 30283 authenticateecpp298 authentication success i1124 010720532331 30281 mastercpp5199 successfully authenticated principal testprincipal at slave219172171810737993 i1124 010720532652 30291 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee515172171810737993 i1124 010720533113 30288 slavecpp763 detecting new master i1124 010720533628 30288 slavecpp4444 received oversubscribable resources from the resource estimator i1124 010720546396 30288 slavecpp858 successfully authenticated with master master172171810737993 i1124 010720547111 30287 mastercpp3878 registering slave at slave219172171810737993 9f2f81738d5e with id ad27bc6016d142399a65235a991f9600s0 i1124 010720547886 30287 registrarcpp439 applied 1 operations in 91121ns attempting to update the registry i1124 010720550647 30287 logcpp683 attempting to append 347 bytes to the log i1124 010720550935 30279 coordinatorcpp348 coordinator attempting to write append action at position 3 i1124 010720551534 30288 slavecpp1252 will retry registration in 3399312ms if necessary i1124 010720551868 30291 replicacpp538 replica received write request for position 3 from 6324172171810737993 i1124 010720557605 30281 slavecpp1252 will retry registration in 16296866ms if necessary i1124 010720557891 30293 mastercpp3866 ignoring register slave message from slave219172171810737993 9f2f81738d5e as admission is already in progress i1124 010720574681 30279 slavecpp1252 will retry registration in 7352632ms if necessary i1124 010720575078 30293 mastercpp3866 ignoring register slave message from slave219172171810737993 9f2f81738d5e as admission is already in progress i1124 010720586236 30291 leveldbcpp341 persisting action 366 bytes to leveldb took 34301173ms i1124 010720586287 30291 replicacpp713 persisted action at 3 i1124 010720587509 30289 replicacpp692 replica received learned notice for position 3 from 00000 i1124 010720611263 30289 leveldbcpp341 persisting action 368 bytes to leveldb took 23677211ms i1124 010720611352 30289 replicacpp713 persisted action at 3 i1124 010720611387 30289 replicacpp698 replica learned append action at position 3 i1124 010720613580 30279 registrarcpp484 successfully updated the registry in 65490944ms i1124 010720613802 30288 logcpp702 attempting to truncate the log to 3 i1124 010720613993 30288 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i1124 010720615281 30289 replicacpp538 replica received write request for position 4 from 6325172171810737993 i1124 010720615883 30279 mastercpp3946 registered slave ad27bc6016d142399a65235a991f9600s0 at slave219172171810737993 9f2f81738d5e with cpus2 mem1024 disk1024 ports3100032000 i1124 010720616261 30282 slavecpp902 registered with master master172171810737993 given slave id ad27bc6016d142399a65235a991f9600s0 i1124 010720616883 30282 fetchercpp79 clearing fetcher cache i1124 010720617261 30280 status_update_managercpp181 resuming sending status updates i1124 010720617766 30282 slavecpp925 checkpointing slaveinfo to tmpreservationendpointstest_unreserveavailableandofferedresources_cszecrmetaslavesad27bc6016d142399a65235a991f9600s0slaveinfo i1124 010720616550 30284 hierarchicalcpp380 added slave ad27bc6016d142399a65235a991f9600s0 9f2f81738d5e with cpus2 mem1024 disk1024 ports3100032000 allocated  i1124 010720618670 30282 slavecpp961 forwarding total oversubscribed resources i1124 010720618932 30282 slavecpp3197 received ping from slaveobserver216172171810737993 i1124 010720619288 30285 mastercpp4288 received update of slave ad27bc6016d142399a65235a991f9600s0 at slave219172171810737993 9f2f81738d5e with total oversubscribed resources i1124 010720619446 30284 hierarchicalcpp1066 no resources available to allocate i1124 010720619526 30284 hierarchicalcpp1159 no inverse offers to send out i1124 010720619568 30284 hierarchicalcpp977 performed allocation for slave ad27bc6016d142399a65235a991f9600s0 in 1108641ms i1124 010720620057 30284 hierarchicalcpp436 slave ad27bc6016d142399a65235a991f9600s0 9f2f81738d5e updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i1124 010720620393 30284 hierarchicalcpp1066 no resources available to allocate i1124 010720620462 30284 hierarchicalcpp1159 no inverse offers to send out i1124 010720620507 30284 hierarchicalcpp977 performed allocation for slave ad27bc6016d142399a65235a991f9600s0 in 395959ns i1124 010720624356 30285 processcpp3067 handling http event for process master with path masterreserve i1124 010720624418 30285 httpcpp336 http post for masterreserve from 172171810748995 i1124 010720626936 30285 mastercpp6224 sending checkpointed resources cpusrole te,1
pass agent work_dir to isolator modules some isolator modules can benefit from access to the agents work_dir for example the dvd isolator httpsgithubcomemccodemesosmoduledvdi is currently forced to mount external volumes in a hardcoded directory making the work_dir accessible to the isolator via isolatorrecover would allow the isolator to mount volumes within the agents work_dir this can be accomplished by simply adding an overloaded signature for isolatorrecover which includes the work_dir as a parameter,1
support default entrypoint and command runtime config in mesos containerizer we need to use the entrypoint and command runtime configuration returned from image to be used in mesos containerizer,3
support workdir runtime configuration from image we need to support workdir runtime configuration returned from image such as dockerfile,2
registryclienttestsimpleregistrypuller doesnt compile with gcc 511 gcc 511 has werrorsigncompare in wall and stumbles over a comparison between signed and unsigned int in provisioner_docker_testscpp,1
introduce status endpoint for quota this endpoint is for querying quota status via the get method,5
introduce remove endpoint for quota this endpoint is for removing quotas via the delete method,3
introduce filter for nonrevocable resources in resources resources class defines some handy filters like revocable unreserved and so on this ticket proposes to add one more nonrevocable,1
remove quota from registry for quota remove request when a remove quota requests hits the endpoint and passes validation quota should be removed from the registry before the allocator is notified about the change,1
registryclienttestsimpleregistrypuller is flaky from asf ci httpsbuildsapacheorgjobmesos1289compilergccconfigurationverbose20enablelibevent20enablessloscentos7label_expdocker7c7chadoopconsole code  run  registryclienttestsimpleregistrypuller i1127 025140235900 362 registry_clientcpp511 response status for url httpslocalhost57828v2librarybusyboxmanifestslatest 401 unauthorized i1127 025140249766 360 registry_clientcpp511 response status for url httpslocalhost57828v2librarybusyboxmanifestslatest 200 ok i1127 025140251137 361 registry_pullercpp195 downloading layer 1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea for image busyboxlatest i1127 025140258514 354 registry_clientcpp511 response status for url httpslocalhost57828v2librarybusyboxblobssha256a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 307 temporary redirect i1127 025140264171 367 libevent_ssl_socketcpp1023 socket error connection reset by peer srctestscontainerizerprovisioner_docker_testscpp1210 failure socketfailure failed accept connection error connection reset by peer  failed  registryclienttestsimpleregistrypuller 349 ms code logs from a previous run that passed code  run  registryclienttestsimpleregistrypuller i1126 184905306396 349 registry_clientcpp511 response status for url httpslocalhost53492v2librarybusyboxmanifestslatest 401 unauthorized i1126 184905321362 347 registry_clientcpp511 response status for url httpslocalhost53492v2librarybusyboxmanifestslatest 200 ok i1126 184905322720 352 registry_pullercpp195 downloading layer 1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea for image busyboxlatest i1126 184905331317 350 registry_clientcpp511 response status for url httpslocalhost53492v2librarybusyboxblobssha256a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 307 temporary redirect i1126 184905370625 352 registry_clientcpp511 response status for url https12700153492 200 ok i1126 184905372102 355 registry_pullercpp294 untarring layer 1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea downloaded from registry to directory output_dir  ok  registryclienttestsimpleregistrypuller 353 ms code,4
contenttypeschedulertest is flaky ssl build ubuntu 1404httpsgithubcomtilltmesosvagrantciblobmasterubuntu14setupsh nonroot test run noformat  22 tests from contenttypeschedulertest  run  contenttypeschedulertestsubscribe0  ok  contenttypeschedulertestsubscribe0 48 ms  aborted at 1448928007 unix time try date d 1448928007 if you are using gnu date   run  contenttypeschedulertestsubscribe1 pc  0x1451b8e testinginternaluntypedfunctionmockerbaseuntypedinvokewith  sigsegv 0x100000030 received by pid 21320 tid 0x2b549e5d4700 from pid 48 stack trace   0x2b54c95940b7 oslinuxchained_handler  0x2b54c9598219 jvm_handle_linux_signal  0x2b5496300340 unknown  0x1451b8e testinginternaluntypedfunctionmockerbaseuntypedinvokewith  0xe2ea6d _zn7testing8internal18functionmockerbaseifvrkst5queuein5mesos2v19scheduler5eventest5dequeis6_sais6_eeeee10invokewitherkst5tupleijsc_ee  0xe2b1bc testinginternalfunctionmockerinvoke  0x1118aed mesosinternaltestsschedulertestcallbacksreceived  0x111c453 _znkst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins0_2v19scheduler5eventest5dequeis8_sais8_eeeeeclijse_eveevrs4_dpot_  0x111c001 _znst5_bindifst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins1_2v19scheduler5eventest5dequeis9_sais9_eeeeest17reference_wrapperis5_est12_placeholderili1eeee6__callivjsf_ejlm0elm1eeeet_ost5tupleijdpt0_eest12_index_tupleijxspt1_eee  0x111b90d _znst5_bindifst7_mem_fnimn5mesos8internal5tests13schedulertest9callbacksefvrkst5queueins1_2v19scheduler5eventest5dequeis9_sais9_eeeeest17reference_wrapperis5_est12_placeholderili1eeeeclijsf_eveet0_dpot_  0x111ae09 std_function_handler_m_invoke  0x2b5493c6da09 stdfunctionoperator  0x2b5493c688ee processasyncexecutorprocessexecute  0x2b5493c6db2a _zzn7process8dispatchi7nothingns_20asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeis8_sais8_eeeeesc_pvsg_sc_sj_eens_6futureit_eerkns_3pidit0_eemso_fsl_t1_t2_t3_et4_t5_t6_enkulpns_11processbaseee_cles11_  0x2b5493c765a4 _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchi7nothingns0_20asyncexecutorprocesserkst8functionifvrkst5queuein5mesos2v19scheduler5eventest5dequeisc_saisc_eeeeesg_pvsk_sg_sn_eens0_6futureit_eerkns0_3pidit0_eemss_fsp_t1_t2_t3_et4_t5_t6_euls2_e_e9_m_invokeerkst9_any_datas2_  0x2b54946b1201 stdfunctionoperator  0x2b549469960f processprocessbasevisit  0x2b549469d480 processdispatcheventvisit  0x9dc0ba processprocessbaseserve  0x2b54946958cc processprocessmanagerresume  0x2b5494692a9c _zzn7process14processmanager12init_threadsevenkulrkst11atomic_boole_cles3_  0x2b549469ccac _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eee6__callivieilm0eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_eee  0x2b549469cc5c _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eeecliieveet0_dpot_  0x2b549469cbee _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeevee9_m_invokeiieeevst12_index_tupleiixspt_eee  0x2b549469cb45 _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeeveeclev  0x2b549469cade _znst6thread5_implist12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis6_eeeveee6_m_runev  0x2b5495b81a40 unknown  0x2b54962f8182 start_thread  0x2b549660847d unknown make3  checklocal segmentation fault make3 leaving directory homevagrantmesosbuildsrc make2  checkam error 2 make2 leaving directory homevagrantmesosbuildsrc make1  check error 2 make1 leaving directory homevagrantmesosbuildsrc make  checkrecursive error 1 noformat,2
install instructions for centos 66 lead to errors running perf after using the current installation instructions in the getting started documentation perf will not run on centos 66 because the version of elfutils included in devtoolset2 is not compatible with the version of perf installed by yum installing and using devtoolset3 however httplinuxwebcernchlinuxscientific6docssoftwarecollectionsshtml fixes this issue this could be resolved by updating the getting started documentation to recommend installing devtoolset3,1
enable env specified in docker image can be returned from docker pull currently docker pull only return an image structure which only contains entrypoint info we have docker inspect as a subprocess inside docker pull which contains many other useful information of a docker image we should be able to support returning environment variables information from the image,3
memorypressuremesostestcgroups_root_slaverecovery is flaky codetitleoutput from passed test  1 test from memorypressuremesostest 10 records in 10 records out 1048576 bytes 10 mb copied 0000430889 s 24 gbs  run  memorypressuremesostestcgroups_root_slaverecovery i1202 110914319327 5062 execcpp134 version 0270 i1202 110914333317 5079 execcpp208 executor registered on slave bea15b359aa14b5796fb29b5f70638acs0 registered executor on ubuntu starting task 4e62294ccfcf4a13b699c6a4b7ac5162 sh c while true do dd count512 bs1m ifdevzero oftemp done forked command at 5085 i1202 110914391739 5077 execcpp254 received reconnect request from slave bea15b359aa14b5796fb29b5f70638acs0 i1202 110914398598 5082 execcpp231 executor reregistered on slave bea15b359aa14b5796fb29b5f70638acs0 reregistered executor on ubuntu shutting down sending sigterm to process tree at pid 5085 killing the following process trees   5085 sh c while true do dd count512 bs1m ifdevzero oftemp done  5086 dd count512 bs1m ifdevzero oftemp   ok  memorypressuremesostestcgroups_root_slaverecovery 1096 ms code codetitleoutput from failed test  1 test from memorypressuremesostest 10 records in 10 records out 1048576 bytes 10 mb copied 0000404489 s 26 gbs  run  memorypressuremesostestcgroups_root_slaverecovery i1202 110915509950 5109 execcpp134 version 0270 i1202 110915568183 5123 execcpp208 executor registered on slave 88734acc718e45b095b9d8f07cea8a9es0 registered executor on ubuntu starting task 14b6bab99f604130bdc444efba262bc6 forked command at 5132 sh c while true do dd count512 bs1m ifdevzero oftemp done i1202 110915665498 5129 execcpp254 received reconnect request from slave 88734acc718e45b095b9d8f07cea8a9es0 i1202 110915670995 5123 execcpp381 executor asked to shutdown shutting down sending sigterm to process tree at pid 5132 srctestscontainerizermemory_pressure_testscpp283 failure usagefailure unknown container ebe90e1572fa4519837b62f43052c913  aborted at 1449083355 unix time try date d 1449083355 if you are using gnu date  code notice that in the failed test the executor is asked to shutdown when it tries to reconnect to the agent,1
memorypressuremesostest tests fail on centos 66 memorypressuremesostestcgroups_root_statistics and memorypressuremesostestcgroups_root_slaverecovery fail on centos 66 it seems that mounted cgroups are not properly cleaned up after previous tests so multiple hierarchies are detected and thus an error is produced code  run  memorypressuremesostestcgroups_root_statistics srctestsmesoscpp849 failure value of _basehierarchyget actual cgroup expected basehierarchy which is tmpmesos_test_cgroup  multiple cgroups base hierarchies detected tmpmesos_test_cgroup cgroup mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems  srctestsmesoscpp932 failure cgroupsdestroyhierarchy cgroupfailure failed to remove cgroup tmpmesos_test_cgroupperf_eventmesos_test device or resource busy  failed  memorypressuremesostestcgroups_root_statistics 12 ms  run  memorypressuremesostestcgroups_root_slaverecovery srctestsmesoscpp849 failure value of _basehierarchyget actual cgroup expected basehierarchy which is tmpmesos_test_cgroup  multiple cgroups base hierarchies detected tmpmesos_test_cgroup cgroup mesos does not support multiple cgroups base hierarchies please unmount the corresponding or all subsystems  srctestsmesoscpp932 failure cgroupsdestroyhierarchy cgroupfailure failed to remove cgroup tmpmesos_test_cgroupperf_eventmesos_test device or resource busy  failed  memorypressuremesostestcgroups_root_slaverecovery 7 ms code,3
respond with methodnotallowed if a request uses an unsupported method we are inconsistent right now in how we respond to endpoint requests with unsupported methods both methodnotallowed and badrequest are used we are also not consistent in the error message we include in the body this ticket proposes use methodnotallowed with standardized message text,1
do not use resourcerole for resources in quota request to be consistent with other operator endpoints and to adhere to the principal of least surprise move role from each resource in quota set request to the request itself resourcerole is used for reserved resources since quota is not a direct reservation request to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved food for thought shall we try to keep internal storage protobufs as close as possible to operators json to provide some sort of a schema or decouple those two for the sake of flexibility,1
investigate remaining flakiness in mastermaintenancetestinverseoffersfilters per comments in mesos3916 the fix for that issue decreased the degree of flakiness but it seems that some intermittent test failures do occur  should be investigated flakiness in task acknowledgment code i1203 182504609817 28732 status_update_managercpp392 received status update acknowledgement uuid 6afd012e8e8841b28239a9b852d07ca1 for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000 w1203 182504610076 28732 status_update_managercpp762 unexpected status update acknowledgement received 6afd012e8e8841b28239a9b852d07ca1 expecting 82fc7a7be64a4f4dab7476abac42b4e6 for update task_running uuid 82fc7a7be64a4f4dab7476abac42b4e6 for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000 e1203 182504610339 28736 slavecpp2339 failed to handle status update acknowledgement uuid 6afd012e8e8841b28239a9b852d07ca1 for task 26305fddedb047648b8a2558f2b2d81b of framework c7900911cc7a4dde92e748fe82cddd9e0000 duplicate acknowledgemen code this is a race between launching and acknowledging two taskshttpsgithubcomapachemesosblob75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5srctestsmaster_maintenance_testscppl1486l1517 the status updates for each task are not necessarily received in the same order as launching the tasks flakiness in first inverse offer filter see this comment in mesos3916httpsissuesapacheorgjirabrowsemesos3916focusedcommentid15027478pagecomatlassianjirapluginsystemissuetabpanelscommenttabpanelcomment15027478 for the explanation the related logs are above the comment,1
add containerinfo to internal task protobuf in what seems like an oversight when containerinfo was added to taskinfo it was not added to our internal task protobuf also unlike the agent it appears that the master does not use protobufcreatetask we should try remove the manual construction in the master in favor of construction through protobufcreatetask partial contents of containerinfo should be exposed through state endpoints on the master and the agent,3
agent should not return partial state when a request is made to state endpoint during recovery currently when a user is hitting statejson on the agent it may return partial state if the agent has failed over and is recovering there is currently no clear way to tell if this is the case when looking at a response so the user may incorrectly interpret the agent as being empty of tasks we could consider exposing the state enum of the agent in the endpoint code enum state  recovering  slave is doing recovery disconnected  slave is not connected to the master running  slave has reregistered terminating  slave is shutting down  state code this may be a bit tricky to maintain as far as backwardscompatibility of the endpoint if we were to alter this enum exposing this would allow users to be more informed about the state of the agent,3
reservationtestaclmultipleoperations is flaky observed from the ci httpsbuildsapacheorgjobmesoscompilergccconfigurationverbose20enablelibevent20enablesslosubuntu3a1404label_expdocker7c7chadoop1319changes,2
libevent_ssl_socket assertion fails have been seeing the following socket receive error frequently code f1204 111247301839 54104 libevent_ssl_socketcpp245 check failed length  0  check failure stack trace   0x7f73227fe5a6 googlelogmessagefail  0x7f73227fe4f2 googlelogmessagesendtolog  0x7f73227fdef4 googlelogmessageflush  0x7f7322800e08 googlelogmessagefatallogmessagefatal  0x7f73227b93e2 processnetworklibeventsslsocketimplrecv_callback  0x7f73227b9182 processnetworklibeventsslsocketimplrecv_callback  0x7f731cbc75cc bufferevent_run_deferred_callbacks_locked  0x7f731cbbdc5d event_base_loop  0x7f73227d9ded processeventlooprun  0x7f73227a3101 _znst12_bind_simpleifpfvvevee9_m_invokeijeeevst12_index_tupleijxspt_eee  0x7f73227a305b std_bind_simpleoperator  0x7f73227a2ff4 stdthread_impl_m_run  0x7f731e0d1a40 unknown  0x7f731de0a182 start_thread  0x7f731db3730d unknown  nil unknown code in this case this was a http get over ssl the url being httpsdseasb33srnrncloudfrontnet443registryv2dockerregistryv2blobssha2564444be94a95984bb47dc3a193f59bf8c04d5e877160b745b119278f38753a6f58fdataexpires1449259252signatureq4cqdr1lbxsiyyvebmetrxlqdgqfhvkgxpbmm3poisn6r07dxizbx6tl1izx9uxdfr5awh8kxwhy8b0dtv3mltzavlnezlhbhbax9qbymd180qvuvrfezwolsmx4b3idvozk0caruu3ev1hbjz5y3olwe2zcrxhewzkq_keypairidapkajech5m7vwis5yz6q steps to reproduce 1 run master 2 run slave from your build directory as as code glog_v1ssl_enabled1ssl_key_filepath_to_keyssl_cert_filepath_to_certsudo e binmesosslavesh  master1270015050  executor_registration_timeout5mins  containerizersmesos  isolationfilesystemlinux  image_providersdocker  docker_puller_timeout600  launcher_dirmesos_build_dirsrclibs  switch_userfalse  docker_pullerregistry code 3 run mesosexecute from your build directory as  code srcmesosexecute  master1270015050  commanduname a  nametest  docker_imageubuntu code,8
expose recovery parameters from hierarchical allocator while implementing recovery in the hierarchical allocator we introduced some internal constants that influence the recovery process allocation_hold_off_recovery_timeout and agent_recovery_factor we should expose these parameters for operators to configure however i am a bit reluctant to expose them as master flags because they are implementation specific it would be nice to combine all hierarchical allocatorrelated flags into one maybe json file similar to how we do it for modules,3
tests for master failover in presence of quota,5
continue test suite execution across crashing tests currently mesostestssh exits when a test crashes this is inconvenient when trying to find out all tests that fail mesostestssh should rate a test that crashes as failed and continue the same way as if the test merely returned with a failure result and exited properly,8
add tests for quota authentication and authorization,3
implement implicit roles see also design doc mesos4000,5
introduce a module for logging executortask output existing executortask logs are logged to files in their sandbox directory with some nuances based on which containerizer is used see background section in linked document a logger for executortask logs has the following requirements  the logger is given a command to run and must handle the stdoutstderr of the command  the handling of stdoutstderr must be resilient across agent failover logging should not stop if the agent fails  logs should be readable presumably via the web ui or via some other modulespecific ui,5
modularize existing plainfile logging for executortask logs launched with the mesos containerizer once a module for executortask output logging has been introduced the default module will mirror the existing behavior executortask stdoutstderr is piped into files within the executors sandbox directory the files are exposed in the web ui via the files endpoint,2
create lightweight executor only and scheduler only mesos eggs currently when running tasks in docker containers if the executor uses the mesosnative python library the execution environment inside the container os native libs etc must match the execution environment outside the container fairly closely in order to load the mesosso library the solution here can be to introduce a much lighter weight python egg mesosexecutor which only includes code and dependencies needed to create and run an mesosexecutordriver executors can then use this native library instead of mesosnative,5
allow interactive terminal for mesos containerizer today mesos containerizer does not have a way to run tasks that require interactive sessions an example use case is running a task that requires a manual password entry from an operator another use case could be debugging gdb,10
parallel make tests does not build all test targets when inside 3rdpartylibprocess running make j8 tests from a clean build does not yield the libprocesstests binary running it a subsequent time triggers more compilation and ends up yielding the libprocesstests binary this suggests the test target is not being built correctly,1
quota doesnt allocate resources on slave joining see attached patch framework1 is not allocated any resources despite the fact that the resources on agent2 can safely be allocated to it without risk of violating quota1 if i understand the intended quota behavior correctly this doesnt seem intended note that if the framework is added _after_ the slaves are added the resources on agent2 are allocated to framework1,5
design document for interactive terminal for mesos containerizer as a first step to address the use cases propose a design document covering the requirement design and implementation details,4
osstrerror_r breaks the windows build osstrerror_r does not exist on windows,1
implement osmkdtemp for windows used basically exclusively for testing this insecure and otherwisenotquitesuitableforprod function needs to work to run what will eventually become the fs tests,5
httpconnectiontestclosingresponse is flaky output of the test code  run  httpconnectiontestclosingresponse i1210 012027048532 26671 processcpp3077 handling http event for process 22 with path 22get 3rdpartylibprocesssrctestshttp_testscpp919 failure actual function call count doesnt match expect_callhttpprocess get_ expected to be called twice actual called once  unsatisfied and active  failed  httpconnectiontestclosingresponse 43 ms code,1
implement windowserror to correspond with errnoerror in the c standard library errno records the last error on a thread you can prettyprint it with strerror in stout we report these errors with errnoerror the windows api has something similar called getlasterror the way to prettyprint this is hilariously unintuitive and terrible so in this case it is actually very beneficial to wrap it with something similar to errnoerror maybe called windowserror,5
clean up libprocess gtest macros this ticket is regarding the libprocess gtest helpers in 3rdpartylibprocessincludeprocessgtesthpp the pattern in this file seems to be a set of macros  await_assert_state_for  await_assert_state  default of 15 seconds  await_state_for  alias for await_assert_state_for  await_state  alias for await_assert_state  await_expect_state_for  await_expect_state  default of 15 seconds 1 await_eq_for should be added for completeness 2 in gtest weve got expect_eq as well as the boolspecific versions expect_true and expect_false we should adopt this pattern in these helpers as well keeping the pattern above in mind the following are missing  await_assert_true_for  await_assert_true  await_assert_false_for  await_assert_false  await_expect_true_for  await_expect_false_for 3 there are http response related macros at the bottom of the file eg await_expect_response_status_eq however these are missing their assert counterparts 4 the reason for 3 presumably is because we reach for expect over assert in general due to the test suite crashing behavior of assert if this is the case it would be worthwhile considering whether macros such as await_ready should alias await_expect_ready rather than await_assert_ready 5 there are a few more missing macros given await_eq_for and await_eq which aliases to await_assert_eq_for and await_assert_eq respectively we should also add await_true_for await_true await_false_for and await_false as well,2
add field vip to message port we would like to extend the mesos protocol buffer port to include an optional repeated string named vip  to map it to a well known virtual ip or virtual hostname for discovery purposes we also want this field exposed in discoveryinfo in statejson,2
fix possible race conditions in registry client tests registryclient tests show flakiness which manifests as socket timeouts or unexpected buffer showing up in the blobs investigate them for possible race conditions,5
add tests for quotas  empty roles no registered frameworks,2
construct the error string in methodnotallowed consider constructing the error string in methodnotallowed rather than at the invocation site currently we want all error messages follow the same pattern so instead of writing code return methodnotallowedpost expecting post received   requestmethod   code we can write something like code methodnotallowedpost requestmethod code,1
ensure contenttype field is set for some responses as pointed out by anandmazumdar in httpsreviewsapacheorgr40905 we should make sure we set the contenttype files for some responses,3
refactor sorter factories in allocator and improve comments around them for clarity we want to refactor the factory section in the allocator and explain the purpose and necessity of all sorters,3
document how the fetcher can reach across a proxy connection the fetcher uses libcurl for downloading content from http https etc there is no source code in the pertinent parts of nethpp that deals with proxy settings however libcurl automatically picks up certain environment variables and adjusts its settings accordingly see man libcurltutorial for details see section proxies subsection environment variables if you follow this recipe in your mesos agent startup script you can use a proxy we should document this in the fetcher cache doc httpmesosapacheorgdocumentationlatestfetcher,1
add a containerlogger module that restrains log sizes one of the major problems this logger module aims to solve is overflowing executortask log files log files are simply written to disk and are not managed other than via occasional garbage collection by the agent process and this only deals with terminated executors we should add a containerlogger module that truncates logs as it reaches a configurable maximum size additionally we should determine if the web uis pailer needs to be changed to deal with logs that are not appendonly this will be a nondefault module which will also serve as an example for how to implement the module,3
modularize plainfile logging for executortask logs launched with the docker containerizer adding a hook inside the docker containerizer is slightly more involved than the mesos containerizer docker executorstasks perform plainfile logging in different places depending on whether the agent is in a docker container itself  agent  code   not in container  dockercontainerizerprocesslaunchexecutorprocess   in container  dockerrun in a mesosdockerexecutor process  this means a containerlogger will need to be loaded or hooked into the mesosdockerexecutor or we will need to change how piping in done in mesosdockerexecutor,3
reserveunreserve dynamic reservation endpoints allow reservations on nonexisting roles when working with dynamic reservations via the reserve and unreserve endpoints it is possible to reserve resources for roles that have not been specified via the roles flag on the master however these roles are not usable because the roles have not been defined nor are they added to the list of roles available per the mailing list changing roles after the fact is not possible at this time that may be another jira more importantly the reserve and unreserve end points should not allow reservation of roles not specified by roles,2
clean up authentication implementation for quota to authenticate quota requests we allowed quotahandler to call private httpauthenticate function once mesos3231 lands we do not need neither this injection nor authenticate calls in the quotahandler,1
implement container logger module metadata recovery the containerloggers are intended to be isolated from agent failover in the same way that executors do not crash when the agent process crashes for default containerlogger s like the sandboxcontainerlogger and the tentatively named truncatingsandboxcontainerlogger the log files are exposed during agent recovery regardless for nondefault containerlogger s the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs this can be implemented as part of containerizerrecover,3
rename shutdown_frameworks to teardown_framework the mesos is now using teardown framework to shutdown a framework but the acls are still using shutdown_framework it is better to rename shutdown_framework to teardown_framework for acl to keep consistent this is a post review request for httpsreviewsapacheorgr40829,2
log recover tests are slow on mac os 10104 some tests take longer than 1s to finish code recovertestautoinitialization 1003 ms recovertestautoinitializationretry 1000 ms code,1
mastertestrecoverresources is slow the mastertestrecoverresources test takes more than 1s to finish on my mac os 10104 code mastertestrecoverresources 1018 ms code,1
mastertestmasterinfoonreelection is slow the mastertestmasterinfoonreelection test takes more than 1s to finish on my mac os 10104 code mastertestmasterinfoonreelection 1024 ms code,1
mastertestlaunchcombinedoffertest is slow the mastertestlaunchcombinedoffertest test takes more than 2s to finish on my mac os 10104 code mastertestlaunchcombinedoffertest 2023 ms code,1
mastertestoffertimeout is slow the mastertestoffertimeout test takes more than 1s to finish on my mac os 10104 code mastertestoffertimeout 1053 ms code,1
oversubscriptiontestupdateallocatoronschedulerfailover is slow the oversubscriptiontestupdateallocatoronschedulerfailover test takes more than 1s to finish on my mac os 10104 code oversubscriptiontestupdateallocatoronschedulerfailover 1018 ms code,1
oversubscriptiontestremovecapabilitiesonschedulerfailover is slow the oversubscriptiontestremovecapabilitiesonschedulerfailover test takes more than 1s to finish on my mac os 10104 code oversubscriptiontestremovecapabilitiesonschedulerfailover 1018 ms code,1
garbagecollectorintegrationtestrestart is slow the garbagecollectorintegrationtestrestart test takes more than 5s to finish on my mac os 10104 code garbagecollectorintegrationtestrestart 5102 ms code,3
hooktestverifyslavelaunchexecutorhook is slow the hooktestverifyslavelaunchexecutorhook test takes more than 5s to finish on my mac os 10104 code hooktestverifyslavelaunchexecutorhook 5061 ms code,1
contenttypeschedulertestdecline is slow the contenttypeschedulertestdecline test takes more than 1s to finish on my mac os 10104 code contenttypeschedulertestdecline0 1022 ms code,1
create a user doc for executor http api we need a user doc similar to the corresponding one for the scheduler http api,3
add persistent volume support to the authorizer this ticket is the first in a series that adds authorization support for persistent volume creation and destruction persistent volumes should be authorized with the principal of the reserving entity framework or master the idea is to introduce create and destroy into the acl code message create   subjects required entity principals  1  objects perhaps the kind of volume allowed permissions  message destroy   subjects required entity principals  1  objects required entity creator_principals  2  code acls for volume creation and destruction must be added to authorizerproto and the appropriate function overloads must be added to the authorizer,1
extend master to authorize persistent volumes this ticket is the second in a series that adds authorization support for persistent volumes methods masterauthorizecreatevolume and masterauthorizedestroyvolume must be added to allow the master to authorize these operations,1
move operator definitions to cpp files and include iosfwd in hpp where possible we often include complex headers like ostream in hpp files to define operator inline eg mesosauthorizerauthorizerhpp instead we can move definitions to corresponding cpp files and replace stream headers with iosfwd for example this is partially done for uri in mesosuriurihpp,3
jenkins builds for centos fail with missing which utility and incorrect javahome jenkins builds are now consistently failing for centos 7 withe the failure checking value of java system property javahome usrlibjvmjava180openjdk180653b17el7x86_64jre configure error could not guess java_home they also fail early on during bootstrap with a missing which command the solution is to update supportdocker_buildsh to install which as well as make sure the proper versions of java are installed during the installation process the problem here is that we install maven before installing java170openjdkdevel causing maven to pull in a dependency on java180openjdk this causes problems with finding the proper javahome in our mesosconfigure script because of the mismatch between the most up to date jre 180 and the most up to date development tools 170 we can either update the script to pull in the 18 devel tools or move our dependence on maven until after our installation of java170openjdkdevel unclear what the best solution is,3
serialize docker v1 image spec as protobuf currently we only support v2 docker manifest serialization method when we read docker image spec locally from disk we should be able to parse v1 docker manifest as protobuf which will make it easier to gather runtime config and other necessary info,2
avoid using absolute urls in documentation pages links from one documentation page to another should not use absolute urls eg httpmesosapacheorgdocumentationlatest for several good reasons for instance absolute urls break when the docs are generatedpreviewed locally,1
create a design doc for dynamic weights a short design doc for dynamic weights it will focus on weights api and the changes to the allocator api,3
design doc for fixed point resources,5
add documentation for api versioning currently we dont have any documentation for  how mesos implements api versioning   how are protobufs versioned and how does mesos handle them internally   what do contributors need to do when they make a change to a external user facing protobuf  the relevant design doc httpsdocsgooglecomdocumentd1iqjo6778h_fu_1zi_yk6szg8qjwqygvgnx7u3h6oueditheadingh2gkbjz6amn7b,3
port processfilehpp,3
mesoscontainerizer tests leak fds pipes if you run binmesostestssh gtest_filtermesoscontainerizer gtest_repeat1 gtest_break_on_failure and then check lsof  grep mesos the number of open pipes will grow linearly with the number of test repetitions,2
add dynamic reservation tests with no principal currently there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal this should be added in order to more comprehensively test the dynamic reservation code,1
enable running tests without authorizer we do not support creating master instance without an authorizer in tests httpsgithubcomapachemesosblobaa497e81c945677c570484a8aa1a8c8b2e979dfdsrctestsclustercppl217 this leads to a segfault when masterflagsacls  none is used in a test while its a valid use case and should be allowed alternatively we use masterflagsacls  acls which triggers creation of localauthorizer with emtpy acls which seems to be semantically equal to the absence of an authorizer given permissive flag is true this equivalence should be verified by a test,3
disk resource reservation is not enforced for persistent volumes if i create a persistent volume on a reserved disk resource i am able to write data in excess of my reserved size disk resource reservation should be enforced just as cpus and mem reservations are enforced,3
test cases for weights  allocation behavior as far as i can see we currently have no test cases for behavior when weights are defined,2
race in ssl socket shutdown libprocess socket shares the ownership of the file descriptor with libevent in the destructor of the libprocess libevent_ssl socket we call ssl shutdown which is executed asynchronously this causes the libprocess socket file descriptor tobe closed and possibly reused when the same file descriptor could be used bylibeventssl since we set the shutdown options as ssl_received_shutdown we leave the any write operations to continue with possibly closed file descriptor this issue manifests as junk characters written to the file that has been handled the closed socket file descriptor by os that has the above issue,5
document that frameworks that participate in a role should cooperate,2
write new logrelated documentation this should include  default logging behavior for master agent framework executor task  masteragent  a summary of logrelated flags  glog specific options  separation of masteragent logs from container logs  the containerlogger module,3
add an example bug due to a lack of defer to the defer documentation in the past some bugs have been introduced into the codebase due to a lack of defer where it should have been used it would be useful to add an example of this to the defer documentation,2
persistentvolumetestbadacldropcreateanddestroy is flaky noformat  run  persistentvolumetestbadacldropcreateanddestroy i1219 095132623245 31878 leveldbcpp174 opened db in 4393596ms i1219 095132624084 31878 leveldbcpp181 compacted db in 709447ns i1219 095132624186 31878 leveldbcpp196 created db iterator in 21252ns i1219 095132624290 31878 leveldbcpp202 seeked to beginning of db in 11391ns i1219 095132624378 31878 leveldbcpp271 iterated through 0 keys in the db in 611ns i1219 095132624505 31878 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1219 095132625195 31904 recovercpp447 starting replica recovery i1219 095132625641 31904 recovercpp473 replica is in empty status i1219 095132627305 31904 replicacpp673 replica in empty status received a broadcasted recover request from 6740172170336408 i1219 095132627749 31904 recovercpp193 received a recover response from a replica in empty status i1219 095132628330 31904 recovercpp564 updating replica status to starting i1219 095132629068 31906 leveldbcpp304 persisting metadata 8 bytes to leveldb took 410494ns i1219 095132629169 31906 replicacpp320 persisted replica status to starting i1219 095132629598 31906 recovercpp473 replica is in starting status i1219 095132630782 31912 replicacpp673 replica in starting status received a broadcasted recover request from 6741172170336408 i1219 095132631166 31901 recovercpp193 received a recover response from a replica in starting status i1219 095132632467 31902 recovercpp564 updating replica status to voting i1219 095132633600 31907 leveldbcpp304 persisting metadata 8 bytes to leveldb took 311370ns i1219 095132633627 31907 replicacpp320 persisted replica status to voting i1219 095132633719 31907 recovercpp578 successfully joined the paxos group i1219 095132633874 31907 recovercpp462 recover process terminated i1219 095132636409 31909 mastercpp365 master bded856d1c7f4fada8bc3629ba8c59d3 60ab6e727501 started on 172170336408 i1219 095132636593 31909 mastercpp367 flags at startup aclscreate_volumes  principals  values creatorprincipal  volume_types  type any   create_volumes  principals  type any  volume_types  type none    allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpsppf7bcredentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue rolesrole1 root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0270_instsharemesoswebui work_dirtmpsppf7bmaster zk_session_timeout10secs i1219 095132637055 31909 mastercpp414 master allowing unauthenticated frameworks to register i1219 095132637068 31909 mastercpp417 master only allowing authenticated slaves to register i1219 095132637094 31909 credentialshpp35 loading credentials for authentication from tmpsppf7bcredentials i1219 095132637403 31909 mastercpp456 using default crammd5 authenticator i1219 095132637555 31909 mastercpp493 authorization enabled w1219 095132637575 31909 mastercpp553 the roles flag is deprecated this flag will be removed in the future see the mesos 027 upgrade notes for more information i1219 095132637806 31897 whitelist_watchercpp77 no whitelist given i1219 095132637820 31910 hierarchicalcpp147 initialized hierarchical allocator process i1219 095132639677 31909 mastercpp1629 the newly elected leader is master172170336408 with id bded856d1c7f4fada8bc3629ba8c59d3 i1219 095132639768 31909 mastercpp1642 elected as the leading master i1219 095132639892 31909 mastercpp1387 recovering from registrar i1219 095132640136 31907 registrarcpp307 recovering registrar i1219 095132640929 31901 logcpp659 attempting to start the writer i1219 095132642199 31912 replicacpp493 replica received implicit promise request from 6742172170336408 with proposal 1 i1219 095132642719 31912 leveldbcpp304 persisting metadata 8 bytes to leveldb took 445876ns i1219 095132642755 31912 replicacpp342 persisted promised to 1 i1219 095132643478 31904 coordinatorcpp238 coordinator attempting to fill missing positions i1219 095132645009 31909 replicacpp388 replica received explicit promise request from 6743172170336408 for position 0 with proposal 2 i1219 095132645356 31909 leveldbcpp341 persisting action 8 bytes to leveldb took 310064ns i1219 095132645382 31909 replicacpp712 persisted action at 0 i1219 095132646662 31909 replicacpp537 replica received write request for position 0 from 6744172170336408 i1219 095132646721 31909 leveldbcpp436 reading position from leveldb took 29298ns i1219 095132647047 31909 leveldbcpp341 persisting action 14 bytes to leveldb took 283424ns i1219 095132647073 31909 replicacpp712 persisted action at 0 i1219 095132647722 31909 replicacpp691 replica received learned notice for position 0 from 00000 i1219 095132648052 31909 leveldbcpp341 persisting action 16 bytes to leveldb took 300825ns i1219 095132648077 31909 replicacpp712 persisted action at 0 i1219 095132648095 31909 replicacpp697 replica learned nop action at position 0 i1219 095132655295 31899 logcpp675 writer started with ending position 0 i1219 095132656543 31905 leveldbcpp436 reading position from leveldb took 32788ns i1219 095132658164 31905 registrarcpp340 successfully fetched the registry 0b in 0ns i1219 095132658604 31905 registrarcpp439 applied 1 operations in 38183ns attempting to update the registry i1219 095132660102 31905 logcpp683 attempting to append 170 bytes to the log i1219 095132660538 31906 coordinatorcpp348 coordinator attempting to write append action at position 1 i1219 095132661872 31906 replicacpp537 replica received write request for position 1 from 6745172170336408 i1219 095132662719 31906 leveldbcpp341 persisting action 189 bytes to leveldb took 483018ns i1219 095132663054 31906 replicacpp712 persisted action at 1 i1219 095132664008 31902 replicacpp691 replica received learned notice for position 1 from 00000 i1219 095132664330 31902 leveldbcpp341 persisting action 191 bytes to leveldb took 287310ns i1219 095132664355 31902 replicacpp712 persisted action at 1 i1219 095132664376 31902 replicacpp697 replica learned append action at position 1 i1219 095132665365 31902 registrarcpp484 successfully updated the registry in 0ns i1219 095132665493 31902 registrarcpp370 successfully recovered registrar i1219 095132665894 31902 mastercpp1439 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i1219 095132665990 31902 hierarchicalcpp165 skipping recovery of hierarchical allocator nothing to recover i1219 095132666266 31902 logcpp702 attempting to truncate the log to 1 i1219 095132666424 31902 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i1219 095132667181 31907 replicacpp537 replica received write request for position 2 from 6746172170336408 i1219 095132667768 31907 leveldbcpp341 persisting action 16 bytes to leveldb took 335947ns i1219 095132668067 31907 replicacpp712 persisted action at 2 i1219 095132668942 31906 replicacpp691 replica received learned notice for position 2 from 00000 i1219 095132669240 31906 leveldbcpp341 persisting action 18 bytes to leveldb took 266566ns i1219 095132669292 31906 leveldbcpp399 deleting 1 keys from leveldb took 27852ns i1219 095132669314 31906 replicacpp712 persisted action at 2 i1219 095132669334 31906 replicacpp697 replica learned truncate action at position 2 i1219 095132691251 31878 containerizercpp141 using isolation posixcpuposixmemfilesystemposix w1219 095132691759 31878 backendcpp48 failed to create bind backend bindbackend requires root privileges i1219 095132697428 31901 slavecpp191 slave started on 228172170336408 i1219 095132697459 31901 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmppersistentvolumetest_badacldropcreateanddestroy_gwltnccredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmppersistentvolumetest_badacldropcreateanddestroy_gwltncfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0270_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024diskrole12048 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmppersistentvolumetest_badacldropcreateanddestroy_gwltnc i1219 095132697963 31901 credentialshpp83 loading credential for authentication from tmppersistentvolumetest_badacldropcreateanddestroy_gwltnccredential i1219 095132698210 31901 slavecpp322 slave using credential for testprincipal i1219 095132698449 31901 resourcescpp478 parsing resources as json failed cpus2mem1024diskrole12048 trying semicolondelimited string format instead i1219 095132699065 31901 slavecpp392 slave resources cpus2 mem1024 diskrole12048 ports3100032000 i1219 095132699137 31901 slavecpp400 slave attributes   i1219 095132699151 31901 slavecpp405 slave hostname 60ab6e727501 i1219 095132699161 31901 slavecpp410 slave checkpoint true i1219 095132699364 31878 schedcpp164 version 0270 i1219 095132700614 31911 schedcpp262 new master detected at master172170336408 i1219 095132700703 31911 schedcpp272 no credentials provided attempting to register without authentication i1219 095132700724 31911 schedcpp714 sending subscribe call to master172170336408 i1219 095132700839 31911 schedcpp747 will retry registration in 620399428ms if necessary i1219 095132701244 31903 mastercpp2197 received subscribe call for framework default at scheduler0333dddc4b4140ed8853a1aadf1f1879172170336408 i1219 095132701313 31903 mastercpp1668 authorizing framework principal testprincipal to receive offers for role role1 i1219 095132701625 31903 mastercpp2268 subscribing framework default with checkpointing disabled and capabilities   i1219 095132702308 31903 hierarchicalcpp260 added framework bded856d1c7f4fada8bc3629ba8c59d30000 i1219 095132702386 31903 hierarchicalcpp1329 no resources available to allocate i1219 095132702422 31903 hierarchicalcpp1423 no inverse offers to send out i1219 095132702448 31903 hierarchicalcpp1079 performed allocation for 0 slaves in 114358ns i1219 095132702638 31903 schedcpp641 framework registered with bded856d1c7f4fada8bc3629ba8c59d30000 i1219 095132702688 31903 schedcpp655 schedulerregistered took 25558ns i1219 095132703553 31901 statecpp58 recovering state from tmppersistentvolumetest_badacldropcreateanddestroy_gwltncmeta i1219 095132704118 31897 status_update_managercpp200 recovering status update manager i1219 095132704407 31907 containerizercpp383 recovering containerizer i1219 095132705373 31907 slavecpp4427 finished recovery i1219 095132705991 31907 slavecpp4599 querying resource estimator for oversubscribable resources i1219 095132706277 31907 slavecpp4613 received oversubscribable resources from the resource estimator i1219 095132706666 31907 slavecpp729 new master detected at master172170336408 i1219 095132706738 31907 slavecpp792 authenticating with master master172170336408 i1219 095132706760 31907 slavecpp797 using default crammd5 authenticatee i1219 095132706886 31899 status_update_managercpp174 pausing sending status updates i1219 095132706941 31907 slavecpp765 detecting new master i1219 095132707036 31899 authenticateecpp121 creating new client sasl connection i1219 095132707291 31910 mastercpp5423 authenticating slave228172170336408 i1219 095132707479 31910 authenticatorcpp413 starting authentication session for crammd5_authenticatee510172170336408 i1219 095132707849 31910 authenticatorcpp98 creating new server sasl connection i1219 095132708082 31910 authenticateecpp212 received sasl authentication mechanisms crammd5 i1219 095132708112 31910 authenticateecpp238 attempting to authenticate with mechanism crammd5 i1219 095132708196 31910 authenticatorcpp203 received sasl authentication start i1219 095132708395 31910 authenticatorcpp325 authentication requires more steps i1219 095132708611 31902 authenticateecpp258 received sasl authentication step i1219 095132708773 31910 authenticatorcpp231 received sasl authentication step i1219 095132708889 31910 auxpropcpp107 request to lookup properties for user testprincipal realm 60ab6e727501 server fqdn 60ab6e727501 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i1219 095132708976 31910 auxpropcpp179 looking up auxiliary property userpassword i1219 095132709096 31910 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i1219 095132709200 31910 auxpropcpp107 request to lookup properties for user testprincipal realm 60ab6e727501 server fqdn 60ab6e727501 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i1219 095132709285 31910 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i1219 095132709363 31910 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i1219 095132709452 31910 authenticatorcpp317 authentication success i1219 095132709707 31910 authenticateecpp298 authentication success i1219 095132710252 31910 slavecpp860 successfully authenticated with master master172170336408 i1219 095132710525 31910 slavecpp1254 will retry registration in 1744437ms if necessary i1219 095132709839 31908 mastercpp5453 successfully authenticated principal testprincipal at slave228172170336408 i1219 095132710985 31908 mastercpp4132 registering slave at slave228172170336408 60ab6e727501 with id bded856d1c7f4fada8bc3629ba8c59d3s0 i1219 095132711645 31908 registrarcpp439 applied 1 operations in 83191ns attempting to update the registry i1219 095132709908 31912 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee510172170336408 i1219 095132713407 31908 logcpp683 attempting to append 343 bytes to the log i1219 095132713646 31912 coordinatorcpp348 coordinator attempting to write append action at position 3 i1219 095132714884 31911 replicacpp537 replica received write request for position 3 from 6758172170336408 i1219 095132715221 31911 leveldbcpp341 persisting action 362 bytes to leveldb took 288909ns i1219 095132715250 31911 replicacpp712 persisted action at 3 i1219 095132716145 31912 replicacpp691 replica received learned notice for position 3 from 00000 i1219 095132716689 31912 leveldbcpp341 persisting action 364 bytes to leveldb took 512217ns i1219 095132716716 31912 replicacpp712 persisted action at 3 i1219 095132716737 31912 replicacpp697 replica learned append action at position 3 i1219 095132718426 31911 registrarcpp484 successfully updated the registry in 0ns i1219 095132719441 31902 slavecpp3371 received ping from slaveobserver228172170336408 i1219 095132719843 31909 logcpp702 attempting to truncate the log to 3 i1219 095132719908 31911 mastercpp4200 registered slave bded856d1c7f4fada8bc3629ba8c59d3s0 at slave228172170336408 60ab6e727501 with cpus2 mem1024 diskrole12048 ports3100032000 i1219 095132720064 31911 slavecpp904 registered with master master172170336408 given slave id bded856d1c7f4fada8bc3629ba8c59d3s0 i1219 095132720088 31911 fetchercpp81 clearing fetcher cache i1219 095132720491 31911 slavecpp927 checkpointing slaveinfo to tmppersistentvolumetest_badacldropcreateanddestroy_gwltncmetaslavesbded856d1c7f4fada8bc3629ba8c59d3s0slaveinfo i1219 095132720844 31909 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i1219 095132720929 31911 slavecpp963 forwarding total oversubscribed resources i1219 095132721017 31903 status_update_managercpp181 resuming sending status updates i1219 095132721099 31911 mastercpp4542 received update of slave bded856d1c7f4fada8bc3629ba8c59d3s0 at slave228172170336408 60ab6e727501 with total oversubscribed resources i1219 095132721141 31905 hierarchicalcpp465 added slave bded856d1c7f4fada8bc3629ba8c59d3s0 60ab6e727501 with cpus2 mem1024 diskrole12048 ports3100032000 allocated  i1219 095132721879 31911 replicacpp537 replica received write request for position 4 from 6759172170336408 i1219 095132722293 31905 hierarchicalcpp1423 no inverse offers to send out i1219 095132722337 31905 hierarchicalcpp1101 performed allocation for slave bded856d1c7f4fada8bc3629ba8c59d3s0 in 1155563ms i1219 095132722681 31905 hierarchicalcpp521 slave bded856d1c7f4fada8bc3629ba8c59d3s0 60ab6e727501 updated with oversubscribed resources total cpus2 mem1024 diskrole12048 ports3100032000 allocated cpus2 mem1024 ports3100032000 diskrole12048 i1219 095132722713 31909 mastercpp5252 sending 1 offers to framework bded856d,1
document how to program with dynamic reservations and persistent volumes specifically some of the gotchas around  retrying reservation attempts after a timeout  fuzzymatching resources to determine whether a reservationpv is successful  represent client state as a state machine and repeatedly move toward successful terminate stats should also point to persistent volume example framework we should also ask gabriel and others arango who have built frameworks with pvsdrs for feedback,3
introduce http endpoint weights for updating weight,5
test for quota status endpoint,3
document containerizer from user perspective add documentation that covers  purpose of containerizers from a use case perspective  what purpose does each containerizer mesos docker compose serve  what criteria could be used to choose a containerizer,3
document isolators from user perspective the documentation should cover  purpose of isolators businessuser perspective  what is the criteria for choosingpicking any set of isolators,4
document isolator internals document isolators from developer perspective possibly covering  linux isolators  posix isolators  filesystem network isolators,4
exposed dockerappc image manifest to mesos containerizer collect docker image manifest from diskwhich contains all runtime configurations and pass it back to provisioner so that mesos containerizer can grab all necessary info from provisioner,2
enable passing docker image environment variables runtime config to provisioner collect environment variables runtime config information from a docker image and save as a map pass it back to provisioner and handling environment variables merge issue,1
enable passing docker image cmd runtime config to provisioner cmd is the command to run when starting a container we should be able to collect cmd config information from a docker image and pass it back to provisioner,1
pull provisioner from linux filesystem isolator to mesos containerizer the rationale behind this change is that many of the image specifications eg dockerappc are not just for filesystems they also specify runtime configurations eg environment variables volumes etc for the container provisioner should return those runtime configurations to the mesos containerizer and mesos containerizer will delegate the isolation of those runtime configurations to the relevant isolator here is what it will be look like eventually we could do those changes in phases 1 provisioner will return a provisioninfo which includes a rootfs and image specific runtime configurations could be the dockerappc manifest 2 then the mesos containerizer will generate a containerconfig a protobuf which includes rootfs sandbox dockerappc manifest similar to ocis host independent configjson and pass that to each isolator in prepare imaging in the future a dockerruntimeisolator takes the docker manifest from containerconfig and prepare the container 3 the isolators prepare function will return a containerlaunchinfo contains environment variables namespaces etc which will be used by mesos containerize to launch containers imaging that information will be passed to the launcher in the future we can do the renaming containerprepareinfo  containerlaunchinfo later,5
consolidate docker store slave flags currently there are too many slave flags for configuring the docker storepuller we can remove the following flags docker_auth_server_port docker_local_archives_dir docker_registry_port docker_puller and consolidate them into the existing flags,3
add mechanism for testing recovery of http based executors currently the slave process generates a process id every time it is initialized via processidgenerate function call this is a problem for testing http executors as it cant retry if there is a disconnection after an agent restart since the prefix is incremented code agent pid before slave112700143915 agent pid after restart slave212700143915 code there are a couple of ways to fix this  add a constructor to slave exclusively for testing that passes on a fixed id instead of relying on idgenerate  currently we delegate to slave1 ie 1 when nothing is specified as the url in libprocess ie 12700143915apiv1executor would delegate to slave112700143915apiv1executor instead of defaulting to 1 we can default to the last known active id,3
examplestestnoexecutorframework runs forever noformat titlegood run  run  examplestestnoexecutorframework i1221 231002721617 32528 execcpp444 ignoring exited event because the driver is aborted using temporary directory tmpexamplestest_noexecutorframework_fcmfln i1221 231002721675 32539 execcpp444 ignoring exited event because the driver is aborted i1221 231002722024 32554 execcpp444 ignoring exited event because the driver is aborted warning logging before initgooglelogging is written to stderr i1221 231005179466 32569 resourcescpp478 parsing resources as json failed cpus01mem32disk32 trying semicolondelimited string format instead i1221 231005180269 32569 loggingcpp172 logging to stderr i1221 231005185768 32569 processcpp998 libprocess is initialized on 172170240874 for 16 cpus i1221 231005200728 32569 leveldbcpp174 opened db in 4184362ms i1221 231005202234 32569 leveldbcpp181 compacted db in 1459268ms i1221 231005202353 32569 leveldbcpp196 created db iterator in 73761ns i1221 231005202383 32569 leveldbcpp202 seeked to beginning of db in 3382ns i1221 231005202405 32569 leveldbcpp271 iterated through 0 keys in the db in 633ns i1221 231005202674 32569 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i1221 231005205301 32604 recovercpp447 starting replica recovery i1221 231005206414 32569 localcpp239 using local authorizer i1221 231005206405 32604 recovercpp473 replica is in empty status i1221 231005209595 32594 replicacpp673 replica in empty status received a broadcasted recover request from 4172170240874 i1221 231005210916 32596 recovercpp193 received a recover response from a replica in empty status i1221 231005211515 32597 mastercpp365 master 3931c1a81cd649eb94c8d01b33bb008e 6ccf2ee56b13 started on 172170240874 i1221 231005211699 32605 recovercpp564 updating replica status to starting i1221 231005211539 32597 mastercpp367 flags at startup aclspermissive false register_frameworks  principals  type some values testprincipal  roles  type some values    run_tasks  principals  type some values testprincipal  users  type some values mesos    allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_slavesfalse authenticatorscrammd5 authorizerslocal credentialstmpexamplestest_noexecutorframework_fcmflncredentials framework_sorterdrf helptrue hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout5secs registry_strictfalse root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0270srcwebui work_dirtmpmesosotpdch zk_session_timeout10secs i1221 231005212323 32597 mastercpp412 master only allowing authenticated frameworks to register i1221 231005212337 32597 mastercpp419 master allowing unauthenticated slaves to register i1221 231005212347 32597 credentialshpp35 loading credentials for authentication from tmpexamplestest_noexecutorframework_fcmflncredentials w1221 231005212442 32597 credentialshpp50 permissions on credentials file tmpexamplestest_noexecutorframework_fcmflncredentials are too open it is recommended that your credentials file is not accessible by others i1221 231005212606 32600 leveldbcpp304 persisting metadata 8 bytes to leveldb took 656857ns i1221 231005212620 32597 mastercpp456 using default crammd5 authenticator i1221 231005212631 32600 replicacpp320 persisted replica status to starting i1221 231005212893 32597 authenticatorcpp518 initializing server sasl i1221 231005213091 32608 recovercpp473 replica is in starting status i1221 231005213958 32595 replicacpp673 replica in starting status received a broadcasted recover request from 5172170240874 i1221 231005214323 32594 recovercpp193 received a recover response from a replica in starting status i1221 231005214689 32595 recovercpp564 updating replica status to voting i1221 231005215353 32596 leveldbcpp304 persisting metadata 8 bytes to leveldb took 487419ns i1221 231005215384 32596 replicacpp320 persisted replica status to voting i1221 231005215481 32605 recovercpp578 successfully joined the paxos group i1221 231005215867 32605 recovercpp462 recover process terminated i1221 231005216111 32569 containerizercpp141 using isolation filesystemposixposixcpuposixmem w1221 231005217021 32569 backendcpp48 failed to create bind backend bindbackend requires root privileges i1221 231005221482 32608 slavecpp191 slave started on 1172170240874 i1221 231005221521 32608 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationfilesystemposixposixcpuposixmem launcherposix launcher_dirmesosmesos0270_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmesosotpdch0 i1221 231005222578 32608 resourcescpp478 parsing resources as json failed cpus2mem10240 trying semicolondelimited string format instead i1221 231005223465 32608 slavecpp392 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i1221 231005223621 32569 containerizercpp141 using isolation filesystemposixposixcpuposixmem i1221 231005223610 32608 slavecpp400 slave attributes   i1221 231005223677 32608 slavecpp405 slave hostname 6ccf2ee56b13 i1221 231005223697 32608 slavecpp410 slave checkpoint true w1221 231005224143 32569 backendcpp48 failed to create bind backend bindbackend requires root privileges i1221 231005226668 32604 slavecpp191 slave started on 2172170240874 i1221 231005226692 32604 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationfilesystemposixposixcpuposixmem launcherposix launcher_dirmesosmesos0270_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmesosotpdch1 i1221 231005227520 32604 resourcescpp478 parsing resources as json failed cpus2mem10240 trying semicolondelimited string format instead i1221 231005228037 32604 slavecpp392 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i1221 231005228148 32604 slavecpp400 slave attributes   i1221 231005228169 32604 slavecpp405 slave hostname 6ccf2ee56b13 i1221 231005228184 32604 slavecpp410 slave checkpoint true i1221 231005229123 32569 containerizercpp141 using isolation filesystemposixposixcpuposixmem i1221 231005229641 32605 statecpp58 recovering state from tmpmesosotpdch0meta w1221 231005229645 32569 backendcpp48 failed to create bind backend bindbackend requires root privileges i1221 231005229636 32595 statecpp58 recovering state from tmpmesosotpdch1meta i1221 231005230242 32605 status_update_managercpp200 recovering status update manager i1221 231005230254 32598 status_update_managercpp200 recovering status update manager i1221 231005230515 32601 containerizercpp383 recovering containerizer i1221 231005230562 32602 containerizercpp383 recovering containerizer i1221 231005232681 32597 auxpropcpp71 initialized inmemory auxiliary property plugin i1221 231005232803 32597 mastercpp493 authorization enabled i1221 231005232867 32600 slavecpp4427 finished recovery i1221 231005232980 32598 slavecpp191 slave started on 3172170240874 i1221 231005233039 32594 slavecpp4427 finished recovery i1221 231005233376 32599 whitelist_watchercpp77 no whitelist given i1221 231005233428 32601 hierarchicalcpp147 initialized hierarchical allocator process i1221 231005233003 32598 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos default_role disk_watch_interval1mins dockerdocker docker_auth_serverauthdockerio docker_auth_server_port443 docker_kill_orphanstrue docker_local_archives_dirtmpmesosimagesdocker docker_pullerlocal docker_puller_timeout60 docker_registryregistry1dockerio docker_registry_port443 docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmesosfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationfilesystemposixposixcpuposixmem launcherposix launcher_dirmesosmesos0270_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor1secs resourcescpus2mem10240 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmesosotpdch2 i1221 231005233744 32600 slavecpp4599 querying resource estimator for oversubscribable resources i1221 231005233749 32598 resourcescpp478 parsing resources as json failed cpus2mem10240 trying semicolondelimited string format instead i1221 231005234222 32598 slavecpp392 slave resources cpus2 mem10240 disk370122e06 ports3100032000 i1221 231005234284 32598 slavecpp400 slave attributes   i1221 231005234299 32598 slavecpp405 slave hostname 6ccf2ee56b13 i1221 231005234311 32598 slavecpp410 slave checkpoint true i1221 231005234338 32600 slavecpp729 new master detected at master172170240874 i1221 231005234376 32604 status_update_managercpp174 pausing sending status updates i1221 231005234424 32600 slavecpp754 no credentials provided attempting to register without authentication i1221 231005234522 32600 slavecpp765 detecting new master i1221 231005234616 32569 schedcpp164 version 0270 i1221 231005234658 32600 slavecpp4613 received oversubscribable resources from the resource estimator i1221 231005234671 32594 slavecpp4599 querying resource estimator for oversubscribable resources i1221 231005234884 32606 slavecpp4613 received oversubscribable resources from the resource estimator i1221 231005235038 32595 status_update_managercpp174 pausing sending status updates i1221 231005235043 32606 slavecpp729 new master detected at master172170240874 i1221 231005235111 32606 slavecpp754 no credentials provided attempting to register without authentication i1221 231005235147 32606 slavecpp765 detecting new master i1221 231005235240 32594 statecpp58 recovering state from tmpmesosotpdch2meta i1221 231005235443 32608 status_update_managercpp200 recovering status update manager i1221 231005235625 32594 containerizercpp383 recovering containerizer i1221 231005236549 32599 slavecpp4427 finished recovery i1221 231005236984 32593 schedcpp262 new master detected at master172170240874 i1221 231005237004 32599 slavecpp4599 querying resource estimator for oversubscribable resources i1221 231005237221 32593 schedcpp318 authenticating with master master172170240874 i1221 231005237277 32593 schedcpp325 using default crammd5 authenticatee i1221 231005237285 32604 status_update_managercpp174 pausing sending status updates i1221 231005237288 32599 slavecpp729 new master detected at master172170240874 i1221 231005237361 32599 slavecpp754 no credentials provided attempting to register without authentication i1221 231005237433 32599 slavecpp765 detecting new master i1221 231005237565 32599 slavecpp4613 received oversubscribable resources from the resource estimator i1221 231005238154 32605 authenticateecpp97 initializing client sasl i1221 231005238315 32605 authenticateecpp121 creating new client sasl connection i1221 231005239640 32597 mastercpp1200 dropping mesosinternalauthenticatemessage message since not elected yet i1221 231005239765 32597 mastercpp1629 the newly elected leader is master172170240874 with id 3931c1a81cd649eb94c8d01b33bb008e i1221 231005239794 32597 mastercpp1642 elected as the leading master i1221 231005239843 32597 mastercpp1387 recovering from registrar i1221 231005240056 32600 registrarcpp307 recovering registrar i1221 231005241477 32608 logcpp659 attempting to start the writer i1221 231005244540 32600 replicacpp493 replica received implicit promise request from 39172170240874 with proposal 1 i1221 231005245358 32600 leveldbcpp304 persisting metadata 8 bytes to leveldb took 776937ns i1221 231005245393 32600 replicacpp342 persisted promised to 1 i1221 231005246625 32601 coordinatorcpp238 coordinator attempting to fill missing positions i1221 231005248757 32605 replicacpp388 replica received explicit promise request from 40172170240874 for position 0 with proposal 2 i1221 231005249214 32605 leveldbcpp341 persisting action 8 bytes to leveldb took 366567ns i1221 231005249246 32605 replicacpp712 persisted action at 0 i1221 231005250998 32599 replicacpp537 replica received write request for position 0 from 41172170240874 i1221 231005251111 32599 leveldbcpp436 reading position from leveldb took 66773ns i1221 231005251734 32599 leveldbcpp341 persisting action 14 bytes to leveldb took 379612ns i1221 231005251759 32599 replicacpp712 persisted action at 0 i1221 231005252555 32601 replicacpp691 replica received learned notice for position 0 from 00000 i1221 231005253010 32601 leveldbcpp341 persisting action 16 bytes to leveldb took 381858ns i1221 231005253036 32601 replicacpp712 persisted action at 0 i1221 231005253068 32601 replicacpp697 replica learned nop action at position 0 i1221 231005254043 32595 logcpp675 writer started with ending position 0 i1221 231005256741 32595 leveldbcpp436 reading position from leveldb took 48607ns i1221 231005260617 32601 registrarcpp340 successfully fetched the registry 0b in 2047616ms i1221 231005260988 32601 registrarcpp439 applied 1 operations in 103123ns attempting to update the registry i1221 231005264700 32604 logcpp683 attempting to append 170 bytes to the log i1221 231005265138 32601 coordinatorcpp348 coordinator attempting to write append action at position 1 i1221 231005266208 32603 replicacpp537 replica received write request for position 1 from 42172170240874 i1221 231005266829 32603 leveldbcpp341 persisting action 189 bytes to leveldb took 551087ns i1221 231005266861 32603 replicacpp712 persisted action at 1 i1221 231005267918 32605 replicacpp691 replica received learned notice for position 1 from 00000 i1221 231005268442 32605 leveldbcpp341 persisting action 191 bytes to leveldb took 453416ns i1221 231005268470 32605 replicacpp712 persisted action at 1 i1221 231005268506 32605 replicacpp697 replica learned append action at position 1 i1221 231005270512 32606 registrarcpp484 successfully updated the registry in 9375232ms i1221 231005270705 32606 registrarcpp370 successfully recovered registrar i1221 231005271045 32602 logcpp702 attempting to truncate the log to 1 i1221 231005271178 32603 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i1221 231005271695 32605,3
remove docker auth server flag we currently use a configured docker auth server from a slave flag to get token auth for docker registry however this doesnt work for private registries as docker registry supports sending down the correct auth server to contact we should remove docker auth server flag completely and ask the docker registry for auth server,3
enable net_cls subsytem in cgroup infrastructure currently the control group infrastructure within mesos supports only the memory and cpu subsystems we need to enhance this infrastructure to support the net_cls subsystem as well details of the net_cls subsystem and its usecases can be found here httpswwwkernelorgdocdocumentationcgroupsnet_clstxt enabling the net_cls will allow us to provide operators to potentially regulate framework traffic on a percontainer basis,5
report volume usage through resourcestatistics posix disk isolator does not currently report volume usage through resourcestatistics posixdiskisolatorprocessusage should be amended to take into account volume usage as well,3
docker executor truncates tasks output when the task is killed im implementing a graceful restarts of our mesosmarathondocker setup and i came to a following issue it was already discussed on httpsgithubcommesospheremarathonissues2876 and guys form mesosphere got to a point that its probably a docker containerizer problem to sum it up when i deploy simple python script to all mesosslaves code usrbinpython from time import sleep import signal import sys import datetime def sigterm_handler_signo _stack_frame print got i  _signo print datetimedatetimenowtime sysstdoutflush sleep2 print datetimedatetimenowtime print ending sysstdoutflush sysexit0 signalsignalsignalsigterm sigterm_handler signalsignalsignalsigint sigterm_handler try print hello i  0 while true i  1 print datetimedatetimenowtime print iteration i  i sysstdoutflush sleep1 finally print goodbye code and i run it through marathon like codejavascript data   args tmpscriptpy instances 1 cpus 01 mem 256 id marathontestapi  code during the app restart i get expected result  the task receives sigterm and dies peacefully during my scriptspecified 2 seconds period but when i wrap this python script in a docker code from node42 run mkdir app add  app workdir app entrypoint  code and run appropriate application by marathon codejavascript data   args scriptpy container  type docker docker  image bydgamarathontestapi  forcepullimage yes  cpus 01 mem 256 instances 1 id marathontestapi  code the task during restart issued from marathon dies immediately without having a chance to do any cleanup,5
correctly handle disk quota usage when volumes are bind mounted into the container in its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox this happens when linux filesystem isolator is used,3
update isolator prepare function to use containerlaunchinfo currently we have the isolators prepare function returning containerprepareinfo protobuf we should enable containerlaunchinfo contains environment variables namespaces etc to be returned which will be used by mesos containerize to launch containers by doing this containerprepareinfo  containerlaunchinfo we can select any necessary information and passing then to launcher,2
draft design doc for multirole frameworks create a document that describes the problems with having only singlerole frameworks and proposes an mvp solution and implementation approach,8
mesos command task doesnt support volumes with image currently volumes are stripped when an image is specified running a command task with mesos containerizer,3
design doc for simple appc image discovery create a design document describing the following  model and abstraction of the discoverer  workflow of the discovery process,5
fsenterrootfs does not work if rootfs is read only i noticed this when i was testing the unified containerizer with the bind mount backend and no volumes the current implementation of fsenter will put the old root under tmp_old_root_xxxxxx in the new rootfs it assumes that tmp is writable in the new rootfs but this might not be true especially if the bind mount backend is used to solve the problem what we can do is to mount tmpfs to tmp in the new rootfs and umount it after pivot_root,2
tests for quota with implicit roles with the introduction of implicit roles mesos3988 we should make sure quota can be set for an inactive role unknown to the master and maybe transition it to the active state,3
protobuf parse should support parsing json object containing json null this bug was exposed by mesos4184 when serializing docker v1 image manifest as protobuf currently protobufparse returns failures when parsing any json containing jsonnull if we have any protobuf field set as jsonnull any other nonrepeated field cannot capture their value for example assuming we have a protobuf message noformat message nested  optional string str  1 repeated string json_null  2  noformat if there exists any field containing jsonnull like below noformat  str message json_null null  noformat when we do protobufparse it would return the following failure noformat failure parse not expecting a json null noformat,1
change documentation links to md right now links either use the form noformatlabeldocumentationlatestfoonoformat or noformatlabelfoomdnoformat we should probably switch to using the latter form consistently  it previews better on github and it will make it easier to have multiple versions of the docs on the website at once in the future,3
add docker uri fetcher plugin based on curl the existing registry client for docker assumes that mesos is built using ssl support and ssl is enabled that means mesos built with libev or if ssl is disabled wont be able to use docker registry client to provision docker images given the new uri fetcher mesos3918 work has been committed we can add a new uri fetcher plugin for docker the plugin will be based on curl so that https and 3xx redirects will be handled automatically the docker registry puller will just use the uri fetcher to get docker images,8
sync up configurationmd and flagscpp the httpsreviewsapacheorgr39923 made some clean up for configurationmd but the related flagscpp was not updated we should update those files as well,1
add authn and authz to maintenance endpoints maintenance endpoints are currently only restricted by firewall settings they should also support authenticationauthorization like other http endpoints,3
accepting an inverse offer prints misleading logs whenever a scheduler accepts an inverse offer mesos will print a line like this in the master logs code w1125 100553155109 29362 mastercpp2897 accept call used invalid offers  932f7d7bf2d442c79391222c19b9d35bo2  offer 932f7d7bf2d442c79391222c19b9d35bo2 is no longer valid code inverse offers should not trigger this warning,1
hdfs operations fail due to prepended  on path for nonhdfs hadoop clients this bug was resolved for the hdfs protocol for mesos3602 but since the process checks for the hdfs protocol at the beginning of the uri the fix does not extend itself to nonhdfs hadoop clients code i0107 012201259490 17678 loggingcpp172 info level logging started i0107 012201259856 17678 fetchercpp422 fetcher info cache_directorytmpmesosfetchslaves530dda5a481a411781543aee637d3b38s3rootitemsactionbypass_cacheuriextracttruevaluemaprfsmesosstormmesos093tgzactionbypass_cacheuriextracttruevaluehttps0121stagurbanairshipcom36373confstormyamlsandbox_directorymntdatamesosslaves530dda5a481a411781543aee637d3b38s3frameworks530dda5a481a411781543aee637d3b380000executorswordcount11452129714runs4443d5acd03449b3bf1208fb9b0d92d0userroot i0107 012201262171 17678 fetchercpp377 fetching uri maprfsmesosstormmesos093tgz i0107 012201262212 17678 fetchercpp248 fetching directly into the sandbox directory i0107 012201262243 17678 fetchercpp185 fetching uri maprfsmesosstormmesos093tgz i0107 012201671777 17678 fetchercpp110 downloading resource with hadoop client from maprfsmesosstormmesos093tgz to mntdatamesosslaves530dda5a481a411781543aee637d3b38s3frameworks530dda5a481a411781543aee637d3b380000executorswordcount11452129714runs4443d5acd03449b3bf1208fb9b0d92d0stormmesos093tgz copytolocal javaneturisyntaxexception expected schemespecific part at index 7 maprfs usage java fsshell copytolocal ignorecrc crc src localdst e0107 012202435556 17678 shellhpp90 command hadoop fs copytolocal maprfsmesosstormmesos093tgz mntdatamesosslaves530dda5a481a411781543aee637d3b38s3frameworks530dda5a481a411781543aee637d3b380000executorswordcount11452129714runs4443d5acd03449b3bf1208fb9b0d92d0stormmesos093tgz failed this is the output failed to fetch maprfsmesosstormmesos093tgz hdfs copytolocal failed failed to execute hadoop fs copytolocal maprfsmesosstormmesos093tgz mntdatamesosslaves530dda5a481a411781543aee637d3b38s3frameworks530dda5a481a411781543aee637d3b380000executorswordcount11452129714runs4443d5acd03449b3bf1208fb9b0d92d0stormmesos093tgz the command was either not found or exited with a nonzero exit status 255 failed to synchronize with slave its probably exited code after a brief chat with jieyu it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it,1
expand the getting started installation instructions the getting started documentation currently contains basic instructions to prepare several platforms for compilation and installation of mesos however these instructions are not sufficient to run and pass all tests in the test suite using all configuration options the installation instructions should be made comprehensive in this respect it may also be desirable to provide scripts that have been verified to prepare a particular base os to build install and test mesos this would be very useful for both developers and users of mesos note that using some features on some platforms requires the installation of software packages from sources that may not be completely reliable in the longterm for example packages which are maintained as personal projects of individuals this should be noted in the instructions accordingly,5
reliably report executor terminations to framework schedulers now that executor terminations are reported unreliably we should investigate queuing up these messages on the agent and resending them periodically until we get an acknowledgement much like status updates do from mesos313 the scheduler interface has a callback for executorlost but currently it is never called,5
protobuf parse should pass error messages when parsing nested json currently when protobufparse handles nested json objects it cannot pass any error message out we should enable showing those error messages,1
publish quota documentation publish and finish the operator guide draft for quota which describes basic usage of the endpoints and few basic and advanced usage cases,3
support get nondefault weights by weights like quota we should also add query logic for weights to keep consistent then roles no longer needs to show weight information,5
persistentvolumetestbadaclnoprincipal is flaky httpsbuildsapacheorgjobmesos1457compilergccconfigurationverbose20enablelibevent20enablessloscentos7label_expdocker7c7chadoopconsolefull noformat  run  persistentvolumetestbadaclnoprincipal i0108 011316117883 1325 leveldbcpp174 opened db in 2614722ms i0108 011316118650 1325 leveldbcpp181 compacted db in 706567ns i0108 011316118702 1325 leveldbcpp196 created db iterator in 24489ns i0108 011316118723 1325 leveldbcpp202 seeked to beginning of db in 2436ns i0108 011316118738 1325 leveldbcpp271 iterated through 0 keys in the db in 397ns i0108 011316118793 1325 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0108 011316119627 1348 recovercpp447 starting replica recovery i0108 011316120352 1348 recovercpp473 replica is in empty status i0108 011316121750 1357 replicacpp673 replica in empty status received a broadcasted recover request from 7084172170232801 i0108 011316122297 1353 recovercpp193 received a recover response from a replica in empty status i0108 011316122747 1350 recovercpp564 updating replica status to starting i0108 011316123625 1354 mastercpp365 master 773d31e8383d4e4baa68f9a3fb9f1fc2 d9632dd1c41e started on 172170232801 i0108 011316123946 1347 leveldbcpp304 persisting metadata 8 bytes to leveldb took 728242ns i0108 011316123999 1347 replicacpp320 persisted replica status to starting i0108 011316123708 1354 mastercpp367 flags at startup aclscreate_volumes  principals  values testprincipal  volume_types  type any   create_volumes  principals  type any  volume_types  type none    allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpf2ra75credentials framework_sorterdrf helpfalse hostname_lookuptrue initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue rolesrole1 root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0270_instsharemesoswebui work_dirtmpf2ra75master zk_session_timeout10secs i0108 011316124219 1354 mastercpp414 master allowing unauthenticated frameworks to register i0108 011316124236 1354 mastercpp417 master only allowing authenticated slaves to register i0108 011316124248 1354 credentialshpp35 loading credentials for authentication from tmpf2ra75credentials i0108 011316124294 1358 recovercpp473 replica is in starting status i0108 011316124644 1354 mastercpp456 using default crammd5 authenticator i0108 011316124820 1354 mastercpp493 authorization enabled w0108 011316124843 1354 mastercpp553 the roles flag is deprecated this flag will be removed in the future see the mesos 027 upgrade notes for more information i0108 011316125154 1348 hierarchicalcpp147 initialized hierarchical allocator process i0108 011316125334 1345 whitelist_watchercpp77 no whitelist given i0108 011316126065 1346 replicacpp673 replica in starting status received a broadcasted recover request from 7085172170232801 i0108 011316126806 1348 recovercpp193 received a recover response from a replica in starting status i0108 011316128237 1354 recovercpp564 updating replica status to voting i0108 011316128402 1359 mastercpp1629 the newly elected leader is master172170232801 with id 773d31e8383d4e4baa68f9a3fb9f1fc2 i0108 011316128489 1359 mastercpp1642 elected as the leading master i0108 011316128523 1359 mastercpp1387 recovering from registrar i0108 011316128756 1355 registrarcpp307 recovering registrar i0108 011316129259 1344 leveldbcpp304 persisting metadata 8 bytes to leveldb took 531437ns i0108 011316129292 1344 replicacpp320 persisted replica status to voting i0108 011316129425 1358 recovercpp578 successfully joined the paxos group i0108 011316129680 1358 recovercpp462 recover process terminated i0108 011316130187 1358 logcpp659 attempting to start the writer i0108 011316131613 1352 replicacpp493 replica received implicit promise request from 7086172170232801 with proposal 1 i0108 011316131983 1352 leveldbcpp304 persisting metadata 8 bytes to leveldb took 333646ns i0108 011316132004 1352 replicacpp342 persisted promised to 1 i0108 011316132627 1348 coordinatorcpp238 coordinator attempting to fill missing positions i0108 011316133896 1349 replicacpp388 replica received explicit promise request from 7087172170232801 for position 0 with proposal 2 i0108 011316134289 1349 leveldbcpp341 persisting action 8 bytes to leveldb took 349652ns i0108 011316134317 1349 replicacpp712 persisted action at 0 i0108 011316135470 1351 replicacpp537 replica received write request for position 0 from 7088172170232801 i0108 011316135537 1351 leveldbcpp436 reading position from leveldb took 36181ns i0108 011316135901 1351 leveldbcpp341 persisting action 14 bytes to leveldb took 308752ns i0108 011316135924 1351 replicacpp712 persisted action at 0 i0108 011316136529 1347 replicacpp691 replica received learned notice for position 0 from 00000 i0108 011316136889 1347 leveldbcpp341 persisting action 16 bytes to leveldb took 327106ns i0108 011316136916 1347 replicacpp712 persisted action at 0 i0108 011316136943 1347 replicacpp697 replica learned nop action at position 0 i0108 011316137707 1359 logcpp675 writer started with ending position 0 i0108 011316138844 1348 leveldbcpp436 reading position from leveldb took 31371ns i0108 011316139878 1356 registrarcpp340 successfully fetched the registry 0b in 0ns i0108 011316140012 1356 registrarcpp439 applied 1 operations in 42063ns attempting to update the registry i0108 011316140797 1355 logcpp683 attempting to append 170 bytes to the log i0108 011316140974 1345 coordinatorcpp348 coordinator attempting to write append action at position 1 i0108 011316141744 1354 replicacpp537 replica received write request for position 1 from 7089172170232801 i0108 011316142226 1354 leveldbcpp341 persisting action 189 bytes to leveldb took 441971ns i0108 011316142251 1354 replicacpp712 persisted action at 1 i0108 011316142860 1351 replicacpp691 replica received learned notice for position 1 from 00000 i0108 011316143198 1351 leveldbcpp341 persisting action 191 bytes to leveldb took 305928ns i0108 011316143223 1351 replicacpp712 persisted action at 1 i0108 011316143241 1351 replicacpp697 replica learned append action at position 1 i0108 011316144271 1354 registrarcpp484 successfully updated the registry in 0ns i0108 011316144435 1354 registrarcpp370 successfully recovered registrar i0108 011316144567 1359 logcpp702 attempting to truncate the log to 1 i0108 011316144780 1359 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0108 011316144989 1348 hierarchicalcpp165 skipping recovery of hierarchical allocator nothing to recover i0108 011316144928 1354 mastercpp1439 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0108 011316145690 1357 replicacpp537 replica received write request for position 2 from 7090172170232801 i0108 011316146072 1357 leveldbcpp341 persisting action 16 bytes to leveldb took 345113ns i0108 011316146097 1357 replicacpp712 persisted action at 2 i0108 011316146667 1358 replicacpp691 replica received learned notice for position 2 from 00000 i0108 011316147060 1358 leveldbcpp341 persisting action 18 bytes to leveldb took 283648ns i0108 011316147116 1358 leveldbcpp399 deleting 1 keys from leveldb took 32174ns i0108 011316147135 1358 replicacpp712 persisted action at 2 i0108 011316147153 1358 replicacpp697 replica learned truncate action at position 2 i0108 011316166832 1325 containerizercpp139 using isolation posixcpuposixmemfilesystemposix w0108 011316167556 1325 backendcpp48 failed to create bind backend bindbackend requires root privileges i0108 011316170526 1349 slavecpp191 slave started on 231172170232801 i0108 011316170718 1349 slavecpp192 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmppersistentvolumetest_badaclnoprincipal_yqjjlycredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmppersistentvolumetest_badaclnoprincipal_yqjjlyfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0270_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024diskrole12048 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmppersistentvolumetest_badaclnoprincipal_yqjjly i0108 011316171269 1349 credentialshpp83 loading credential for authentication from tmppersistentvolumetest_badaclnoprincipal_yqjjlycredential i0108 011316171505 1349 slavecpp322 slave using credential for testprincipal i0108 011316171747 1349 resourcescpp481 parsing resources as json failed cpus2mem1024diskrole12048 trying semicolondelimited string format instead i0108 011316172266 1349 slavecpp392 slave resources cpus2 mem1024 diskrole12048 ports3100032000 i0108 011316172327 1349 slavecpp400 slave attributes   i0108 011316172340 1349 slavecpp405 slave hostname d9632dd1c41e i0108 011316172353 1349 slavecpp410 slave checkpoint true i0108 011316173418 1353 statecpp58 recovering state from tmppersistentvolumetest_badaclnoprincipal_yqjjlymeta i0108 011316173521 1325 schedcpp164 version 0270 i0108 011316174054 1345 status_update_managercpp200 recovering status update manager i0108 011316174289 1353 containerizercpp387 recovering containerizer i0108 011316174295 1356 schedcpp268 new master detected at master172170232801 i0108 011316174387 1356 schedcpp278 no credentials provided attempting to register without authentication i0108 011316174409 1356 schedcpp722 sending subscribe call to master172170232801 i0108 011316174515 1356 schedcpp755 will retry registration in 1699889272secs if necessary i0108 011316174653 1349 mastercpp2197 received subscribe call for framework noprincipal at schedulerbf0ed267b4c4412d9fb084c85cd2fbce172170232801 i0108 011316174823 1349 mastercpp1668 authorizing framework principal  to receive offers for role role1 i0108 011316175250 1347 mastercpp2268 subscribing framework noprincipal with checkpointing disabled and capabilities   i0108 011316175359 1353 slavecpp4429 finished recovery i0108 011316175715 1345 hierarchicalcpp260 added framework 773d31e8383d4e4baa68f9a3fb9f1fc20000 i0108 011316175734 1351 schedcpp649 framework registered with 773d31e8383d4e4baa68f9a3fb9f1fc20000 i0108 011316175792 1345 hierarchicalcpp1329 no resources available to allocate i0108 011316175833 1345 hierarchicalcpp1423 no inverse offers to send out i0108 011316175853 1353 slavecpp4601 querying resource estimator for oversubscribable resources i0108 011316175869 1345 hierarchicalcpp1079 performed allocation for 0 slaves in 127881ns i0108 011316175923 1351 schedcpp663 schedulerregistered took 27956ns i0108 011316176110 1353 slavecpp729 new master detected at master172170232801 i0108 011316176187 1353 slavecpp792 authenticating with master master172170232801 i0108 011316176216 1353 slavecpp797 using default crammd5 authenticatee i0108 011316176398 1357 status_update_managercpp174 pausing sending status updates i0108 011316176404 1353 slavecpp765 detecting new master i0108 011316176463 1358 authenticateecpp121 creating new client sasl connection i0108 011316176553 1353 slavecpp4615 received oversubscribable resources from the resource estimator i0108 011316176709 1353 mastercpp5445 authenticating slave231172170232801 i0108 011316176823 1359 authenticatorcpp413 starting authentication session for crammd5_authenticatee516172170232801 i0108 011316177135 1348 authenticatorcpp98 creating new server sasl connection i0108 011316177373 1356 authenticateecpp212 received sasl authentication mechanisms crammd5 i0108 011316177399 1356 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0108 011316177502 1344 authenticatorcpp203 received sasl authentication start i0108 011316177563 1344 authenticatorcpp325 authentication requires more steps i0108 011316177680 1346 authenticateecpp258 received sasl authentication step i0108 011316177848 1354 authenticatorcpp231 received sasl authentication step i0108 011316177883 1354 auxpropcpp107 request to lookup properties for user testprincipal realm d9632dd1c41e server fqdn d9632dd1c41e sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0108 011316177894 1354 auxpropcpp179 looking up auxiliary property userpassword i0108 011316177944 1354 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0108 011316177994 1354 auxpropcpp107 request to lookup properties for user testprincipal realm d9632dd1c41e server fqdn d9632dd1c41e sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0108 011316178014 1354 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0108 011316178040 1354 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0108 011316178066 1354 authenticatorcpp317 authentication success i0108 011316178256 1355 authenticateecpp298 authentication success i0108 011316178315 1354 mastercpp5475 successfully authenticated principal testprincipal at slave231172170232801 i0108 011316178356 1355 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee516172170232801 i0108 011316178710 1354 slavecpp860 successfully authenticated with master master172170232801 i0108 011316178865 1354 slavecpp1254 will retry registration in 13009431ms if necessary i0108 011316179138 1350 mastercpp4154 registering slave at slave231172170232801 d9632dd1c41e with id 773d31e8383d4e4baa68f9a3fb9f1fc2s0 i0108 011316179628 1345 registrarcpp439 applied 1 operations in 71663ns attempting to update the registry i0108 011316180505 1356 logcpp683 attempting to append 343 bytes to the log i0108 011316180711 1352 coordinatorcpp348 coordinator attempting to write append action at position 3 i0108 011316181499 1350 replicacpp537 replica received write request for position 3 from 7103172170232801 i0108 011316182080 1350 leveldbcpp341 persisting action 362 bytes to leveldb took 537757ns i0108 011316182112 1350 replicacpp712 persisted action at 3 i0108 011316182749 1351 replicacpp691 replica received learned notice for position 3 from 00000 i0108 011316183120 1351 leveldbcpp341 persisting action 364 bytes to leveldb took 340999ns i0108 011316183151 1351 replicacpp712 persisted action at 3 i0108 011316183177 1351 replicacpp697 replica learned append action at position 3 i0108 011316184787 1348 registrarcpp484 successfully updated the registry in 0ns i0108 011316185287 1348 logcpp702 attempting to truncate the log to 3 i0108 011316185484 1349 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0108 011316186043 1353 slavecpp3371 received ping from slaveobserver230172170232801 i0108 011316186074 1345 mastercpp4222 registered slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 at slave231172170232801 d9632dd1c41e with cpus2 mem1024 diskrole12048 ports3100032000 i0108 011316186224 1353 slavecpp904 registered with master master172170232801 given slave id 773d31e8383d4e4baa68f9a3fb9f1fc2s0 i0108 011316186441 1353 fetchercpp81 clearing fetcher cache i0108 011316186486 1349 hierarchicalcpp465 added slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 d9632dd1c41e with cpus2 mem1024 diskrole12048 ports3100032000 allocated  i0108 011316186658 1346 status_update_managercpp181 resuming sending status updates i0108 011316186885 1353 slavecpp927 checkpointing slaveinfo to tmppersistentvolumetest_badaclnoprincipal_yqjjlymetaslaves773d31e8383d4e4baa68f9a3fb9f1fc2s0slaveinfo i0108 011316186905 1350 replicacpp537 replica received write request for position 4 from 7104172170232801 i0108 011316187595 1350 leveldbcpp341 persisting action 16 bytes to leveldb took 645704ns i0108 011316187628 1350 replicacpp712 persisted action at 4 i0108 011316188347 1349 hierarchicalcpp1423 no inverse offers to send out i0108 011316188475 1349 hierarchicalcpp1101 performed allocation for slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 in 1861833ms i0108 011316188560 1348 replicacpp691 replica received learned notice for position 4 from 00000 i0108 011316188385 1353 slavecpp963 forwarding total oversubscribed resources i0108 011316189275 1344 mastercpp5274 sending 1 offers to framework 773d31e8383d4e4baa68f9a3fb9f1fc20000 noprincipal at schedulerbf0ed267b4c4412d9fb084c85cd2fbce172170232801 i0108 011316189792 1344 mastercpp4564 received update of slave 773d31e8383d4e4baa68f9a3fb9f1fc2s0 at slave2311,1
slavetestlaunchtaskinfowithcontainerinfo cannot be execute in isolation executing slavetestlaunchtaskinfowithcontainerinfo from 468b8ec under os x 10105 in isolation fails due to missing cleanup code  binmesostestssh gtest_filterslavetestlaunchtaskinfowithcontainerinfo source directory abcdefsrcmesos build directory abcdefsrcmesosbuild  we cannot run any docker tests because docker tests not supported on nonlinux systems  usrbinnc usrbincurl note google test filter  slavetestlaunchtaskinfowithcontainerinfohealthchecktestroot_docker_dockerhealthytaskhealthchecktestroot_docker_dockerhealthstatuschangehierarchicalallocator_benchmark_testdeclineoffershooktestroot_docker_verifyslaveprelaunchdockerhookslavetestroot_runtaskwithcommandinfowithoutuserslavetestdisabled_root_runtaskwithcommandinfowithuserdockercontainerizertestroot_docker_launchdockercontainerizertestroot_docker_killdockercontainerizertestroot_docker_usagedockercontainerizertestroot_docker_recoverdockercontainerizertestroot_docker_skiprecovernondockerdockercontainerizertestroot_docker_logsdockercontainerizertestroot_docker_default_cmddockercontainerizertestroot_docker_default_cmd_overridedockercontainerizertestroot_docker_default_cmd_argsdockercontainerizertestroot_docker_slaverecoverytaskcontainerdockercontainerizertestdisabled_root_docker_slaverecoveryexecutorcontainerdockercontainerizertestroot_docker_nc_portmappingdockercontainerizertestroot_docker_launchsandboxwithcolondockercontainerizertestroot_docker_destroywhilefetchingdockercontainerizertestroot_docker_destroywhilepullingdockercontainerizertestroot_docker_executorcleanupwhenlaunchfaileddockercontainerizertestroot_docker_fetchfailuredockercontainerizertestroot_docker_dockerpullfailuredockercontainerizertestroot_docker_dockerinspectdiscarddockertestroot_docker_interfacedockertestroot_docker_parsing_versiondockertestroot_docker_checkcommandwithshelldockertestroot_docker_checkportresourcedockertestroot_docker_cancelpulldockertestroot_docker_mountrelativedockertestroot_docker_mountabsolutecopybackendtestroot_copybackendslaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave0slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave1slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave2slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave3slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave4slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave5slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave6slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave7slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave8slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave9slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave10slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave11slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave12slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave13slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave14slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave15slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave16slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave17slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave18slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave19slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave20slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave21slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave22slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave23slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave24slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave25slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave26slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave27slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave28slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave29slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave30slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave31slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave32slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave33slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave34slaveandframeworkcounthierarchicalallocator_benchmark_testaddandupdateslave35slavecountregistrar_benchmark_testperformance0slavecountregistrar_benchmark_testperformance1slavecountregistrar_benchmark_testperformance2slavecountregistrar_benchmark_testperformance3  running 1 test from 1 test case  global test environment setup  1 test from slavetest  run  slavetestlaunchtaskinfowithcontainerinfo  ok  slavetestlaunchtaskinfowithcontainerinfo 79 ms  1 test from slavetest 79 ms total  global test environment teardown srctestsenvironmentcpp569 failure failed tests completed with child processes remaining  54487 abcdefsrcmesosbuildsrclibsmesostests gtest_filterslavetestlaunchtaskinfowithcontainerinfo  54503 binsh abcdefsrcmesosbuildsrcmesoscontainerizer launch commandshelltruevalueabcdefsrcmesosbuildsrcmesosexecutor commandscommands directorytmp helpfalse pipe_read10 pipe_write13 usertest  1 test from 1 test case ran 87 ms total  passed  1 test  failed  0 tests listed below 0 failed tests code,1
refactor appc provisioner tests current tests can be refactored so that we can reuse some common tasks like test image creation this will benefit future tests like appc image puller tests,2
document supported file types for archive extraction by fetcher the mesos fetcher extracts specified uris if requested to do so by the scheduler however the documentation at httpmesosapacheorgdocumentationlatestfetcher doesnt list the file types extensions that will be extracted by the fetcher the relevant codehttpsgithubcomapachemesosblobmastersrclauncherfetchercppl63 specifies an exhaustive list of extensions that will be extracted the documentation should be updated to match,1
implement a simple windows version of direnthpp for compatibility,5
create utilities for common shell commands used we spawn shell for command line utilities like tar untar sha256 etc would be great for resuse if we can create a common utilities classfile for all these utilities,5
add parameters to apply patches quiet added a parameters to apply the patches quiet so its easy for contributor to apply patches with c,1
allow operators to assign net_cls major handles to mesos agents the net_cls cgroup allows operators to assign a 16bit major and 16bit minor network handle to tasks associated with a specific net_cls cgroup in mesos we need to give the operator the ability to fix the 16bit major handle used in an agent fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy say deny all for all container traffic till the container is allocated a minor handle a simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup,1
implement a networkhandle manager for net_cls cgroup subsystem as part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will allocated to containers when they are associated with a net_cls cgroup the networkhandle manager needs to provide the following functionality a during normal operation keep track of the free and allocated network handles there can be a total of 64k such network handles b on startup learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent,8
gmock warning in reservationtestaclmultipleoperations noformat  run  reservationtestaclmultipleoperations gmock warning uninteresting mock function call  returning directly function call shutdown0x7fa2a311b300 stack trace  ok  reservationtestaclmultipleoperations 174 ms  1 test from reservationtest 174 ms total noformat seems to occur nondeterministically for me maybe once per 50 runs or so osx 1010,1
gmock warning in hooktestverifyslaveruntaskhook hooktestverifyslavetaskstatusdecorator noformat  run  hooktestverifyslaveruntaskhook gmock warning uninteresting mock function call  returning directly function call shutdown0x7ff079cb2420 stack trace  ok  hooktestverifyslaveruntaskhook 51 ms  run  hooktestverifyslavetaskstatusdecorator gmock warning uninteresting mock function call  returning directly function call shutdown0x7ff079cbb790 stack trace  ok  hooktestverifyslavetaskstatusdecorator 54 ms noformat occurs nondeterministically for me osx 1010,1
gmock warning in slavetestcontainerupdatedbeforetaskreachesexecutor noformat  run  slavetestcontainerupdatedbeforetaskreachesexecutor gmock warning uninteresting mock function call  returning directly function call shutdown0x7fe189cae850 stack trace  ok  slavetestcontainerupdatedbeforetaskreachesexecutor 51 ms noformat occurs nondeterministically for me on osx 1010 perhaps one run in ten,1
gmock warning on offerrescinded in reservationtest fixture several tests involving checkpointing of resources in the reservationtest fixture are throwing gmock warnings occasionally here is the output of gtest_filterreservationtest binmesostestssh gtest_repeat10000 gtest_break_on_failure1  grep b 3 a 6 warning code  we cannot run any docker tests because docker tests not supported on nonlinux systems   ok  reservationtestmasterfailover 89 ms  run  reservationtestcompatiblecheckpointedresources gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec320fab0 65537c10285c419eb89f191283402d85o1 stack trace  ok  reservationtestcompatiblecheckpointedresources 52 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 45 ms    ok  reservationtestcompatiblecheckpointedresources 46 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec796f220 bf4e1b5202db47638be03c759c80f1bao1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 63 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 45 ms    ok  reservationtestcompatiblecheckpointedresources 42 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec7ad92b0 42a9f1ff122e4df79530a96126e36f84o1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 65 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 46 ms    ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 49 ms  run  reservationtestincompatiblecheckpointedresources gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec7af4310 d5e1005fabb84bfd92e03976ee150fbfo1 stack trace  ok  reservationtestincompatiblecheckpointedresources 94 ms  run  reservationtestgoodaclreservethenunreserve  ok  reservationtestgoodaclreservethenunreserve 57 ms    ok  reservationtestcompatiblecheckpointedresources 43 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec7cdadc0 36e15f52329946fa850d970097fef8e2o1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 62 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 46 ms    ok  reservationtestcompatiblecheckpointedresources 47 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feec8c1b580 c8dd35ab736340e08e208c7dc76a8497o1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 62 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 45 ms    ok  reservationtestcompatiblecheckpointedresources 47 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feecbd9b5b0 031c21488a204532b77fb6200c3791c8o1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 62 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 46 ms    ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 47 ms  run  reservationtestincompatiblecheckpointedresources gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feecd52adb0 edc5a322b2204b13a39b99a523b172bao1 stack trace  ok  reservationtestincompatiblecheckpointedresources 76 ms  run  reservationtestgoodaclreservethenunreserve  ok  reservationtestgoodaclreservethenunreserve 63 ms    ok  reservationtestsendingcheckpointresourcesmessage 45 ms  run  reservationtestresourcescheckpointing gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f015df8 0x7feecfe16f00 09a90e67a40f4e4288021a5644733a06o1 stack trace  ok  reservationtestresourcescheckpointing 60 ms  run  reservationtestmasterfailover  ok  reservationtestmasterfailover 89 ms    ok  reservationtestcompatiblecheckpointedresources 43 ms  run  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes gmock warning uninteresting mock function call  returning directly function call offerrescinded0x7fff5f014960 0x7feecacceba0 8496598428cd4bc8b25b746583477d09o1 stack trace  ok  reservationtestcompatiblecheckpointedresourceswithpersistentvolumes 58 ms  run  reservationtestincompatiblecheckpointedresources  ok  reservationtestincompatiblecheckpointedresources 68 ms code,2
limit the number of processes created by libprocess currently libprocess will create max8 number of cpu cores processes during the initialization see httpsgithubcomapachemesosblob02603rdpartylibprocesssrcprocesscppl2146 for details this should be ok for a normal machine which has no much cores eg 16 32 but for a powerful machine which may have a large number of cores eg an ibm power machine may have 192 cores this will cause too much worker threads which are not necessary and since libprocess is widely used in mesos master agent scheduler executor it may also cause some performance issue for example when user creates a docker container via mesos in a mesos agent which is running on a powerful machine with 192 cores the dockercontainerizer in mesos agent will create a dedicated executor for the container and there will be 192 worker threads in that executor and if user creates 1000 docker containers in that machine then there will be 1000 executors ie 1000  192 worker threads which is a large number and may thrash the os,1
gmock warning in roletestimplicitrolestaticreservation noformat  run  roletestimplicitrolestaticreservation gmock warning uninteresting mock function call  returning directly function call shutdown0x7fe37a4752f0 stack trace  ok  roletestimplicitrolestaticreservation 52 ms noformat,1
expose net_cls network handles in agents state endpoint we need to expose net_cls network handles associated with containers to operators and network utilities that would use these network handles to enforce network policy in order to achieve the above we need to add a new field in the networkinfo protobuf say nethandles and update this field when a container gets assigned to a net_cls cgroup the containerstatus protobuf already has the networkinfo protobuf as a nested message and the containerstatus itself is exposed to operators as part of taskinfo for tasks associated with the container in an agents statejson,2
gmock warning in dockercontainerizertestroot_docker_dockerinspectdiscard the following gmock warning was seen on centos 71 code  run  dockercontainerizertestroot_docker_dockerinspectdiscard gmock warning uninteresting mock function call  returning directly function call executorlost0x7ffdd74f73e0 0x7f3e3c00fa20 e1 0x7f3e3c00f4b0 cf212bb4c8c54a43b71fc17b27458627s0 1 stack trace  ok  dockercontainerizertestroot_docker_dockerinspectdiscard 405 ms code,2
create common taruntar utility function as part of refactoring and creating a common place to add all command utilities add tar and untar as the first poc,3
formating issues and broken links in documentation the online documentation has a number of bad formatting issues and broken links eg mesosprovidermd,1
add a roles field to frameworkinfo to represent multiple roles per framework a new repeated string field for roles is needed,1
add roles validation code to master a frameworkinfo can only have one of role or roles a natural location for this appears to be under validationoperationvalidate,5
add internal migration from role to roles to master if only the role field is given add it as single entry to roles add a note to changelogrelease notes on deprecation of the existing role field file a jira issue for removal of that migration code once the deprecation cycle is over,3
migrate all existing uses of frameworkinforole to frameworkinforoles,3
add tracking of the role a resource was offered for if a framework can have multiple roles we need a way to identify for which of the frameworks role a resource was offered for eg for resource recovery and reconciliation,5
make hierarchicalallocatorprocess set a resources active role during allocation the concrete implementation here depends on the implementation strategy used to solve mesos4367,3
document semantics of slavelost we should clarify the semantics of this callback  is it always invoked or just a hint  can a slave ever come back from slavelost  what happens to persistent resources on a lost slave the new ha framework development guide might be a good place to put some of this information,2
document units associated with resource types we should document the units associated with memory and disk resources,1
add source to resourcediskinfo source is used to describe the extra information about the source of a disk resource we will support path type first and then block later noformat message source  enum type  path  1 block  2  message path   path to the folder eg mntraiddisk0 required string root  1 required double total_size  2  message block   path to the device file eg devsda1 devvgv1  it can be a physical partition or a logical volume lvm required string device  1  required type type  1 optional path path  2 optional block block  3   noformat,1
design doc for reservation ids,3
adjust resource arithmetics for diskinfosource since we added the source for diskinfo we need to adjust the resource arithmetics for that that includes equality check addable check subtractable check etc,2
improve upgrade compatibility documentation investigate and document upgrade compatibility for 027 release,3
change the principal in reservationinfo to optional with the addition of http endpoints for reserve and unreserve it is now desirable to allow dynamic reservations without a principal in the case where http authentication is disabled to allow for this we will change the principal field in reservationinfo from required to optional for backwardscompatibility however the master should currently invalidate any reservationinfo messages that do not have this field set,1
support docker runtime configuration env var from image we need to support env var configuration returned from docker image in mesos containerizer,2
offers and inverseoffers cannot be accepted in the same accept call problem  in masteraccept validationoffervalidate returns an error when an inverseoffer is included in the list of offerids in an accept call  if an offer is part of the same accept the master sees errorissome and returns a task_lost for normal offers httpsgithubcomapachemesosblobfafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3srcmastermastercppl3117 heres a regression test httpsreviewsapacheorgr42092 proprosal the question is whether we want to allow the mixing of offers and inverseoffers arguments for mixing  the designstructure of the maintenance originally intended to overload accept and decline to take inverse offers  enforcing nonmixing may require breaking changes to schedulerproto arguments against mixing  some semantics are difficult to explain what does it mean to supply inverseoffers with offeroperations what about decline with offers and inverseoffers including a reason  what happens if we presumably add a third type of offer  does it make sense to task_lost valid normal offers if inverseoffers are invalid,2
deprecate authenticate master flag in favor of authenticate_frameworks flag to be consistent with authenticate_slaves and authenticate_http flags we should rename authenticate to authenticate_frameworks flag this should be done via deprecation cycle 1 release x supports both authenticate and authenticate_frameworks flags 2 release x  n supports only authenticate_frameworks flag,1
shared volumes design doc review  approve design doc,3
draft design document for resource revocability by default create a design document for setting offered resources as revocable by default greedy frameworks can then temporarily use resources set aside to satisfy quota,8
add persistent volume endpoint tests with no principal there are currently no persistent volume endpoint tests that do not use a principal they should be added,1
rename containerprepareinfo to containerlaunchinfo for isolators the name containerprepareinfo does not really capture the purpose of this struct containerlaunchinfo better captures the purpose of this struct containerlaunchinfo is returned by the isolator prepare function it contains information about how a container should be launched eg environment variables namespaces commands etc the information will be used by the mesos containerizer when launching the container,2
synchronously handle authz errors for the scheduler endpoint currently any authz errors for the scheduler endpoint are handled asynchronously as frameworkerrormessage here is an example code if authorizationerrorissome  loginfo  refusing subscription of framework     frameworkinfoname       authorizationerrorgetmessage frameworkerrormessage message messageset_messageauthorizationerrorgetmessage httpsendmessage httpclose return  code we would like to handle such errors synchronously when the request is received similar to what other endpoints like reservequota do we already have the relevant functions authorizexxx etc in mastercpp we should just make the requests pass through once the relevant future from the authorizexxx function is fulfilled,5
create persistent volume directories based on diskinfosource currently we always create persistent volumes from root disk and the persistent volumes are directories with diskinfosource being added we should create the persistent volume accordingly based on the information in diskinfosource this ticket handles the case where diskinfosourcetype is path in that case we should create subdirectories and use the same layout as slavework_dir see the relevant code here code void slavecheckpointresources   creates persistent volumes that do not exist and schedules  releasing those persistent volumes that are no longer needed   todojieyu consider introducing a volume manager once we start  to support multiple disks or raw disks depending on the  diskinfo we may want to create either directories under a root  directory or lvm volumes from a given device resources volumes  newcheckpointedresourcespersistentvolumes foreach const resource volume volumes   this is validated in master check_nevolumerole  string path  pathsgetpersistentvolumepath flagswork_dir volumerole volumediskpersistenceid if osexistspath  check_someosmkdirpath true  failed to create persistent volume at   path      code,2
update filesystem isolators to look for persistent volume directories from the correct location this is related to mesos4400 since persistent volume directories can be created from non root disk now we need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in diskinfosource see relevant code in code futurenothing posixfilesystemisolatorprocessupdate futurenothing linuxfilesystemisolatorprocessupdate code,2
check paths in diskinfosourcepath exist during slave initialization we have two options here we can either check and fail if it does not exists or we can create if it does not exist like we did for slavework_dir,2
introduce protobuf for quota set request to document quota request json schema and simplify request processing introduce a quotarequest protobuf wrapper,3
traverse all roles for quota allocation there might be a bug in how resources are allocated to multiple quotaed roles if one roles quota is met we need to investigate this behavior,3
implement stoutoswindowsrmdirhpp,5
prevent allocator from crashing on successful recovery there might be a bug that may crash the master as pointed out by bmahler in httpsreviewsapacheorgr42222 noformat it looks like if we trip the resume call in addslave this delayed resume will crash the master due to the checkpaused that currently resides in resume noformat,3
document that reserve createvolumes endpoints can return misleading success the docs for the reserve endpoint say noformat 200 ok success the requested resources have been reserved noformat this is not true the master returns 200 when the request has been validated and a checkpointresourcesmessage has been sent to the agent but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed same behavior applies to unreserve createvolumes and destroyvolumes we should _either_ 1 accurately document what 200 return code means 2 change the implementation to wait for the agents next checkpoint to succeed and to include the effect of the operation before returning success to the http client,3
introduce filtering test abstractions for http events to libprocess we need a test abstraction for httpevent similar to the already existing ones for dispatchevent messageevent in libprocess the abstraction can look similar in semantics to the already existing future_dispatchfuture_message,3
implement a callback testing interface for the executor library currently we do not have a mocking based callback interface for the executor library this should look similar to the ongoing work for mesos3339 ie the corresponding issue for the scheduler library the interface should allow us to set expectations like we do for the driver an example code expect_callexecutor connected times1 code,3
install 3rdparty package boost glog protobuf and picojson when installing mesos mesos modules depend on having these packages installed with the exact version as mesos was compiled with,3
update masterhttpstatesummary to use jsonify update statesummary to use jsonify to stay consistent with state http endpoint,3
disable the test registryclienttestbadtokenserveraddress as we are retiring registry client disable this test which looks flaky,1
add dependency message to appcimagemanifest protobuf appcimagemanifest protobuf currently lacks dependencies which is necessary for image discovery,1
fix appc cachedimage image validation currently image validation is done assuming that the images filename will have digest sha512 information this is not part of the spec httpsgithubcomappcspecblobmasterspecdiscoverymd the spec specifies the tuple image name labels as unique identifier for discovering an image,1
allocate revocable resources beyond quota guarantee h4 status quo currently resources allocated to frameworks in a role with quota aka quotaed role beyond quota guarantee are marked nonrevocable this impacts our flexibility for revoking them if we decide so in the future h4 proposal once quota guarantee is satisfied we must not necessarily further allocate resources as nonrevocable instead we can mark all offers resources beyond guarantee as revocable when in the future revocableinfo evolves frameworks will get additional information about revocability of the resource ie allocation slack h4 caveats though it seems like a simple change it has several implications h6 fairness currently the hierarchical allocator considers revocable resources as regular resources when doing fairness calculations this may prevent frameworks getting nonrevocable resources as part of their roles quota guarantee if they accept some revocable resources as well consider the following scenario a single framework in a role with quota set to 10 cpus is allocated 10 cpus as nonrevocable resources as part of its quota and additionally 2 revocable cpus now a task using 2 nonrevocable cpus finishes and its resources are returned total allocation for the role is 8 nonrevocable  2 revocable however the role may not be offered additional 2 nonrevocable since its total allocation satisfies quota h6 resource math if we allocate nonrevocable resources as revocable we should make sure we do accounting right either we should update total agent resources and mark them as revocable as well or bookkeep resources as nonrevocable and convert them to revocable when necessary h6 coarsegrained nature of allocation the hierarchical allocator performs coarsegrained allocation meaning it always allocates the entire remaining agent resources to a single framework this may lead to overallocating some resources as nonrevocable beyond quota guarantee h6 quotas smaller than fair share if a quota set for a role is smaller than its fair share it may reduce the amount of resources offered to this role if frameworks in it do not accept revocable resources this is probably the most important consequence of the proposed change operators may set quota to get guarantees but may observe a decrease in amount of resources a role gets which is not intuitive,8
refactor allocator recovery allocator recovery code can be improved for readability bmahler left some thoughts about it in httpsreviewsapacheorgr42222,3
design doc for reservation labels,3
labels equality behavior is wrong noformat testrevocableresourcetest labelsemantics  labels labels1 labels labels2 labels1add_labelscopyfromcreatelabelfoo bar labels1add_labelscopyfromcreatelabelfoo bar labels2add_labelscopyfromcreatelabelfoo bar labels2add_labelscopyfromcreatelabelbaz qux bool eq  labels1  labels2 loginfo  equal   eq  true  false  noformat output noformat  run  revocableresourcetestlabelsemantics i0120 131525207223 2078158848 resources_testscpp1990 equal true  ok  revocableresourcetestlabelsemantics 0 ms noformat this behavior seems pretty problematic,5
segfault on agent during executor startup when repeatedly performing our system tests we have found that we get a segfault on one of the agents it probably occurs about one time in ten i have attached the full log from that agent ive attached the log from the agent that failed and the master although i think this is less helpful to reproduce  i have no idea it seems to occur at certain times eg like if a packet is created right on a minute boundary or something but i dont think its something caused by our code because the timestamps are stamped by mesos i was surprised not to find a bug already open,1
improve documentation around roles principals authz and reservations  what is the difference between a role and a principal  why do some acl entities reference roles but others reference principals in a typical organization what realworld entities would my roles vs principals map to the acl documentation could use more information about the motivation of acls and examples of configuring acls to meet realworld security policies  we should give some examples of making reservations when the role and principal are different and why you would want to do that  we should add an example to the acl page that includes setting acls for reservations andor persistent volumes,2
create common sha512 compute utility function add common utility function for computing digests start with sha512 since its immediately needed by appc image fetcher,2
implement tests for the new executor library we need to add tests for the executor library srcexecutorexecutorcpp one possible approach would be to use the existing tests in srctestsscheduler_testscpp and make them use the new executor library,3
implement authn handling on the scheduler library currently we do not have the ability of passing credentials via the scheduler library once the master supports authn handling for the scheduler endpoint we would need to add this support to the library,3
enable frameworkexecutor message optimization for http api currently we support sending frameworkexecutor messages directly as an optimization this is not currently possible with using the scheduler http api we should think about exploring possible alternatives for supporting this optimization,13
enable executorframework message optimization for http api currently we support sending executorframework messages directly as an optimization this is not currently possible with using the scheduler http api we should think about exploring possible alternatives for supporting this optimization,13
implement waitpid in windows,5
implement process queryingcounting in windows,2
reviewbot seemed to be crashing reviewboard server when posting large reviews the bot is currently tripping on this review httpsreviewsapacheorgr42506 see builds 10973 to 10978 jfarrell looked at the server logs and said he saw mysql going away message when the mesos bot was making these requests i think that error is a bit misleading because it happens only for this review which has a huge error log due to bad patch the bot has successfully posted reviews for other review requests which had no error log good patch one way to fix this would be to just post a tail of the error log and perhaps link to jenkins console or some other service for the longer error text,2
implement reservation labels,5
introduce status interface in containerizer in the containerizer during container isolation the isolators end up modifying the state of the containers examples would be ip address allocation to a container by the network isolator or net_cls handle allocation by the cgroupnet_cls isolator often times the state of the container needs to be exposed to operators through the statejson endpoint for eg operators or frameworks might want to know the ipaddress configured on a particular container or the net_cls handle associated with a container to configure the right tc rules however at present there is no clean interface for the slave to retrieve the state of a container from the containerizer for any of the launched containers thus we need to introduce a status interface in the containerizer base class in order for the slave to expose container state information in its statejson,2
define a cgroupinfo protobuf to expose cgroup isolator configuration within mesoscontainerizer we have an isolator associated with each linux cgroup subsystem the isolators apply subsystem specific configuration on the containers before launching the containers for eg cgroupnet_cls isolator applies net_cls handles cgroupmem isolator applies memory quotas cgroupscpushare isolator configures cpu shares currently there is no message structure defined to capture the configuration information of the container for each cgroup isolator that has been applied to the container we therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container this protobuf will be filled in by the cgroup isolator and will be stored as part of containerconfig in the containerizer,1
the cgroupsnet_cls isolator needs to expose handles in the containerstatus the cgroupnet_cls isolator is responsible for allocating network handles to containers launched within a net_cls cgroup the cgroupnet_cls isolator needs to expose these handles to the containerizer as part of the containerstatus when the containerizer queries the status method of the isolator the information itself will go as part of a cgroupinfo protobuf that will be defined as part of mesos4488,1
get container status information in slave as part of mesos4487 an interface will be introduce into the containerizer to allow agents to retrieve container state information the agent needs to use this interface to retrieve container state information during status updates from the executor the container state information can be then use by the agent to expose various isolator specific configuration for eg ip address allocated by network isolators net_cls handles allocated by cgroupsnet_cls isolator that has been applied to the container in the statejson endpoint,3
add ability to create symlink on windows,3
implement size usage and other disk metrics reporting on windows,3
delete oschown on windows,1
refactor oshpp to be less monolithic and more crossplatform compatible,1
docker provisioner store should reuse existing layers in the cache currently the docker provisioner store will download all the layers associated with an image if the image is not found locally even though some layers of it might already exist in the cache this is problematic because anytime a user deploys a new image mesos will fetch all layers of that new image even though most of the layers are already cached locally,5
expose executorinfo and taskinfo for isolators currently we do not have these info for isolator image once we have docker runtime isolator commandinfo is necessary to support either custom executor or command executor,2
hierarchical allocator performance is slow due to quota since we do not strip the nonscalar resources during the resource arithmetic for quota the performance can degrade significantly as currently resource arithmetic is expensive one approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars this is valid as quota can currently only be set for scalar resource types,3
posix disk isolator should ignore disk quota enforcement for mount type disk resources we assume mount type disk is exclusive and the underlying filesystem will enforce the quota ie the application wont be able to exceed the quota and will get a write error it the disk is full therefore theres no need to enforce its quota in posix disk isolator,2
render quota status consistently with other endpoints currently quota status endpoint returns a collection of quotainfo protos converted to json an example response looks like this codexml  infos   role role1 guarantee   name cpus role  type scalar scalar  value 12    name mem role  type scalar scalar  value 6144       code presence of some fields eg role is misleading to address this issue and make the output more informative we should probably introduce a model function for quotastatus,3
build failure when using gcc49  signedunsigned mismatch when building the current master the following happens when using gcc49 noformat mv f examplesdepspersistent_volume_frameworkpersistent_volume_frameworktpo examplesdepspersistent_volume_frameworkpersistent_volume_frameworkpo g49 dpackage_namemesos dpackage_tarnamemesos dpackage_version0270 dpackage_stringmesos 0270 dpackage_bugreport dpackage_url dpackagemesos dversion0270 dstdc_headers1 dhave_sys_types_h1 dhave_sys_stat_h1 dhave_stdlib_h1 dhave_string_h1 dhave_memory_h1 dhave_strings_h1 dhave_inttypes_h1 dhave_stdint_h1 dhave_unistd_h1 dhave_dlfcn_h1 dlt_objdirlibs dhave_pthread_prio_inherit1 dhave_pthread1 dhave_libz1 dhave_libcurl1 dhave_apr_pools_h1 dhave_libapr_11 dhave_svn_version_h1 dhave_libsvn_subr_11 dhave_svn_delta_h1 dhave_libsvn_delta_11 dhave_libsasl21 i isrc wall werror dlibdirusrlocallib dpkglibexecdirusrlocallibexecmesos dpkgdatadirusrlocalsharemesos iinclude i3rdpartylibprocessinclude i3rdpartylibprocess3rdpartystoutinclude iinclude iincludemesos isystem 3rdpartylibprocess3rdpartyboost1530 i3rdpartylibprocess3rdpartypicojson130 dpicojson_use_int64 d__stdc_format_macros i3rdpartylibprocess3rdpartyprotobuf250src i3rdpartylibprocess3rdpartyglog033src i3rdpartylibprocess3rdpartyglog033src i3rdpartyleveldbinclude i3rdpartyzookeeper345srccinclude i3rdpartyzookeeper345srccgenerated i3rdpartylibprocess3rdpartyprotobuf250src dsource_diruserstilldevelopmentmesosprivatebuild dbuild_diruserstilldevelopmentmesosprivatebuild i3rdpartylibprocess3rdpartygmock170gtestinclude i3rdpartylibprocess3rdpartygmock170include iusrlocaloptopensslinclude iusrlocaloptlibeventinclude iusrlocaloptsubversionincludesubversion1 iusrincludeapr1 iusrincludeapr10 d_thread_safe pthread g1 o0 wnounusedlocaltypedefs stdc11 dgtest_use_own_tr1_tuple1 dgtest_lang_cxx11 mt testsmesos_testscontainer_logger_testso md mp mf testsdepsmesos_testscontainer_logger_teststpo c o testsmesos_testscontainer_logger_testso test f testscontainer_logger_testscpp  echo srctestscontainer_logger_testscpp mv f slaveqos_controllersdepsmesos_testsloadtpo slaveqos_controllersdepsmesos_testsloadpo g49 dpackage_namemesos dpackage_tarnamemesos dpackage_version0270 dpackage_stringmesos 0270 dpackage_bugreport dpackage_url dpackagemesos dversion0270 dstdc_headers1 dhave_sys_types_h1 dhave_sys_stat_h1 dhave_stdlib_h1 dhave_string_h1 dhave_memory_h1 dhave_strings_h1 dhave_inttypes_h1 dhave_stdint_h1 dhave_unistd_h1 dhave_dlfcn_h1 dlt_objdirlibs dhave_pthread_prio_inherit1 dhave_pthread1 dhave_libz1 dhave_libcurl1 dhave_apr_pools_h1 dhave_libapr_11 dhave_svn_version_h1 dhave_libsvn_subr_11 dhave_svn_delta_h1 dhave_libsvn_delta_11 dhave_libsasl21 i isrc wall werror dlibdirusrlocallib dpkglibexecdirusrlocallibexecmesos dpkgdatadirusrlocalsharemesos iinclude i3rdpartylibprocessinclude i3rdpartylibprocess3rdpartystoutinclude iinclude iincludemesos isystem 3rdpartylibprocess3rdpartyboost1530 i3rdpartylibprocess3rdpartypicojson130 dpicojson_use_int64 d__stdc_format_macros i3rdpartylibprocess3rdpartyprotobuf250src i3rdpartylibprocess3rdpartyglog033src i3rdpartylibprocess3rdpartyglog033src i3rdpartyleveldbinclude i3rdpartyzookeeper345srccinclude i3rdpartyzookeeper345srccgenerated i3rdpartylibprocess3rdpartyprotobuf250src dsource_diruserstilldevelopmentmesosprivatebuild dbuild_diruserstilldevelopmentmesosprivatebuild i3rdpartylibprocess3rdpartygmock170gtestinclude i3rdpartylibprocess3rdpartygmock170include iusrlocaloptopensslinclude iusrlocaloptlibeventinclude iusrlocaloptsubversionincludesubversion1 iusrincludeapr1 iusrincludeapr10 d_thread_safe pthread g1 o0 wnounusedlocaltypedefs stdc11 dgtest_use_own_tr1_tuple1 dgtest_lang_cxx11 mt testsmesos_testscontainerizero md mp mf testsdepsmesos_testscontainerizertpo c o testsmesos_testscontainerizero test f testscontainerizercpp  echo srctestscontainerizercpp in file included from 3rdpartylibprocess3rdpartygmock170includegmockinternalgmockinternalutilsh470 from 3rdpartylibprocess3rdpartygmock170includegmockgmockactionsh46 from 3rdpartylibprocess3rdpartygmock170includegmockgmockh58 from srctestscontainer_logger_testscpp21 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth in instantiation of testingassertionresult testinginternalcmphelperleconst char const char const t1 const t2 with t1  int t2  long long unsigned int srctestscontainer_logger_testscpp4673 required from here 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth157928 error comparison between signed and unsigned integer expressions werrorsigncompare gtest_impl_cmp_helper_le   3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth156212 note in definition of macro gtest_impl_cmp_helper_ if val1 op val2   3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth in instantiation of testingassertionresult testinginternalcmphelpergeconst char const char const t1 const t2 with t1  int t2  long long unsigned int srctestscontainer_logger_testscpp4683 required from here 3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth158328 error comparison between signed and unsigned integer expressions werrorsigncompare gtest_impl_cmp_helper_ge   3rdpartylibprocess3rdpartygmock170gtestincludegtestgtesth156212 note in definition of macro gtest_impl_cmp_helper_ if val1 op val2   mv f testsdepsmesos_testsanonymous_teststpo testsdepsmesos_testsanonymous_testspo noformat,1
containerloggertestlogrotate_rotateinsandbox breaks when running on centos6 noformat 172458step 77 logrotate bad argument version unknown error 172458step 77 f0126 172457913729 4503 container_logger_testscpp380 check_somecontainerizer failed to create container logger failed to create container logger module org_apache_mesos_logrotatecontainerlogger error creating module instance for org_apache_mesos_logrotatecontainerlogger 172458step 77  check failure stack trace  172458step 77  0x7f11ae0d2d40 googlelogmessagefail 172458step 77  0x7f11ae0d2c9c googlelogmessagesendtolog 172458step 77  0x7f11ae0d2692 googlelogmessageflush 172458step 77  0x7f11ae0d544c googlelogmessagefatallogmessagefatal 172458step 77  0x983927 _checkfatal_checkfatal 172458step 77  0xa9a18b mesosinternaltestscontainerloggertest_logrotate_rotateinsandbox_testtestbody 172458step 77  0x1623a4e testinginternalhandlesehexceptionsinmethodifsupported 172458step 77  0x161eab2 testinginternalhandleexceptionsinmethodifsupported 172458step 77  0x15ffdfd testingtestrun 172458step 77  0x160058b testingtestinforun 172458step 77  0x1600bc6 testingtestcaserun 172458step 77  0x1607515 testinginternalunittestimplrunalltests 172458step 77  0x16246dd testinginternalhandlesehexceptionsinmethodifsupported 172458step 77  0x161f608 testinginternalhandleexceptionsinmethodifsupported 172458step 77  0x1606245 testingunittestrun 172458step 77  0xde36b6 run_all_tests 172458step 77  0xde32cc main 172458step 77  0x7f11a8896d5d __libc_start_main 172458step 77  0x981fc9 unknown noformat,1
introduce docker runtime isolator currently docker image default configuration are included in provisioninfo we should grab necessary config from provisioninfo into containerinfo and handle all these runtime informations inside of docker runtime isolator return a containerlaunchinfo containing working_dir env and merged commandinfo etc,3
introduce a status interface for isolators while launching a container mesos isolators end up configuringmodifying various properties of the container for eg cgroup isolators mem cpu net_cls configurechange the properties associated with their respective subsystems before launching a container similary network isolator netmodules port mapping configure the ip address and ports associated with a container currently there are not interface in the isolator to extract the run time state of these properties for a given container therefore a status method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator,1
enable benchmark tests in asf ci it would be nice to enable benchmark tests in the asf ci so that we can catch performance regressions esp during releases,3
include the allocated portion of reserved resources in the role sorter for drf reserved resources should be accounted for fairness calculation whether they are allocated or not since they model a long or forever running task that is the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as nonrevocable in the shortterm we should at least account for the allocated portion of the reservation,1
include allocated portion of the reserved resources in the quota role sorter for drf similar to mesos4526 reserved resources should be accounted for in the quota role sorter regardless of their allocation state in the shortterm we should at least account them if they are allocated,1
account for reserved resources in the quota guarantee check reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool,2
update the allocator to not offer unreserved resources beyond quota eventually we will want to offer unreserved resources as revocable beyond the roles quota rather than offering nonrevocable resources beyond the roles quotas guarantee in the short term we choose to not offer resources beyond a roles quota,2
netclsisolatortestroot_cgroups_netclsisolate is flaky while running the command noformat sudo binmesostestssh gtest_filtercgroupsanyhierarchywithcpumemorytestroot_cgroups_listencgroupsanyhierarchymemorypressuretestroot_increaserss gtest_repeat10 gtest_break_on_failure noformat one eventually gets the following output noformat  run  netclsisolatortestroot_cgroups_netclsisolate srctestscontainerizerisolator_testscpp870 failure containerizer could not create isolator cgroupsnet_cls unexpected subsystems found attached to the hierarchy sysfscgroupnet_clsnet_prio  failed  netclsisolatortestroot_cgroups_netclsisolate 75 ms noformat,1
document multidisk support,2
resources object can be mutated through the public api the resources object current allows mutation of its internal state through the public mutable iterator interface this can cause issues when the mutation involved stripping certain qualifiers on a resource as they will not be summed together at the end of the mutation even though they should be the contains math will not work correctly if two addable resources are not summed together on the lhs of the contains check,3
logrotate containerlogger may not handle fd ownership correctly one of the patches for mesos4136 introduced the fdtypeowned enum for subprocessiofd the way the logrotate module uses this is slightly incorrect  the module starts a subprocess with an output subprocesspipe  that pipes fd is passed into another subprocess via subprocessiofdpipe ioowned  when the second subprocess starts the pipes fd is closed in the parent  when the first subprocess terminates the existing code will try to close the pipe again this effectively closes a random fd,1
add abstractions of owned and shared file descriptors to libprocess libprocess currently manages file descriptors as plain int s this leads to some easily missed bugs regarding duplicated or closed fds we should introduce an abstraction like unique_ptr and shared_ptr so that fd ownership can be expressed alongside the affected code,3
exclude paths in posix disk isolator should be absolute paths since du exclude uses pattern matching a relative path might accidentally matches an irrelevant directoryfile for instance noformat tmptestpath  tree   aaa   exc   file  exc  file 3 directories 2 files tmptestpath  du exclude tmptestpathexc tmptestpath 8 tmptestpathaaaexc 12 tmptestpathaaa 16 tmptestpath tmptestpath  du exclude exc tmptestpath 4 tmptestpathaaa 8 tmptestpath tmptestpath  noformat,2
netclsisolatortestroot_cgroups_netclsisolate fails on centos 6 this test fails in my centos 6 vm due to a cgroups issue code  run  netclsisolatortestroot_cgroups_netclsisolate i0127 191506637328 25347 execcpp134 version 0280 i0127 191506648378 25378 execcpp208 executor registered on slave 6edafba09dbd4e6eb10ec6f935e58d41s0 registered executor on localhost starting task b745d88e3fbe4af980b3e43484e37acf sh c sleep 1000 forked command at 25385 srctestscontainerizerisolator_testscpp926 failure pids failed to read cgroups control cgroupprocs sysfscgroupnet_cls is not a valid hierarchy i0127 191506662083 25376 execcpp381 executor asked to shutdown shutting down sending sigterm to process tree at pid 25385  failed  netclsisolatortestroot_cgroups_netclsisolate 335 ms code,1
masterquotatestavailableresourcesafterrescinding is flaky can be reproduced by running glog_v1 gtest_filtermasterquotatestavailableresourcesafterrescinding binmesostestssh gtest_shuffle gtest_break_on_failure gtest_repeat1000 verbose h5 verbose log from a bad run code  run  masterquotatestavailableresourcesafterrescinding i0128 122027568657 2080858880 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0128 122027570142 2080858880 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0128 122027583225 2080858880 leveldbcpp174 opened db in 6241us i0128 122027584353 2080858880 leveldbcpp181 compacted db in 1026us i0128 122027584429 2080858880 leveldbcpp196 created db iterator in 12us i0128 122027584442 2080858880 leveldbcpp202 seeked to beginning of db in 7us i0128 122027584453 2080858880 leveldbcpp271 iterated through 0 keys in the db in 6us i0128 122027584475 2080858880 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0128 122027584918 300445696 recovercpp447 starting replica recovery i0128 122027585113 300445696 recovercpp473 replica is in empty status i0128 122027585916 297226240 replicacpp673 replica in empty status received a broadcasted recover request from 182741921681782451278 i0128 122027586086 297762816 recovercpp193 received a recover response from a replica in empty status i0128 122027586449 297226240 recovercpp564 updating replica status to starting i0128 122027587204 300445696 leveldbcpp304 persisting metadata 8 bytes to leveldb took 624us i0128 122027587242 300445696 replicacpp320 persisted replica status to starting i0128 122027587376 299372544 recovercpp473 replica is in starting status i0128 122027588050 300982272 replicacpp673 replica in starting status received a broadcasted recover request from 182751921681782451278 i0128 122027588235 300445696 recovercpp193 received a recover response from a replica in starting status i0128 122027588572 297762816 recovercpp564 updating replica status to voting i0128 122027588850 297226240 leveldbcpp304 persisting metadata 8 bytes to leveldb took 140us i0128 122027588879 297226240 replicacpp320 persisted replica status to voting i0128 122027588975 299909120 recovercpp578 successfully joined the paxos group i0128 122027589154 299909120 recovercpp462 recover process terminated i0128 122027599486 298835968 mastercpp374 master 531344bd56f44e4f8f6fa6a9d36058c7 alexrfritzbox started on 1921681782451278 i0128 122027599520 298835968 mastercpp376 flags at startup acls allocation_interval50ms allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialsprivatetmpnlzpsocredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue rolesrole1role2 root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirprivatetmpnlzpsomaster zk_session_timeout10secs i0128 122027599753 298835968 mastercpp421 master only allowing authenticated frameworks to register i0128 122027599769 298835968 mastercpp426 master only allowing authenticated slaves to register i0128 122027599781 298835968 credentialshpp35 loading credentials for authentication from privatetmpnlzpsocredentials i0128 122027600082 298835968 mastercpp466 using default crammd5 authenticator i0128 122027600163 298835968 mastercpp535 using default basic http authenticator i0128 122027600327 298835968 mastercpp569 authorization enabled w0128 122027600345 298835968 mastercpp629 the roles flag is deprecated this flag will be removed in the future see the mesos 027 upgrade notes for more information i0128 122027600497 297762816 whitelist_watchercpp77 no whitelist given i0128 122027600503 297226240 hierarchicalcpp144 initialized hierarchical allocator process i0128 122027601965 297226240 mastercpp1710 the newly elected leader is master1921681782451278 with id 531344bd56f44e4f8f6fa6a9d36058c7 i0128 122027601995 297226240 mastercpp1723 elected as the leading master i0128 122027602007 297226240 mastercpp1468 recovering from registrar i0128 122027602083 300445696 registrarcpp307 recovering registrar i0128 122027602460 297226240 logcpp659 attempting to start the writer i0128 122027603514 299909120 replicacpp493 replica received implicit promise request from 182771921681782451278 with proposal 1 i0128 122027603734 299909120 leveldbcpp304 persisting metadata 8 bytes to leveldb took 205us i0128 122027603768 299909120 replicacpp342 persisted promised to 1 i0128 122027604194 299909120 coordinatorcpp238 coordinator attempting to fill missing positions i0128 122027605311 299372544 replicacpp388 replica received explicit promise request from 182781921681782451278 for position 0 with proposal 2 i0128 122027605468 299372544 leveldbcpp341 persisting action 8 bytes to leveldb took 133us i0128 122027605494 299372544 replicacpp712 persisted action at 0 i0128 122027606441 298835968 replicacpp537 replica received write request for position 0 from 182791921681782451278 i0128 122027606492 298835968 leveldbcpp436 reading position from leveldb took 29us i0128 122027606665 298835968 leveldbcpp341 persisting action 14 bytes to leveldb took 151us i0128 122027606688 298835968 replicacpp712 persisted action at 0 i0128 122027607244 297226240 replicacpp691 replica received learned notice for position 0 from 00000 i0128 122027607409 297226240 leveldbcpp341 persisting action 16 bytes to leveldb took 152us i0128 122027607441 297226240 replicacpp712 persisted action at 0 i0128 122027607457 297226240 replicacpp697 replica learned nop action at position 0 i0128 122027607853 297226240 logcpp675 writer started with ending position 0 i0128 122027608649 299372544 leveldbcpp436 reading position from leveldb took 158us i0128 122027609539 298835968 registrarcpp340 successfully fetched the registry 0b in 7426816ms i0128 122027609763 298835968 registrarcpp439 applied 1 operations in 54us attempting to update the registry i0128 122027610216 300982272 logcpp683 attempting to append 186 bytes to the log i0128 122027610297 298835968 coordinatorcpp348 coordinator attempting to write append action at position 1 i0128 122027611016 299909120 replicacpp537 replica received write request for position 1 from 182801921681782451278 i0128 122027611188 299909120 leveldbcpp341 persisting action 205 bytes to leveldb took 153us i0128 122027611222 299909120 replicacpp712 persisted action at 1 i0128 122027611843 299909120 replicacpp691 replica received learned notice for position 1 from 00000 i0128 122027612004 299909120 leveldbcpp341 persisting action 207 bytes to leveldb took 147us i0128 122027612035 299909120 replicacpp712 persisted action at 1 i0128 122027612052 299909120 replicacpp697 replica learned append action at position 1 i0128 122027612742 300982272 registrarcpp484 successfully updated the registry in 2924032ms i0128 122027612846 300982272 registrarcpp370 successfully recovered registrar i0128 122027612936 298835968 logcpp702 attempting to truncate the log to 1 i0128 122027613005 297762816 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0128 122027613323 298299392 mastercpp1520 recovered 0 slaves from the registry 147b  allowing 10mins for slaves to reregister i0128 122027613364 298835968 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0128 122027613966 300445696 replicacpp537 replica received write request for position 2 from 182811921681782451278 i0128 122027614131 300445696 leveldbcpp341 persisting action 16 bytes to leveldb took 151us i0128 122027614166 300445696 replicacpp712 persisted action at 2 i0128 122027614660 299372544 replicacpp691 replica received learned notice for position 2 from 00000 i0128 122027614828 299372544 leveldbcpp341 persisting action 18 bytes to leveldb took 158us i0128 122027614876 299372544 leveldbcpp399 deleting 1 keys from leveldb took 28us i0128 122027614898 299372544 replicacpp712 persisted action at 2 i0128 122027614915 299372544 replicacpp697 replica learned truncate action at position 2 i0128 122027625591 2080858880 containerizercpp143 using isolation posixcpuposixmemfilesystemposix i0128 122027629758 298299392 slavecpp192 slave started on 8711921681782451278 i0128 122027629791 298299392 slavecpp193 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 container_disk_watch_interval15secs containerizersmesos credentialtmpmasterquotatest_availableresourcesafterrescinding_gs9qcfcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmasterquotatest_availableresourcesafterrescinding_gs9qcffetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirusersalexprojectsmesosbuilddefaultsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirtmpmasterquotatest_availableresourcesafterrescinding_gs9qcf i0128 122027630067 298299392 credentialshpp83 loading credential for authentication from tmpmasterquotatest_availableresourcesafterrescinding_gs9qcfcredential i0128 122027630223 298299392 slavecpp323 slave using credential for testprincipal i0128 122027630360 298299392 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0128 122027630818 298299392 slavecpp463 slave resources cpus2 mem1024 disk1024 ports3100032000 i0128 122027630869 298299392 slavecpp471 slave attributes   i0128 122027630882 298299392 slavecpp476 slave hostname alexrfritzbox i0128 122027631352 300982272 statecpp58 recovering state from tmpmasterquotatest_availableresourcesafterrescinding_gs9qcfmeta i0128 122027631515 299909120 status_update_managercpp200 recovering status update manager i0128 122027631702 298835968 containerizercpp390 recovering containerizer i0128 122027632589 297226240 provisionercpp245 provisioner recovery complete i0128 122027632807 298835968 slavecpp4495 finished recovery i0128 122027633539 298835968 slavecpp4667 querying resource estimator for oversubscribable resources i0128 122027633752 300445696 status_update_managercpp174 pausing sending status updates i0128 122027633754 298835968 slavecpp795 new master detected at master1921681782451278 i0128 122027633806 298835968 slavecpp858 authenticating with master master1921681782451278 i0128 122027633824 298835968 slavecpp863 using default crammd5 authenticatee i0128 122027633903 298835968 slavecpp831 detecting new master i0128 122027633913 299372544 authenticateecpp121 creating new client sasl connection i0128 122027634016 298835968 slavecpp4681 received oversubscribable resources from the resource estimator i0128 122027634076 297226240 mastercpp5521 authenticating slave8711921681782451278 i0128 122027634130 299372544 authenticatorcpp413 starting authentication session for crammd5_authenticatee17411921681782451278 i0128 122027634255 297226240 authenticatorcpp98 creating new server sasl connection i0128 122027634348 300982272 authenticateecpp212 received sasl authentication mechanisms crammd5 i0128 122027634367 300982272 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0128 122027634454 298835968 authenticatorcpp203 received sasl authentication start i0128 122027634515 298835968 authenticatorcpp325 authentication requires more steps i0128 122027634572 298835968 authenticateecpp258 received sasl authentication step i0128 122027634706 297226240 authenticatorcpp231 received sasl authentication step i0128 122027634757 297226240 auxpropcpp107 request to lookup properties for user testprincipal realm alexrfritzbox server fqdn alexrfritzbox sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0128 122027634771 297226240 auxpropcpp179 looking up auxiliary property userpassword i0128 122027634793 297226240 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0128 122027634809 297226240 auxpropcpp107 request to lookup properties for user testprincipal realm alexrfritzbox server fqdn alexrfritzbox sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0128 122027634819 297226240 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0128 122027634827 297226240 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0128 122027634893 297226240 authenticatorcpp317 authentication success i0128 122027634958 298835968 authenticateecpp298 authentication success i0128 122027635030 298299392 mastercpp5551 successfully authenticated principal testprincipal at slave8711921681782451278 i0128 122027635079 300445696 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee17411921681782451278 i0128 122027635195 299372544 slavecpp926 successfully authenticated with master master1921681782451278 i0128 122027635273 299372544 slavecpp1320 will retry registration in 5823453ms if necessary i0128 122027635365 299909120 mastercpp4235 registering slave at slave8711921681782451278 alexrfritzbox with id 531344bd56f44e4f8f6fa6a9d36058c7s0 i0128 122027635542 297762816 registrarcpp439 applied 1 operations in 41us attempting to update the registry i0128 122027635889 299372544 logcpp683 attempting to append 358 bytes to the log i0128 122027636011 298299392 coordinatorcpp348 coordinator attempting to write append action at position 3 i0128 122027636693 300982272 replicacpp537 replica received write request for position 3 from 182951921681782451278 i0128 122027636860 300982272 leveldbcpp341 persisting action 377 bytes to leveldb took 139us i0128 122027636885 300982272 replicacpp712 persisted action at 3 i0128 122027637380 299909120 replicacpp691 replica received learned notice for position 3 from 00000 i0128 122027637547 299909120 leveldbcpp341 persisting action 379 bytes to leveldb took 132us i0128 122027637573 299909120 replicacpp712 persisted action at 3 i0128 122027637589 299909120 replicacpp697 replica learned append action at position 3 i0128 122027638362 298835968 registrarcpp484 successfully updated the registry in 277504ms i0128 122027638589 300445696 logcpp702 attempting to truncate the log to 3 i0128 122027638684 298299392 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0128 122027638825 300445696 slavecpp3435 received ping from slaveobserver8711921681782451278 i0128 122027639081 300982272 hierarchicalcpp473 added slave 531344bd56f44e4f8f6fa6a9d36058c7s0 alexrfritzbox with cpus2 mem1024 disk1024 ports3100032000 allocated  i0128 122027639117 299909120 mastercpp4303 registered slave 531344bd56f44e4f8f6fa6a9d36058c7s0 at slave8711921681782451278 alexrfritzbox with cpus2 mem1024 disk1024 ports3100032000 i0128 122027639165 300982272 hierarchicalcpp1403 no resources available to allocate i0128 122027639168 297226240 slavecpp970 registered with master master1921681782451278 given slave id 531344bd56f44e4f8f6fa6a9d36058c7s0 i0128 122027639189 297226240 fetchercpp81 clearing fetcher cache i0128 122027639183 300982272 hierarchicalcpp1116 performed allocation for slave 531344bd56f44e4f8f6fa6a9d36058c7s0 in 77us i0128 122027639348 297762816 status_update_managercpp181 resuming sending status updates i0128 122027639519 298835968 replicacpp537 replica received write request for position 4 from 182961921681782451278 i0128 122027639678 298835968 leveldbcpp341 persisting action 16 bytes to leveldb took 142us i0128 122027639708 298835968 replicacpp712 persisted action at 4 i0128 122027640115 300982272 replicacpp691 replica received learned notice for position 4 from 00000 i0128 122027640276 300982272 leveldbcpp341 persisting action 18 bytes to leveldb took 137us i0128 122027640312 300982272 leveldbcpp399 deleting 2 keys from leveldb took 21us i0128 122027640326 300982272 replicacpp712 persisted action at 4 i0128 122027640336 300982272 replicacpp697 replica learned truncate action at position 4 i0128 122027642145 297226240 slavecpp993 checkpointing slaveinfo to tmpmasterquotatest_availableresourcesafterrescinding_gs9qcfmetaslaves531344bd56f44e4f8f6fa6a9d36058c7s0slaveinfo i0128 122027643354 297226240 slavecpp1029 forwarding total oversubscribed resources i0128 122027643458 300445696 mastercpp4644 received update of slave 531344bd56f44e4f8f6fa6a9d36058c7s0 at slave8711921681782451278 alexrfritzbox with total oversubscribed resources i0128 122027643710 298299392 hierarchicalcpp531 slave 531344bd56f44e4f8f6fa6a9d36058c7s0 alexrfritzbox updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i0128 122027643769 298299392 hierarchicalcpp1403 no resources available to allocate i0128 122027643805 2,3
propose design doc for agent partitioning behavior,8
propose design doc for reliable floating point behavior,3
mesos agents needs to reresolve hosts in zk string on leader change  failure to connect sample mesos agent log httpsgistgithubcombrndnmtthwsfb846fa988487250a809 note zookeeper has a function to change the list of servers at runtime httpsgithubcomapachezookeeperblob735ea78909e67c648a4978c8d31d63964986af73srccsrczookeepercl1207l1232 this comes up when using an aws autoscalinggroup for managing the set of masters the agent when it comes up the first time resolves the zk string once all the hosts that were in the original string fail each fails is replaced by a new machine which has the same dns name the agent just keeps spinning in an internal loop never reresolving the dns names two solutions i see are 1 update the list of servers  reresolve 2 have the agent detect it hasnt connected recently and kill itself which will force a reresolution when the agent starts back up,3
investigate test suite crashes after zk socket disconnections showed up on asf ci httpsbuildsapacheorgjobmesoscompilerclangconfigurationverbose20enablelibevent20enablesslosubuntu1404label_expdocker7c7chadoop1579console the test crashed with the following logs code  run  contenttypeexecutorhttpapitestdefaultaccept1 i0129 020035137161 31926 leveldbcpp174 opened db in 118902333ms i0129 020035187021 31926 leveldbcpp181 compacted db in 49836241ms i0129 020035187088 31926 leveldbcpp196 created db iterator in 33825ns i0129 020035187109 31926 leveldbcpp202 seeked to beginning of db in 7965ns i0129 020035187121 31926 leveldbcpp271 iterated through 0 keys in the db in 6350ns i0129 020035187165 31926 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0129 020035188433 31950 recovercpp447 starting replica recovery i0129 020035188796 31950 recovercpp473 replica is in empty status i0129 020035190021 31949 replicacpp673 replica in empty status received a broadcasted recover request from 11817172170360904 i0129 020035190569 31958 recovercpp193 received a recover response from a replica in empty status i0129 020035190994 31959 recovercpp564 updating replica status to starting i0129 020035191522 31953 mastercpp374 master 823f2212bf284dd6959d796029d32afb 90665f991b70 started on 172170360904 i0129 020035191640 31953 mastercpp376 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpb9o6zqcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout25secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0280_instsharemesoswebui work_dirtmpb9o6zqmaster zk_session_timeout10secs i0129 020035191926 31953 mastercpp421 master only allowing authenticated frameworks to register i0129 020035191936 31953 mastercpp426 master only allowing authenticated slaves to register i0129 020035191943 31953 credentialshpp35 loading credentials for authentication from tmpb9o6zqcredentials i0129 020035192229 31953 mastercpp466 using default crammd5 authenticator i0129 020035192366 31953 mastercpp535 using default basic http authenticator i0129 020035192530 31953 mastercpp569 authorization enabled i0129 020035192719 31950 whitelist_watchercpp77 no whitelist given i0129 020035192756 31957 hierarchicalcpp144 initialized hierarchical allocator process i0129 020035194291 31955 mastercpp1710 the newly elected leader is master172170360904 with id 823f2212bf284dd6959d796029d32afb i0129 020035194335 31955 mastercpp1723 elected as the leading master i0129 020035194350 31955 mastercpp1468 recovering from registrar i0129 020035194545 31958 registrarcpp307 recovering registrar i0129 020035220226 31948 leveldbcpp304 persisting metadata 8 bytes to leveldb took 29150097ms i0129 020035220262 31948 replicacpp320 persisted replica status to starting i0129 020035220484 31959 recovercpp473 replica is in starting status i0129 020035221220 31954 replicacpp673 replica in starting status received a broadcasted recover request from 11819172170360904 i0129 020035221539 31959 recovercpp193 received a recover response from a replica in starting status i0129 020035221871 31954 recovercpp564 updating replica status to voting i0129 020035245329 31949 leveldbcpp304 persisting metadata 8 bytes to leveldb took 23326002ms i0129 020035245367 31949 replicacpp320 persisted replica status to voting i0129 020035245522 31955 recovercpp578 successfully joined the paxos group i0129 020035245800 31955 recovercpp462 recover process terminated i0129 020035246181 31951 logcpp659 attempting to start the writer i0129 020035247228 31953 replicacpp493 replica received implicit promise request from 11820172170360904 with proposal 1 i0129 020035270472 31953 leveldbcpp304 persisting metadata 8 bytes to leveldb took 23225846ms i0129 020035270510 31953 replicacpp342 persisted promised to 1 i0129 020035271306 31957 coordinatorcpp238 coordinator attempting to fill missing positions i0129 020035272373 31949 replicacpp388 replica received explicit promise request from 11821172170360904 for position 0 with proposal 2 i0129 020035295600 31949 leveldbcpp341 persisting action 8 bytes to leveldb took 23181008ms i0129 020035295639 31949 replicacpp712 persisted action at 0 i0129 020035296815 31950 replicacpp537 replica received write request for position 0 from 11822172170360904 i0129 020035296879 31950 leveldbcpp436 reading position from leveldb took 43203ns i0129 020035320659 31950 leveldbcpp341 persisting action 14 bytes to leveldb took 23753935ms i0129 020035320699 31950 replicacpp712 persisted action at 0 i0129 020035321394 31950 replicacpp691 replica received learned notice for position 0 from 00000 i0129 020035345837 31950 leveldbcpp341 persisting action 16 bytes to leveldb took 24358655ms i0129 020035345877 31950 replicacpp712 persisted action at 0 i0129 020035345898 31950 replicacpp697 replica learned nop action at position 0 i0129 020035346683 31950 logcpp675 writer started with ending position 0 i0129 020035347913 31957 leveldbcpp436 reading position from leveldb took 55621ns i0129 020035349047 31947 registrarcpp340 successfully fetched the registry 0b in 154395904ms i0129 020035349185 31947 registrarcpp439 applied 1 operations in 46347ns attempting to update the registry i0129 020035350008 31952 logcpp683 attempting to append 170 bytes to the log i0129 020035350132 31957 coordinatorcpp348 coordinator attempting to write append action at position 1 i0129 020035351042 31953 replicacpp537 replica received write request for position 1 from 11823172170360904 i0129 020035370906 31953 leveldbcpp341 persisting action 189 bytes to leveldb took 19829257ms i0129 020035370946 31953 replicacpp712 persisted action at 1 i0129 020035371840 31952 replicacpp691 replica received learned notice for position 1 from 00000 i0129 020035396082 31952 leveldbcpp341 persisting action 191 bytes to leveldb took 24218894ms i0129 020035396122 31952 replicacpp712 persisted action at 1 i0129 020035396144 31952 replicacpp697 replica learned append action at position 1 i0129 020035397250 31954 registrarcpp484 successfully updated the registry in 4799104ms i0129 020035397452 31954 registrarcpp370 successfully recovered registrar i0129 020035397678 31946 logcpp702 attempting to truncate the log to 1 i0129 020035397881 31956 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0129 020035398066 31951 mastercpp1520 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0129 020035398111 31957 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0129 020035398982 31955 replicacpp537 replica received write request for position 2 from 11824172170360904 i0129 020035421293 31955 leveldbcpp341 persisting action 16 bytes to leveldb took 22286476ms i0129 020035421339 31955 replicacpp712 persisted action at 2 i0129 020035422046 31944 replicacpp691 replica received learned notice for position 2 from 00000 i0129 020035446316 31944 leveldbcpp341 persisting action 18 bytes to leveldb took 24246177ms i0129 020035446406 31944 leveldbcpp399 deleting 1 keys from leveldb took 84415ns i0129 020035446466 31944 replicacpp712 persisted action at 2 i0129 020035446491 31944 replicacpp697 replica learned truncate action at position 2 i0129 020035452579 31957 slavecpp192 slave started on 372172170360904 i0129 020035452620 31957 slavecpp193 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpcontenttype_executorhttpapitest_defaultaccept_1_r4guhmcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpcontenttype_executorhttpapitest_defaultaccept_1_r4guhmfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0280_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpcontenttype_executorhttpapitest_defaultaccept_1_r4guhm i0129 020035453012 31957 credentialshpp83 loading credential for authentication from tmpcontenttype_executorhttpapitest_defaultaccept_1_r4guhmcredential i0129 020035453191 31957 slavecpp323 slave using credential for testprincipal i0129 020035453368 31957 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0129 020035453853 31957 slavecpp463 slave resources cpus2 mem1024 disk1024 ports3100032000 i0129 020035453938 31957 slavecpp471 slave attributes   i0129 020035453953 31957 slavecpp476 slave hostname 90665f991b70 i0129 020035454794 31950 statecpp58 recovering state from tmpcontenttype_executorhttpapitest_defaultaccept_1_r4guhmmeta i0129 020035455080 31948 status_update_managercpp200 recovering status update manager i0129 020035455225 31926 schedcpp222 version 0280 i0129 020035455535 31956 slavecpp4495 finished recovery i0129 020035455798 31945 schedcpp326 new master detected at master172170360904 i0129 020035455879 31945 schedcpp382 authenticating with master master172170360904 i0129 020035455904 31945 schedcpp389 using default crammd5 authenticatee i0129 020035455943 31956 slavecpp4667 querying resource estimator for oversubscribable resources i0129 020035456167 31950 authenticateecpp121 creating new client sasl connection i0129 020035456218 31953 status_update_managercpp174 pausing sending status updates i0129 020035456219 31956 slavecpp795 new master detected at master172170360904 i0129 020035456298 31956 slavecpp858 authenticating with master master172170360904 i0129 020035456323 31956 slavecpp863 using default crammd5 authenticatee i0129 020035456490 31948 authenticateecpp121 creating new client sasl connection i0129 020035456492 31956 slavecpp831 detecting new master i0129 020035456588 31946 mastercpp5521 authenticating scheduler93e745f00e484a8fb22793569976c5e8172170360904 i0129 020035456686 31956 slavecpp4681 received oversubscribable resources from the resource estimator i0129 020035456805 31953 authenticatorcpp413 starting authentication session for crammd5_authenticatee804172170360904 i0129 020035456878 31946 mastercpp5521 authenticating slave372172170360904 i0129 020035457124 31953 authenticatorcpp413 starting authentication session for crammd5_authenticatee805172170360904 i0129 020035457157 31948 authenticatorcpp98 creating new server sasl connection i0129 020035457373 31946 authenticateecpp212 received sasl authentication mechanisms crammd5 i0129 020035457381 31951 authenticatorcpp98 creating new server sasl connection i0129 020035457491 31946 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0129 020035457598 31946 authenticateecpp212 received sasl authentication mechanisms crammd5 i0129 020035457612 31951 authenticatorcpp203 received sasl authentication start i0129 020035457635 31946 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0129 020035457680 31951 authenticatorcpp325 authentication requires more steps i0129 020035457767 31954 authenticatorcpp203 received sasl authentication start i0129 020035457768 31948 authenticateecpp258 received sasl authentication step i0129 020035457830 31954 authenticatorcpp325 authentication requires more steps i0129 020035457885 31948 authenticatorcpp231 received sasl authentication step i0129 020035457918 31948 auxpropcpp107 request to lookup properties for user testprincipal realm 90665f991b70 server fqdn 90665f991b70 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0129 020035457933 31948 auxpropcpp179 looking up auxiliary property userpassword i0129 020035457954 31959 authenticateecpp258 received sasl authentication step i0129 020035457993 31948 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0129 020035458031 31948 auxpropcpp107 request to lookup properties for user testprincipal realm 90665f991b70 server fqdn 90665f991b70 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0129 020035458050 31948 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0129 020035458065 31948 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0129 020035458096 31948 authenticatorcpp317 authentication success i0129 020035458112 31944 authenticatorcpp231 received sasl authentication step i0129 020035458142 31944 auxpropcpp107 request to lookup properties for user testprincipal realm 90665f991b70 server fqdn 90665f991b70 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0129 020035458173 31944 auxpropcpp179 looking up auxiliary property userpassword i0129 020035458206 31954 authenticateecpp298 authentication success i0129 020035458256 31957 mastercpp5551 successfully authenticated principal testprincipal at scheduler93e745f00e484a8fb22793569976c5e8172170360904 i0129 020035458206 31944 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0129 020035458360 31944 auxpropcpp107 request to lookup properties for user testprincipal realm 90665f991b70 server fqdn 90665f991b70 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0129 020035458382 31944 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0129 020035458397 31944 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0129 020035458489 31944 authenticatorcpp317 authentication success i0129 020035458623 31953 schedcpp471 successfully authenticated with master master172170360904 i0129 020035458649 31953 schedcpp780 sending subscribe call to master172170360904 i0129 020035458653 31956 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee804172170360904 i0129 020035458673 31951 authenticateecpp298 authentication success i0129 020035458709 31952 mastercpp5551 successfully authenticated principal testprincipal at slave372172170360904 i0129 020035458906 31955 slavecpp926 successfully authenticated with master master172170360904 i0129 020035458983 31956 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee805172170360904 i0129 020035459033 31955 slavecpp1320 will retry registration in 7075135ms if necessary i0129 020035459128 31953 schedcpp813 will retry registration in 86579738ms if necessary i0129 020035459193 31950 mastercpp4235 registering slave at slave372172170360904 90665f991b70 with id 823f2212bf284dd6959d796029d32afbs0 i0129 020035459489 31950 mastercpp2278 received subscribe call for framework default at scheduler93e745f00e484a8fb22793569976c5e8172170360904 i0129 020035459513 31950 mastercpp1749 authorizing framework principal testprincipal to receive offers for role  i0129 020035459516 31959 registrarcpp439 applied 1 operations in 62499ns attempting to update the registry i0129 020035459766 31956 mastercpp2349 subscribing framework default with checkpointing disabled and capabilities   i0129 020035460095 31955 logcpp683 attempting to append 339 bytes to the log i0129 020035460192 31948 hierarchicalcpp265 added framework 823f2212bf284dd6959d796029d32afb0000 i0129 020035460247 31956 schedcpp707 framework registered with 823f2212bf284dd6959d796029d32afb0000 i0129 020035460314 31958 coordinatorcpp348 coordinator attempting to write append action at position 3 i0129 020035460388 31948 hierarchicalcpp1403 no resources available to allocate i0129 020035460449 31948 hierarchicalcpp1498 no inverse offers to send out i0129 020035460402 31956 schedcpp721 schedulerregistered took 136519ns i0129 020035460482 31948 hierarchicalcpp1096 performed allocation for 0 slaves in 158218ns i0129 020035461187 31944 replicacpp537 replica received write request for position 3 from 11829172170360904 i0129 020035467929 31954 slavecpp1320 will retry registration in 14701381ms if necessary i0129 020035468183 31952 mastercpp4223 ignoring register slave message from slave372172170360904 90665f991b70 as admission is already in progress i0129 020035483300 31959 slavecpp1320 will retry registration in 8003223ms if necessary i0129 020035483500 31946 mastercpp4223 ignoring register slave message from slave372172170360904 90665f991b70 as admission is already in progress i0129 020035491843 31945 slavecpp1320 will retry registration in 52952447ms if necessary i0129 020035491962 31948 mastercpp4223,3
automatically generate commandline flag documentation to ensure that the commandline flag documentation in configurationmd stays in sync with the help strings in the various flagscpp files it could be beneficial to automate the generation of those docs such a script could be run as part of the build process ensuring that changes to the help strings would show up in the documentation as well in addition to parsing and formatting the help strings for display as html this could also involve specifying collections of flags to be grouped together in order to provide logical structure to the configurationmd documentation,3
reduce the running time of benchmark tests currently benchmark tests take a long time 5 hours it would be nice to reduce the total time taken by the benchmark tests to enable us to run them on asf ci command to run only benchmark tests code mesos_benchmark1 gtest_filterbenchmark make check code,2
run benchmark tests in asf ci the build job is already created on asf ci httpsbuildsapacheorgjobmesosbenchmarks but is currently disabled due to mesos4558,2
mesos ui shows wrong count for started tasks the task started field shows the number of tasks in state tasks_starting as opposed to those in task_running state,2
separate appc protobuf messages to its own file it would be cleaner to keep the appc protobuf messages separate from other mesos messages,2
avoid unnecessary temporary stdstring constructions and copies in jsonify a few of the critical code paths in jsonify involve unnecessary temporary string construction and copies inherited from the json for example stringstrim is used to remove trailing 0s from printing doubles we print doubles a lot and therefore constructing a temporary stdstring on printing of every double is extremely costly this ticket captures the work involved in avoiding them,1
deprecate task_starting state we currently have the following task stages  task_staging  set by slave  task_starting  set by the executor   task_running  set by the executor when the task is running  task_xxx  task termination statuses the confusion here is about task_starting this is the state between task_staging and task_running and is somewhat nonintuitive for the reader further looks like no where in the source code we are setting the task_starting state why shouldnt we just deprecateremove it,2
dockerfetcherplugintestinternet_curl_fetchimage seems flaky noformat configure enablessl enablelibevent  make check noformat noformat gtest_repeat1 gtest_break_on_failure gtest_filterdockerfetcherplugintestinternet_curl_fetchimage noformat failed at the 22nd run noformat  run  dockerfetcherplugintestinternet_curl_fetchimage srctestsuri_fetcher_testscpp276 failure failed to wait 15secs for fetchergetfetchuri dir  aborted at 1454207653 unix time try date d 1454207653 if you are using gnu date  pc  0x167023a testingunittestaddtestpartresult  sigsegv 0x0 received by pid 19868 tid 0x7f500fc877c0 from pid 0 stack trace   0x7f5008f368d0 unknown  0x167023a testingunittestaddtestpartresult  0x1664c73 testinginternalasserthelperoperator  0x146ac6f mesosinternaltestsdockerfetcherplugintest_internet_curl_fetchimage_testtestbody  0x168dc70 testinginternalhandlesehexceptionsinmethodifsupported  0x1688cc8 testinginternalhandleexceptionsinmethodifsupported  0x166a013 testingtestrun  0x166a7a1 testingtestinforun  0x166addc testingtestcaserun  0x167172b testinginternalunittestimplrunalltests  0x168e8ff testinginternalhandlesehexceptionsinmethodifsupported  0x168981e testinginternalhandleexceptionsinmethodifsupported  0x167045b testingunittestrun  0xe2d476 run_all_tests  0xe2d08c main  0x7f5008b9fb45 unknown  0x9c6bf9 unknown noformat,1
design doc for scheduler http stream ids this ticket is for the design of http stream ids for use with http schedulers these ids allow mesos to distinguish between different instances of http framework schedulers,5
fix appc image caching to share with image fetcher as appc image fetcher is being developed image cache needs to be shared between store and the image fetcher,3
introduce a stout helper for which we may want to add a helper to stoutoshpp that will natively emulate the functionality of the linux utility which ie code optionstring whichconst string command  optionstring path  osgetenvpath  loop through path and return the first one which osexists return none  code this helper may be useful  for test filters in srctestsenvironmentcpp  a few tests in srctestscontainerizerport_mapping_testscpp  the sha512 utility in srccommoncommand_utilscpp  as runtime checks in the logrotatecontainerlogger  etc,2
statejson serving duplicate active fields statejson is serving duplicate active fields in frameworks see the framework 47df96c23f854bc5b781709b2c30c7520000 in the attached file,1
rename examplesevent_call_frameworkcpp to examplestest_http_frameworkcpp we already have examplestest_frameworkcpp for testing pid based frameworks we would ideally want to rename event_call_framework to correctly reflect that its an example for http based framework,1
update rakefile for mesos site generation the stuff in site directory needs some updates to make it easier to generate updates for mesosapacheorg site,2
add test case for reservations with same role different principals we dont have a test case that covers subject we probably should,2
reserve and createvolumes endpoints allow operations for any role when frameworks reserve resources the validation of the operation ensures that the role of the reservation matches the role of the framework for the case of the reserve operator endpoint however the operator has no role to validate so this check isnt performed this means that if an acl exists which authorizes a frameworks principal to reserve resources that same principal can be used to reserve resources for _any_ role through the operator endpoint we should restrict reservations made through the operator endpoint to specified roles a few possibilities  the object of the reserve_resources acl could be changed from resources to roles  a second acl could be added for authorization of reserve operations with an object of role  our conception of the resources object in the reserve_resources acl could be expanded to include role information ie diskrole1memrole1,3
add common appc spec utilities add common utility functions such as   validating image information against actual data in the image directory  getting list of dependencies at depth 1 for an image  getting image path simple image discovery,2
logrotate containerlogger should not remove ip from environment the logrotatecontainerlogger starts libprocessusing subprocesses libprocess initialization will attempt to resolve the ip from the hostname if a dns service is not available this step will fail which terminates the logger subprocess prematurely since the logger subprocesses live on the agent they should use the same libprocess_ip supplied to the agent,1
use stdquoted for strings in error messages wed like to have a consistent format for error strings through the code base as per this comment mesos3772httpsissuesapacheorgjirabrowsemesos3772focusedcommentid14965652pagecomatlassianjirapluginsystemissuetabpanelscommenttabpanelcomment14965652 we can then overload the stream operator to make sur strings are quoted as needed note we need to first require compilers that support c14 for now we have to wait for msvc to be part of that list,3
root_docker_dockerhealthytask is flaky log from teamcity that is running sudo binmesostestssh on aws ec2 instances noformat 182714step 88  8 tests from healthchecktest 182714step 88  run  healthchecktesthealthytask 182717step 88  ok  healthchecktesthealthytask 2222 ms 182717step 88  run  healthchecktestroot_docker_dockerhealthytask 182736step 88 srctestshealth_check_testscpp388 failure 182736step 88 failed to wait 15secs for termination 182736step 88 f0204 182735981302 23085 loggingcpp64 raw pure virtual method called 182736step 88  0x7f7077055e1c googlelogmessagefail 182736step 88  0x7f707705ba6f googlerawlog__ 182736step 88  0x7f70760f76c9 __cxa_pure_virtual 182736step 88  0xa9423c mesosinternaltestsclusterslavesshutdown 182736step 88  0x1074e45 mesosinternaltestsmesostestshutdownslaves 182736step 88  0x1074de4 mesosinternaltestsmesostestshutdown 182736step 88  0x1070ec7 mesosinternaltestsmesostestteardown 182736step 88  0x16eb7b2 testinginternalhandlesehexceptionsinmethodifsupported 182736step 88  0x16e61a9 testinginternalhandleexceptionsinmethodifsupported 182736step 88  0x16c56aa testingtestrun 182736step 88  0x16c5e89 testingtestinforun 182736step 88  0x16c650a testingtestcaserun 182736step 88  0x16cd1f6 testinginternalunittestimplrunalltests 182736step 88  0x16ec513 testinginternalhandlesehexceptionsinmethodifsupported 182736step 88  0x16e6df1 testinginternalhandleexceptionsinmethodifsupported 182736step 88  0x16cbe26 testingunittestrun 182736step 88  0xe54c84 run_all_tests 182736step 88  0xe54867 main 182736step 88  0x7f7071560a40 unknown 182736step 88  0x9b52d9 _start 182736step 88 aborted core dumped 182736step 88 process exited with code 134 noformat happens with ubuntu 1504 centos 6 centos 7 _quite_ often,2
subprocess should be more intelligent about settinginheriting libprocess environment variables mostly copied from this commenthttpsissuesapacheorgjirabrowsemesos4598focusedcommentid15133497pagecomatlassianjirapluginsystemissuetabpanelscommenttabpanelcomment15133497 a subprocess inheriting the environment variables libprocess_ may run into some accidental fatalities   subprocess uses libprocess  subprocess is something else   subprocess setsinherits the same port by accident  bind failure  exit  nothing happens    subprocess sets a different port on purpose  bind success   nothing happens     means this is usually the case but not 100 a complete fix would look something like  if the subprocess call gets environment  none we should automatically remove libprocess_port from the inherited environment  the parts of executorenvironmenthttpsgithubcomapachemesosblamemastersrcslavecontainerizercontainerizercppl265 dealing with libprocess  libmesos should be refactored into libprocess as a helper we would use this helper for the containerizer fetcher and containerlogger module  if the subprocess call is given libprocess_port  osgetenvlibprocess_port we can logwarn and unset the env var locally,2
passing a lambda to dispatch always matches the template returning void the following idiom does not currently compile code futurenothing initialized  dispatchpid    nothing  return nothing  code this seems nonintuitive because the following template exists for dispatch code template typename r futurer dispatchconst upid pid const stdfunctionr f  stdshared_ptrpromiser promisenew promiser stdshared_ptrstdfunctionvoidprocessbase f_ new stdfunctionvoidprocessbase processbase  promisesetf  internaldispatchpid f_ return promisefuture  code however lambdas cannot be implicitly cast to a corresponding stdfunctionr type to make this work you have to explicitly type the lambda before passing it to dispatch code stdfunctionnothing f    return nothing  futurenothing initialized  dispatchpid f code we should add template support to allow lambdas to be passed to dispatch without explicit typing,5
update vendored zookeeper to 348 see httpzookeeperapacheorgdocr348releasenoteshtml for improvements  bug fixes added a new patch that solved zookeeper1643httpsissuesapacheorgjirabrowsezookeeper1643 the original patch httpsgithubcomapachezookeepercommit46b565e6abd8423c43f1bb8da782d76bac7c392c,3
slaverecoverytest0cleanuphttpexecutor is flaky just saw this failure on the asf ci code  run  slaverecoverytest0cleanuphttpexecutor i0206 002244791671 2824 leveldbcpp174 opened db in 2539372ms i0206 002244792459 2824 leveldbcpp181 compacted db in 740473ns i0206 002244792510 2824 leveldbcpp196 created db iterator in 24164ns i0206 002244792532 2824 leveldbcpp202 seeked to beginning of db in 1831ns i0206 002244792548 2824 leveldbcpp271 iterated through 0 keys in the db in 342ns i0206 002244792605 2824 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0206 002244793256 2847 recovercpp447 starting replica recovery i0206 002244793480 2847 recovercpp473 replica is in empty status i0206 002244794538 2847 replicacpp673 replica in empty status received a broadcasted recover request from 9472172170243484 i0206 002244795040 2848 recovercpp193 received a recover response from a replica in empty status i0206 002244795644 2848 recovercpp564 updating replica status to starting i0206 002244796519 2850 leveldbcpp304 persisting metadata 8 bytes to leveldb took 752810ns i0206 002244796545 2850 replicacpp320 persisted replica status to starting i0206 002244796725 2848 recovercpp473 replica is in starting status i0206 002244797828 2857 replicacpp673 replica in starting status received a broadcasted recover request from 9473172170243484 i0206 002244798355 2850 recovercpp193 received a recover response from a replica in starting status i0206 002244799193 2850 recovercpp564 updating replica status to voting i0206 002244799583 2855 mastercpp376 master 0b206a40a9c34d44a5bd8032d60a32ca 6632562f1ade started on 172170243484 i0206 002244799609 2855 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpn2fxqvcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0280_instsharemesoswebui work_dirtmpn2fxqvmaster zk_session_timeout10secs i0206 002244799991 2855 mastercpp423 master only allowing authenticated frameworks to register i0206 002244800009 2855 mastercpp428 master only allowing authenticated slaves to register i0206 002244800020 2855 credentialshpp35 loading credentials for authentication from tmpn2fxqvcredentials i0206 002244800245 2850 leveldbcpp304 persisting metadata 8 bytes to leveldb took 679345ns i0206 002244800370 2850 replicacpp320 persisted replica status to voting i0206 002244800397 2855 mastercpp468 using default crammd5 authenticator i0206 002244800693 2855 mastercpp537 using default basic http authenticator i0206 002244800815 2855 mastercpp571 authorization enabled i0206 002244801216 2850 recovercpp578 successfully joined the paxos group i0206 002244801604 2850 recovercpp462 recover process terminated i0206 002244801759 2856 whitelist_watchercpp77 no whitelist given i0206 002244801725 2847 hierarchicalcpp144 initialized hierarchical allocator process i0206 002244803982 2855 mastercpp1712 the newly elected leader is master172170243484 with id 0b206a40a9c34d44a5bd8032d60a32ca i0206 002244804026 2855 mastercpp1725 elected as the leading master i0206 002244804059 2855 mastercpp1470 recovering from registrar i0206 002244804424 2855 registrarcpp307 recovering registrar i0206 002244805202 2855 logcpp659 attempting to start the writer i0206 002244806782 2856 replicacpp493 replica received implicit promise request from 9475172170243484 with proposal 1 i0206 002244807368 2856 leveldbcpp304 persisting metadata 8 bytes to leveldb took 547939ns i0206 002244807395 2856 replicacpp342 persisted promised to 1 i0206 002244808375 2856 coordinatorcpp238 coordinator attempting to fill missing positions i0206 002244809460 2848 replicacpp388 replica received explicit promise request from 9476172170243484 for position 0 with proposal 2 i0206 002244809929 2848 leveldbcpp341 persisting action 8 bytes to leveldb took 427561ns i0206 002244809967 2848 replicacpp712 persisted action at 0 i0206 002244811035 2850 replicacpp537 replica received write request for position 0 from 9477172170243484 i0206 002244811149 2850 leveldbcpp436 reading position from leveldb took 36452ns i0206 002244811532 2850 leveldbcpp341 persisting action 14 bytes to leveldb took 318924ns i0206 002244811615 2850 replicacpp712 persisted action at 0 i0206 002244812532 2850 replicacpp691 replica received learned notice for position 0 from 00000 i0206 002244813117 2850 leveldbcpp341 persisting action 16 bytes to leveldb took 476530ns i0206 002244813143 2850 replicacpp712 persisted action at 0 i0206 002244813166 2850 replicacpp697 replica learned nop action at position 0 i0206 002244813984 2848 logcpp675 writer started with ending position 0 i0206 002244815549 2848 leveldbcpp436 reading position from leveldb took 31800ns i0206 002244817061 2848 registrarcpp340 successfully fetched the registry 0b in 12591104ms i0206 002244817319 2848 registrarcpp439 applied 1 operations in 63480ns attempting to update the registry i0206 002244818780 2845 logcpp683 attempting to append 170 bytes to the log i0206 002244818981 2845 coordinatorcpp348 coordinator attempting to write append action at position 1 i0206 002244819941 2845 replicacpp537 replica received write request for position 1 from 9478172170243484 i0206 002244820582 2845 leveldbcpp341 persisting action 189 bytes to leveldb took 600949ns i0206 002244820608 2845 replicacpp712 persisted action at 1 i0206 002244821552 2845 replicacpp691 replica received learned notice for position 1 from 00000 i0206 002244821934 2845 leveldbcpp341 persisting action 191 bytes to leveldb took 352813ns i0206 002244821960 2845 replicacpp712 persisted action at 1 i0206 002244821979 2845 replicacpp697 replica learned append action at position 1 i0206 002244823447 2845 registrarcpp484 successfully updated the registry in 5987072ms i0206 002244823580 2845 registrarcpp370 successfully recovered registrar i0206 002244823833 2845 logcpp702 attempting to truncate the log to 1 i0206 002244824203 2845 mastercpp1522 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0206 002244824291 2845 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0206 002244824645 2845 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0206 002244825222 2850 replicacpp537 replica received write request for position 2 from 9479172170243484 i0206 002244825742 2850 leveldbcpp341 persisting action 16 bytes to leveldb took 481617ns i0206 002244825772 2850 replicacpp712 persisted action at 2 i0206 002244826748 2852 replicacpp691 replica received learned notice for position 2 from 00000 i0206 002244827368 2852 leveldbcpp341 persisting action 18 bytes to leveldb took 588591ns i0206 002244827432 2852 leveldbcpp399 deleting 1 keys from leveldb took 33059ns i0206 002244827450 2852 replicacpp712 persisted action at 2 i0206 002244827468 2852 replicacpp697 replica learned truncate action at position 2 i0206 002244838011 2824 containerizercpp149 using isolation posixcpuposixmemfilesystemposix w0206 002244838873 2824 backendcpp48 failed to create bind backend bindbackend requires root privileges i0206 002244843785 2857 slavecpp193 slave started on 172170243484 i0206 002244843819 2857 slavecpp194 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpslaverecoverytest_0_cleanuphttpexecutor_kaxwvwcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpslaverecoverytest_0_cleanuphttpexecutor_kaxwvwfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0280_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpslaverecoverytest_0_cleanuphttpexecutor_kaxwvw i0206 002244844292 2857 credentialshpp83 loading credential for authentication from tmpslaverecoverytest_0_cleanuphttpexecutor_kaxwvwcredential i0206 002244844518 2857 slavecpp324 slave using credential for testprincipal i0206 002244844696 2857 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0206 002244845243 2857 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 i0206 002244845326 2857 slavecpp472 slave attributes   i0206 002244845342 2857 slavecpp477 slave hostname 6632562f1ade i0206 002244845953 2824 schedcpp222 version 0280 i0206 002244846853 2848 schedcpp326 new master detected at master172170243484 i0206 002244846936 2848 schedcpp382 authenticating with master master172170243484 i0206 002244846958 2848 schedcpp389 using default crammd5 authenticatee i0206 002244847692 2858 statecpp58 recovering state from tmpslaverecoverytest_0_cleanuphttpexecutor_kaxwvwmeta i0206 002244848108 2850 status_update_managercpp200 recovering status update manager i0206 002244848325 2852 containerizercpp397 recovering containerizer i0206 002244848603 2845 authenticateecpp121 creating new client sasl connection i0206 002244849719 2845 mastercpp5523 authenticating scheduler63899759d7fc42b2837157484f352895172170243484 i0206 002244850052 2852 authenticatorcpp413 starting authentication session for crammd5_authenticatee662172170243484 i0206 002244850227 2854 provisionercpp245 provisioner recovery complete i0206 002244850410 2852 authenticatorcpp98 creating new server sasl connection i0206 002244850692 2852 authenticateecpp212 received sasl authentication mechanisms crammd5 i0206 002244850720 2852 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0206 002244850805 2852 authenticatorcpp203 received sasl authentication start i0206 002244850862 2852 authenticatorcpp325 authentication requires more steps i0206 002244850939 2852 authenticateecpp258 received sasl authentication step i0206 002244851027 2852 authenticatorcpp231 received sasl authentication step i0206 002244851052 2852 auxpropcpp107 request to lookup properties for user testprincipal realm 6632562f1ade server fqdn 6632562f1ade sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0206 002244851063 2852 auxpropcpp179 looking up auxiliary property userpassword i0206 002244851102 2852 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0206 002244851121 2852 auxpropcpp107 request to lookup properties for user testprincipal realm 6632562f1ade server fqdn 6632562f1ade sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0206 002244851130 2852 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0206 002244851136 2852 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0206 002244851150 2852 authenticatorcpp317 authentication success i0206 002244851219 2850 authenticateecpp298 authentication success i0206 002244851310 2850 mastercpp5553 successfully authenticated principal testprincipal at scheduler63899759d7fc42b2837157484f352895172170243484 i0206 002244851485 2849 slavecpp4496 finished recovery i0206 002244852154 2843 schedcpp471 successfully authenticated with master master172170243484 i0206 002244852175 2843 schedcpp776 sending subscribe call to master172170243484 i0206 002244852262 2843 schedcpp809 will retry registration in 939183679ms if necessary i0206 002244852375 2844 mastercpp2280 received subscribe call for framework default at scheduler63899759d7fc42b2837157484f352895172170243484 i0206 002244852448 2844 mastercpp1751 authorizing framework principal testprincipal to receive offers for role  i0206 002244852699 2852 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee662172170243484 i0206 002244852782 2844 mastercpp2351 subscribing framework default with checkpointing enabled and capabilities   i0206 002244853056 2849 slavecpp4668 querying resource estimator for oversubscribable resources i0206 002244853421 2856 hierarchicalcpp265 added framework 0b206a40a9c34d44a5bd8032d60a32ca0000 i0206 002244853513 2856 hierarchicalcpp1403 no resources available to allocate i0206 002244853582 2844 schedcpp703 framework registered with 0b206a40a9c34d44a5bd8032d60a32ca0000 i0206 002244853613 2852 slavecpp4682 received oversubscribable resources from the resource estimator i0206 002244853663 2844 schedcpp717 schedulerregistered took 53762ns i0206 002244853899 2843 slavecpp796 new master detected at master172170243484 i0206 002244853955 2854 status_update_managercpp174 pausing sending status updates i0206 002244853997 2856 hierarchicalcpp1498 no inverse offers to send out i0206 002244853960 2843 slavecpp859 authenticating with master master172170243484 i0206 002244854035 2843 slavecpp864 using default crammd5 authenticatee i0206 002244854030 2856 hierarchicalcpp1096 performed allocation for 0 slaves in 581355ns i0206 002244854182 2843 slavecpp832 detecting new master i0206 002244854277 2854 authenticateecpp121 creating new client sasl connection i0206 002244854517 2843 mastercpp5523 authenticating slave172170243484 i0206 002244854603 2854 authenticatorcpp413 starting authentication session for crammd5_authenticatee663172170243484 i0206 002244854836 2855 authenticatorcpp98 creating new server sasl connection i0206 002244855013 2852 authenticateecpp212 received sasl authentication mechanisms crammd5 i0206 002244855044 2852 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0206 002244855139 2855 authenticatorcpp203 received sasl authentication start i0206 002244855186 2855 authenticatorcpp325 authentication requires more steps i0206 002244855263 2855 authenticateecpp258 received sasl authentication step i0206 002244855352 2855 authenticatorcpp231 received sasl authentication step i0206 002244855381 2855 auxpropcpp107 request to lookup properties for user testprincipal realm 6632562f1ade server fqdn 6632562f1ade sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0206 002244855389 2855 auxpropcpp179 looking up auxiliary property userpassword i0206 002244855419 2855 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0206 002244855438 2855 auxpropcpp107 request to lookup properties for user testprincipal realm 6632562f1ade server fqdn 6632562f1ade sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0206 002244855448 2855 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0206 002244855453 2855 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0206 002244855464 2855 authenticatorcpp317 authentication success i0206 002244855540 2851 authenticateecpp298 authentication success i0206 002244855721 2851 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee663172170243484 i0206 002244855832 2852 slavecpp927 successfully authenticated with master master172170243484 i0206 002244855615 2855 mastercpp5553 successfully authenticated principal testprincipal at slave172170243484 i0206 002244855973 2852 slavecpp1321 will retry registration in 9327708ms if necessary i0206 002244856145 2854 mastercpp4237 registering slave at slave172170243484 6632562f1ade with id 0b206a40a9c34d44a5bd8032d60a32cas0 i0206 002244856598 2851 registrarcpp439 applied 1 operations in 59112ns attempting to update the registry i0206 002244857403 2851 logcpp683 attempting to append 339 bytes to the log i0206 002244857525 2855 coordinatorcpp348 coordinator attempting to write append action at position 3 i0206 002244858482 2844 replicacpp537 replica received write request for position 3 from 9493172170243484 i0206 002244858755 2844 leveldbcpp341 persisting action 358 bytes to leveldb took 228484ns i0206 002244858855 2844 replicacpp712 persisted action at 3 i0206 002244859751 2852 replicacpp691 replica received learned notice for position 3 from 00000 i0206 002244860332 2852 leveldbcpp341 persisting action 360 bytes to leveldb took 549638ns i0206 002244860358 2852 replicacpp712 persisted action at 3 i0206 002244860411 2852 replicacpp697 replica learned append action at position 3 i0206 002244862709 2856 registrarcpp484 succe,3
containerloggertestdefaulttosandbox is flaky just saw this failure on the asf ci code  run  containerloggertestdefaulttosandbox i0206 012503766458 2824 leveldbcpp174 opened db in 72979786ms i0206 012503811712 2824 leveldbcpp181 compacted db in 45162067ms i0206 012503811810 2824 leveldbcpp196 created db iterator in 26090ns i0206 012503811828 2824 leveldbcpp202 seeked to beginning of db in 3173ns i0206 012503811839 2824 leveldbcpp271 iterated through 0 keys in the db in 497ns i0206 012503811900 2824 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0206 012503812785 2849 recovercpp447 starting replica recovery i0206 012503813043 2849 recovercpp473 replica is in empty status i0206 012503814668 2854 replicacpp673 replica in empty status received a broadcasted recover request from 371172170837843 i0206 012503815210 2849 recovercpp193 received a recover response from a replica in empty status i0206 012503815732 2854 recovercpp564 updating replica status to starting i0206 012503819664 2857 mastercpp376 master 914b62f995f64c57a7e39b06e2c1c8de 74ef606c4063 started on 172170837843 i0206 012503819703 2857 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmph5vu5icredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0280_instsharemesoswebui work_dirtmph5vu5imaster zk_session_timeout10secs i0206 012503820241 2857 mastercpp423 master only allowing authenticated frameworks to register i0206 012503820257 2857 mastercpp428 master only allowing authenticated slaves to register i0206 012503820269 2857 credentialshpp35 loading credentials for authentication from tmph5vu5icredentials i0206 012503821110 2857 mastercpp468 using default crammd5 authenticator i0206 012503821311 2857 mastercpp537 using default basic http authenticator i0206 012503821636 2857 mastercpp571 authorization enabled i0206 012503821979 2846 hierarchicalcpp144 initialized hierarchical allocator process i0206 012503822057 2846 whitelist_watchercpp77 no whitelist given i0206 012503825460 2847 mastercpp1712 the newly elected leader is master172170837843 with id 914b62f995f64c57a7e39b06e2c1c8de i0206 012503825512 2847 mastercpp1725 elected as the leading master i0206 012503825533 2847 mastercpp1470 recovering from registrar i0206 012503825835 2847 registrarcpp307 recovering registrar i0206 012503848212 2854 leveldbcpp304 persisting metadata 8 bytes to leveldb took 32226093ms i0206 012503848299 2854 replicacpp320 persisted replica status to starting i0206 012503848702 2854 recovercpp473 replica is in starting status i0206 012503850728 2858 replicacpp673 replica in starting status received a broadcasted recover request from 373172170837843 i0206 012503851230 2854 recovercpp193 received a recover response from a replica in starting status i0206 012503852018 2854 recovercpp564 updating replica status to voting i0206 012503881681 2854 leveldbcpp304 persisting metadata 8 bytes to leveldb took 29184163ms i0206 012503881772 2854 replicacpp320 persisted replica status to voting i0206 012503882058 2854 recovercpp578 successfully joined the paxos group i0206 012503882258 2854 recovercpp462 recover process terminated i0206 012503883076 2854 logcpp659 attempting to start the writer i0206 012503885040 2854 replicacpp493 replica received implicit promise request from 374172170837843 with proposal 1 i0206 012503915132 2854 leveldbcpp304 persisting metadata 8 bytes to leveldb took 29980589ms i0206 012503915215 2854 replicacpp342 persisted promised to 1 i0206 012503916038 2856 coordinatorcpp238 coordinator attempting to fill missing positions i0206 012503917659 2856 replicacpp388 replica received explicit promise request from 375172170837843 for position 0 with proposal 2 i0206 012503948698 2856 leveldbcpp341 persisting action 8 bytes to leveldb took 30974607ms i0206 012503948786 2856 replicacpp712 persisted action at 0 i0206 012503950920 2849 replicacpp537 replica received write request for position 0 from 376172170837843 i0206 012503951011 2849 leveldbcpp436 reading position from leveldb took 44263ns i0206 012503982026 2849 leveldbcpp341 persisting action 14 bytes to leveldb took 30947321ms i0206 012503982225 2849 replicacpp712 persisted action at 0 i0206 012503983867 2849 replicacpp691 replica received learned notice for position 0 from 00000 i0206 012504015499 2849 leveldbcpp341 persisting action 16 bytes to leveldb took 30957888ms i0206 012504015591 2849 replicacpp712 persisted action at 0 i0206 012504015682 2849 replicacpp697 replica learned nop action at position 0 i0206 012504016666 2849 logcpp675 writer started with ending position 0 i0206 012504017881 2855 leveldbcpp436 reading position from leveldb took 56779ns i0206 012504018934 2852 registrarcpp340 successfully fetched the registry 0b in 193048064ms i0206 012504019076 2852 registrarcpp439 applied 1 operations in 38180ns attempting to update the registry i0206 012504020100 2844 logcpp683 attempting to append 170 bytes to the log i0206 012504020288 2855 coordinatorcpp348 coordinator attempting to write append action at position 1 i0206 012504021323 2844 replicacpp537 replica received write request for position 1 from 377172170837843 i0206 012504054726 2844 leveldbcpp341 persisting action 189 bytes to leveldb took 33309419ms i0206 012504054818 2844 replicacpp712 persisted action at 1 i0206 012504055933 2844 replicacpp691 replica received learned notice for position 1 from 00000 i0206 012504088142 2844 leveldbcpp341 persisting action 191 bytes to leveldb took 32116643ms i0206 012504088230 2844 replicacpp712 persisted action at 1 i0206 012504088265 2844 replicacpp697 replica learned append action at position 1 i0206 012504090070 2856 registrarcpp484 successfully updated the registry in 7090816ms i0206 012504090338 2851 logcpp702 attempting to truncate the log to 1 i0206 012504090358 2856 registrarcpp370 successfully recovered registrar i0206 012504090507 2847 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0206 012504090867 2858 mastercpp1522 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0206 012504091449 2858 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0206 012504092280 2857 replicacpp537 replica received write request for position 2 from 378172170837843 i0206 012504125702 2857 leveldbcpp341 persisting action 16 bytes to leveldb took 33192265ms i0206 012504125804 2857 replicacpp712 persisted action at 2 i0206 012504127400 2857 replicacpp691 replica received learned notice for position 2 from 00000 i0206 012504157727 2857 leveldbcpp341 persisting action 18 bytes to leveldb took 30268594ms i0206 012504157905 2857 leveldbcpp399 deleting 1 keys from leveldb took 88436ns i0206 012504157941 2857 replicacpp712 persisted action at 2 i0206 012504157984 2857 replicacpp697 replica learned truncate action at position 2 i0206 012504166174 2824 containerizercpp149 using isolation posixcpuposixmemfilesystemposix w0206 012504166954 2824 backendcpp48 failed to create bind backend bindbackend requires root privileges i0206 012504172008 2844 slavecpp193 slave started on 9172170837843 i0206 012504172046 2844 slavecpp194 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpcontainerloggertest_defaulttosandbox_fmakswcredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpcontainerloggertest_defaulttosandbox_fmakswfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0280_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpcontainerloggertest_defaulttosandbox_fmaksw i0206 012504172569 2844 credentialshpp83 loading credential for authentication from tmpcontainerloggertest_defaulttosandbox_fmakswcredential i0206 012504172886 2844 slavecpp324 slave using credential for testprincipal i0206 012504173141 2844 resourcescpp564 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0206 012504173620 2844 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 i0206 012504173686 2844 slavecpp472 slave attributes   i0206 012504173702 2844 slavecpp477 slave hostname 74ef606c4063 i0206 012504174816 2847 statecpp58 recovering state from tmpcontainerloggertest_defaulttosandbox_fmakswmeta i0206 012504175441 2847 status_update_managercpp200 recovering status update manager i0206 012504175678 2858 containerizercpp397 recovering containerizer i0206 012504177573 2858 provisionercpp245 provisioner recovery complete i0206 012504178231 2847 slavecpp4496 finished recovery i0206 012504178834 2847 slavecpp4668 querying resource estimator for oversubscribable resources i0206 012504179405 2847 slavecpp796 new master detected at master172170837843 i0206 012504179500 2847 slavecpp859 authenticating with master master172170837843 i0206 012504179525 2847 slavecpp864 using default crammd5 authenticatee i0206 012504179656 2858 status_update_managercpp174 pausing sending status updates i0206 012504179798 2847 slavecpp832 detecting new master i0206 012504179891 2852 authenticateecpp121 creating new client sasl connection i0206 012504179916 2847 slavecpp4682 received oversubscribable resources from the resource estimator i0206 012504180286 2847 mastercpp5523 authenticating slave9172170837843 i0206 012504180569 2847 authenticatorcpp413 starting authentication session for crammd5_authenticatee32172170837843 i0206 012504181000 2847 authenticatorcpp98 creating new server sasl connection i0206 012504181315 2847 authenticateecpp212 received sasl authentication mechanisms crammd5 i0206 012504181387 2847 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0206 012504181562 2847 authenticatorcpp203 received sasl authentication start i0206 012504181648 2847 authenticatorcpp325 authentication requires more steps i0206 012504181843 2847 authenticateecpp258 received sasl authentication step i0206 012504182034 2853 authenticatorcpp231 received sasl authentication step i0206 012504182071 2853 auxpropcpp107 request to lookup properties for user testprincipal realm 74ef606c4063 server fqdn 74ef606c4063 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0206 012504182093 2853 auxpropcpp179 looking up auxiliary property userpassword i0206 012504182145 2853 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0206 012504182173 2853 auxpropcpp107 request to lookup properties for user testprincipal realm 74ef606c4063 server fqdn 74ef606c4063 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0206 012504182185 2853 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0206 012504182193 2853 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0206 012504182211 2853 authenticatorcpp317 authentication success i0206 012504182333 2849 authenticateecpp298 authentication success i0206 012504182422 2853 mastercpp5553 successfully authenticated principal testprincipal at slave9172170837843 i0206 012504182510 2853 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee32172170837843 i0206 012504182945 2849 slavecpp927 successfully authenticated with master master172170837843 i0206 012504183178 2849 slavecpp1321 will retry registration in 987937ms if necessary i0206 012504183466 2852 mastercpp4237 registering slave at slave9172170837843 74ef606c4063 with id 914b62f995f64c57a7e39b06e2c1c8des0 i0206 012504184039 2845 registrarcpp439 applied 1 operations in 89453ns attempting to update the registry i0206 012504185288 2856 logcpp683 attempting to append 339 bytes to the log i0206 012504185672 2850 coordinatorcpp348 coordinator attempting to write append action at position 3 i0206 012504186674 2846 replicacpp537 replica received write request for position 3 from 392172170837843 i0206 012504195863 2856 slavecpp1321 will retry registration in 11038094ms if necessary i0206 012504196233 2856 mastercpp4225 ignoring register slave message from slave9172170837843 74ef606c4063 as admission is already in progress i0206 012504208094 2856 slavecpp1321 will retry registration in 27881223ms if necessary i0206 012504208472 2856 mastercpp4225 ignoring register slave message from slave9172170837843 74ef606c4063 as admission is already in progress i0206 012504216698 2846 leveldbcpp341 persisting action 358 bytes to leveldb took 29961291ms i0206 012504216789 2846 replicacpp712 persisted action at 3 i0206 012504218246 2845 replicacpp691 replica received learned notice for position 3 from 00000 i0206 012504237861 2846 slavecpp1321 will retry registration in 1006941ms if necessary i0206 012504238221 2846 mastercpp4225 ignoring register slave message from slave9172170837843 74ef606c4063 as admission is already in progress i0206 012504239858 2856 slavecpp1321 will retry registration in 167305686ms if necessary i0206 012504240044 2856 mastercpp4225 ignoring register slave message from slave9172170837843 74ef606c4063 as admission is already in progress i0206 012504241482 2845 leveldbcpp341 persisting action 360 bytes to leveldb took 23193162ms i0206 012504241524 2845 replicacpp712 persisted action at 3 i0206 012504241557 2845 replicacpp697 replica learned append action at position 3 i0206 012504243746 2844 registrarcpp484 successfully updated the registry in 59587072ms i0206 012504244210 2857 logcpp702 attempting to truncate the log to 3 i0206 012504244344 2845 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0206 012504244597 2856 mastercpp4305 registered slave 914b62f995f64c57a7e39b06e2c1c8des0 at slave9172170837843 74ef606c4063 with cpus2 mem1024 disk1024 ports3100032000 i0206 012504244746 2843 slavecpp3436 received ping from slaveobserver8172170837843 i0206 012504244976 2845 hierarchicalcpp473 added slave 914b62f995f64c57a7e39b06e2c1c8des0 74ef606c4063 with cpus2 mem1024 disk1024 ports3100032000 allocated  i0206 012504245072 2843 slavecpp971 registered with master master172170837843 given slave id 914b62f995f64c57a7e39b06e2c1c8des0 i0206 012504245121 2843 fetchercpp81 clearing fetcher cache i0206 012504245146 2845 hierarchicalcpp1403 no resources available to allocate i0206 012504245178 2845 hierarchicalcpp1116 performed allocation for slave 914b62f995f64c57a7e39b06e2c1c8des0 in 159744ns i0206 012504245465 2846 status_update_managercpp181 resuming sending status updates i0206 012504245776 2843 slavecpp994 checkpointing slaveinfo to tmpcontainerloggertest_defaulttosandbox_fmakswmetaslaves914b62f995f64c57a7e39b06e2c1c8des0slaveinfo i0206 012504245745 2846 replicacpp537 replica received write request for position 4 from 393172170837843 i0206 012504246273 2843 slavecpp1030 forwarding total oversubscribed resources i0206 012504246507 2850 mastercpp4646 received update of slave 914b62f995f64c57a7e39b06e2c1c8des0 at slave9172170837843 74ef606c4063 with total oversubscribed resources i0206 012504247180 2824 schedcpp222 version 0280 i0206 012504247155 2850 hierarchicalcpp531 slave 914b62f995f64c57a7e39b06e2c1c8des0 74ef606c4063 updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i0206 012504247357 2850 hierarchicalcpp1403 no resources available to allocate i0206 012504247406 2850 hierarchicalcpp1116 performed allocation for slave 914b62f995f64c57a7e39b06e2c1c8des0 in 183250ns i0206 012504247938 2854 schedcpp326 new master detected at master172170837843 i0206 012504248157 2854 schedcpp382 authenticating with master master172170837843 i0206 012504248265 2854 schedcpp389 using default crammd5 authenticatee i0206 012504248769 2854 authenticateecpp121 creating new client sasl connection i0206 012504249311 2854 mastercpp5523 authenticating schedulerf50aad7578d04d9fb1a4488d5ab932d6172170837843 i0206 012504249646 2854 authenticatorcpp413 starting authentication sess,1
remove markdown files from doxygen pages the doxygen html pages corresponding to doc markdown files are redundant and have broken links they dont serve any reasonable purpose in doxygen site,1
update configurationmd with cgroups_net_cls_primary_handle agent flag as part of the net_cls epic we introduce an agent flag called cgroup_net_cls_primary_handle  we need to update configurationmd with the corresponding help string,1
add a stub nvidia gpu isolator well first wire up a skeleton nvidia gpu isolator which needs to be guarded by a configure flag due to the dependency on nvml,3
add allocation metrics for gpus resources allocation metrics are currently hardcoded to include only cpus mem disk resources well need to add gpus to the list to start possibly following up on the todo to remove the hardcoding see httpsgithubcomapachemesosblob0270srcmastermetricscppl266l269 httpsgithubcomapachemesosblob0270srcslavemetricscppl123l126,1
implement nvidia gpu isolation wo filesystem isolation enabled the nvidia gpu isolator will need to use the device cgroup to restrict access to gpu resources and will need to recover this information after agent failover for now this will require that the operator specifies the gpu devices via a flag to handle filesystem isolation requires that we provide mechanisms for operators to inject volumes with the necessary libraries into all containers using gpu resources well tackle this in a separate ticket,5
support nvidia gpus with filesystem isolation enabled in mesos containerizer when filesystem isolation is enabled in the mesos containerizer containers that use nvidia gpu resources need access to gpu libraries residing on the host well need to provide a means for operators to inject the necessary volumes into all containers that use gpus resources see the nvidiadocker project for more details nvidiadockertoolssrcnvidiavolumesgohttpsgithubcomnvidianvidiadockerblobfda10b2d27bf5578cc5337c23877f827e4d1ed77toolssrcnvidiavolumesgol50l103,13
implement fault tolerance tests for the http scheduler api currently the http v1 api does not have fault tolerance tests similar to the one in srctestsfault_tolerance_testscpp for more information see mesos3355,5
implement partition tests for the http scheduler api currently the http v1 api does not have partition tests similar to the one in srctestspartition_testscpp for more information see mesos3355,5
tests will dereference stack allocated agent objects upon assertionexpectation failure tests that use the startslave test helper are generally fragile when the test fails an assertexpect in the middle of the test this is because the startslave helper takes raw pointer arguments which may be stackallocated in case of an assert failure the test immediately exits destroying stack allocated objects and proceeds onto test cleanup the test cleanup may dereference some of these destroyed objects leading to a test crash like code 182736step 88 f0204 182735981302 23085 loggingcpp64 raw pure virtual method called 182736step 88  0x7f7077055e1c googlelogmessagefail 182736step 88  0x7f707705ba6f googlerawlog__ 182736step 88  0x7f70760f76c9 __cxa_pure_virtual 182736step 88  0xa9423c mesosinternaltestsclusterslavesshutdown 182736step 88  0x1074e45 mesosinternaltestsmesostestshutdownslaves 182736step 88  0x1074de4 mesosinternaltestsmesostestshutdown 182736step 88  0x1070ec7 mesosinternaltestsmesostestteardown code the startslave helper should take shared_ptr arguments instead this also means that we can remove the shutdown helper from most of these tests,5
tests will dereference stack allocated master objects upon assertionexpectation failure tests that use the startmaster test helper are generally fragile when the test fails an assertexpect in the middle of the test this is because the startmaster helper takes raw pointer arguments which may be stackallocated in case of an assert failure the test immediately exits destroying stack allocated objects and proceeds onto test cleanup the test cleanup may dereference some of these destroyed objects leading to a test crash like code 182736step 88 f0204 182735981302 23085 loggingcpp64 raw pure virtual method called 182736step 88  0x7f7077055e1c googlelogmessagefail 182736step 88  0x7f707705ba6f googlerawlog__ 182736step 88  0x7f70760f76c9 __cxa_pure_virtual 182736step 88  0xa9423c mesosinternaltestsclusterslavesshutdown 182736step 88  0x1074e45 mesosinternaltestsmesostestshutdownslaves 182736step 88  0x1074de4 mesosinternaltestsmesostestshutdown 182736step 88  0x1070ec7 mesosinternaltestsmesostestteardown code the startmaster helper should take shared_ptr arguments instead this also means that we can remove the shutdown helper from most of these tests,5
add parent hook to subprocess,3
docker process executor can die with agent unit on systemd,1
posix process executor can die with agent unit on systemd,1
logrotate container logger can die with agent unit on systemd,1
add loginfo in cgroupsnet_cls for debugging allocation of net_cls handles we need to add loginfo during the prepare phase of cgroupsnet_cls for debugging management of net_cls handles within the isolator,1
document net_cls isolator in docsmesoscontainerizermd we need to add a section in the doc to describe how to use cgroupsnet_cls isolator,1
expose persistent volume information in http endpoints the perslave reserved_resources information returned by state does not seem to include information about persistent volumes this makes it hard for operators to use the destroyvolumes endpoint,3
add common compression utility we need gzip uncompress utility for appc image fetching functionality the images are tar  gziped and they needs to be first uncompressed so that we can compute sha 512 checksum on it,2
cgroup_info not being exposed in statejson when composingcontainerizer is used the composingcontainerizer currently does not have a status method this results in no containerstatus being updated in the agent when uses composingcontainerizer to launch containers this would specifically happen when the agent is launched with containerizerdockermesos,1
status updates from executor can be forwarded out of order by the agent previously all status update messages from the executor were forwarded by the agent to the master in the order that they had been received however that seems to be no longer valid due to a recently introduced change in the agent code  before sending update we need to retrieve the container status containerizerstatusexecutorcontainerid onanydeferself slave_statusupdate update pid executorid lambda_1 code this can sometimes lead to status updates being sent out of order depending on the order the future is fulfilled from the call to status,1
linux filesystem isolator tests are flaky linuxfilesystemisolatortestroot_imageinvolumewithrootfilesystem sometimes fails on centos 7 with this kind of output noformat srctestscontainerizerfilesystem_isolator_testscpp1054 failure failed to wait 2mins for launch noformat linuxfilesystemisolatortestroot_multiplecontainers often has this output noformat srctestscontainerizerfilesystem_isolator_testscpp1138 failure failed to wait 1mins for launch1 noformat whether ssl is configured makes no difference this test may also fail on other platforms but more rarely,3
cannot disable systemd support on certain platforms the systemd init system is available but not used not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init  monit system,1
root_docker_logs is flaky noformat 180625step 88  run  dockercontainerizertestroot_docker_logs 180625step 88 i0215 170625256103 1740 leveldbcpp174 opened db in 6548327ms 180625step 88 i0215 170625258002 1740 leveldbcpp181 compacted db in 1837816ms 180625step 88 i0215 170625258059 1740 leveldbcpp196 created db iterator in 22044ns 180625step 88 i0215 170625258076 1740 leveldbcpp202 seeked to beginning of db in 2347ns 180625step 88 i0215 170625258091 1740 leveldbcpp271 iterated through 0 keys in the db in 571ns 180625step 88 i0215 170625258152 1740 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 180625step 88 i0215 170625258936 1758 recovercpp447 starting replica recovery 180625step 88 i0215 170625259177 1758 recovercpp473 replica is in empty status 180625step 88 i0215 170625260327 1757 replicacpp673 replica in empty status received a broadcasted recover request from 1360817230223939785 180625step 88 i0215 170625260545 1758 recovercpp193 received a recover response from a replica in empty status 180625step 88 i0215 170625261065 1757 mastercpp376 master 112363e2c68049468feed0626ed8b21e ip172302239mesosphereio started on 17230223939785 180625step 88 i0215 170625261209 1761 recovercpp564 updating replica status to starting 180625step 88 i0215 170625261086 1757 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmphnclljcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmphnclljmaster zk_session_timeout10secs 180625step 88 i0215 170625261446 1757 mastercpp423 master only allowing authenticated frameworks to register 180625step 88 i0215 170625261456 1757 mastercpp428 master only allowing authenticated slaves to register 180625step 88 i0215 170625261462 1757 credentialshpp35 loading credentials for authentication from tmphnclljcredentials 180625step 88 i0215 170625261723 1757 mastercpp468 using default crammd5 authenticator 180625step 88 i0215 170625261855 1757 mastercpp537 using default basic http authenticator 180625step 88 i0215 170625262022 1757 mastercpp571 authorization enabled 180625step 88 i0215 170625262177 1755 hierarchicalcpp144 initialized hierarchical allocator process 180625step 88 i0215 170625262177 1758 whitelist_watchercpp77 no whitelist given 180625step 88 i0215 170625262899 1760 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1517992ms 180625step 88 i0215 170625262924 1760 replicacpp320 persisted replica status to starting 180625step 88 i0215 170625263144 1754 recovercpp473 replica is in starting status 180625step 88 i0215 170625264010 1757 mastercpp1712 the newly elected leader is master17230223939785 with id 112363e2c68049468feed0626ed8b21e 180625step 88 i0215 170625264044 1757 mastercpp1725 elected as the leading master 180625step 88 i0215 170625264061 1757 mastercpp1470 recovering from registrar 180625step 88 i0215 170625264117 1760 replicacpp673 replica in starting status received a broadcasted recover request from 1361017230223939785 180625step 88 i0215 170625264197 1758 registrarcpp307 recovering registrar 180625step 88 i0215 170625264827 1756 recovercpp193 received a recover response from a replica in starting status 180625step 88 i0215 170625265219 1757 recovercpp564 updating replica status to voting 180625step 88 i0215 170625267302 1754 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1887739ms 180625step 88 i0215 170625267326 1754 replicacpp320 persisted replica status to voting 180625step 88 i0215 170625267453 1759 recovercpp578 successfully joined the paxos group 180625step 88 i0215 170625267632 1759 recovercpp462 recover process terminated 180625step 88 i0215 170625268007 1757 logcpp659 attempting to start the writer 180625step 88 i0215 170625269055 1759 replicacpp493 replica received implicit promise request from 1361117230223939785 with proposal 1 180625step 88 i0215 170625270488 1759 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1406068ms 180625step 88 i0215 170625270511 1759 replicacpp342 persisted promised to 1 180625step 88 i0215 170625271078 1761 coordinatorcpp238 coordinator attempting to fill missing positions 180625step 88 i0215 170625272146 1756 replicacpp388 replica received explicit promise request from 1361217230223939785 for position 0 with proposal 2 180625step 88 i0215 170625273478 1756 leveldbcpp341 persisting action 8 bytes to leveldb took 1297217ms 180625step 88 i0215 170625273500 1756 replicacpp712 persisted action at 0 180625step 88 i0215 170625274355 1757 replicacpp537 replica received write request for position 0 from 1361317230223939785 180625step 88 i0215 170625274405 1757 leveldbcpp436 reading position from leveldb took 25294ns 180625step 88 i0215 170625275800 1757 leveldbcpp341 persisting action 14 bytes to leveldb took 1362978ms 180625step 88 i0215 170625275823 1757 replicacpp712 persisted action at 0 180625step 88 i0215 170625276348 1755 replicacpp691 replica received learned notice for position 0 from 00000 180625step 88 i0215 170625277765 1755 leveldbcpp341 persisting action 16 bytes to leveldb took 1391531ms 180625step 88 i0215 170625277788 1755 replicacpp712 persisted action at 0 180625step 88 i0215 170625277802 1755 replicacpp697 replica learned nop action at position 0 180625step 88 i0215 170625278336 1754 logcpp675 writer started with ending position 0 180625step 88 i0215 170625279371 1755 leveldbcpp436 reading position from leveldb took 29214ns 180625step 88 i0215 170625280272 1758 registrarcpp340 successfully fetched the registry 0b in 1602688ms 180625step 88 i0215 170625280385 1758 registrarcpp439 applied 1 operations in 31040ns attempting to update the registry 180625step 88 i0215 170625281054 1755 logcpp683 attempting to append 210 bytes to the log 180625step 88 i0215 170625281165 1757 coordinatorcpp348 coordinator attempting to write append action at position 1 180625step 88 i0215 170625281780 1757 replicacpp537 replica received write request for position 1 from 1361417230223939785 180625step 88 i0215 170625283159 1757 leveldbcpp341 persisting action 229 bytes to leveldb took 1348041ms 180625step 88 i0215 170625283184 1757 replicacpp712 persisted action at 1 180625step 88 i0215 170625283695 1759 replicacpp691 replica received learned notice for position 1 from 00000 180625step 88 i0215 170625285059 1759 leveldbcpp341 persisting action 231 bytes to leveldb took 1334577ms 180625step 88 i0215 170625285084 1759 replicacpp712 persisted action at 1 180625step 88 i0215 170625285099 1759 replicacpp697 replica learned append action at position 1 180625step 88 i0215 170625285910 1758 registrarcpp484 successfully updated the registry in 546816ms 180625step 88 i0215 170625286043 1758 registrarcpp370 successfully recovered registrar 180625step 88 i0215 170625286121 1755 logcpp702 attempting to truncate the log to 1 180625step 88 i0215 170625286301 1756 coordinatorcpp348 coordinator attempting to write truncate action at position 2 180625step 88 i0215 170625286478 1759 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover 180625step 88 i0215 170625286476 1754 mastercpp1522 recovered 0 slaves from the registry 171b  allowing 10mins for slaves to reregister 180625step 88 i0215 170625287137 1755 replicacpp537 replica received write request for position 2 from 1361517230223939785 180625step 88 i0215 170625289104 1755 leveldbcpp341 persisting action 16 bytes to leveldb took 1938609ms 180625step 88 i0215 170625289127 1755 replicacpp712 persisted action at 2 180625step 88 i0215 170625289667 1759 replicacpp691 replica received learned notice for position 2 from 00000 180625step 88 i0215 170625290956 1759 leveldbcpp341 persisting action 18 bytes to leveldb took 1256421ms 180625step 88 i0215 170625291007 1759 leveldbcpp399 deleting 1 keys from leveldb took 28064ns 180625step 88 i0215 170625291021 1759 replicacpp712 persisted action at 2 180625step 88 i0215 170625291038 1759 replicacpp697 replica learned truncate action at position 2 180625step 88 i0215 170625300550 1760 slavecpp193 slave started on 39317230223939785 180625step 88 i0215 170625300573 1760 slavecpp194 flags at startup appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpdockercontainerizertest_root_docker_logs_a4ns2ncredential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpdockercontainerizertest_root_docker_logs_a4ns2nfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpdockercontainerizertest_root_docker_logs_a4ns2n 180625step 88 i0215 170625300868 1760 credentialshpp83 loading credential for authentication from tmpdockercontainerizertest_root_docker_logs_a4ns2ncredential 180625step 88 i0215 170625301030 1760 slavecpp324 slave using credential for testprincipal 180625step 88 i0215 170625301180 1760 resourcescpp576 parsing resources as json failed cpus2mem1024disk1024ports3100032000 180625step 88 trying semicolondelimited string format instead 180625step 88 i0215 170625301553 1760 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 180625step 88 i0215 170625301609 1760 slavecpp472 slave attributes   180625step 88 i0215 170625301620 1760 slavecpp477 slave hostname ip172302239mesosphereio 180625step 88 i0215 170625302417 1757 statecpp58 recovering state from tmpdockercontainerizertest_root_docker_logs_a4ns2nmeta 180625step 88 i0215 170625302515 1740 schedcpp222 version 0280 180625step 88 i0215 170625302772 1755 status_update_managercpp200 recovering status update manager 180625step 88 i0215 170625302956 1758 dockercpp559 recovering docker containers 180625step 88 i0215 170625303050 1761 schedcpp326 new master detected at master17230223939785 180625step 88 i0215 170625303133 1754 slavecpp4565 finished recovery 180625step 88 i0215 170625303154 1761 schedcpp382 authenticating with master master17230223939785 180625step 88 i0215 170625303169 1761 schedcpp389 using default crammd5 authenticatee 180625step 88 i0215 170625303364 1759 authenticateecpp121 creating new client sasl connection 180625step 88 i0215 170625303467 1754 slavecpp4737 querying resource estimator for oversubscribable resources 180625step 88 i0215 170625303668 1756 mastercpp5523 authenticating scheduler806c70e31cf6418faa306bb26db42d1817230223939785 180625step 88 i0215 170625303707 1760 status_update_managercpp174 pausing sending status updates 180625step 88 i0215 170625303707 1754 slavecpp796 new master detected at master17230223939785 180625step 88 i0215 170625303767 1755 authenticatorcpp413 starting authentication session for crammd5_authenticatee82917230223939785 180625step 88 i0215 170625303791 1754 slavecpp859 authenticating with master master17230223939785 180625step 88 i0215 170625303805 1754 slavecpp864 using default crammd5 authenticatee 180625step 88 i0215 170625303956 1754 slavecpp832 detecting new master 180625step 88 i0215 170625303971 1761 authenticateecpp121 creating new client sasl connection 180625step 88 i0215 170625303984 1760 authenticatorcpp98 creating new server sasl connection 180625step 88 i0215 170625304131 1754 slavecpp4751 received oversubscribable resources from the resource estimator 180625step 88 i0215 170625304275 1757 mastercpp5523 authenticating slave39317230223939785 180625step 88 i0215 170625304344 1754 authenticateecpp212 received sasl authentication mechanisms crammd5 180625step 88 i0215 170625304369 1754 authenticateecpp238 attempting to authenticate with mechanism crammd5 180625step 88 i0215 170625304373 1761 authenticatorcpp413 starting authentication session for crammd5_authenticatee83017230223939785 180625step 88 i0215 170625304440 1757 authenticatorcpp203 received sasl authentication start 180625step 88 i0215 170625304491 1757 authenticatorcpp325 authentication requires more steps 180625step 88 i0215 170625304548 1754 authenticatorcpp98 creating new server sasl connection 180625step 88 i0215 170625304582 1761 authenticateecpp258 received sasl authentication step 180625step 88 i0215 170625304688 1761 authenticatorcpp231 received sasl authentication step 180625step 88 i0215 170625304714 1761 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302239mesosphereio server fqdn ip172302239mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false 180625step 88 i0215 170625304723 1761 auxpropcpp179 looking up auxiliary property userpassword 180625step 88 i0215 170625304767 1761 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 180625step 88 i0215 170625304805 1761 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302239mesosphereio server fqdn ip172302239mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true 180625step 88 i0215 170625304817 1761 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 180625step 88 i0215 170625304824 1761 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true 180625step 88 i0215 170625304836 1761 authenticatorcpp317 authentication success 180625step 88 i0215 170625304841 1758 authenticateecpp212 received sasl authentication mechanisms crammd5 180625step 88 i0215 170625304870 1758 authenticateecpp238 attempting to authenticate with mechanism crammd5 180625step 88 i0215 170625304909 1757 authenticateecpp298 authentication success 180625step 88 i0215 170625304983 1756 authenticatorcpp203 received sasl authentication start 180625step 88 i0215 170625305033 1756 authenticatorcpp325 authentication requires more steps 180625step 88 i0215 170625305042 1759 mastercpp5553 successfully authenticated principal testprincipal at scheduler806c70e31cf6418faa306bb26db42d1817230223939785 180625step 88 i0215 170625305071 1755 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee82917230223939785 180625step 88 i0215 170625305124 1756 authenticateecpp258 received sasl authentication step 180625step 88 i0215 170625305222 1758 schedcpp471 successfully authenticated with master master17230223939785 180625step 88 i0215 170625305246 1758 schedcpp776 sending subscribe call to master17230223939785 180625step 88 i0215 170625305286 1760 authenticatorcpp231 received sasl authentication step 180625step 88 i0215 170625305310 1760 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302239mesosphereio server fqdn ip172302239mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false 180625step 88 i0215 170625305318 1760 auxpropcpp179 looking up auxiliary property userpassword 180625step 88 i0215 170625305344 1760 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 180625step 88 i0215 170625305363 1758 schedcpp809 will retry registration in 1888777185secs if necessary 180625step 88 i0215 170625305379 1760 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302239mesosphereio server fqdn ip172302239mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true 180625step 88 i0215 170625305397 1760 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 180625step 8,2
upgrade vendored protobuf to 261 we currently vendor protobuf 250 we should upgrade to protobuf 261 this introduces various bugfixes performance improvements and at least one new feature we might want to eventually take advantage of map data type afaik there should be no backward compatibility concerns,3
document docker runtime isolator should include the following information what features are currently supported in docker runtime isolator how to use the docker runtime isolator user manual compare the different semantics vs docker containerizer and explain why,2
create base docker image for test suite this should be widely used for unified containerizer testing should basically include at least one layer repositories for each layer root file system as a layer tar ball docker image json manifest docker version,3
implement master failover tests for the scheduler library currently the scheduler library creates its own masterdetector object internally we would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover,3
implement reliable floating point for scalar resources design doc httpsdocsgooglecomdocumentd14qlxjzsfipfynbx0usljr0gelsq8hdzjuww6kay_dxcedituspsharing,5
design doc for v1 operator api we need to design how the v1 operator api all the http endpoints exposed by masteragent that are not for schedulerexecutor interactions looks and works,8
reorganize 3rdparty directory this issues is currently being discussed in the dev mailing list httpwwwmailarchivecomdevmesosapacheorgmsg34349html,5
add a hierarchicalallocator benchmark with reservation labels with labels being part of the reservationinfo we should ensure that we dont observe a significant performance degradation in the allocator,3
slaveteststateendpoint is flaky code  run  slaveteststateendpoint srctestsslave_testscpp1220 failure value of statevaluesstart_timeasjsonnumberasint actual 1458159086 expected static_castintclocknowsecs which is 1458159085  failed  slaveteststateendpoint 193 ms code even though this test does clockpause before starting the agent theres a possibility that a numifiedstringified double to not equal itself even after rounding to the nearest int,1
allow reserve operations by a principal without reservationinfoprincipal currently we require a framework or operator to specify reservationinfoprincipal when they reserve resources this isnt necessary however we already know the principal and can fill in the field if it isnt set already,2
document default value of offer_timeout there isnt a default value ie offers do not timeout by default but we should clarify this in flagscpp and configurationmd,1
make stout configuration modular and consumable by downstream eg libprocess and agent stout configuration is replicated in at least 3 configuration files  stout itself libprocess and agent more will follow in the future we should make a stoutconfigurecmake that can be included by any package downstream,1
enable zlib on windows,1
remove force field from the subscribe call in v1 scheduler api wei introduced the force field in subscribe call to deal with scheduler partition cases having thought a bit more and discussing with few other folks anandmazumdar greggomann i think we can get away from not having that field in the v1 api the obvious advantage of removing the field is that framework devs dont have to think about howwhen to set the field the current semantics are a bit confusing the new workflow when a master receives a subscribe call is that master always accepts this call and closes any existing connection after sending error event from the same scheduler identified by framework id the expectation from schedulers is that they must close the old subscribe connection before resending a new subscribe call lets look at some tricky scenarios and see how this works and why it is safe 1 connection disconnection  the scheduler but not  the master scheduler sees the disconnection and sends a new subscribe call master sends error on the old connection wont be received by the scheduler because the connection is already closed and closes it 2 connection disconnection  master but not  scheduler scheduler realizes this from lack of heartbeat events it then closes its existing connection and sends a new subscribe call master accepts the new subscribe call there is no old connection to close on the master as it is already closed 3 scheduler failover but no disconnection  master newly elected scheduler sends a subscribe call master sends error event and closes the old connection wont be received because the old scheduler failed over 4 if scheduler a got partitioned but is alive and connected with master and scheduler b got elected as new leader when scheduler b sends subscribe master sends error and closes the connection from scheduler a master accepts scheduler bs connection typically scheduler a aborts after receiving error and gets restarted after restart it wont become the leader because scheduler b is already elected 5 scheduler sends subscribe times out closes the subscribe connection a and sends a new subscribe b master receives subscribe b and then receives subscribe a but doesnt see as disconnection yet master first accepts subscribe b after it receives subscribe a it sends error to subscribe b and closes that connection when it accepts subscribe a and tries to send subscribed event the connection closure is detected scheduler retries the subscribe connection after a backoff i think this is a rare enough race for it to happen continuously in a loop,5
reviewbot should not fail hard if there are circular dependencies in a review chain instead of failing hard reviewbot should post an error to the review that a circular dependency is detected,2
make destdirpath install broken there is a missing destdir prefix in the installdatahook that causes destdir builds to be broken,2
add allocator metric for number of completed allocation runs,1
add allocator metric for number of offers each framework received a counter for the number of allocations to a framework can be used to monitor allocation progress eg when agents are added to a cluster and as other frameworks are added or removed currently an offer by the hierarchical allocator to a framework consists of a list of resources on possibly many agents resources might be offered in order to satisfy outstanding quota or for fairness to capture allocations on fine granularity we should not count the number of offers but instead the pieces making up that offer as such a metric would better resolve the effect of changes eg addingremoving a framework,2
add allocator metrics for total vs offeredallocated resources exposing the current allocation breakdown as seen by the allocator will allow us to correlated the corresponding metrics in the master with what the allocator sees we should expose at least allocated or available and total,2
expose allocation algorithm latency via a metric the allocation algorithm has grown to become fairly expensive gaining visibility into its latency enables monitoring and alerting similar allocator timingrelated information is already exposed in the log but should also be exposed via an endpoint,1
add allocator metric for number of active offer filters to diagnose scenarios where frameworks unexpectedly do not receive offers information on currently active filters are needed,1
add allocator metric for currently satisfied quotas we currently expose information on set quotas via dedicated quota endpoints to diagnose allocator problems one additionally needs information about used quotas,2
add allocator metric for currrent dominant shares of frameworks and roles,5
document scheduler driver calls in framework development guide the interface examples are slightly out of sync with schedulerhpp most notably missing the new acceptoffers call,2
update frameworks to use jsonify this should let us remove the duplicated code in httpcpp between modelframework and jsonfullframework,3
dockercontainerizertestroot_docker_launchwithpersistentvolumes fails on centos 6 this test passes consistently on other oss but fails consistently on centos 6 verbose logs from test failure code  run  dockercontainerizertestroot_docker_launchwithpersistentvolumes i0222 181612327957 26681 leveldbcpp174 opened db in 7466102ms i0222 181612330528 26681 leveldbcpp181 compacted db in 2540139ms i0222 181612330580 26681 leveldbcpp196 created db iterator in 16908ns i0222 181612330592 26681 leveldbcpp202 seeked to beginning of db in 1403ns i0222 181612330600 26681 leveldbcpp271 iterated through 0 keys in the db in 315ns i0222 181612330634 26681 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0222 181612331082 26698 recovercpp447 starting replica recovery i0222 181612331289 26698 recovercpp473 replica is in empty status i0222 181612332162 26703 replicacpp673 replica in empty status received a broadcasted recover request from 1376117230214835274 i0222 181612332701 26701 recovercpp193 received a recover response from a replica in empty status i0222 181612333230 26699 recovercpp564 updating replica status to starting i0222 181612334102 26698 mastercpp376 master 652149b439324d8bba6f8c9d9045be70 ip172302148mesosphereio started on 17230214835274 i0222 181612334116 26698 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpqehlbscredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpqehlbsmaster zk_session_timeout10secs i0222 181612334354 26698 mastercpp423 master only allowing authenticated frameworks to register i0222 181612334363 26698 mastercpp428 master only allowing authenticated slaves to register i0222 181612334369 26698 credentialshpp35 loading credentials for authentication from tmpqehlbscredentials i0222 181612335366 26698 mastercpp468 using default crammd5 authenticator i0222 181612335492 26698 mastercpp537 using default basic http authenticator i0222 181612335623 26698 mastercpp571 authorization enabled i0222 181612335752 26703 leveldbcpp304 persisting metadata 8 bytes to leveldb took 2314693ms i0222 181612335769 26700 whitelist_watchercpp77 no whitelist given i0222 181612335778 26703 replicacpp320 persisted replica status to starting i0222 181612335821 26697 hierarchicalcpp144 initialized hierarchical allocator process i0222 181612335965 26701 recovercpp473 replica is in starting status i0222 181612336771 26703 replicacpp673 replica in starting status received a broadcasted recover request from 1376317230214835274 i0222 181612337191 26696 recovercpp193 received a recover response from a replica in starting status i0222 181612337635 26700 recovercpp564 updating replica status to voting i0222 181612337671 26703 mastercpp1712 the newly elected leader is master17230214835274 with id 652149b439324d8bba6f8c9d9045be70 i0222 181612337698 26703 mastercpp1725 elected as the leading master i0222 181612337713 26703 mastercpp1470 recovering from registrar i0222 181612337828 26696 registrarcpp307 recovering registrar i0222 181612339972 26702 leveldbcpp304 persisting metadata 8 bytes to leveldb took 206039ms i0222 181612339994 26702 replicacpp320 persisted replica status to voting i0222 181612340082 26700 recovercpp578 successfully joined the paxos group i0222 181612340267 26700 recovercpp462 recover process terminated i0222 181612340591 26699 logcpp659 attempting to start the writer i0222 181612341594 26698 replicacpp493 replica received implicit promise request from 1376417230214835274 with proposal 1 i0222 181612343598 26698 leveldbcpp304 persisting metadata 8 bytes to leveldb took 197941ms i0222 181612343619 26698 replicacpp342 persisted promised to 1 i0222 181612344182 26698 coordinatorcpp238 coordinator attempting to fill missing positions i0222 181612345285 26702 replicacpp388 replica received explicit promise request from 1376517230214835274 for position 0 with proposal 2 i0222 181612347275 26702 leveldbcpp341 persisting action 8 bytes to leveldb took 1960198ms i0222 181612347296 26702 replicacpp712 persisted action at 0 i0222 181612348201 26703 replicacpp537 replica received write request for position 0 from 1376617230214835274 i0222 181612348247 26703 leveldbcpp436 reading position from leveldb took 21399ns i0222 181612350667 26703 leveldbcpp341 persisting action 14 bytes to leveldb took 239166ms i0222 181612350690 26703 replicacpp712 persisted action at 0 i0222 181612351191 26696 replicacpp691 replica received learned notice for position 0 from 00000 i0222 181612353152 26696 leveldbcpp341 persisting action 16 bytes to leveldb took 1935798ms i0222 181612353173 26696 replicacpp712 persisted action at 0 i0222 181612353188 26696 replicacpp697 replica learned nop action at position 0 i0222 181612353639 26696 logcpp675 writer started with ending position 0 i0222 181612354508 26697 leveldbcpp436 reading position from leveldb took 25625ns i0222 181612355274 26696 registrarcpp340 successfully fetched the registry 0b in 17406976ms i0222 181612355357 26696 registrarcpp439 applied 1 operations in 20977ns attempting to update the registry i0222 181612355929 26697 logcpp683 attempting to append 210 bytes to the log i0222 181612356032 26703 coordinatorcpp348 coordinator attempting to write append action at position 1 i0222 181612356657 26698 replicacpp537 replica received write request for position 1 from 1376717230214835274 i0222 181612358566 26698 leveldbcpp341 persisting action 229 bytes to leveldb took 1881945ms i0222 181612358588 26698 replicacpp712 persisted action at 1 i0222 181612359081 26697 replicacpp691 replica received learned notice for position 1 from 00000 i0222 181612361002 26697 leveldbcpp341 persisting action 231 bytes to leveldb took 1894331ms i0222 181612361023 26697 replicacpp712 persisted action at 1 i0222 181612361038 26697 replicacpp697 replica learned append action at position 1 i0222 181612361883 26697 registrarcpp484 successfully updated the registry in 6482944ms i0222 181612361981 26697 registrarcpp370 successfully recovered registrar i0222 181612362052 26701 logcpp702 attempting to truncate the log to 1 i0222 181612362167 26703 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0222 181612362421 26696 mastercpp1522 recovered 0 slaves from the registry 171b  allowing 10mins for slaves to reregister i0222 181612362447 26698 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0222 181612362911 26701 replicacpp537 replica received write request for position 2 from 1376817230214835274 i0222 181612364760 26701 leveldbcpp341 persisting action 16 bytes to leveldb took 1819954ms i0222 181612364783 26701 replicacpp712 persisted action at 2 i0222 181612365384 26697 replicacpp691 replica received learned notice for position 2 from 00000 i0222 181612367961 26697 leveldbcpp341 persisting action 18 bytes to leveldb took 255143ms i0222 181612368015 26697 leveldbcpp399 deleting 1 keys from leveldb took 28196ns i0222 181612368028 26697 replicacpp712 persisted action at 2 i0222 181612368044 26697 replicacpp697 replica learned truncate action at position 2 i0222 181612376824 26703 slavecpp193 slave started on 39617230214835274 i0222 181612376838 26703 slavecpp194 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpdockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1credential default_role disk_watch_interval1mins dockerdocker docker_auth_serverhttpsauthdockerio docker_kill_orphanstrue docker_puller_timeout60 docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpdockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1fetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpu2mem2048diskrole12048 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpdockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1 i0222 181612377109 26703 credentialshpp83 loading credential for authentication from tmpdockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1credential i0222 181612377300 26703 slavecpp324 slave using credential for testprincipal i0222 181612377439 26703 resourcescpp576 parsing resources as json failed cpu2mem2048diskrole12048 trying semicolondelimited string format instead i0222 181612377804 26703 slavecpp464 slave resources cpu2 mem2048 diskrole12048 cpus8 ports3100032000 i0222 181612377881 26703 slavecpp472 slave attributes   i0222 181612377889 26703 slavecpp477 slave hostname ip172302148mesosphereio i0222 181612378779 26701 statecpp58 recovering state from tmpdockercontainerizertest_root_docker_launchwithpersistentvolumes_u5vzx1meta i0222 181612379092 26697 status_update_managercpp200 recovering status update manager i0222 181612379156 26681 schedcpp222 version 0280 i0222 181612379250 26697 dockercpp722 recovering docker containers i0222 181612379421 26703 slavecpp4565 finished recovery i0222 181612379627 26700 schedcpp326 new master detected at master17230214835274 i0222 181612379735 26703 slavecpp4737 querying resource estimator for oversubscribable resources i0222 181612379765 26700 schedcpp382 authenticating with master master17230214835274 i0222 181612379781 26700 schedcpp389 using default crammd5 authenticatee i0222 181612379964 26696 status_update_managercpp174 pausing sending status updates i0222 181612379992 26702 authenticateecpp121 creating new client sasl connection i0222 181612380030 26697 slavecpp796 new master detected at master17230214835274 i0222 181612380106 26697 slavecpp859 authenticating with master master17230214835274 i0222 181612380127 26697 slavecpp864 using default crammd5 authenticatee i0222 181612380188 26699 mastercpp5526 authenticating scheduler1850b1cd33964479b2f347ee6c3fa27017230214835274 i0222 181612380269 26700 authenticatorcpp413 starting authentication session for crammd5_authenticatee83217230214835274 i0222 181612380280 26698 authenticateecpp121 creating new client sasl connection i0222 181612380307 26697 slavecpp832 detecting new master i0222 181612380450 26697 slavecpp4751 received oversubscribable resources from the resource estimator i0222 181612380452 26699 mastercpp5526 authenticating slave39617230214835274 i0222 181612380506 26698 authenticatorcpp98 creating new server sasl connection i0222 181612380540 26697 authenticatorcpp413 starting authentication session for crammd5_authenticatee83317230214835274 i0222 181612380635 26700 authenticateecpp212 received sasl authentication mechanisms crammd5 i0222 181612380659 26700 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0222 181612380762 26700 authenticatorcpp203 received sasl authentication start i0222 181612380765 26701 authenticatorcpp98 creating new server sasl connection i0222 181612380843 26700 authenticatorcpp325 authentication requires more steps i0222 181612380911 26698 authenticateecpp212 received sasl authentication mechanisms crammd5 i0222 181612380931 26702 authenticateecpp258 received sasl authentication step i0222 181612380936 26698 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0222 181612381036 26702 authenticatorcpp231 received sasl authentication step i0222 181612381052 26698 authenticatorcpp203 received sasl authentication start i0222 181612381062 26702 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302148 server fqdn ip172302148 sasl_auxprop_override false sasl_auxprop_authzid false i0222 181612381072 26702 auxpropcpp179 looking up auxiliary property userpassword i0222 181612381104 26702 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0222 181612381104 26698 authenticatorcpp325 authentication requires more steps i0222 181612381134 26702 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302148 server fqdn ip172302148 sasl_auxprop_override false sasl_auxprop_authzid true i0222 181612381142 26702 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0222 181612381147 26702 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0222 181612381162 26702 authenticatorcpp317 authentication success i0222 181612381184 26698 authenticateecpp258 received sasl authentication step i0222 181612381247 26699 authenticateecpp298 authentication success i0222 181612381283 26696 authenticatorcpp231 received sasl authentication step i0222 181612381311 26696 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302148 server fqdn ip172302148 sasl_auxprop_override false sasl_auxprop_authzid false i0222 181612381325 26696 auxpropcpp179 looking up auxiliary property userpassword i0222 181612381319 26701 mastercpp5556 successfully authenticated principal testprincipal at scheduler1850b1cd33964479b2f347ee6c3fa27017230214835274 i0222 181612381345 26700 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee83217230214835274 i0222 181612381361 26696 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0222 181612381397 26696 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302148 server fqdn ip172302148 sasl_auxprop_override false sasl_auxprop_authzid true i0222 181612381413 26696 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0222 181612381422 26696 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0222 181612381441 26696 authenticatorcpp317 authentication success i0222 181612381548 26698 schedcpp471 successfully authenticated with master master17230214835274 i0222 181612381563 26698 schedcpp776 sending subscribe call to master17230214835274 i0222 181612381634 26700 authenticateecpp298 authentication success i0222 181612381660 26698 schedcpp809 will retry registration in 77060771ms if necessary i0222 181612381675 26697 mastercpp5556 successfully authenticated principal testprincipal at slave39617230214835274 i0222 181612381734 26702 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee83317230214835274 i0222 181612381811 26697 mastercpp2280 received subscribe call for framework default at scheduler1850b1cd33964479b2f347ee6c3fa27017230214835274 i0222 181612381882 26697 mastercpp1751 authorizing framework principal testprincipal to receive offers for role role1 i0222 181612382004 26698 slavecpp927 successfully authenticated with master master17230214835274 i0222 181612382123 26698 slavecpp1321 will retry registration in 81941ms if necessary i0222 181612382282 26701 mastercpp4240 registering slave at slave39617230214835274 ip172302148mesosphereio with id 652149b439324d8bba6f8c9d9045be70s0 i0222 181612382482 26701 mastercpp2351 subscribing framework default with checkpointing disabled and capabilities   i0222 181612382612 26703 registrarcpp439 applied 1 operations in 46327ns attempting to update the registry i0222 181612382829 26699 hierarchicalcpp265 added framework 652149b439324d8bba6f8c9d9045be700000 i0222 181612382910 26699 hierarchicalcpp1434 no resources available to allocate i0222 181612382915 26701 schedcpp703 framework registered with 652149b439324d8bba6f8c9d9045be700000 i0222 181612382936 26699 hierarchicalcpp1529 no inverse offers to send out i0222 181612382953 26699 hierarchicalcpp1127 performed allocation for 0 slaves in 89949ns i0222 181612382982 26701 schedcpp717 schedulerregistered took 46498ns i0222 181612383536 26698 logcpp683 attempting to append 423 bytes to the log i0222 181612383628 26699 coordinatorcpp348 coordinator attempting to write append action at position 3 i0222 181612384196 26700 replicacpp537 replica received write request for position 3 from 1377517230214835274 i0222 181612386602 26700 leveldbcpp341 persisting action 442 bytes to leveldb took 2377119ms i0222 181612386625 26700 replicacpp712 persisted action at 3 i0222 181612387104 26698 replicacpp691 replica received learned notice for position 3 from 00000 i0222 181612389159 26698 leveldbcpp341 persisting action 444 bytes to leveldb took 2032301ms i0222 181612389181 26698 replicacpp712 persisted action at 3 i0222 181612389196 26698 replicacpp697 replica learned append action at position 3 i0222 181612390281 26698 registrarcpp484 suc,2
cmake add leveldb library to 3rdparty external builds,3
containerloggertestmesoscontainerizerrecover cannot be executed in isolation some cleanup of spawned processes is missing in containerloggertestmesoscontainerizerrecover so that when the test is run in isolation the global teardown might find lingering processes code  running 1 test from 1 test case  global test environment setup  1 test from containerloggertest  run  containerloggertestmesoscontainerizerrecover  ok  containerloggertestmesoscontainerizerrecover 13 ms  1 test from containerloggertest 13 ms total  global test environment teardown srctestsenvironmentcpp728 failure failed tests completed with child processes remaining  7112 somepathsrcmesosbuildsrclibsmesostests gtest_filtercontainerloggertestmesoscontainerizerrecover  7130 sh  1 test from 1 test case ran 23 ms total  passed  1 test  failed  0 tests listed below 0 failed tests code observered on os x with clangtrunk and an unoptimized build,1
add appc image fetcher tests mesos now has support for fetching appc images add tests that verifies the new component,3
move htb out of containers currently we set a fixed htb bandwidth in each of the container which makes it impossible to share the link if idle as the first step we should move it out of the containers into the qdisc hierarchy of the physical interface,3
document mesos executor expects all ssl_ environment variables to be set i was trying to run docker containers in a fully sslized mesos cluster but ran into problems because the executor was failing with a failed to shutdown socket with fd 10 transport endpoint is not connected my understanding of why this is happening is because the executor was trying to report its status to mesos slave over https but doesnt have the appropriate certsenv setup inside the executor thanks to mslackbotjoseph for helping me figure this out on mesos it turns out the executor expects all ssl_ variables to be set inside commandinfoenvironment which gets picked up by the executor to successfully reports its status to the slave this part of __executor needing all the ssl_ variables to be set in its environment__ is missing in the mesos ssl transitioning guide i request you to please add this vital information to the doc,2
the executors field is exposed under a backwards incompatible schema in 0260 the masters state endpoint generated the following code     frameworks      executors   command  argv  uris  value usersmparkprojectsmesosbuildoptsrclonglivedexecutor  executor_id default framework_id 0ea528a964ba417f98ea9c4b8d418db60000 name long lived executor c resources  cpus 0 disk 0 mem 0  slave_id 8a51367803a14cb59279c3c0c591f1d8s0            code in 0271 the executorinfo is mistakenly exposed in the raw protobuf schema code     frameworks      executors   command  shell true value usersmparkprojectsmesosbuildoptsrclonglivedexecutor  executor_id  value default  framework_id  value 368a5a49480b41f6a13b24a69c92a72e0000  name long lived executor c slave_id 8a51367803a14cb59279c3c0c591f1d8s0 source cpp_long_lived_framework            code this is a backwards incompatible api change,2
mesos containerizer should get uidgids before pivot_root currently we call ossuuser after pivot_root this is problematic because etcpasswd and etcgroup might be missing in containers root filesystem we should instead get the uidgids before pivot_root and call setuidsetgroups after pivot_root,3
add a name field into networkinfo this allows the framework writer to specify the name of the network they want their container to join why not using groups thats because there might be multiple groups under a single network eg admin vs user public vs private etc,1
add networkcni isolator for mesos containerizer see the design doc for more context mesos4742 the isolator will interact with cni plugins to create the network for the container to join,8
expose metrics and gauges for fetcher cache usage and hit rate to evaluate the fetcher cache and calibrate the value of the fetcher_cache_size flag it would be useful to have metrics and gauges on agents that expose operational statistics like cache hit rate occupied cache size and time spent downloading resources that were not present,2
add agent flags to allow operators to specify cni plugin and config directories according to design doc we plan to add the following flags network_cni_plugins_dir location of the cni plugin binaries the networkcni isolator will find cni plugins under this directory so that it can execute the plugins to adddelete container from the cni networks it is the operators responsibility to install the cni plugin binaries in the specified directory network_cni_config_dir location of the cni network configuration files for each network that containers launched in mesos agent can connect to the operator should install a network configuration file in json format in the specified directory,2
setup proper dns resolver for containers in networkcni isolator please get more context from the design doc mesos4742 the cni plugin will return the dns information about the network the networkcni isolator needs to properly setup etcresolvconf for the container we should consider the following cases 1 container is using host filesystem 2 container is using a different filesystem 3 custom executor and command executor,5
add test mock for cni plugins in order to test the networkcni isolator we need to mock the behavior of an cni plugin one option is to write a mock script which acts as a cni plugin the isolator will talk to the mock script the same way it talks to an actual cni plugin the mock script can just join the host network,5
the networkcni isolator should report assigned ip address in order for service discovery to work in some cases the networkcni isolator needs to report the assigned ip address through the isolatorstatus interface,3
mastermaintenancetestinverseoffers is flaky mesos4169 significantly sped up this test but also surfaced some more flakiness this can be fixed in the same way as mesos4059 verbose logs from asf centos7 build code  run  mastermaintenancetestinverseoffers i0224 223553714018 1948 leveldbcpp174 opened db in 2034387ms i0224 223553714663 1948 leveldbcpp181 compacted db in 608839ns i0224 223553714709 1948 leveldbcpp196 created db iterator in 19043ns i0224 223553714844 1948 leveldbcpp202 seeked to beginning of db in 2330ns i0224 223553714956 1948 leveldbcpp271 iterated through 0 keys in the db in 518ns i0224 223553715092 1948 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0224 223553715646 1968 recovercpp447 starting replica recovery i0224 223553715915 1981 recovercpp473 replica is in empty status i0224 223553717067 1972 replicacpp673 replica in empty status received a broadcasted recover request from 4533172170136678 i0224 223553717445 1981 recovercpp193 received a recover response from a replica in empty status i0224 223553717888 1978 recovercpp564 updating replica status to starting i0224 223553718585 1979 leveldbcpp304 persisting metadata 8 bytes to leveldb took 525061ns i0224 223553718618 1979 replicacpp320 persisted replica status to starting i0224 223553718827 1982 recovercpp473 replica is in starting status i0224 223553719728 1969 replicacpp673 replica in starting status received a broadcasted recover request from 4534172170136678 i0224 223553719974 1971 recovercpp193 received a recover response from a replica in starting status i0224 223553720369 1970 recovercpp564 updating replica status to voting i0224 223553720789 1982 leveldbcpp304 persisting metadata 8 bytes to leveldb took 322308ns i0224 223553720823 1982 replicacpp320 persisted replica status to voting i0224 223553720968 1982 recovercpp578 successfully joined the paxos group i0224 223553721101 1982 recovercpp462 recover process terminated i0224 223553721698 1982 mastercpp376 master aab18b6178114c43a672d1a63818c880 4db5fa128d2d started on 172170136678 i0224 223553721719 1982 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatefalse authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpmjbcwpcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0280_instsharemesoswebui work_dirtmpmjbcwpmaster zk_session_timeout10secs i0224 223553722039 1982 mastercpp425 master allowing unauthenticated frameworks to register i0224 223553722053 1982 mastercpp428 master only allowing authenticated slaves to register i0224 223553722061 1982 credentialshpp35 loading credentials for authentication from tmpmjbcwpcredentials i0224 223553722394 1982 mastercpp468 using default crammd5 authenticator i0224 223553722525 1982 mastercpp537 using default basic http authenticator i0224 223553722661 1982 mastercpp571 authorization enabled i0224 223553722813 1968 hierarchicalcpp144 initialized hierarchical allocator process i0224 223553722846 1980 whitelist_watchercpp77 no whitelist given i0224 223553724957 1977 mastercpp1712 the newly elected leader is master172170136678 with id aab18b6178114c43a672d1a63818c880 i0224 223553725000 1977 mastercpp1725 elected as the leading master i0224 223553725023 1977 mastercpp1470 recovering from registrar i0224 223553725306 1967 registrarcpp307 recovering registrar i0224 223553725808 1977 logcpp659 attempting to start the writer i0224 223553727145 1973 replicacpp493 replica received implicit promise request from 4536172170136678 with proposal 1 i0224 223553727728 1973 leveldbcpp304 persisting metadata 8 bytes to leveldb took 424560ns i0224 223553727828 1973 replicacpp342 persisted promised to 1 i0224 223553729080 1973 coordinatorcpp238 coordinator attempting to fill missing positions i0224 223553731009 1979 replicacpp388 replica received explicit promise request from 4537172170136678 for position 0 with proposal 2 i0224 223553731580 1979 leveldbcpp341 persisting action 8 bytes to leveldb took 478479ns i0224 223553731613 1979 replicacpp712 persisted action at 0 i0224 223553734354 1979 replicacpp537 replica received write request for position 0 from 4538172170136678 i0224 223553734485 1979 leveldbcpp436 reading position from leveldb took 60879ns i0224 223553735877 1979 leveldbcpp341 persisting action 14 bytes to leveldb took 1324061ms i0224 223553735930 1979 replicacpp712 persisted action at 0 i0224 223553737061 1970 replicacpp691 replica received learned notice for position 0 from 00000 i0224 223553738881 1970 leveldbcpp341 persisting action 16 bytes to leveldb took 1772814ms i0224 223553738939 1970 replicacpp712 persisted action at 0 i0224 223553738975 1970 replicacpp697 replica learned nop action at position 0 i0224 223553740136 1976 logcpp675 writer started with ending position 0 i0224 223553741750 1976 leveldbcpp436 reading position from leveldb took 74863ns i0224 223553743479 1976 registrarcpp340 successfully fetched the registry 0b in 1811968ms i0224 223553743755 1976 registrarcpp439 applied 1 operations in 56670ns attempting to update the registry i0224 223553745604 1978 logcpp683 attempting to append 170 bytes to the log i0224 223553745905 1977 coordinatorcpp348 coordinator attempting to write append action at position 1 i0224 223553746968 1981 replicacpp537 replica received write request for position 1 from 4539172170136678 i0224 223553747480 1981 leveldbcpp341 persisting action 189 bytes to leveldb took 456947ns i0224 223553747609 1981 replicacpp712 persisted action at 1 i0224 223553750448 1981 replicacpp691 replica received learned notice for position 1 from 00000 i0224 223553751158 1981 leveldbcpp341 persisting action 191 bytes to leveldb took 535163ns i0224 223553751258 1981 replicacpp712 persisted action at 1 i0224 223553751389 1981 replicacpp697 replica learned append action at position 1 i0224 223553753149 1979 registrarcpp484 successfully updated the registry in 9228032ms i0224 223553753324 1979 registrarcpp370 successfully recovered registrar i0224 223553753593 1979 logcpp702 attempting to truncate the log to 1 i0224 223553753805 1979 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0224 223553754055 1981 mastercpp1522 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0224 223553754349 1979 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0224 223553755764 1977 replicacpp537 replica received write request for position 2 from 4540172170136678 i0224 223553756459 1977 leveldbcpp341 persisting action 16 bytes to leveldb took 488559ns i0224 223553756561 1977 replicacpp712 persisted action at 2 i0224 223553757932 1972 replicacpp691 replica received learned notice for position 2 from 00000 i0224 223553758400 1972 leveldbcpp341 persisting action 18 bytes to leveldb took 343827ns i0224 223553758539 1972 leveldbcpp399 deleting 1 keys from leveldb took 34231ns i0224 223553758658 1972 replicacpp712 persisted action at 2 i0224 223553758782 1972 replicacpp697 replica learned truncate action at position 2 i0224 223553778059 1978 slavecpp193 slave started on 115172170136678 i0224 223553778105 1978 slavecpp194 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpmastermaintenancetest_inverseoffers_ywqvffcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmastermaintenancetest_inverseoffers_ywqvfffetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostnamemaintenancehost hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0280_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmastermaintenancetest_inverseoffers_ywqvff i0224 223553778609 1978 credentialshpp83 loading credential for authentication from tmpmastermaintenancetest_inverseoffers_ywqvffcredential i0224 223553779175 1978 slavecpp324 slave using credential for testprincipal i0224 223553779520 1978 resourcescpp576 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0224 223553780192 1978 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 i0224 223553780362 1978 slavecpp472 slave attributes   i0224 223553780483 1978 slavecpp477 slave hostname maintenancehost i0224 223553782126 1967 statecpp58 recovering state from tmpmastermaintenancetest_inverseoffers_ywqvffmeta i0224 223553782892 1969 status_update_managercpp200 recovering status update manager i0224 223553783242 1969 slavecpp4565 finished recovery i0224 223553784001 1969 slavecpp4737 querying resource estimator for oversubscribable resources i0224 223553784678 1969 slavecpp796 new master detected at master172170136678 i0224 223553784874 1967 status_update_managercpp174 pausing sending status updates i0224 223553784808 1969 slavecpp859 authenticating with master master172170136678 i0224 223553784945 1969 slavecpp864 using default crammd5 authenticatee i0224 223553785181 1969 slavecpp832 detecting new master i0224 223553785326 1969 slavecpp4751 received oversubscribable resources from the resource estimator i0224 223553785557 1969 authenticateecpp121 creating new client sasl connection i0224 223553786227 1969 mastercpp5526 authenticating slave115172170136678 i0224 223553786492 1969 authenticatorcpp413 starting authentication session for crammd5_authenticatee298172170136678 i0224 223553786962 1969 authenticatorcpp98 creating new server sasl connection i0224 223553787274 1969 authenticateecpp212 received sasl authentication mechanisms crammd5 i0224 223553787308 1969 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0224 223553787400 1969 authenticatorcpp203 received sasl authentication start i0224 223553787470 1969 authenticatorcpp325 authentication requires more steps i0224 223553787884 1972 authenticateecpp258 received sasl authentication step i0224 223553787992 1972 authenticatorcpp231 received sasl authentication step i0224 223553788027 1972 auxpropcpp107 request to lookup properties for user testprincipal realm 4db5fa128d2d server fqdn 4db5fa128d2d sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0224 223553788040 1972 auxpropcpp179 looking up auxiliary property userpassword i0224 223553788090 1972 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0224 223553788122 1972 auxpropcpp107 request to lookup properties for user testprincipal realm 4db5fa128d2d server fqdn 4db5fa128d2d sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0224 223553788136 1972 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0224 223553788146 1972 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0224 223553788164 1972 authenticatorcpp317 authentication success i0224 223553788331 1972 authenticateecpp298 authentication success i0224 223553788439 1972 mastercpp5556 successfully authenticated principal testprincipal at slave115172170136678 i0224 223553788529 1972 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee298172170136678 i0224 223553788988 1972 slavecpp927 successfully authenticated with master master172170136678 i0224 223553789139 1972 slavecpp1321 will retry registration in 1535786ms if necessary i0224 223553789515 1972 mastercpp4240 registering slave at slave115172170136678 maintenancehost with id aab18b6178114c43a672d1a63818c880s0 i0224 223553790577 1972 registrarcpp439 applied 1 operations in 78745ns attempting to update the registry i0224 223553791128 1971 processcpp3141 handling http event for process master with path mastermaintenanceschedule i0224 223553791877 1971 httpcpp501 http post for mastermaintenanceschedule from 172170145095 i0224 223553793313 1972 logcpp683 attempting to append 343 bytes to the log i0224 223553793586 1972 coordinatorcpp348 coordinator attempting to write append action at position 3 i0224 223553794533 1971 replicacpp537 replica received write request for position 3 from 4547172170136678 i0224 223553794862 1971 leveldbcpp341 persisting action 362 bytes to leveldb took 283614ns i0224 223553794893 1971 replicacpp712 persisted action at 3 i0224 223553796646 1979 replicacpp691 replica received learned notice for position 3 from 00000 i0224 223553797102 1972 slavecpp1321 will retry registration in 17198963ms if necessary i0224 223553797186 1979 leveldbcpp341 persisting action 364 bytes to leveldb took 498502ns i0224 223553797230 1979 replicacpp712 persisted action at 3 i0224 223553797260 1979 replicacpp697 replica learned append action at position 3 i0224 223553797417 1972 mastercpp4228 ignoring register slave message from slave115172170136678 maintenancehost as admission is already in progress i0224 223553799119 1978 registrarcpp484 successfully updated the registry in 845824ms i0224 223553799613 1978 registrarcpp439 applied 1 operations in 176193ns attempting to update the registry i0224 223553800472 1972 mastercpp4308 registered slave aab18b6178114c43a672d1a63818c880s0 at slave115172170136678 maintenancehost with cpus2 mem1024 disk1024 ports3100032000 i0224 223553800623 1978 logcpp702 attempting to truncate the log to 3 i0224 223553801255 1969 hierarchicalcpp473 added slave aab18b6178114c43a672d1a63818c880s0 maintenancehost with cpus2 mem1024 disk1024 ports3100032000 allocated  i0224 223553801301 1978 slavecpp971 registered with master master172170136678 given slave id aab18b6178114c43a672d1a63818c880s0 i0224 223553801331 1978 fetchercpp81 clearing fetcher cache i0224 223553801431 1969 hierarchicalcpp1434 no resources available to allocate i0224 223553801466 1969 hierarchicalcpp1147 performed allocation for slave aab18b6178114c43a672d1a63818c880s0 in 162751ns i0224 223553801532 1969 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0224 223553801867 1978 slavecpp994 checkpointing slaveinfo to tmpmastermaintenancetest_inverseoffers_ywqvffmetaslavesaab18b6178114c43a672d1a63818c880s0slaveinfo i0224 223553801877 1969 status_update_managercpp181 resuming sending status updates i0224 223553802898 1977 replicacpp537 replica received write request for position 4 from 4548172170136678 i0224 223553803252 1978 slavecpp1030 forwarding total oversubscribed resources i0224 223553803640 1970 mastercpp4649 received update of slave aab18b6178114c43a672d1a63818c880s0 at slave115172170136678 maintenancehost with total oversubscribed resources i0224 223553803858 1977 leveldbcpp341 persisting action 16 bytes to leveldb took 912626ns i0224 223553803889 1977 replicacpp712 persisted action at 4 i0224 223553804144 1978 slavecpp3482 received ping from slaveobserver117172170136678 i0224 223553804535 1971 hierarchicalcpp531 slave aab18b6178114c43a672d1a63818c880s0 maintenancehost updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i0224 223553804684 1971 hierarchicalcpp1434 no resources available to allocate i0224 223553804714 1971 hierarchicalcpp1147 performed allocation for slave aab18b6178114c43a672d1a63818c880s0 in 131453ns i0224 223553805541 1967 replicacpp691 replica received learned notice for position 4 from 00000 i0224 223553805941 1967 leveldbcpp341 persisting action 18 bytes to leveldb took 366444ns i0224 223553806015 1967 leveldbcpp399 deleting 2 keys from leveldb took 42808ns i0224 223553806041 1967 replicacpp712 persisted action at 4 i0224 223553806066 1967 replicacpp697 replica learned truncate action at position 4 i0224 223553807355 1978 logcpp683 attempting to append 465 bytes to the log i0224 223553807551 1978 coordinatorcpp348 coordinator attempting to write append action at position 5 i0224 223553809638 1979 replicacpp537 replica received write request for position 5 from 4549172170136678 i0224 223553810858 1979 leveldbcpp341 persisting action 484 bytes to leveldb took 1167663ms i0224 223553810904 1979 replicacpp712 persisted action at 5 i0224 223553811997 1979 replicacpp691 replica received learned notice for position 5 from 00000 i0224 223553,1
document the networkcni isolator we need to document this isolator in mesoscontainerizermd eg how to configure it whats the prerequisite etc,3
taskinfoexecutorinfo should include finegrained ownershipnamespacing we need a way to assign finegrained ownership to tasksexecutors so that multiuser frameworks can tell mesos to associate the task with a user identity rather than just the framework principalrole then when an http user requests to view the tasks sandbox contents or kill the task or list all tasks the authorizer can determine whether to allowdenyfilter the request based on finergrained userlevel ownership some systems may want taskinfoowner to represent a group rather than an individual user thats fine as long as the framework sets the field to the group id in such a way that a groupaware authorizer can interpret it,2
libprocess metricssnapshot endpoint rate limiting should be configurable currently the metricssnapshot endpoint in libprocess has a hardcodedhttpsgithubcomapachemesosblob02713rdpartylibprocessincludeprocessmetricsmetricshppl52 rate limit of 2 requests per second code metricsprocess  processbasemetrics limiter2 seconds1  code this should be configurable via a libprocess environment variable so that users can control this when initializing libprocess,2
add appcruntime isolator for runtime isolation for appc images appc image also contains runtime information like exec env workingdirectory etc httpsgithubcomappcspecblobmasterspecacimd similar to docker images we need to support a subset of them mainly exec env and workingdirectory,13
remove user and rootfs flags in windows launcher,2
executor env variables should not be leaked to the command task currently command task inherits the env variables of the command executor this is less ideal because the command executor environment variables include some mesos internal env variables like mesos_xxx and libprocess_xxx also this behavior does not match what docker containerizer does we should construct the env variables from scratch for the command task rather than relying on inheriting the env variables from the command executor,3
disable rate limiting of the global metrics endpoint for mesostests execution once we can optionally disable rate limiting in the global metrics endpoint with mesos4776 we should disable the rate limiting during the execution of mesostests  rate limiting makes it cumbersome to repeatedly hit the endpoint since one would not want to interfere with the rate limiting  rate limiting might incur additional wait time which might slown down tests,3
slavetestmetricsslavelauncherrors test relies on implicit blocking behavior hitting the global metrics endpoint the test attempts to observe a change in the slavecontainer_launch_errors metric but does not wait for the triggering action to take place currently the test passes since hitting the endpoint blocks for some rate limitrelated time which provides under many circumstances enough wait time for the action to take place,1
reorganize acl subjectobject descriptions the authorization documentation would benefit from a reorganization of the acl subjectobject descriptions instead of simple lists of the available subjects and objects it would be nice to see a table showing which subject and object is used with each action,5
http endpoint docs should use shorter paths my understanding is that the recommended path for the v1 scheduler api is apiv1scheduler but the http endpoint docshttpmesosapacheorgdocumentationlatestendpoints for this endpoint list the path as masterapiv1scheduler the filename of the doc page is also in the master subdirectory similarly we document the master state endpoint as masterstate whereas the preferred name is now just state and so on for most of the other endpoints unlike we the v1 api we might want to consider backward compatibility and document both forms  not sure but certainly it seems like we should encourage people to use the shorter paths not the longer ones,2
revert external linkage of symbols in masterconstantshpp srcmasterconstantshpp contains code  todobmahler it appears there may be a bug with gcc412 in which the  duration constants were not being initialized when having static linkage  this issue did not manifest in newer gccs specifically 421 was ok  so weve moved these to have external linkage but perhaps in the future  we can revert this code from commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a we should investigate whether this is still necessary on supported compilers it likely is not,1
add documentation around using the docker containerizer on centos 6 support for persistent volumes was added to the docker containerizer in mesos3413 however this does not work on centos 6 on centos 6 the same docker run v  operation does not perform a recursive bind whereas on every other os supported by mesos docker does a recursive bind docker has already dropped support for centos 6httpsgithubcomdockerdockerissues14365 so we should add precautionary documentation in case anyone tries to use the docker containerizer on centos 6,1
add a couple of registrar tests for weights endpoint,2
make existing scheduler library tests use the callback interface we need to migrate the existing tests in srctestsscheduler_testscpp to use the new callback interface introduced in mesos3339 the changes to srctestsmaster_maintenance_testscpp would be done when mesos4831 is resolved for an example see schedulertestschedulerfailover which already uses this new interface,5
updated createframeworkinfo for hierarchical_allocator_testscpp the function of createframeworkinfo in hierarchical_allocator_testscpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources,1
update leveldb patch file to suport powerpc le see httpsgithubcomgoogleleveldbreleasestagv118 for improvements  bug fixes the motivation is that leveldb 118 has officially supported ibm power ppc64le so this is needed by mesos4312httpsissuesapacheorgjirabrowsemesos4312 update since someone updated leveldb to 14 so i only update the patch file to support powerpc le because i dont think upgrade 3rdparty library frequently is a good thing,3
update vendored libev to 422 the motivation is that libev 422 has officially supported ibm power ppc64le so this is needed by mesos4312httpsissuesapacheorgjirabrowsemesos4312,3
update ryhttpparser1c3624a to nodejshttpparser 261 see httpsgithubcomnodejshttpparserreleasestagv261 the motivation is that nodejshttpparser 261 has officially supported ibm power ppc64le so this is needed by mesos4312httpsissuesapacheorgjirabrowsemesos4312,3
iotestbufferedread writes to the current directory libprocesss iotestbufferedread writes to the current directory this is bad for a number of reasons eg  should the test fail data might be leaked to random locations  the test cannot be executed from a writeonly directory or  executing the same test in parallel would race on the existence of the created file and show bogus behavior the test should probably be executed from a temporary directory eg via stouts temporarydirectorytest fixture,1
provisionerdockerpullertestroot_internet_curl_shellcommand fails noformat 094646  step 1111  run  provisionerdockerregistrypullertestroot_internet_curl_shellcommand 094646w step 1111 i0229 094646628413 1166 leveldbcpp174 opened db in 4242882ms 094646w step 1111 i0229 094646629926 1166 leveldbcpp181 compacted db in 1483621ms 094646w step 1111 i0229 094646629966 1166 leveldbcpp196 created db iterator in 15498ns 094646w step 1111 i0229 094646629977 1166 leveldbcpp202 seeked to beginning of db in 1405ns 094646w step 1111 i0229 094646629984 1166 leveldbcpp271 iterated through 0 keys in the db in 239ns 094646w step 1111 i0229 094646630015 1166 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 094646w step 1111 i0229 094646630470 1183 recovercpp447 starting replica recovery 094646w step 1111 i0229 094646630702 1180 recovercpp473 replica is in empty status 094646w step 1111 i0229 094646631767 1182 replicacpp673 replica in empty status received a broadcasted recover request from 1456717230212437431 094646w step 1111 i0229 094646632115 1183 recovercpp193 received a recover response from a replica in empty status 094646w step 1111 i0229 094646632450 1186 recovercpp564 updating replica status to starting 094646w step 1111 i0229 094646633476 1186 mastercpp375 master 3fbb2fb04f18498ba4409acbf6923a13 ip172302124mesosphereio started on 17230212437431 094646w step 1111 i0229 094646633491 1186 mastercpp377 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmp4uxxowcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmp4uxxowmaster zk_session_timeout10secs 094646w step 1111 i0229 094646633677 1186 mastercpp422 master only allowing authenticated frameworks to register 094646w step 1111 i0229 094646633685 1186 mastercpp427 master only allowing authenticated slaves to register 094646w step 1111 i0229 094646633692 1186 credentialshpp35 loading credentials for authentication from tmp4uxxowcredentials 094646w step 1111 i0229 094646633851 1183 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1191043ms 094646w step 1111 i0229 094646633873 1183 replicacpp320 persisted replica status to starting 094646w step 1111 i0229 094646633894 1186 mastercpp467 using default crammd5 authenticator 094646w step 1111 i0229 094646634003 1186 mastercpp536 using default basic http authenticator 094646w step 1111 i0229 094646634062 1184 recovercpp473 replica is in starting status 094646w step 1111 i0229 094646634109 1186 mastercpp570 authorization enabled 094646w step 1111 i0229 094646634249 1187 whitelist_watchercpp77 no whitelist given 094646w step 1111 i0229 094646634255 1184 hierarchicalcpp144 initialized hierarchical allocator process 094646w step 1111 i0229 094646634884 1187 replicacpp673 replica in starting status received a broadcasted recover request from 1456917230212437431 094646w step 1111 i0229 094646635278 1181 recovercpp193 received a recover response from a replica in starting status 094646w step 1111 i0229 094646635742 1187 recovercpp564 updating replica status to voting 094646w step 1111 i0229 094646636391 1180 mastercpp1711 the newly elected leader is master17230212437431 with id 3fbb2fb04f18498ba4409acbf6923a13 094646w step 1111 i0229 094646636415 1180 mastercpp1724 elected as the leading master 094646w step 1111 i0229 094646636430 1180 mastercpp1469 recovering from registrar 094646w step 1111 i0229 094646636554 1187 registrarcpp307 recovering registrar 094646w step 1111 i0229 094646637111 1181 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1120322ms 094646w step 1111 i0229 094646637133 1181 replicacpp320 persisted replica status to voting 094646w step 1111 i0229 094646637218 1186 recovercpp578 successfully joined the paxos group 094646w step 1111 i0229 094646637354 1186 recovercpp462 recover process terminated 094646w step 1111 i0229 094646637715 1182 logcpp659 attempting to start the writer 094646w step 1111 i0229 094646638617 1184 replicacpp493 replica received implicit promise request from 1457017230212437431 with proposal 1 094646w step 1111 i0229 094646639700 1184 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1057386ms 094646w step 1111 i0229 094646639722 1184 replicacpp342 persisted promised to 1 094646w step 1111 i0229 094646640251 1184 coordinatorcpp238 coordinator attempting to fill missing positions 094646w step 1111 i0229 094646641274 1185 replicacpp388 replica received explicit promise request from 1457117230212437431 for position 0 with proposal 2 094646w step 1111 i0229 094646642371 1185 leveldbcpp341 persisting action 8 bytes to leveldb took 1061574ms 094646w step 1111 i0229 094646642396 1185 replicacpp712 persisted action at 0 094646w step 1111 i0229 094646643299 1186 replicacpp537 replica received write request for position 0 from 1457217230212437431 094646w step 1111 i0229 094646643349 1186 leveldbcpp436 reading position from leveldb took 21735ns 094646w step 1111 i0229 094646644448 1186 leveldbcpp341 persisting action 14 bytes to leveldb took 106671ms 094646w step 1111 i0229 094646644469 1186 replicacpp712 persisted action at 0 094646w step 1111 i0229 094646645077 1181 replicacpp691 replica received learned notice for position 0 from 00000 094646w step 1111 i0229 094646646174 1181 leveldbcpp341 persisting action 16 bytes to leveldb took 1069097ms 094646w step 1111 i0229 094646646198 1181 replicacpp712 persisted action at 0 094646w step 1111 i0229 094646646211 1181 replicacpp697 replica learned nop action at position 0 094646w step 1111 i0229 094646646716 1182 logcpp675 writer started with ending position 0 094646w step 1111 i0229 094646647538 1183 leveldbcpp436 reading position from leveldb took 21456ns 094646w step 1111 i0229 094646648298 1186 registrarcpp340 successfully fetched the registry 0b in 1171072ms 094646w step 1111 i0229 094646648388 1186 registrarcpp439 applied 1 operations in 21138ns attempting to update the registry 094646w step 1111 i0229 094646648947 1187 logcpp683 attempting to append 210 bytes to the log 094646w step 1111 i0229 094646649050 1183 coordinatorcpp348 coordinator attempting to write append action at position 1 094646w step 1111 i0229 094646649655 1187 replicacpp537 replica received write request for position 1 from 1457317230212437431 094646w step 1111 i0229 094646650725 1187 leveldbcpp341 persisting action 229 bytes to leveldb took 1041938ms 094646w step 1111 i0229 094646650748 1187 replicacpp712 persisted action at 1 094646w step 1111 i0229 094646651198 1181 replicacpp691 replica received learned notice for position 1 from 00000 094646w step 1111 i0229 094646652312 1181 leveldbcpp341 persisting action 231 bytes to leveldb took 1092268ms 094646w step 1111 i0229 094646652335 1181 replicacpp712 persisted action at 1 094646w step 1111 i0229 094646652349 1181 replicacpp697 replica learned append action at position 1 094646w step 1111 i0229 094646653095 1187 registrarcpp484 successfully updated the registry in 4664064ms 094646w step 1111 i0229 094646653236 1187 registrarcpp370 successfully recovered registrar 094646w step 1111 i0229 094646653306 1181 logcpp702 attempting to truncate the log to 1 094646w step 1111 i0229 094646653476 1184 coordinatorcpp348 coordinator attempting to write truncate action at position 2 094646w step 1111 i0229 094646653642 1183 mastercpp1521 recovered 0 slaves from the registry 171b  allowing 10mins for slaves to reregister 094646w step 1111 i0229 094646653659 1181 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover 094646w step 1111 i0229 094646654270 1181 replicacpp537 replica received write request for position 2 from 1457417230212437431 094646w step 1111 i0229 094646655357 1181 leveldbcpp341 persisting action 16 bytes to leveldb took 1055267ms 094646w step 1111 i0229 094646655378 1181 replicacpp712 persisted action at 2 094646w step 1111 i0229 094646655850 1184 replicacpp691 replica received learned notice for position 2 from 00000 094646w step 1111 i0229 094646657009 1184 leveldbcpp341 persisting action 18 bytes to leveldb took 1137223ms 094646w step 1111 i0229 094646657059 1184 leveldbcpp399 deleting 1 keys from leveldb took 26459ns 094646w step 1111 i0229 094646657074 1184 replicacpp712 persisted action at 2 094646w step 1111 i0229 094646657089 1184 replicacpp697 replica learned truncate action at position 2 094646w step 1111 i0229 094646665710 1166 containerizercpp149 using isolation dockerruntimefilesystemlinux 094646w step 1111 i0229 094646672399 1166 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 094646w step 1111 e0229 094646676822 1166 shellhpp93 command hadoop version 21 failed this is the output 094646w step 1111 sh hadoop command not found 094646w step 1111 e0229 094646676851 1166 fetchercpp58 failed to create uri fetcher plugin hadoop failed to create hdfs client failed to execute hadoop version 21 the command was either not found or exited with a nonzero exit status 127 094646w step 1111 i0229 094646678383 1166 linuxcpp81 making tmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfv a shared mount 094646w step 1111 i0229 094646687223 1180 slavecpp193 slave started on 42217230212437431 094646w step 1111 i0229 094646687248 1180 slavecpp194 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfvcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfvfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_providersdocker image_provisioner_backendcopy initialize_driver_loggingtrue isolationdockerruntimefilesystemlinux launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfv 094646w step 1111 i0229 094646687531 1180 credentialshpp83 loading credential for authentication from tmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfvcredential 094646w step 1111 i0229 094646687666 1180 slavecpp324 slave using credential for testprincipal 094646w step 1111 i0229 094646687798 1180 resourcescpp572 parsing resources as json failed cpus2mem1024disk1024ports3100032000 094646w step 1111 trying semicolondelimited string format instead 094646w step 1111 i0229 094646688151 1180 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 094646w step 1111 i0229 094646688207 1180 slavecpp472 slave attributes   094646w step 1111 i0229 094646688217 1180 slavecpp477 slave hostname ip172302124mesosphereio 094646w step 1111 i0229 094646689259 1187 statecpp58 recovering state from tmpprovisionerdockerregistrypullertest_root_internet_curl_shellcommand_5bwcfvmeta 094646w step 1111 i0229 094646689394 1166 schedcpp222 version 0280 094646w step 1111 i0229 094646689497 1180 status_update_managercpp200 recovering status update manager 094646w step 1111 i0229 094646689798 1182 containerizercpp407 recovering containerizer 094646w step 1111 i0229 094646690021 1186 schedcpp326 new master detected at master17230212437431 094646w step 1111 i0229 094646690146 1186 schedcpp382 authenticating with master master17230212437431 094646w step 1111 i0229 094646690162 1186 schedcpp389 using default crammd5 authenticatee 094646w step 1111 i0229 094646690378 1181 authenticateecpp121 creating new client sasl connection 094646w step 1111 i0229 094646690688 1186 mastercpp5540 authenticating scheduler52603476875a49a885d4c98d102cdfab17230212437431 094646w step 1111 i0229 094646690801 1184 authenticatorcpp413 starting authentication session for crammd5_authenticatee87717230212437431 094646w step 1111 i0229 094646691025 1181 authenticatorcpp98 creating new server sasl connection 094646w step 1111 i0229 094646691314 1180 authenticateecpp212 received sasl authentication mechanisms crammd5 094646w step 1111 i0229 094646691339 1180 authenticateecpp238 attempting to authenticate with mechanism crammd5 094646w step 1111 i0229 094646691437 1180 authenticatorcpp203 received sasl authentication start 094646w step 1111 i0229 094646691490 1180 authenticatorcpp325 authentication requires more steps 094646w step 1111 i0229 094646691581 1180 authenticateecpp258 received sasl authentication step 094646w step 1111 i0229 094646691684 1180 authenticatorcpp231 received sasl authentication step 094646w step 1111 i0229 094646691712 1180 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302124mesosphereio server fqdn ip172302124mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false 094646w step 1111 i0229 094646691726 1180 auxpropcpp179 looking up auxiliary property userpassword 094646w step 1111 i0229 094646691768 1180 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 094646w step 1111 i0229 094646691802 1180 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302124mesosphereio server fqdn ip172302124mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true 094646w step 1111 i0229 094646691817 1180 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 094646w step 1111 i0229 094646691829 1180 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true 094646w step 1111 i0229 094646691848 1180 authenticatorcpp317 authentication success 094646w step 1111 i0229 094646691944 1186 authenticateecpp298 authentication success 094646w step 1111 i0229 094646692011 1185 mastercpp5570 successfully authenticated principal testprincipal at scheduler52603476875a49a885d4c98d102cdfab17230212437431 094646w step 1111 i0229 094646692056 1187 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee87717230212437431 094646w step 1111 i0229 094646692308 1184 schedcpp471 successfully authenticated with master master17230212437431 094646w step 1111 i0229 094646692325 1184 schedcpp776 sending subscribe call to master17230212437431 094646w step 1111 i0229 094646692399 1184 schedcpp809 will retry registration in 954231367ms if necessary 094646w step 1111 i0229 094646692505 1183 mastercpp2279 received subscribe call for framework default at scheduler52603476875a49a885d4c98d102cdfab17230212437431 094646w step 1111 i0229 094646692553 1183 mastercpp1750 authorizing framework principal testprincipal to receive offers for role  094646w step 1111 i0229 094646692836 1184 mastercpp2350 subscribing framework default with checkpointing disabled and capabilities   094646w step 1111 i0229 094646692942 1183 metadata_managercpp188 no images to load from disk docker provisioner image storage path tmpmesosstoredockerstoredimages does not exist 094646w step 1111 i0229 094646693208 1180 provisionercpp245 provisioner recovery complete 094646w step 1111 i0229 094646693295 1186 hierarchicalcpp265 added framework 3fbb2fb04f18498ba4409acbf6923a130000 094646w step 1111 i0229 094646693357 1186 hierarchicalcpp1437 no resources available to allocate 094646w step 1111 i0229 094646693397 1186 hierarchicalcpp1532 no inverse offers to send out 094646w step 1111 i0229 094646693424 1186,3
implement base tests for unified container using local puller using command line executor to test shell commands with local docker images,2
remove internal usage of deprecated json endpoints we still use the deprecated json internally ui tests documentation,3
add end to end testing for appc images add tests that covers integration test of the appc provisioner feature with mesos containerizer,3
add documentation for appc image discovery add documentation for the appc image discovery feature that covers  use case  implementation detail simple discovery,3
need to set exposed ports from docker images into containerconfig most docker images have an expose command associated with them this tells the container runtime the tcp ports that the microservice wishes to expose to the outside world with the unified containerizer project since mesoscontainerizer is going to natively support docker images it is imperative that the mesos container run time have a mechanism to expose ports listed in a docker image the first step to achieve this is to extract this information from the docker image and set in the containerconfig  the containerconfig can then be used to pass this information to any isolator for eg networkcni isolator that will install port forwarding rules to expose the desired ports,1
introduce a port field in imagemanifest in order to set exposed ports for a container networking isolators such as networkcni need to learn about ports that a container wishes to be exposed to the outside world this can be achieved by adding a field to the imagemanifest protobuf and allowing the imageprovisioner to set these fields to inform the isolator of the ports that the container wishes to be exposed,1
add support for local image fetching in appc provisioner currently appc image provisioner supports https fetching it would be valuable to add support for local file pathuri based fetching,2
implement port forwarding in networkcni isolator most docker and appc images wish to expose ports that microservices are listening on to the outside world when containers are running on bridged or ptp networking this can be achieved by installing port forwarding rules on the agent using iptables this can be done in the networkcni isolator the reason we would like this functionality to be implemented in the networkcni isolator and not a cni plugin is that the specifications currently do not support specifying port forwarding rules further to install these rules the isolator needs two pieces of information the exposed ports and the ip address associated with the container bother are available to the isolator,2
filesystemlinux isolator does not unmount orphaned persistent volumes a persistent volume can be orphaned when  a framework registers with checkpointing enabled  the framework starts a task  a persistent volume  the agent exits the task continues to run  something wipes the agents meta directory this removes the checkpointed framework info from the agent  the agent comes back and recovers the framework for the task is not found so the task is considered orphaned now the agent currently does not unmount the persistent volume saying with glog_v1 code i0229 235542078940 5635 linuxcpp711 ignoring cleanup request for unknown container a35189d385d54d02b56867f675b6dc97 code test implemented here httpsreviewsapacheorgr44122,2
masters slave reregister logic does not update version field the masters logic for reregistering a slave does not update the version field if the slave reregisters with a new version,1
remove grace_period_seconds field from shutdown event v1 protobuf there are two ways in which a shutdown of executor can be triggered 1 if it receives an explicit shutdown message from the agent 2 if the recovery timeout period has elapsed and the executor still hasnt been able to reconnect with the agent currently the executor library relies on the field grace_period_seconds having a default value of 5 seconds to handle the second scenario httpsgithubcomapachemesosblobmastersrcexecutorexecutorcppl608 the driver used to trigger the grace period via a constant defined in srcslaveconstantscpp httpsgithubcomapachemesosblobmastersrcexecexeccppl92 the agent may want to force a shorter shutdown grace period eg oversubscription eviction may have shorter deadline in the future for now we can just read the value via an environment variable,3
bind docker runtime isolator with docker image provider if image provider is specified as docker but dockerruntime is not set it would be not meaningful because of no executables a check should be added to make sure docker runtime isolator is on if using docker as image provider,1
dockercontainerizertestroot_docker_recoverorphanedpersistentvolumes exits when the tmp directory is bindmounted if the tmp directory where mesos tests create temporary directories is a bind mount the test suite will exit here code  run  dockercontainerizertestroot_docker_recoverorphanedpersistentvolumes i0226 031726722806 1097 leveldbcpp174 opened db in 12587676ms i0226 031726723496 1097 leveldbcpp181 compacted db in 636999ns i0226 031726723536 1097 leveldbcpp196 created db iterator in 18271ns i0226 031726723547 1097 leveldbcpp202 seeked to beginning of db in 1555ns i0226 031726723554 1097 leveldbcpp271 iterated through 0 keys in the db in 363ns i0226 031726723593 1097 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0226 031726724128 1117 recovercpp447 starting replica recovery i0226 031726724367 1117 recovercpp473 replica is in empty status i0226 031726725237 1117 replicacpp673 replica in empty status received a broadcasted recover request from 1381017230215151934 i0226 031726725744 1114 recovercpp193 received a recover response from a replica in empty status i0226 031726726356 1111 mastercpp376 master 5cc57c0ef1ad4107893f420ed1a1db1a ip172302151mesosphereio started on 17230215151934 i0226 031726726369 1118 recovercpp564 updating replica status to starting i0226 031726726378 1111 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpdjhtvqcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpdjhtvqmaster zk_session_timeout10secs i0226 031726726605 1111 mastercpp423 master only allowing authenticated frameworks to register i0226 031726726616 1111 mastercpp428 master only allowing authenticated slaves to register i0226 031726726632 1111 credentialshpp35 loading credentials for authentication from tmpdjhtvqcredentials i0226 031726726860 1111 mastercpp468 using default crammd5 authenticator i0226 031726726977 1111 mastercpp537 using default basic http authenticator i0226 031726727092 1111 mastercpp571 authorization enabled i0226 031726727243 1118 hierarchicalcpp144 initialized hierarchical allocator process i0226 031726727285 1116 whitelist_watchercpp77 no whitelist given i0226 031726728852 1114 mastercpp1712 the newly elected leader is master17230215151934 with id 5cc57c0ef1ad4107893f420ed1a1db1a i0226 031726728876 1114 mastercpp1725 elected as the leading master i0226 031726728891 1114 mastercpp1470 recovering from registrar i0226 031726728977 1117 registrarcpp307 recovering registrar i0226 031726731503 1112 leveldbcpp304 persisting metadata 8 bytes to leveldb took 4977811ms i0226 031726731539 1112 replicacpp320 persisted replica status to starting i0226 031726731711 1111 recovercpp473 replica is in starting status i0226 031726732501 1114 replicacpp673 replica in starting status received a broadcasted recover request from 1381217230215151934 i0226 031726732862 1111 recovercpp193 received a recover response from a replica in starting status i0226 031726733264 1117 recovercpp564 updating replica status to voting i0226 031726733836 1118 leveldbcpp304 persisting metadata 8 bytes to leveldb took 388246ns i0226 031726733855 1118 replicacpp320 persisted replica status to voting i0226 031726733979 1113 recovercpp578 successfully joined the paxos group i0226 031726734149 1113 recovercpp462 recover process terminated i0226 031726734478 1111 logcpp659 attempting to start the writer i0226 031726735523 1114 replicacpp493 replica received implicit promise request from 1381317230215151934 with proposal 1 i0226 031726736130 1114 leveldbcpp304 persisting metadata 8 bytes to leveldb took 576451ns i0226 031726736150 1114 replicacpp342 persisted promised to 1 i0226 031726736709 1115 coordinatorcpp238 coordinator attempting to fill missing positions i0226 031726737771 1114 replicacpp388 replica received explicit promise request from 1381417230215151934 for position 0 with proposal 2 i0226 031726738386 1114 leveldbcpp341 persisting action 8 bytes to leveldb took 583184ns i0226 031726738404 1114 replicacpp712 persisted action at 0 i0226 031726739312 1118 replicacpp537 replica received write request for position 0 from 1381517230215151934 i0226 031726739367 1118 leveldbcpp436 reading position from leveldb took 26157ns i0226 031726740638 1118 leveldbcpp341 persisting action 14 bytes to leveldb took 1238477ms i0226 031726740669 1118 replicacpp712 persisted action at 0 i0226 031726741158 1118 replicacpp691 replica received learned notice for position 0 from 00000 i0226 031726742878 1118 leveldbcpp341 persisting action 16 bytes to leveldb took 1697254ms i0226 031726742902 1118 replicacpp712 persisted action at 0 i0226 031726742916 1118 replicacpp697 replica learned nop action at position 0 i0226 031726743393 1117 logcpp675 writer started with ending position 0 i0226 031726744370 1112 leveldbcpp436 reading position from leveldb took 34329ns i0226 031726745240 1117 registrarcpp340 successfully fetched the registry 0b in 1621888ms i0226 031726745350 1117 registrarcpp439 applied 1 operations in 30460ns attempting to update the registry i0226 031726746016 1111 logcpp683 attempting to append 210 bytes to the log i0226 031726746119 1116 coordinatorcpp348 coordinator attempting to write append action at position 1 i0226 031726746798 1114 replicacpp537 replica received write request for position 1 from 1381617230215151934 i0226 031726747251 1114 leveldbcpp341 persisting action 229 bytes to leveldb took 411333ns i0226 031726747269 1114 replicacpp712 persisted action at 1 i0226 031726747808 1113 replicacpp691 replica received learned notice for position 1 from 00000 i0226 031726749511 1113 leveldbcpp341 persisting action 231 bytes to leveldb took 1673488ms i0226 031726749534 1113 replicacpp712 persisted action at 1 i0226 031726749550 1113 replicacpp697 replica learned append action at position 1 i0226 031726750422 1111 registrarcpp484 successfully updated the registry in 5021952ms i0226 031726750560 1111 registrarcpp370 successfully recovered registrar i0226 031726750635 1112 logcpp702 attempting to truncate the log to 1 i0226 031726750751 1113 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0226 031726751096 1116 mastercpp1522 recovered 0 slaves from the registry 171b  allowing 10mins for slaves to reregister i0226 031726751126 1111 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0226 031726751561 1118 replicacpp537 replica received write request for position 2 from 1381717230215151934 i0226 031726751999 1118 leveldbcpp341 persisting action 16 bytes to leveldb took 406823ns i0226 031726752018 1118 replicacpp712 persisted action at 2 i0226 031726752521 1113 replicacpp691 replica received learned notice for position 2 from 00000 i0226 031726754161 1113 leveldbcpp341 persisting action 18 bytes to leveldb took 1614888ms i0226 031726754210 1113 leveldbcpp399 deleting 1 keys from leveldb took 26384ns i0226 031726754225 1113 replicacpp712 persisted action at 2 i0226 031726754240 1113 replicacpp697 replica learned truncate action at position 2 i0226 031726765103 1115 slavecpp193 slave started on 39917230215151934 i0226 031726765130 1115 slavecpp194 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpdockercontainerizertest_root_docker_recoverorphanedpersistentvolumes_ajoespcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpdockercontainerizertest_root_docker_recoverorphanedpersistentvolumes_ajoespfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpu2mem2048diskrole12048 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpdockercontainerizertest_root_docker_recoverorphanedpersistentvolumes_ajoesp i0226 031726765403 1115 credentialshpp83 loading credential for authentication from tmpdockercontainerizertest_root_docker_recoverorphanedpersistentvolumes_ajoespcredential i0226 031726765573 1115 slavecpp324 slave using credential for testprincipal i0226 031726765733 1115 resourcescpp576 parsing resources as json failed cpu2mem2048diskrole12048 trying semicolondelimited string format instead i0226 031726766185 1115 slavecpp464 slave resources cpu2 mem2048 diskrole12048 cpus8 ports3100032000 i0226 031726766242 1115 slavecpp472 slave attributes   i0226 031726766250 1115 slavecpp477 slave hostname ip172302151mesosphereio i0226 031726767325 1097 schedcpp222 version 0280 i0226 031726767390 1111 statecpp58 recovering state from tmpdockercontainerizertest_root_docker_recoverorphanedpersistentvolumes_ajoespmeta i0226 031726767603 1115 status_update_managercpp200 recovering status update manager i0226 031726767865 1113 dockercpp726 recovering docker containers i0226 031726767971 1111 schedcpp326 new master detected at master17230215151934 i0226 031726768045 1111 schedcpp382 authenticating with master master17230215151934 i0226 031726768059 1111 schedcpp389 using default crammd5 authenticatee i0226 031726768070 1118 slavecpp4565 finished recovery i0226 031726768273 1112 authenticateecpp121 creating new client sasl connection i0226 031726768435 1118 slavecpp4737 querying resource estimator for oversubscribable resources i0226 031726768565 1111 mastercpp5526 authenticating schedulerc59020d6385e48a38a109e5c3f1dbd9217230215151934 i0226 031726768661 1118 slavecpp796 new master detected at master17230215151934 i0226 031726768659 1115 authenticatorcpp413 starting authentication session for crammd5_authenticatee83917230215151934 i0226 031726768679 1113 status_update_managercpp174 pausing sending status updates i0226 031726768728 1118 slavecpp859 authenticating with master master17230215151934 i0226 031726768743 1118 slavecpp864 using default crammd5 authenticatee i0226 031726768865 1118 slavecpp832 detecting new master i0226 031726768868 1112 authenticatorcpp98 creating new server sasl connection i0226 031726768908 1114 authenticateecpp121 creating new client sasl connection i0226 031726769003 1118 slavecpp4751 received oversubscribable resources from the resource estimator i0226 031726769103 1115 authenticateecpp212 received sasl authentication mechanisms crammd5 i0226 031726769131 1115 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0226 031726769209 1116 mastercpp5526 authenticating slave39917230215151934 i0226 031726769253 1114 authenticatorcpp203 received sasl authentication start i0226 031726769295 1115 authenticatorcpp413 starting authentication session for crammd5_authenticatee84017230215151934 i0226 031726769307 1114 authenticatorcpp325 authentication requires more steps i0226 031726769403 1117 authenticateecpp258 received sasl authentication step i0226 031726769495 1114 authenticatorcpp98 creating new server sasl connection i0226 031726769531 1115 authenticatorcpp231 received sasl authentication step i0226 031726769554 1115 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302151mesosphereio server fqdn ip172302151mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0226 031726769562 1115 auxpropcpp179 looking up auxiliary property userpassword i0226 031726769608 1115 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0226 031726769629 1115 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302151mesosphereio server fqdn ip172302151mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0226 031726769637 1115 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0226 031726769642 1115 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0226 031726769654 1115 authenticatorcpp317 authentication success i0226 031726769728 1117 authenticateecpp298 authentication success i0226 031726769769 1112 authenticateecpp212 received sasl authentication mechanisms crammd5 i0226 031726769767 1118 mastercpp5556 successfully authenticated principal testprincipal at schedulerc59020d6385e48a38a109e5c3f1dbd9217230215151934 i0226 031726769803 1112 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0226 031726769798 1114 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee83917230215151934 i0226 031726769881 1112 authenticatorcpp203 received sasl authentication start i0226 031726769932 1112 authenticatorcpp325 authentication requires more steps i0226 031726769981 1117 schedcpp471 successfully authenticated with master master17230215151934 i0226 031726770004 1117 schedcpp776 sending subscribe call to master17230215151934 i0226 031726770064 1118 authenticateecpp258 received sasl authentication step i0226 031726770102 1117 schedcpp809 will retry registration in 1937819802secs if necessary i0226 031726770165 1115 authenticatorcpp231 received sasl authentication step i0226 031726770193 1115 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302151mesosphereio server fqdn ip172302151mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0226 031726770207 1115 auxpropcpp179 looking up auxiliary property userpassword i0226 031726770213 1116 mastercpp2280 received subscribe call for framework default at schedulerc59020d6385e48a38a109e5c3f1dbd9217230215151934 i0226 031726770241 1115 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0226 031726770274 1115 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302151mesosphereio server fqdn ip172302151mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0226 031726770277 1116 mastercpp1751 authorizing framework principal testprincipal to receive offers for role role1 i0226 031726770298 1115 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0226 031726770331 1115 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0226 031726770349 1115 authenticatorcpp317 authentication success i0226 031726770428 1118 authenticateecpp298 authentication success i0226 031726770442 1116 mastercpp5556 successfully authenticated principal testprincipal at slave39917230215151934 i0226 031726770547 1116 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee84017230215151934 i0226 031726770846 1116 mastercpp2351 subscribing framework default with checkpointing enabled and capabilities   i0226 031726770866 1118 slavecpp927 successfully authenticated with master master17230215151934 i0226 031726770966 1118 slavecpp1321 will retry registration in 1453415ms if necessary i0226 031726771225 1115 hierarchicalcpp265 added framework 5cc57c0ef1ad4107893f420ed1a1db1a0000 i0226 031726771275 1118 schedcpp703 framework registered with 5cc57c0ef1ad4107893f420ed1a1db1a0000 i0226 031726771299 1115 hierarchicalcpp1434 no resources available to allocate i0226 031726771328 1115 hierarchicalcpp1529 no inverse offers to send out i0226 031726771344 1118 schedcpp717 schedulerregistered took 50146ns i0226 031726771356 1116 mastercpp4240 registering slave at slave39917230215151934 ip172302151mesosphereio with id 5cc57c0ef1ad4107893f420ed1a1db1as0 i0226 031726771348 1115 hierarchicalcpp1127 performed allocation for 0 slaves in 101438ns i0226 031726771860 1114 registrarcpp439 applied 1 operations in 59672ns attempting to update the registry i0226 031726772645 1117 logcpp683 attempting to append 423 bytes to the log i0226 031726772758 1112 coordinatorcpp348 coordinator attempting to write append action at position 3 i0226 031726773435 1117 replicacpp537 replica received write request for position 3 from 1382417230215151934 i0226 031726773586 1111 slavecpp1321 will retry registration in 274261ms if necessary i0226 031726773682 1115 mastercpp4228 ignoring register slave message from slave39917230215151934 ip172302151mesosphereio as admission is already in progress i0226 031726773937 1117 leveldbcpp341 persisting action 442 bytes to level,2
poor allocator performance with labeled resources andor persistent volumes modifying the hierarchicalallocator_benchmark_testresourcelabels benchmark from httpsreviewsapacheorgr43686 to use distinct labels between different slaves performance regresses from 2 seconds to 3 minutes the culprit seems to be the way in which the allocator merges together resources reserved resource labels or persistent volume ids inhibit merging which causes performance to be much worse,5
add file fetcher plugin add support for file based uri fetcher this could be useful for container image provisioning from local file system,2
cgroupsanyhierarchywithfreezertestroot_cgroups_destroytracedprocess is flaky verbose logs code  run  cgroupsanyhierarchywithfreezertestroot_cgroups_destroytracedprocess i0302 004314127846 11755 cgroupscpp2427 freezing cgroup sysfscgroupfreezermesos_test i0302 004314267411 11758 cgroupscpp1409 successfully froze cgroup sysfscgroupfreezermesos_test after 13946496ms i0302 004314409395 11751 cgroupscpp2445 thawing cgroup sysfscgroupfreezermesos_test i0302 004314551304 11751 cgroupscpp1438 successfullly thawed cgroup sysfscgroupfreezermesos_test after 141811968ms srctestscontainerizercgroups_testscpp949 failure value of waitpidpid status 0 actual 23809 expected 1 srctestscontainerizercgroups_testscpp950 failure value of __errno_location  actual 0 expected 10  failed  cgroupsanyhierarchywithfreezertestroot_cgroups_destroytracedprocess 1055 ms code,2
fix rmdir for windows this is due to a bug in mesos4415 that landed for 0270,1
move placement new processes into the freezer cgroup into a parent hook the linux launcher places new processes into the freezer cgroup this is currently done by a combination of childsetup function blocking the new process until parent is done and the parent placing child process into the cgroup and then signaling child to continue parenthooks support this behavior blocking child until some work is done in the parent in a much cleaner way,3
remove internal usage of deprecated shutdownframework acl shutdownframework acl was deprecated a couple of versions ago in favor of the teardownframework message its deprecation cycle came with 027 that means we should remove the message and its references in the code base,2
add authentication to master endpoints before we can add authorization around operator endpoints we need to add authentication support so that unauthenticated requests are denied when authenticate_http is enabled and so that the principal is passed into route,2
agent authn research spike research the master authentication flags to see what changes will be necessary for agent http authentication write up a 12 page summarydesign doc,2
add agent flags for http authentication flags should be added to the agent to 1 enable http authentication authenticate_http 2 specify credentials http_credentials 3 specify http authenticators authenticators,2
add authentication to agent endpoints state and flags the state and flags endpoints are installed in srcslaveslavecpp and thus are straightforward to make authenticated other agent endpoints require a bit more consideration and are tracked in mesos4902 for more information on agent endpoints see httpmesosapacheorgdocumentationlatestendpoints or search for route in the source code code  grep rn route src grep v master grep v tests grep v json srcversionversioncpp75 route version_help versionprocessversion srcfilesfilescpp150 routebrowse srcfilesfilescpp153 routeread srcfilesfilescpp156 routedownload srcfilesfilescpp159 routedebug srcslaveslavecpp580 routeapiv1executor srcslaveslavecpp595 routestate srcslaveslavecpp601 routeflags srcslaveslavecpp607 routehealth srcslavemonitorcpp100 routestatistics  grep rn route 3rdparty grep v tests grep v readme grep v examples grep v help grep v processpp 3rdpartylibprocessincludeprocessprofilerhpp34 routestart start_help profilerstart 3rdpartylibprocessincludeprocessprofilerhpp35 routestop stop_help profilerstop 3rdpartylibprocessincludeprocesssystemhpp70 routestatsjson statshelp systemstats 3rdpartylibprocessincludeprocesslogginghpp44 routetoggle toggle_help thistoggle code,3
update changelog with net_cls isolator need to update the changelog for 028 release,1
make changes to executor v1 library around managing connections while implementing pipelining changes for the scheduler library mesos3570 we noticed a couple of small bugs that we would like to fix in the executor library  dont pass connection objects to defer callbacks as they can sometimes lead to deadlocks  minor cleanups around not accepting subscribe call if one is currently in progress  create a random uuid connectionid before we initiate a connection to the agent as in some scenarios we can accept connection attempts from stale connections,3
add explicit upgrade instructions to the docs the documentation currently contains perversion upgrade guidelines which for recent releases only outlines the upgrade concerns for that version without detailing explicit upgrade instructions we should add explicit upgrade instructions to the top of the upgrades documentation which can be supplemented by the perversion concerns this is done within the upgrade docs for some early versions with text like code in order to upgrade a running cluster install the new master binaries and restart the masters upgrade the schedulers by linking the latest native library and mesos jar if necessary restart the schedulers install the new slave binaries and restart the slaves upgrade the executors by linking the latest native library and mesos jar if necessary code instructions to this effect should be featured prominently in the doc,1
add a script to install the nvidia gdk on a host this script can be used to install the nvidia gdk for cuda 75 on a mesos development machine the purpose of the nvidia gdk is to provide all the necessary header files nvmlh and library files libnvidiamlso necessary to build mesos with nvidia gpu support if the machine on which mesos is being compiled doesnt have any gpus then libnvidiamlso consists only of stubs allowing mesos to build and run but not actually do anything useful under the hood this enables us to build a gpuenabled mesos on a development machine without gpus and then deploy it to a production machine with gpus and be reasonably sure it will work,2
add configure flags to build with nvidia gpu support the configure flags can be used to enable nvidia gpu support as well as specify the installation directories of the nvml header and library files if not already installed in standard includelibrary paths on the system they will also be used to conditionally build support for nvidia gpus into mesos,2
add nvidia gpu isolator tests we need to be able to run unit tests that verify gpu isolation as well as run full blown tests that actually exercise the gpus these tests should only build when the proper configure flags are set for enabling nvidia gpu support,2
add flag to specify available nvidia gpus on an agents command line in the initial gpu support we will not do autodiscovery of gpus on an agent as such an operator will need to specify a flag on the command line listing all of the gpus available on the system,3
add gpus as an explicit resource we will add gpus as an explicitly recognized resource in mesos akin to cpus memory ports and disk in the containerizer we will verify that the number of gpu resources passed in via the resources flag matches the list of gpus passed in via the nvidia_gpus flag in the future we will add autodiscovery so this matching is unnecessary however we will always have to pass gpus as a resource to make any gpu available on the system unlike for cpus and memory where the default is probed,3
persistentvolumetests do not need to set up acls the persistentvolumetest s have a custom helper for setting up acls in the masterflags code acls acls hashsetstring roles foreach const frameworkinfo framework frameworks  mesosaclregisterframework acl  aclsadd_register_frameworks aclmutable_principalsadd_valuesframeworkprincipal aclmutable_rolesadd_valuesframeworkrole rolesinsertframeworkrole  flagsacls  acls flagsroles  stringsjoin roles code this is no longer necessary with implicit roles,1
dump the contents of the sandbox when a test fails berndmesos added this logic for extra info about a rare flaky test httpsgithubcomapachemesosblobd26baee1f377aedb148ad04cc004bb38b85ee4f6srctestsfetcher_cache_testscppl249l259 this information is useful regardless of the test type and should be generalized for clusterslave ie  when a clusterslave is destructed it can detect if the test has failed  if so navigate through its own work_dir and print sandboxes andor other useful debugging info also see the refactor in mesos4634,3
add documentation about container image support,5
mesos containerizer cant handle top level docker image like alpine must use libraryalpine this can be demonstrated with the mesosexecute command  docker containerizer with image alpine success code sudo buildsrcmesosexecute docker_imagealpine containerizerdocker namejustatest commandsleep 1000 masterlocalhost5050 code  mesos containerizer with image alpine failure code sudo buildsrcmesosexecute docker_imagealpine containerizermesos namejustatest commandsleep 1000 masterlocalhost5050 code  mesos containerizer with image libraryalpine success code sudo buildsrcmesosexecute docker_imagelibraryalpine containerizermesos namejustatest commandsleep 1000 masterlocalhost5050 code in the slave logs code ea446083 9c838da86af34c0007 i0306 163241418269 3403 metadata_managercpp159 looking for image alpinelatest i0306 163241418699 3403 registry_pullercpp194 pulling image alpinelatest from dockermanifestregistry1dockerio443alpinelatesthttps to tmpmesostest storedockerstagingka7mlq e0306 163243098131 3400 slavecpp3773 container 4bf9132d9a574baaa78ce7164e93ace6 for executor justatest of framework 4f055c6f1bea4460839c838da86af34c0 007 failed to start collect failed unexpected http response 401 unauthorized code curl command executed code  sudo sysdig a p evttime proccmdline evttypeexecve and procnamecurl 164253198998042 curl s s l d  httpsregistry1dockerio443v2alpinemanifestslatest 164253784958541 curl s s l d  httpsauthdockeriotokenserviceregistrydockerioscoperepositoryalpinepull 164254294192024 curl s s l d  h authorization bearer eyjhbgcioijfuzi1niisinr5cci6ikpxvcising1yyi6wyjnsuldthpdq0fku2dbd0lcqwdjqkfequtcz2dxagtqt1bruurbakjhtvvrd1fnwurwuvferxp0uk5gb3ppa2rytjbrnldgulfsrhbjvfrsuk9rovvwrmc2tmtgrlf6cfnuve5et2tgu01rttzumfkztnpwq1zrvkjpa2xhulvrnlexazftekflrncwee5uqtjnalv4t1rvmu5ewmfgdzb4tmpbmk1quxhpvfuxtkrayu1fwxhsrejdqmdovkjbtvrpmghhu1uwnldgzfzwam8yuvzksu9swlpuvek2ttfnmvrecfnwrekxt2s5vfnrbzztmvexumpwwvrssklpbfjmtmtnnlmxukxoanbcuvv0vu1ga3dfd1lis29asxpqmenbuvljs29asxpqmerbuwneuwdbrxl2uzivdei3t3jlmkvxcgrdefdts1nqv1n2vmj2twurwgvftunvmdbyqji0akniuvhrefdmoss0muxqmlznq29bk0rmrkiwvjbgzgdwajlowu5rl2pxt0jzaknccnpbt0jntlziuthcqwy4rujbtunbsuf3rhdzrfzsmgxcqwd3qmdzrvzsmgxbrejfqmdovkhrnevquve3u0vaslrucflwmvzxt2paqlywzzzwbgxotwpveldevk1pbepvtwpvnlqxtkttanbmvkrwr09sae9va2c2vkvzmlnecexwrxmyt2tgqlmxuxdsz1levliwakjeohdqwue3vvrsyu16cehwemrkt2xovvvfutztrtawvvrwufzgullpalpculvnnlvrmhprenbcvwpkre9roudoemm2uwxarlfucepsa1zkt2towk5vc3ddz1ljs29asxpqmevbd0leu1fbd1jnswhbtxzit2h4chhrtktqsdrhmfbns0lfdxrmtjztrdfvmws4zejovgxuwvfudkfpruf0yvjgsgjsr2o4zlvsszz4uvjhrurvqm1zz3dzelr3z3bmagjbzznoumfvpsjdfqeyjhy2nlc3mioltdlcjhdwqioijyzwdpc3ryes5kb2nrzxiuaw8ilcjlehaioje0ntcyodi4nzqsimlhdci6mtq1nzi4mju3ncwiaxnzijoiyxv0ac5kb2nrzxiuaw8ilcjqdgkioijaogtynxzxnejmwknirs1icvjiacisim5izii6mtq1nzi4mju3ncwic3viijoiin0c2wtjq_pm0buparhmqjdfh6ztiahcvgn3tfwizeclsgxlvq_saqxaalnzkwaql2chj7nphx0gwael_28aw httpsregistry1dockerio443v2alpinemanifestslatest code also got the same result with ubuntu docker image,3
update glog patch to support powerpc le this is a part of powerpc le porting,1
rescind all outstanding offers after changing some weights,2
add support for command and arguments to mesosexecute commandinfo protobuf support two kinds of command code  there are two ways to specify the command  1 if shell  true the command will be launched via shell  ie binsh c value the value specified will be  treated as the shell command the arguments will be ignored  2 if shell  false the command will be launched by passing  arguments to an executable the value specified will be  treated as the filename of the executable the arguments  will be treated as the arguments to the executable this is  similar to how posix exec families launch processes ie  execlpvalue arguments0 arguments1  code the mesosexecute cannot handle 2 now enabling 2 can help with testing and running one off tasks,5
support mesos containerizer force_pull_image option currently for unified containerizer images that are already cached by metadata manager cannot be updated user has to delete corresponding images in store if an update is need we should support force_pull_image option for unified containerizer to provide override option if existed,3
default cmd is executed as an incorrect command when mesos containerizer launch a container using a docker image which only container default cmd the executable command is is a incorrect sequence for example if an image default entrypoint is null cmd is sh user defines shellfalse value is none and arguments as c echo hello world the executable command is sh c echo hello world sh which is incorrect it should be sh sh c echo hello world instead this problem is only exposed for the case sh0 value0 argv1 entrypoint0 cmd1,2
implement runtime isolator tests there different cases in docker runtime isolator some special cases should be tested with unique test case to verify the docker runtime isolator logic is correct,5
add a containers endpoint to the agent to list all the active containers this endpoint will be similar to monitorstatisticsjson endpoint but itll also contain the container_status about the container see containerstatus in mesosproto well eventually deprecate the monitorstatisticsjson endpoint,8
add authentication to libprocess endpoints in addition to the endpoints addressed by mesos4850 and mesos5152 the following endpoints would also benefit from http authentication  profiler  loggingtoggle  metricssnapshot adding http authentication to these endpoints is a bit more complicated because they are defined at the libprocess level while working on mesos4850 it became apparent that since our tests use the same instance of libprocess for both master and agent different default authentication realms must be used for masteragent so that http authentication can be independently enableddisabled for each we should establish a mechanism for making an endpoint authenticated that allows us to 1 install an endpoint like files whose code is shared by the master and agent with different authentication realms for the master and agent 2 avoid hardcoding a default authentication realm into libprocess to permit the use of different authentication realms for the master and agent and to keep applicationlevel concerns from leaking into libprocess another option would be to use a single default authentication realm and always enable or disable http authentication for both the master and agent in tests however this wouldnt allow us to test scenarios where http authentication is enabled on one but disabled on the other,5
allow multiple loads of module manifests the modulemanagerload is designed to be called exactly once during a process lifetime this works well for masteragent environments however it can fail in scheduler environments for example a single scheduler binary might implement multiple scheduler drivers causing multiple calls to modulemanagerload leading to a failure,3
tasks cannot be killed forcefully currently there is no way for a scheduler to instruct the executor to kill a certain task immediately skipping any possible timeouts and  or kill policies this may be desirable in cases like eg the kill policy is 10 minutes but something went wrong so the scheduler decides to issue a forceful kill,5
introduce kill policy for tasks a task may require some time to clean up or even a special mechanism to issue a kill request currently its a sigterm followed by sigkill introducing kill policies per task will help address these issue,5
deprecate the docker_stop_timeout agent flag instead a combination of executor_shutdown_grace_period agent flag and optionally task kill policies should be used,1
executor driver does not respect executor shutdown grace period executor shutdown grace period configured on the agent is propagated to executors via the mesos_executor_shutdown_grace_period environment variable the executor driver must use this timeout to delay the hard shutdown of the related executor,1
linuxfilesystemisolatortestroot_multiplecontainers fails observed on our ci noformat 093415  step 1111  run  linuxfilesystemisolatortestroot_multiplecontainers 093419w step 1111 i0309 093419906719 2357 linuxcpp81 making tmpmlvlnv a shared mount 093419w step 1111 i0309 093419923548 2357 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 093419w step 1111 i0309 093419924705 2376 containerizercpp666 starting container da610f7fa7094de894d374f4a520619b for executor test_executor1 of framework  093419w step 1111 i0309 093419925355 2371 provisionercpp285 provisioning image rootfs tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0 for container da610f7fa7094de894d374f4a520619b 093419w step 1111 i0309 093419925881 2377 copycpp127 copying layer path tmpmlvlnvtest_image1 to rootfs tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0 093430w step 1111 i0309 093430835127 2376 linuxcpp355 bind mounting work directory from tmpmlvlnvslavestest_slaveframeworksexecutorstest_executor1runsda610f7fa7094de894d374f4a520619b to tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0mntmesossandbox for container da610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430835392 2376 linuxcpp683 changing the ownership of the persistent volume at tmpmlvlnvvolumesrolestest_rolepersistent_volume_id with uid 0 and gid 0 093430w step 1111 i0309 093430840425 2376 linuxcpp723 mounting tmpmlvlnvvolumesrolestest_rolepersistent_volume_id to tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0mntmesossandboxvolume for persistent volume disktest_rolepersistent_volume_idvolume32 of container da610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430843878 2374 linux_launchercpp304 cloning child process with flags  clone_newns 093430w step 1111 i0309 093430848302 2371 containerizercpp666 starting container fe4729c51e634cc6a2e3fe5006ffe087 for executor test_executor2 of framework  093430w step 1111 i0309 093430848758 2371 containerizercpp1392 destroying container da610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430848865 2373 provisionercpp285 provisioning image rootfs tmpmlvlnvprovisionercontainersfe4729c51e634cc6a2e3fe5006ffe087backendscopyrootfses518b246443dd47b09648e78aedde6917 for container fe4729c51e634cc6a2e3fe5006ffe087 093430w step 1111 i0309 093430849449 2375 copycpp127 copying layer path tmpmlvlnvtest_image2 to rootfs tmpmlvlnvprovisionercontainersfe4729c51e634cc6a2e3fe5006ffe087backendscopyrootfses518b246443dd47b09648e78aedde6917 093430w step 1111 i0309 093430854038 2374 cgroupscpp2427 freezing cgroup sysfscgroupfreezermesosda610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430856693 2372 cgroupscpp1409 successfully froze cgroup sysfscgroupfreezermesosda610f7fa7094de894d374f4a520619b after 2608128ms 093430w step 1111 i0309 093430859237 2377 cgroupscpp2445 thawing cgroup sysfscgroupfreezermesosda610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430861454 2377 cgroupscpp1438 successfullly thawed cgroup sysfscgroupfreezermesosda610f7fa7094de894d374f4a520619b after 2176us 093430w step 1111 i0309 093430934608 2378 containerizercpp1608 executor for container da610f7fa7094de894d374f4a520619b has exited 093430w step 1111 i0309 093430937692 2372 linuxcpp798 unmounting volume tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0mntmesossandboxvolume for container da610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430937742 2372 linuxcpp817 unmounting sandboxwork directory tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0mntmesossandbox for container da610f7fa7094de894d374f4a520619b 093430w step 1111 i0309 093430938129 2375 provisionercpp330 destroying container rootfs at tmpmlvlnvprovisionercontainersda610f7fa7094de894d374f4a520619bbackendscopyrootfses0d7e047a50f1490bbb5800e9c49628d0 for container da610f7fa7094de894d374f4a520619b 093445  step 1111 srctestscontainerizerfilesystem_isolator_testscpp1318 failure 093445  step 1111 failed to wait 15secs for wait1 093448  step 1111  failed  linuxfilesystemisolatortestroot_multiplecontainers 32341 ms noformat,3
processormanager delegate should be an optionstring not just a string currently the delegate field in the processmanager is just a string type we check for existence of a delegate by comparing delegate   using an option is the preferred method for things like this,1
allow modules to express if they are multiinstantiable and thread safe a module might be instantiated multiple time eg multiple schedulers in the same java process instantiating an authenticator module within the same process the current mechanism doesnt provide a way through the module api to forbid multiple instantiations it is up to the module to check and return error on prior instantiation along similar lines a module should be able to express threadsafety concerns typically a module running in masteragent doesnt have to be concerned about thread safety if it uses libprocess api however we should investigate how it plays in the scheduler environment,8
replace nonpod static variables in modulemanagerchpp with pod eqivalents,3
cache module manifests while loading in modulemanager since the module managers are allowed to load the same module multiple times we should be caching the module manifests to avoid cases where the module tries to trick the module manager by changing modulebase fields before the next call to modulemanagerload,3
setup proper etchostname etchosts and etcresolvconf for containers in networkcni isolator the networkcni isolator needs to properly setup etchostname and etchosts for the container with a hostname eg randomly generated and the assigned ip returned by cni plugin we should consider the following cases 1 container is using host filesystem 2 container is using a different filesystem 3 custom executor and command executor,5
add a list parser for comma separated integers in flags some flags require lists of integers to be passed in we should have an explicit parser for this instead of relying on ad hoc solutions,2
the flag parser for hashmapstring string should live in stout not mesos the title says it all,1
remove all get calls on option  try variables in the resources abstraction when possible get calls should be replaced by  for option  try variables this ticket only proposes a blanket change for this in the resource abstraction files not the code base as a whole this is in preparation for introducing the new gpu resource without this change i would need to use the old get calls instead i propose to fix the old code surrounding it so that consistency has me doing it the right way,1
propose design for authorization based filtering for endpoints the design doc can be found here httpsdocsgooglecomdocumentd1m27s7otsfj8afzckloz00g_wcvrl32i9lyl6g22gwey,5
registrar http authentication now that the master and agents in progress provide http authentication the registrar should do the same see httpmesosapacheorgdocumentationlatestendpointsregistrarregistry,3
enable help to include authentication status of endpoint as we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones dont,2
investigate container security options for mesos containerizer we should investigate the following to improve the container security for mesos containerizer and come up with a list of features that we want to support in mvp 1 capabilities 2 user namespace 3 seccomp 4 selinux 5 apparmor we should investigate what other container systems are doing regarding security 1 k8s httpsgithubcomkuberneteskubernetesblobmasterpkgapiv1typesgol2905 2 dockerhttpsdocsdockercomenginesecuritysecurity 3 ocihttpsgithubcomopencontainersspecsblobmasterconfigmd,5
support docker registry authentication,5
support specifying percontainer docker registry currently we only support a per agent flag to specify the docker registry we should instead allow people to specify the registry as part of the docker image name like docker pull does,3
support update existing quota we want to support updating an existing quota without the cycle of delete and recreate this avoids the possible starvation risk of losing the quota between delete and recreate and also makes the interface friendly design doc httpsdocsgooglecomdocumentd1c8fjy9_n0w04ftuq_b_kzm6s0eepu7eyvyfup14dsys,8
docker runtime isolator tests may cause disk issue currently slave working directory is used as docker store dir and archive dir which is problematic because slave work dir is exactly environmentmkdtemp it will get cleaned up until the end of the whole test but the runtime isolator local puller tests cp the hosts rootfs which size is relatively big cleanup has to be done by each test tear down,2
reduce the size of linuxrootfs in tests right now linuxrootfs copies files from the host filesystem to construct a chrootable rootfs we copy a lot of unnecessary files making it very large we can potentially strip a lot files,13
improve overlay backend so that its writable currently the overlay backend will provision a readonly fs we can use an empty directory from the container sandbox to act as the upper layer so that its writable,5
executor shutdown grace period should be configurable currently executor shutdown grace period is specified by an agent flag which is propagated to executors via the mesos_executor_shutdown_grace_period environment variable there is no way to adjust this timeout for the needs of a particular executor to tackle this problem we propose to introduce an optional shutdown_grace_period field in executorinfo,3
implement reconnect funtionality in the scheduler library currently there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library srcschedulerschedulercpp it is specifically useful in scenarios where there is a one way network partition with the master due to this the scheduler has not received any heartbeat events from the master in this case the scheduler might want to force a reconnection attempt with the master instead of relying on the disconnected callback,3
enable actors to pass an authentication realm to libprocess to prepare for mesos4902 the mesos master and agent need a way to pass the desired authentication realm to libprocess since some endpoints like profiler get installed in libprocess the masteragent should be able to specify during initialization what authentication realm the libprocesslevel endpoints will be authenticated under,2
add authentication to files endpoints to protect access authz to masteragent logs as well as executor sandboxes we need authentication on the files endpoints adding http authentication to these endpoints is a bit complicated since they are defined in code that is shared by the master and agent while working on mesos4850 it became apparent that since our tests use the same instance of libprocess for both master and agent different default authentication realms must be used for masteragent so that http authentication can be independently enableddisabled for each we should establish a mechanism for making an endpoint authenticated that allows us to 1 install an endpoint like files whose code is shared by the master and agent with different authentication realms for the master and agent 2 avoid hardcoding a default authentication realm into libprocess to permit the use of different authentication realms for the master and agent and to keep applicationlevel concerns from leaking into libprocess another option would be to use a single default authentication realm and always enable or disable http authentication for both the master and agent in tests however this wouldnt allow us to test scenarios where http authentication is enabled on one but disabled on the other,5
containerloggertestlogrotate_rotateinsandbox is flaky the logger subprocesses may exit before we reach the waitpid in the test if this happens waitpid will return a 1 as the process no longer exists verbose logs code  run  containerloggertestlogrotate_rotateinsandbox i0316 142851329337 1242 clustercpp139 creating default local authorizer i0316 142851332823 1242 leveldbcpp174 opened db in 3079559ms i0316 142851333916 1242 leveldbcpp181 compacted db in 1054247ms i0316 142851333979 1242 leveldbcpp196 created db iterator in 21450ns i0316 142851334005 1242 leveldbcpp202 seeked to beginning of db in 2205ns i0316 142851334025 1242 leveldbcpp271 iterated through 0 keys in the db in 410ns i0316 142851334089 1242 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0316 142851334661 1275 recovercpp447 starting replica recovery i0316 142851335044 1275 recovercpp473 replica is in empty status i0316 142851336207 1262 replicacpp673 replica in empty status received a broadcasted recover request from 484172170345919 i0316 142851336730 1270 recovercpp193 received a recover response from a replica in empty status i0316 142851337257 1275 recovercpp564 updating replica status to starting i0316 142851338001 1267 leveldbcpp304 persisting metadata 8 bytes to leveldb took 537200ns i0316 142851338032 1267 replicacpp320 persisted replica status to starting i0316 142851338183 1261 mastercpp376 master c7653f6033e944069f62dc74c906bf83 2cbb23302fe5 started on 172170345919 i0316 142851338295 1263 recovercpp473 replica is in starting status i0316 142851338213 1261 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmpxtqwkscredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0290_instsharemesoswebui work_dirtmpxtqwksmaster zk_session_timeout10secs i0316 142851338562 1261 mastercpp423 master only allowing authenticated frameworks to register i0316 142851338572 1261 mastercpp428 master only allowing authenticated slaves to register i0316 142851338580 1261 credentialshpp35 loading credentials for authentication from tmpxtqwkscredentials i0316 142851338877 1261 mastercpp468 using default crammd5 authenticator i0316 142851339030 1262 replicacpp673 replica in starting status received a broadcasted recover request from 485172170345919 i0316 142851339246 1261 mastercpp537 using default basic http authenticator i0316 142851339393 1261 mastercpp571 authorization enabled i0316 142851339390 1266 recovercpp193 received a recover response from a replica in starting status i0316 142851339606 1271 whitelist_watchercpp77 no whitelist given i0316 142851339607 1275 hierarchicalcpp144 initialized hierarchical allocator process i0316 142851340077 1268 recovercpp564 updating replica status to voting i0316 142851340533 1270 leveldbcpp304 persisting metadata 8 bytes to leveldb took 331558ns i0316 142851340558 1270 replicacpp320 persisted replica status to voting i0316 142851340672 1270 recovercpp578 successfully joined the paxos group i0316 142851340827 1270 recovercpp462 recover process terminated i0316 142851341684 1270 mastercpp1806 the newly elected leader is master172170345919 with id c7653f6033e944069f62dc74c906bf83 i0316 142851341717 1270 mastercpp1819 elected as the leading master i0316 142851341740 1270 mastercpp1508 recovering from registrar i0316 142851341954 1263 registrarcpp307 recovering registrar i0316 142851342499 1273 logcpp659 attempting to start the writer i0316 142851343616 1266 replicacpp493 replica received implicit promise request from 487172170345919 with proposal 1 i0316 142851344183 1266 leveldbcpp304 persisting metadata 8 bytes to leveldb took 536941ns i0316 142851344208 1266 replicacpp342 persisted promised to 1 i0316 142851344825 1267 coordinatorcpp238 coordinator attempting to fill missing positions i0316 142851346009 1276 replicacpp388 replica received explicit promise request from 488172170345919 for position 0 with proposal 2 i0316 142851346371 1276 leveldbcpp341 persisting action 8 bytes to leveldb took 327890ns i0316 142851346393 1276 replicacpp712 persisted action at 0 i0316 142851347363 1267 replicacpp537 replica received write request for position 0 from 489172170345919 i0316 142851347414 1267 leveldbcpp436 reading position from leveldb took 24861ns i0316 142851347774 1267 leveldbcpp341 persisting action 14 bytes to leveldb took 323654ns i0316 142851347796 1267 replicacpp712 persisted action at 0 i0316 142851348323 1276 replicacpp691 replica received learned notice for position 0 from 00000 i0316 142851348714 1276 leveldbcpp341 persisting action 16 bytes to leveldb took 361981ns i0316 142851348738 1276 replicacpp712 persisted action at 0 i0316 142851348760 1276 replicacpp697 replica learned nop action at position 0 i0316 142851349318 1274 logcpp675 writer started with ending position 0 i0316 142851350275 1267 leveldbcpp436 reading position from leveldb took 23849ns i0316 142851351171 1271 registrarcpp340 successfully fetched the registry 0b in 9173248ms i0316 142851351300 1271 registrarcpp439 applied 1 operations in 32119ns attempting to update the registry i0316 142851351989 1272 logcpp683 attempting to append 170 bytes to the log i0316 142851352108 1266 coordinatorcpp348 coordinator attempting to write append action at position 1 i0316 142851352802 1263 replicacpp537 replica received write request for position 1 from 490172170345919 i0316 142851353313 1263 leveldbcpp341 persisting action 189 bytes to leveldb took 474854ns i0316 142851353338 1263 replicacpp712 persisted action at 1 i0316 142851354101 1273 replicacpp691 replica received learned notice for position 1 from 00000 i0316 142851354483 1273 leveldbcpp341 persisting action 191 bytes to leveldb took 338210ns i0316 142851354507 1273 replicacpp712 persisted action at 1 i0316 142851354529 1273 replicacpp697 replica learned append action at position 1 i0316 142851355444 1275 registrarcpp484 successfully updated the registry in 4084224ms i0316 142851355569 1275 registrarcpp370 successfully recovered registrar i0316 142851355697 1268 logcpp702 attempting to truncate the log to 1 i0316 142851355870 1269 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0316 142851356016 1274 mastercpp1616 recovered 0 slaves from the registry 131b  allowing 10mins for slaves to reregister i0316 142851356032 1272 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0316 142851356761 1273 replicacpp537 replica received write request for position 2 from 491172170345919 i0316 142851357203 1273 leveldbcpp341 persisting action 16 bytes to leveldb took 406053ns i0316 142851357226 1273 replicacpp712 persisted action at 2 i0316 142851357718 1270 replicacpp691 replica received learned notice for position 2 from 00000 i0316 142851358093 1270 leveldbcpp341 persisting action 18 bytes to leveldb took 345370ns i0316 142851358175 1270 leveldbcpp399 deleting 1 keys from leveldb took 57us i0316 142851358201 1270 replicacpp712 persisted action at 2 i0316 142851358220 1270 replicacpp697 replica learned truncate action at position 2 i0316 142851368399 1242 containerizercpp149 using isolation posixcpuposixmemfilesystemposix w0316 142851406371 1242 backendcpp66 failed to create bind backend bindbackend requires root privileges i0316 142851410480 1266 slavecpp193 slave started on 12172170345919 i0316 142851410518 1266 slavecpp194 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs container_loggerorg_apache_mesos_logrotatecontainerlogger containerizersmesos credentialtmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gycredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gyfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0290_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gy i0316 142851411118 1266 credentialshpp83 loading credential for authentication from tmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gycredential i0316 142851411381 1266 slavecpp324 slave using credential for testprincipal i0316 142851411696 1266 resourcescpp572 parsing resources as json failed cpus2mem1024disk1024ports3100032000 trying semicolondelimited string format instead i0316 142851412075 1266 slavecpp464 slave resources cpus2 mem1024 disk1024 ports3100032000 i0316 142851412148 1266 slavecpp472 slave attributes   i0316 142851412160 1266 slavecpp477 slave hostname 2cbb23302fe5 i0316 142851413516 1263 statecpp58 recovering state from tmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gymeta i0316 142851413774 1266 status_update_managercpp200 recovering status update manager i0316 142851414029 1276 containerizercpp407 recovering containerizer i0316 142851415222 1269 provisionercpp245 provisioner recovery complete i0316 142851415650 1268 slavecpp4565 finished recovery i0316 142851416115 1268 slavecpp4737 querying resource estimator for oversubscribable resources i0316 142851416365 1268 slavecpp796 new master detected at master172170345919 i0316 142851416448 1276 status_update_managercpp174 pausing sending status updates i0316 142851416445 1268 slavecpp859 authenticating with master master172170345919 i0316 142851416522 1268 slavecpp864 using default crammd5 authenticatee i0316 142851416671 1268 slavecpp832 detecting new master i0316 142851416731 1275 authenticateecpp121 creating new client sasl connection i0316 142851416807 1268 slavecpp4751 received oversubscribable resources from the resource estimator i0316 142851417006 1263 mastercpp5659 authenticating slave12172170345919 i0316 142851417103 1262 authenticatorcpp413 starting authentication session for crammd5_authenticatee38172170345919 i0316 142851417348 1273 authenticatorcpp98 creating new server sasl connection i0316 142851417548 1266 authenticateecpp212 received sasl authentication mechanisms crammd5 i0316 142851417582 1266 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0316 142851417696 1264 authenticatorcpp203 received sasl authentication start i0316 142851417753 1264 authenticatorcpp325 authentication requires more steps i0316 142851417948 1265 authenticateecpp258 received sasl authentication step i0316 142851418107 1267 authenticatorcpp231 received sasl authentication step i0316 142851418159 1267 auxpropcpp107 request to lookup properties for user testprincipal realm 2cbb23302fe5 server fqdn 2cbb23302fe5 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0316 142851418180 1267 auxpropcpp179 looking up auxiliary property userpassword i0316 142851418233 1267 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0316 142851418270 1267 auxpropcpp107 request to lookup properties for user testprincipal realm 2cbb23302fe5 server fqdn 2cbb23302fe5 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0316 142851418289 1267 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0316 142851418300 1267 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0316 142851418323 1267 authenticatorcpp317 authentication success i0316 142851418414 1264 authenticateecpp298 authentication success i0316 142851418473 1269 mastercpp5689 successfully authenticated principal testprincipal at slave12172170345919 i0316 142851418514 1275 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee38172170345919 i0316 142851418781 1276 slavecpp927 successfully authenticated with master master172170345919 i0316 142851418937 1276 slavecpp1321 will retry registration in 1983001ms if necessary i0316 142851419108 1262 mastercpp4370 registering slave at slave12172170345919 2cbb23302fe5 with id c7653f6033e944069f62dc74c906bf83s0 i0316 142851419643 1266 registrarcpp439 applied 1 operations in 75642ns attempting to update the registry i0316 142851420670 1272 logcpp683 attempting to append 339 bytes to the log i0316 142851420820 1269 coordinatorcpp348 coordinator attempting to write append action at position 3 i0316 142851421495 1270 slavecpp1321 will retry registration in 1437257ms if necessary i0316 142851421716 1275 mastercpp4358 ignoring register slave message from slave12172170345919 2cbb23302fe5 as admission is already in progress i0316 142851422107 1267 replicacpp537 replica received write request for position 3 from 505172170345919 i0316 142851423033 1267 leveldbcpp341 persisting action 358 bytes to leveldb took 762815ns i0316 142851423066 1267 replicacpp712 persisted action at 3 i0316 142851424069 1267 replicacpp691 replica received learned notice for position 3 from 00000 i0316 142851424232 1264 slavecpp1321 will retry registration in 6601292ms if necessary i0316 142851424342 1269 mastercpp4358 ignoring register slave message from slave12172170345919 2cbb23302fe5 as admission is already in progress i0316 142851424686 1267 leveldbcpp341 persisting action 360 bytes to leveldb took 574743ns i0316 142851424757 1267 replicacpp712 persisted action at 3 i0316 142851424792 1267 replicacpp697 replica learned append action at position 3 i0316 142851426441 1272 registrarcpp484 successfully updated the registry in 6721024ms i0316 142851426677 1262 logcpp702 attempting to truncate the log to 3 i0316 142851426808 1264 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0316 142851427584 1261 slavecpp3482 received ping from slaveobserver11172170345919 i0316 142851428213 1262 hierarchicalcpp473 added slave c7653f6033e944069f62dc74c906bf83s0 2cbb23302fe5 with cpus2 mem1024 disk1024 ports3100032000 allocated  i0316 142851427865 1266 mastercpp4438 registered slave c7653f6033e944069f62dc74c906bf83s0 at slave12172170345919 2cbb23302fe5 with cpus2 mem1024 disk1024 ports3100032000 i0316 142851428270 1267 slavecpp971 registered with master master172170345919 given slave id c7653f6033e944069f62dc74c906bf83s0 i0316 142851428412 1265 replicacpp537 replica received write request for position 4 from 506172170345919 i0316 142851428443 1267 fetchercpp81 clearing fetcher cache i0316 142851428503 1262 hierarchicalcpp1453 no resources available to allocate i0316 142851428535 1262 hierarchicalcpp1150 performed allocation for slave c7653f6033e944069f62dc74c906bf83s0 in 205421ns i0316 142851428750 1273 status_update_managercpp181 resuming sending status updates i0316 142851429157 1265 leveldbcpp341 persisting action 16 bytes to leveldb took 695258ns i0316 142851429225 1267 slavecpp994 checkpointing slaveinfo to tmpcontainerloggertest_logrotate_rotateinsandbox_jhp0gymetaslavesc7653f6033e944069f62dc74c906bf83s0slaveinfo i0316 142851429275 1265 replicacpp712 persisted action at 4 i0316 142851429759 1267 slavecpp1030 forwarding total oversubscribed resources i0316 142851430055 1265 mastercpp4782 received update of slave c7653f6033e944069f62dc74c906bf83s0 at slave12172170345919 2cbb23302fe5 with total oversubscribed resources i0316 142851430614 1271 replicacpp691 replica received learned notice for position 4 from 00000 i0316 142851430891 1242 schedcpp222 version 0290 i0316 142851431043 1265 hierarchicalcpp531 slave c7653f6033e944069f62dc74c906bf83s0 2cbb23302fe5 updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated  i0316 142851431236 1271 leveldbcpp341 persisting action 18 bytes to leveldb took 536892ns i0316 142851431267 1265 hierarchicalcpp1453 no resources available to allocate i0316 142851431584 1271 leveldbcpp399 deleting 2 keys from leveldb took 66904ns i0316 142851431538 1273 schedcpp326 new master detected at master172170345919 i0316 142851431622 1271 replicacpp712 persisted action at 4 i0316 142851431623 1265 hierarchicalcpp1150 performed allocation for slave c7653f6033e944069f62dc74c906bf83s0 in 518588ns i0316 142851431660 1271 replicacpp697 replica learned truncate action at position 4 i0316 142851431711 1273 schedcpp382,1
support for mesos releases as part of mesos reaching 10 we need to formalize the policy of supporting mesos releases some specific questions we need to answer  what fixes should we backports to older releases  how many old releases are supported  should we have a lts version  what is the cadence of major minor and patch releases,8
add more examples of json resources to docs the configuration documentation currently only shows examples of scalar resource types in json format the structures of json resources are a bit complicated so it would be very helpful to include examples of ranges sets and text resource types as well,1
update mesosexecute with appc changes mesosexecute cli application currently does not have support for appc images adding support would make integration tests easier,3
update example long running to use v1 api we need to modify the long running test framework similar to srcexampleslong_lived_frameworkcpp to use the v1 api this would allow us to vet the v1 api and the scheduler library in test clusters,5
mastertestslavesendpointtwoslaves is flaky observed on arch linux with gcc 6 running in a virtualbox vm  run  mastertestslavesendpointtwoslaves mesos2srctestsmaster_testscpp1710 failure value of arraygetvaluessize actual 1 expected 2u which is 2  failed  mastertestslavesendpointtwoslaves 86 ms seems to fail nondeterministically perhaps more often when there is concurrent cpu load on the machine,2
destroy a container while its provisioning can lead to leaked provisioned directories here is the possible sequence of events 1 containerizerlaunch 2 provisionerprovision is called it is fetching the image 3 executor registration timed out 4 containerizerdestroy is called 5 containerstate is still in preparing 6 provisionerdestroy is called so we can be calling provisionerdestory while provisionerprovision hasnt finished yet provisionerdestroy might just skip since theres no information about the container yet and later provisioner will prepare the root filesystem this root filesystem will not be destroyed as destroy already finishes,3
sandbox uri does not work outisde mesos http server the sandbox uri of a framework does not work if i just copy paste it to the browser for example the following sandbox uri http17217015050slaves50f87c7379ef4f2a95f0b2b4062b2de6s0frameworks50f87c7379ef4f2a95f0b2b4062b2de60009executorsdriver201603211550160001browse should redirect to http17217015050slaves50f87c7379ef4f2a95f0b2b4062b2de6s0browsepath2ftmp2fmesos2fslaves2f50f87c7379ef4f2a95f0b2b4062b2de6s02fframeworks2f50f87c7379ef4f2a95f0b2b4062b2de600092fexecutors2fdriver2016032115501600012fruns2f6053348331fb4353987df3393911cc80 yet it fails with the message failed to find slaves navigate to the slaves sandbox via the mesos ui and redirects to http17217015050 it is an issue for me because im working on expanding the mesos spark ui with sandbox uri the other option is to get the slave info and parse the json file there and get executor paths not so straightforward or elegant though moreover i dont see the runscontainer_id in the mesos proto api i guess this is hidden info this is the needed piece of info to rewrite the uri without redirection,3
problematic forkclone performance at high load creating a new subprocess in mesos involves forkingcloning a new process in most cases executors perf  the parent of the new process is the agentslave process this can lead to problematic behavior especially when creating several new processes at the same time the problem here is that the normal fork or clone syscall used by libprocess provides a copyonwrite cow view of the parents address space until the child execs its new binary note that during the time between fork and exec mesos does several setup actions such as placing the new processes in systemd units or assigning them to the freezer cgroup this cow property of the address space implies that existing memory is marked as readonly and any write will trigger a pagefault and a newly created page note this behavior also extends to the parent process and hence any write will be very costly we simulated the number of pagefaults when forkingcloning new processes by this benchmark httpsgithubcomjoerg84forkingbenchmark results can be seen here httpsdocsgooglecompresentationd1sujkavhdrutlppfjy3q1yhing5fomw3hbbedzuhz7a8,8
clarify docs on reserve and createvolumes without authentication for both reservations and persistent volume creation the behavior of the http endpoints differs slightly from that of the framework operations due to the implementation of http authentication it is not possible for a frameworkoperator to provide a principal when http authentication is disabled this means that when http authentication is disabled the endpoint handlers will _always_ receive none as the principal associated with the request and thus if authorization is enabled the request will only succeed if the none principal is authorized to do stuff the docs should be updated to explain this behavior explicitly,1
enforce that diskinfo principal is equal to frameworkoperator principal currently we require that reservationinfoprincipal be equal to the principal provided for authentication which means that when http authentication is disabled this field cannot be set based on comments in mesosproto the original intention was to enforce this same constraint for persistenceprincipal but it seems that we dont enforce it this should be changed to make the two fields equivalent with one exception when the frameworkoperator principal is none we should allow the principal in diskinfo to take any value along the same lines as mesos5212,3
add example for mesosexecute usage of appc images in containerimagemd example usage for appc flags and images needs to be added to containerimagemd,3
installation of mesos python package is incomplete the installation of mesos python package is incomplete ie the files clipy futurespy and httppy are not installed code  configure enablepython  make install destdirpwdd  pythonpathpwddusrlocallibpython27sitepackagespythonpath python c from mesos import http traceback most recent call last file string line 1 in module importerror cannot import name http code this appears to be first broken with d1d70b9 mesos3969 upgraded bundled pip to 712httpsreviewsapacheorgr40630 bisecting in pipland shows that our install becomes broken for pip601 and later we are using pip712,2
add docker volume driver isolator for mesos containerizer the isolator will interact with docker volume driver plugins to mount and unmount external volumes to container,8
call and event type enums in schedulerproto should be optional having a required type enum has backwards compatibility issues when adding new enum types see mesos4997 for details,2
call and event type enums in executorproto should be optional having a required type enum has backwards compatibility issues when adding new enum types see mesos4997 for details,2
add a reconnect method to the c scheduler library a reconnect method on the library would allow the scheduler to force a reconnection disconnect and reconnect by the library this might be used by the scheduler to react to lack of heartbeats,3
mesoscontainerizerprovisionertestdestroywhileprovisioning is flaky observed on the apache jenkins noformat  run  mesoscontainerizerprovisionertestprovisionfailed i0324 133856284261 2948 containerizercpp666 starting container test_container for executor executor of framework  i0324 133856285825 2939 containerizercpp1421 destroying container test_container i0324 133856285854 2939 containerizercpp1424 waiting for the provisioner to complete for container test_container  ok  mesoscontainerizerprovisionertestprovisionfailed 7 ms  run  mesoscontainerizerprovisionertestdestroywhileprovisioning i0324 133856291187 2944 containerizercpp666 starting container c2316963c6cb4c7fa3b917ca5931e5b2 for executor executor of framework  i0324 133856292157 2944 containerizercpp1421 destroying container c2316963c6cb4c7fa3b917ca5931e5b2 i0324 133856292179 2944 containerizercpp1424 waiting for the provisioner to complete for container c2316963c6cb4c7fa3b917ca5931e5b2 f0324 133856292899 2944 containerizercpp752 check failed containers_containscontainerid  check failure stack trace   0x2ac9973d0ae4 googlelogmessagefail  0x2ac9973d0a30 googlelogmessagesendtolog  0x2ac9973d0432 googlelogmessageflush  0x2ac9973d3346 googlelogmessagefatallogmessagefatal  0x2ac996af897c mesosinternalslavemesoscontainerizerprocess_launch  0x2ac996b1f18a _zzn7process8dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns1_11containeriderk6optionins1_8taskinfoeerkns1_12executorinfoerkssrks8_isserkns1_7slaveiderkns_3pidins3_5slaveeeebrks8_ins3_13provisioninfoees5_sa_sd_sssi_sl_sq_bsu_eens_6futureit_eerknso_it0_eems10_fsz_t1_t2_t3_t4_t5_t6_t7_t8_t9_et10_t11_t12_t13_t14_t15_t16_t17_t18_enkulpns_11processbaseee_cles1p_  0x2ac996b479d9 _znst17_function_handlerifvpn7process11processbaseeezns0_8dispatchibn5mesos8internal5slave25mesoscontainerizerprocesserkns5_11containeriderk6optionins5_8taskinfoeerkns5_12executorinfoerkssrksc_isserkns5_7slaveiderkns0_3pidins7_5slaveeeebrksc_ins7_13provisioninfoees9_se_sh_sssm_sp_su_bsy_eens0_6futureit_eerknss_it0_eems14_fs13_t1_t2_t3_t4_t5_t6_t7_t8_t9_et10_t11_t12_t13_t14_t15_t16_t17_t18_euls2_e_e9_m_invokeerkst9_any_datas2_  0x2ac997334fef stdfunctionoperator  0x2ac99731b1c7 processprocessbasevisit  0x2ac997321154 processdispatcheventvisit  0x9a699c processprocessbaseserve  0x2ac9973173c0 processprocessmanagerresume  0x2ac99731445a _zzn7process14processmanager12init_threadsevenkulrkst11atomic_boole_cles3_  0x2ac997320916 _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eee6__callivieilm0eeeet_ost5tupleiidpt0_eest12_index_tupleiixspt1_eee  0x2ac9973208c6 _znst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis3_eeecliieveet0_dpot_  0x2ac997320858 _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeevee9_m_invokeiieeevst12_index_tupleiixspt_eee  0x2ac9973207af _znst12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis4_eeeveeclev  0x2ac997320748 _znst6thread5_implist12_bind_simpleifst5_bindifzn7process14processmanager12init_threadseveulrkst11atomic_boole_st17reference_wrapperis6_eeeveee6_m_runev  0x2ac9989aea60 unknown  0x2ac999125182 start_thread  0x2ac99943547d unknown make4 leaving directory mesosmesos0290_buildsrc make4  checklocal aborted make3  checkam error 2 make3 leaving directory mesosmesos0290_buildsrc make2  check error 2 make2 leaving directory mesosmesos0290_buildsrc make1  checkrecursive error 1 make1 leaving directory mesosmesos0290_build make  distcheck error 1 build step execute shell marked build as failure noformat,2
enable authenticated login in the webui the webui hits a number of endpoints to get the data that it displays state metricssnapshot filesbrowse filesread and maybe others once authentication is enabled on these endpoints we need to add a login prompt to the webui so that users can provide credentials,2
copy provisioner cannot replace directory with symlink im trying to play with the new image provisioner on our custom docker images but one of layer failed to get copied possibly due to a dangling symlink error log with glog_v1 quote i0324 054248926678 15067 copycpp127 copying layer path tmpmesosstoredockerlayers5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1rootfs to rootfs varlibmesosprovisionercontainers5f05be6cc9704539aa64fd0eef2ec7aebackendscopyrootfses507173f3e31648a3a96e5fdea9ffe9f6 e0324 054249028506 15062 slavecpp3773 container 5f05be6cc9704539aa64fd0eef2ec7ae for executor test of framework 75932a8915144011bafebeb6a208bb2d0004 failed to start collect failed collect failed failed to copy layer cp cannot overwrite directory varlibmesosprovisionercontainers5f05be6cc9704539aa64fd0eef2ec7aebackendscopyrootfses507173f3e31648a3a96e5fdea9ffe9f6etcapt with nondirectory quote content of _tmpmesosstoredockerlayers5df0888641196b88dcc1b97d04c74839f02a73b8a194a79e134426d6a8fcb0f1rootfsetcapt_ points to a nonexisting absolute path cannot provide exact path but its a result of us trying to mount apt keys into docker container at build time i believe what happened is that we executed a script at build time which contains equivalent of quote rm rf etcapt  ln sf buildmountpoint etcapt quote,3
authorization action enum does not support upgrades we need to make the action enum optional in authorizationrequest and add an unknown  0 enum value see mesos4997 for details,2
remove plain text credential format after deprecation cycle currently two formats of credentials are supported json code credentials   principal sherman secret kitesurf  code and a deprecated new line file code principal1 secret1 pricipal2 secret2 code we deprecated the new line format in 029 and should remove it after the deprecation cycle ends,3
design doc for ordered message delivery in libprocess,3
temporary directories created by environmentmkdtemp cleanup can be problematic currently in mesos test we have the temporary directories created by environmentmkdtemp cleaned up until the end of the test suite which can be problematic for instance if we have many tests in a test suite each of those tests is performing large size disk readwrite in its temp dir which may lead to out of disk issue on some resource limited machines we should have these temp dir created by environmentmkdtemp cleaned up during each test teardown currently we only clean up the sandbox for each test,1
refactore subproces setup functions executing arbitrary setup functions while creating new processes is dangerous as all functions called have to be async safe as setup functions are used for only very few purposes setsid chdir monitoring and killing a process see upcoming review it makes sense to support them safely via parameters to subprocess another common use of child setup are is to block the child while doing some work in the parent this pattern can be more cleanly expressed with parenthooks,3
design linux capability support for mesos containerizer we should at least support the following cases 1 a root user has reduced capability 2 a nonroot user has the capability of cap_net_admin to do eg tcpdump,5
create helpers for manipulating linux capabilities these helpers can either based on some existing library eg libcap or use system calls directly,5
namespace the stout flags a recent name collision occurred when updating the 3rdparty httpparser library httpsgithubcomapachemesoscommit94df63f72146501872a06c6487e94bdfd0f23025 we should put stouts flags namespace within another suitable namespace perhaps stoutflags to avoid such collisions,2
slaveagent rename phase i  update strings in the log message and standard output this is a sub ticket of mesos3780 in this ticket we will rename all the slave to agent in the log messages and standard output,2
replace masterslave terminology phase i  update strings in the shell scripts outputs this is a sub ticket of mesos3780 in this ticket we will rename slave to agent in the shell script outputs,1
slaveagent rename phase i  update strings in error messages and other strings this is a sub ticket of mesos3780 in this ticket we will update all the slave to agent in the error messages and other strings in the code,3
update the longlivedframework example to run on test clusters there are a couple of problems with the longlived framework that prevent it from being deployed easily on an actual cluster  the framework will greedily accept all offers it runs one executor per agent in the cluster  the framework assumes the longlivedexecutor binary is available on each agent this is generally only true in the build environment or in singleagent test environments  the framework does not specify an resources with the executor this is required by many isolators  the framework has no metrics,3
remove default value for the agent work_dir following a crash report from the user we need to be more explicit about the dangers of using tmp as agent work_dir in addition we can remove the default value for the work_dir flag forcing users to explicitly set the work directory for the agent,2
support docker private registry default docker config for docker private registry with authentication docker containerizer should support using a default dockerconfigjson file or the old dockercfg file locally which is prehandled by operators the default docker config file should be exposed by a new agent flag docker_config,3
upgrade httpparser to v262,3
introduce more flexible subprocess interface for child options we introduced a number of parameters to the subprocess interface with mesos5049 adding all options explicitly to the subprocess interface makes it inflexible we should investigate a flexible options which still prevents arbitrary code to be executed,2
refactor the clone option to subprocess the clone option in subprocess is only used at least in the mesos codebase to specify custom namespace flags to clone it feels having the clone function in the subprocess interface is too explicit for this functionality,2
document taskstatus reasons we should document the possible reason values that can be found in the taskstatus message,1
fix a bug in the nvidia gpu device isolator that exposes a discrepancy between clang and gcc in using declarations there appears to be a discrepancy between clang and gcc which allows clang to accept using declarations of the form using ns_namename that contain nested classes structs and enums after the name field in the declaration eg using ns_namenameenum the language for describing this functionality is ambiguous in the c11 specification as referenced here httpencppreferencecomwcpplanguagenamespaceusingdeclarations,1
add cmake build to docker_buildsh add the cmake build system to docker_buildsh to automatically test the build on jenkins alongside gcc and clang,2
design a shortterm solution for a typed error handling mechanism,2
capture the error code in errnoerror and windowserror the errnoerror and windowserror classes simply construct the error string via a mechanism such as strerror they should also capture the error code as it is an essential piece of information for such an error type,2
introduce an additional template parameter to try for typed error add an additional template parameter e to the try class template code template typename t typename e  error class try      code,3
update networkconnect to use the typed error state of try networkconnect function returns a tryint currently and the caller is required to inspect the state of errno outofband networkconnect should really return something like a tryint errnoerror,2
introduce windowssocketerror windowserror invokes getlasterror to retrieve the error code windows has a wsagetlasterror function which at the interface level is intended for failed socket operations we should introduce a windowssocketerror which invokes wsagetlasterror and use them accordingly,2
networkcni isolator crashes when launched without the network_cni_plugins_dir flag if we start the agent with the isolationnetworkcni but do not specify the network_cni_plugins_dir flag the agent crashes with the following stack dump 0x00007ffff2324cc9 in __gi_raise sigsigentry6 at nptlsysdepsunixsysvlinuxraisec56 56 nptlsysdepsunixsysvlinuxraisec no such file or directory gdb bt 0 0x00007ffff2324cc9 in __gi_raise sigsigentry6 at nptlsysdepsunixsysvlinuxraisec56 1 0x00007ffff23280d8 in __gi_abort  at abortc89 2 0x00007ffff231db86 in __assert_fail_base fmt0x7ffff246e830 sssu ssassertion s failednn assertionassertionentry0x451f5c issome filefileentry0x451f65 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp linelineentry111 functionfunctionentry0x45294a const t optionstdbasic_stringchar get const  t  stdbasic_stringchar at assertc92 3 0x00007ffff231dc32 in __gi___assert_fail assertion0x451f5c issome file0x451f65 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp line111 function0x45294a const t optionstdbasic_stringchar get const  t  stdbasic_stringchar at assertc101 4 0x0000000000432c0d in optionstdstringget const  this0x6c1ea8 at 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp111 python exception class indexerror list index out of range 5 0x00007ffff63ef7cc in mesosinternalslavenetworkcniisolatorprocessrecover this0x6c1e70 statesempty stdlist orphans at srcslavecontainerizermesosisolatorsnetworkcnicnicpp331 6 0x00007ffff60cddd8 in operator this0x7fffc0001e00 process0x6c1ef8 at 3rdpartylibprocessincludeprocessdispatchhpp239 7 0x00007ffff60cd972 in std_function_handlervoid processprocessbase processfuturenothing processdispatchnothing mesosinternalslavemesosisolatorprocess stdlistmesosslavecontainerstate stdallocatormesosslavecontainerstate  const hashsetmesoscontainerid stdhashmesoscontainerid stdequal_tomesoscontainerid  const stdlistmesosslavecontainerstate stdallocatormesosslavecontainerstate  hashsetmesoscontainerid stdhashmesoscontainerid stdequal_tomesoscontainerid  processpidmesosinternalslavemesosisolatorprocess const processfuturenothing mesosinternalslavemesosisolatorprocessstdlistmesosslavecontainerstate stdallocatormesosslavecontainerstate  const hashsetmesoscontainerid stdhashmesoscontainerid stdequal_tomesoscontainerid  const stdlistmesosslavecontainerstate stdallocatormesosslavecontainerstate  hashsetmesoscontainerid stdhashmesoscontainerid stdequal_tomesoscontainerid lambdaprocessprocessbase1_m_invokestd_any_data const processprocessbase __functor __args0x6c1ef8 at usrbinlibgccx86_64linuxgnu48includec48functional2071 8 0x00007ffff6a6bf38 in stdfunctionvoid processprocessbaseoperatorprocessprocessbase const this0x7fffc0001d70 __args0x6c1ef8 at usrbinlibgccx86_64linuxgnu48includec48functional2471 9 0x00007ffff6a561b4 in processprocessbasevisit this0x6c1ef8 event at 3rdpartylibprocesssrcprocesscpp3130 10 0x00007ffff6aac5fe in processdispatcheventvisit this0x7fffc0001570 visitor0x6c1ef8 at 3rdpartylibprocessincludeprocesseventhpp161 11 0x00007ffff55e9c91 in processprocessbaseserve this0x6c1ef8 event at 3rdpartylibprocessincludeprocessprocesshpp82 12 0x00007ffff6a53ed4 in processprocessmanagerresume this0x67cca0 process0x6c1ef8 at 3rdpartylibprocesssrcprocesscpp2570 13 0x00007ffff6a5bff5 in operator this0x697d70 joining at 3rdpartylibprocesssrcprocesscpp2218 14 0x00007ffff6a5bf33 in std_bindprocessprocessmanagerinit_threads_1 stdreference_wrapperstdatomic_bool const__callvoid  0ulstdtuple std_index_tuple0ul this0x697d70 __argsunknown type in homevagrantmesospheremesosbuildsrclibslibmesos0290so cu 0x45bb552 die 0x469efe5 at usrbinlibgccx86_64linuxgnu48includec48functional1295 15 0x00007ffff6a5bee6 in std_bindprocessprocessmanagerinit_threads_1 stdreference_wrapperstdatomic_bool constoperator void this0x697d70 at usrbinlibgccx86_64linuxgnu48includec48functional1353 16 0x00007ffff6a5be95 in std_bind_simplestd_bindprocessprocessmanagerinit_threads_1 stdreference_wrapperstdatomic_bool const _m_invokestd_index_tuple this0x697d70 at usrbinlibgccx86_64linuxgnu48includec48functional1731 17 0x00007ffff6a5be65 in std_bind_simplestd_bindprocessprocessmanagerinit_threads_1 stdreference_wrapperstdatomic_bool const operator this0x697d70 at usrbinlibgccx86_64linuxgnu48includec48functional1720 18 0x00007ffff6a5be3c in stdthread_implstd_bind_simplestd_bindprocessprocessmanagerinit_threads_1 stdreference_wrapperstdatomic_bool const  _m_run this0x697d58 at usrbinlibgccx86_64linuxgnu48includec48thread115 19 0x00007ffff2b98a60 in   from usrlibx86_64linuxgnulibstdcso6 20 0x00007ffff26bb182 in start_thread arg0x7fffeb92d700 at pthread_createc312 21 0x00007ffff23e847d in clone  at sysdepsunixsysvlinuxx86_64clones111 gdb frame 4 4 0x0000000000432c0d in optionstdstringget const  this0x6c1ea8 at 3rdpartylibprocess3rdpartystoutincludestoutoptionhpp111,1
flagsparse does not handle empty string correctly a missing default for quorum size has generated the following master config code mesos_work_dirvarlibmesosmaster mesos_zkzkzk12181zk22181zk32181mesos mesos_quorum mesos_port5050 mesos_clustermesos mesos_log_dirvarlogmesos mesos_logbufsecs1 mesos_logging_levelinfo code this was causing each elected leader to attempt replica recovery eg groupcpp700 trying to get mesoslog_replicas0000000012 in zookeeper and eventually mastercpp1458 recovery failed failed to recover registrar failed to perform fetch within 1mins full log on one of the masters httpsgistgithubcomclehene09a9ddfe49b92a5deb4c1b421f63479e all masters and zk nodes were reachable over the network also once the quorum was configured the master recovery protocol finished gracefully,2
grant access to devnvidiactl and devnvidiauvm in the nvidia gpu isolator calls to nvidiasmi fail inside a container even if access to a gpu has been granted moreover access to devnvidiactl is actually required for a container to do anything useful with a gpu even if it has access to it we should grantrevoke access to devnvidiactl and devnvidiauvm as gpus are added and removed from a container in the nvidia gpu isolator,2
pivot_root is not available on powerpc when compile on ppc64le it will through error message srclinuxfscpp4432 error error pivot_root is not available the current code logic in srclinuxfscpp is code ifdef __nr_pivot_root int ret  syscall__nr_pivot_root newrootc_str putoldc_str elif __x86_64__  a workaround for systems that have an old glib but have a new  kernel the magic number 155 is the syscall number for  pivot_root on the x86_64 architecture see  archx86syscallssyscall_64tbl int ret  syscall155 newrootc_str putoldc_str else error pivot_root is not available endif code there is no old glib version and the new kernel version it will never run code in ifdef __nr_pivot_root condition and when i build on ubuntu 1604it has the latest linux kernel and glibc it still cant step into the ifdef __nr_pivot_root condition for powerpc case i added another condition code elif __powerpc__  __ppc__  __powerpc64__  __ppc64__  a workaround for powerpc the magic number 203 is the syscall  number for pivot_root on the powerpc architecture see  httpsw3challscomsyscallsarchpowerpc_64 int ret  syscall203 newrootc_str putoldc_str code,1
task_killing is not supported by mesosexecute recently task_killing state mesos4547 have been introduced to mesos we should add support for this feature to mesosexecute,3
commit message hook iterates over words rather than lines for line in commit_message iterates over one word at a time rather than one line at a time we should use the following pattern instead code while read line do  done  commit_message code,2
commit message hook iterates over the commented lines currently the commit message hook iterates over the commented lines for example if there is a modified file for which its path is longer than 72 characters the commit hook errors out we should skip over the commented lines,2
reset libprocess_ip in networkcni isolator currently the libprocess_ip environment variable was being set to the agent ip if the environment variable has not be defined by the framework for containers having their own ip address as with containers on cni networks this becomes a problem since the command executor tries to bind to the libprocess_ip that does not exist in its network namespace and fails thus for containers launched on cni networks the libprocess_ip should not be set or rather is set to 0000 allowing the container to bind to the ip address provided by the cni network,1
persistentvolumetestaccesspersistentvolume is flaky observed on asf ci code  run  diskresourcepersistentvolumetestaccesspersistentvolume0 i0405 172919134435 31837 clustercpp139 creating default local authorizer i0405 172919251143 31837 leveldbcpp174 opened db in 116386403ms i0405 172919310050 31837 leveldbcpp181 compacted db in 5880688ms i0405 172919310180 31837 leveldbcpp196 created db iterator in 37145ns i0405 172919310199 31837 leveldbcpp202 seeked to beginning of db in 4212ns i0405 172919310210 31837 leveldbcpp271 iterated through 0 keys in the db in 410ns i0405 172919310279 31837 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0405 172919311069 31861 recovercpp447 starting replica recovery i0405 172919311362 31861 recovercpp473 replica is in empty status i0405 172919312641 31861 replicacpp673 replica in empty status received a broadcasted recover request from 14359172170443972 i0405 172919313045 31860 recovercpp193 received a recover response from a replica in empty status i0405 172919313608 31860 recovercpp564 updating replica status to starting i0405 172919316416 31867 mastercpp376 master 9565ff6ff1b642598430690e635c391f 4090d10eba90 started on 172170443972 i0405 172919316470 31867 mastercpp378 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmp0a9elucredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0290_instsharemesoswebui work_dirtmp0a9elumaster zk_session_timeout10secs i0405 172919316938 31867 mastercpp427 master only allowing authenticated frameworks to register i0405 172919316951 31867 mastercpp432 master only allowing authenticated agents to register i0405 172919316961 31867 credentialshpp37 loading credentials for authentication from tmp0a9elucredentials i0405 172919317402 31867 mastercpp474 using default crammd5 authenticator i0405 172919317643 31867 mastercpp545 using default basic http authenticator i0405 172919317854 31867 mastercpp583 authorization enabled i0405 172919318081 31864 whitelist_watchercpp77 no whitelist given i0405 172919318079 31861 hierarchicalcpp144 initialized hierarchical allocator process i0405 172919320838 31864 mastercpp1826 the newly elected leader is master172170443972 with id 9565ff6ff1b642598430690e635c391f i0405 172919320888 31864 mastercpp1839 elected as the leading master i0405 172919320909 31864 mastercpp1526 recovering from registrar i0405 172919321218 31871 registrarcpp331 recovering registrar i0405 172919347045 31860 leveldbcpp304 persisting metadata 8 bytes to leveldb took 33164133ms i0405 172919347126 31860 replicacpp320 persisted replica status to starting i0405 172919347611 31869 recovercpp473 replica is in starting status i0405 172919349215 31871 replicacpp673 replica in starting status received a broadcasted recover request from 14361172170443972 i0405 172919349653 31870 recovercpp193 received a recover response from a replica in starting status i0405 172919350236 31866 recovercpp564 updating replica status to voting i0405 172919388882 31864 leveldbcpp304 persisting metadata 8 bytes to leveldb took 3838299ms i0405 172919388993 31864 replicacpp320 persisted replica status to voting i0405 172919389369 31856 recovercpp578 successfully joined the paxos group i0405 172919389735 31856 recovercpp462 recover process terminated i0405 172919390476 31868 logcpp659 attempting to start the writer i0405 172919392125 31862 replicacpp493 replica received implicit promise request from 14362172170443972 with proposal 1 i0405 172919430706 31862 leveldbcpp304 persisting metadata 8 bytes to leveldb took 38505062ms i0405 172919430816 31862 replicacpp342 persisted promised to 1 i0405 172919431918 31856 coordinatorcpp238 coordinator attempting to fill missing positions i0405 172919433725 31861 replicacpp388 replica received explicit promise request from 14363172170443972 for position 0 with proposal 2 i0405 172919472491 31861 leveldbcpp341 persisting action 8 bytes to leveldb took 38659492ms i0405 172919472595 31861 replicacpp712 persisted action at 0 i0405 172919474556 31864 replicacpp537 replica received write request for position 0 from 14364172170443972 i0405 172919474652 31864 leveldbcpp436 reading position from leveldb took 49423ns i0405 172919528175 31864 leveldbcpp341 persisting action 14 bytes to leveldb took 53443616ms i0405 172919528300 31864 replicacpp712 persisted action at 0 i0405 172919529389 31865 replicacpp691 replica received learned notice for position 0 from 00000 i0405 172919571137 31865 leveldbcpp341 persisting action 16 bytes to leveldb took 41676495ms i0405 172919571254 31865 replicacpp712 persisted action at 0 i0405 172919571302 31865 replicacpp697 replica learned nop action at position 0 i0405 172919572322 31856 logcpp675 writer started with ending position 0 i0405 172919574060 31861 leveldbcpp436 reading position from leveldb took 83200ns i0405 172919575417 31864 registrarcpp364 successfully fetched the registry 0b in 0ns i0405 172919575565 31864 registrarcpp463 applied 1 operations in 46419ns attempting to update the registry i0405 172919576517 31857 logcpp683 attempting to append 170 bytes to the log i0405 172919576849 31857 coordinatorcpp348 coordinator attempting to write append action at position 1 i0405 172919578390 31857 replicacpp537 replica received write request for position 1 from 14365172170443972 i0405 172919780277 31857 leveldbcpp341 persisting action 189 bytes to leveldb took 201808617ms i0405 172919780366 31857 replicacpp712 persisted action at 1 i0405 172919782024 31857 replicacpp691 replica received learned notice for position 1 from 00000 i0405 172919823770 31857 leveldbcpp341 persisting action 191 bytes to leveldb took 41667662ms i0405 172919823851 31857 replicacpp712 persisted action at 1 i0405 172919823889 31857 replicacpp697 replica learned append action at position 1 i0405 172919825701 31867 registrarcpp508 successfully updated the registry in 0ns i0405 172919825929 31867 registrarcpp394 successfully recovered registrar i0405 172919826015 31857 logcpp702 attempting to truncate the log to 1 i0405 172919826262 31867 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0405 172919827647 31867 replicacpp537 replica received write request for position 2 from 14366172170443972 i0405 172919828018 31857 mastercpp1634 recovered 0 agents from the registry 131b  allowing 10mins for agents to reregister i0405 172919828065 31861 hierarchicalcpp171 skipping recovery of hierarchical allocator nothing to recover i0405 172919865555 31867 leveldbcpp341 persisting action 16 bytes to leveldb took 37822178ms i0405 172919865661 31867 replicacpp712 persisted action at 2 i0405 172919866921 31867 replicacpp691 replica received learned notice for position 2 from 00000 i0405 172919907341 31867 leveldbcpp341 persisting action 18 bytes to leveldb took 40356649ms i0405 172919907531 31867 leveldbcpp399 deleting 1 keys from leveldb took 91109ns i0405 172919907560 31867 replicacpp712 persisted action at 2 i0405 172919907599 31867 replicacpp697 replica learned truncate action at position 2 i0405 172919923305 31837 resourcescpp572 parsing resources as json failed cpus2mem2048 trying semicolondelimited string format instead i0405 172919926491 31837 containerizercpp155 using isolation posixcpuposixmemfilesystemposix w0405 172919927836 31837 backendcpp66 failed to create bind backend bindbackend requires root privileges i0405 172919932029 31862 slavecpp200 agent started on 441172170443972 i0405 172919932086 31862 slavecpp201 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7accredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7acfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_credentialstmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7achttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0290_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcesnamecpusrolescalarvalue20typescalarnamememrolescalarvalue20480typescalarnamediskrolerole1scalarvalue40960typescalar revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7ac i0405 172919932665 31862 credentialshpp86 loading credential for authentication from tmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7accredential i0405 172919932934 31862 slavecpp338 agent using credential for testprincipal i0405 172919932968 31862 credentialshpp37 loading credentials for authentication from tmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7achttp_credentials i0405 172919933284 31862 slavecpp390 using default basic http authenticator i0405 172919934916 31837 schedcpp222 version 0290 i0405 172919935566 31862 slavecpp589 agent resources cpus2 mem2048 diskrole14096 ports3100032000 i0405 172919935664 31862 slavecpp597 agent attributes   i0405 172919935679 31862 slavecpp602 agent hostname 4090d10eba90 i0405 172919938390 31864 statecpp57 recovering state from tmpdiskresource_persistentvolumetest_accesspersistentvolume_0_fjs7acmeta i0405 172919940608 31869 schedcpp326 new master detected at master172170443972 i0405 172919940749 31869 schedcpp382 authenticating with master master172170443972 i0405 172919940773 31869 schedcpp389 using default crammd5 authenticatee i0405 172919942371 31869 authenticateecpp121 creating new client sasl connection i0405 172919942873 31859 mastercpp5679 authenticating schedulerbdf68f7fd93847eda132bb3f218628bf172170443972 i0405 172919943156 31859 authenticatorcpp413 starting authentication session for crammd5_authenticatee896172170443972 i0405 172919943507 31863 authenticatorcpp98 creating new server sasl connection i0405 172919943740 31859 authenticateecpp212 received sasl authentication mechanisms crammd5 i0405 172919943783 31859 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0405 172919943892 31859 authenticatorcpp203 received sasl authentication start i0405 172919943977 31859 authenticatorcpp325 authentication requires more steps i0405 172919944066 31859 authenticateecpp258 received sasl authentication step i0405 172919944164 31859 authenticatorcpp231 received sasl authentication step i0405 172919944193 31859 auxpropcpp107 request to lookup properties for user testprincipal realm 4090d10eba90 server fqdn 4090d10eba90 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0405 172919944206 31859 auxpropcpp179 looking up auxiliary property userpassword i0405 172919944268 31859 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0405 172919944300 31859 auxpropcpp107 request to lookup properties for user testprincipal realm 4090d10eba90 server fqdn 4090d10eba90 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0405 172919944313 31859 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0405 172919944321 31859 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0405 172919944339 31859 authenticatorcpp317 authentication success i0405 172919944541 31859 authenticateecpp298 authentication success i0405 172919944655 31859 mastercpp5709 successfully authenticated principal testprincipal at schedulerbdf68f7fd93847eda132bb3f218628bf172170443972 i0405 172919944737 31859 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee896172170443972 i0405 172919945111 31859 schedcpp472 successfully authenticated with master master172170443972 i0405 172919945132 31859 schedcpp777 sending subscribe call to master172170443972 i0405 172919945591 31859 schedcpp810 will retry registration in 37280738ms if necessary i0405 172919945744 31865 mastercpp2346 received subscribe call for framework default at schedulerbdf68f7fd93847eda132bb3f218628bf172170443972 i0405 172919945838 31865 mastercpp1865 authorizing framework principal testprincipal to receive offers for role role1 i0405 172919946194 31865 mastercpp2417 subscribing framework default with checkpointing disabled and capabilities   i0405 172919946866 31866 hierarchicalcpp266 added framework 9565ff6ff1b642598430690e635c391f0000 i0405 172919946974 31866 hierarchicalcpp1490 no resources available to allocate i0405 172919947010 31866 hierarchicalcpp1585 no inverse offers to send out i0405 172919947054 31865 schedcpp704 framework registered with 9565ff6ff1b642598430690e635c391f0000 i0405 172919947074 31866 hierarchicalcpp1141 performed allocation for 0 agents in 178242ns i0405 172919947124 31865 schedcpp718 schedulerregistered took 38907ns i0405 172919948712 31866 status_update_managercpp200 recovering status update manager i0405 172919948901 31866 containerizercpp416 recovering containerizer i0405 172919951021 31866 provisionercpp245 provisioner recovery complete i0405 172919951802 31866 slavecpp4773 finished recovery i0405 172919952518 31866 slavecpp4945 querying resource estimator for oversubscribable resources i0405 172919953248 31866 slavecpp928 new master detected at master172170443972 i0405 172919953305 31865 status_update_managercpp174 pausing sending status updates i0405 172919953626 31866 slavecpp991 authenticating with master master172170443972 i0405 172919953716 31866 slavecpp996 using default crammd5 authenticatee i0405 172919954074 31866 slavecpp964 detecting new master i0405 172919954167 31861 authenticateecpp121 creating new client sasl connection i0405 172919954372 31866 slavecpp4959 received oversubscribable resources from the resource estimator i0405 172919954756 31866 mastercpp5679 authenticating slave441172170443972 i0405 172919954944 31861 authenticatorcpp413 starting authentication session for crammd5_authenticatee897172170443972 i0405 172919955368 31863 authenticatorcpp98 creating new server sasl connection i0405 172919955687 31861 authenticateecpp212 received sasl authentication mechanisms crammd5 i0405 172919955801 31861 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0405 172919956075 31861 authenticatorcpp203 received sasl authentication start i0405 172919956279 31861 authenticatorcpp325 authentication requires more steps i0405 172919956455 31861 authenticateecpp258 received sasl authentication step i0405 172919956676 31861 authenticatorcpp231 received sasl authentication step i0405 172919956815 31861 auxpropcpp107 request to lookup properties for user testprincipal realm 4090d10eba90 server fqdn 4090d10eba90 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0405 172919956907 31861 auxpropcpp179 looking up auxiliary property userpassword i0405 172919957044 31861 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0405 172919957166 31861 auxpropcpp107 request to lookup properties for user testprincipal realm 4090d10eba90 server fqdn 4090d10eba90 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0405 172919957264 31861 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0405 172919957353 31861 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0405 172919957449 31861 authenticatorcpp317 authentication success i0405 172919957664 31857 authenticateecpp298 authentication success i0405 172919957813 31857 mastercpp5709 successfully authenticated principal testprincipal at slave441172170443972 i0405 172919958008 31861 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee897172170443972 i0405 172919958732 31857 slavecpp1061 successfully authenticated with master master172170443972 i0405 172919958930 31857 slavecpp1457 will retry registration in 18568334ms if necessary i0405 172919959262 31857 mastercpp4390 registering agent at slave441172170443972 4090d10eba90 with id 9565ff6ff1b642598430690e635c391fs0 i0405 172919959934 31857 registrarcpp463 applied 1 operations in 99197ns attempting to update the registry i0405 172919961587 31857 logcpp683 attempting to append 343 bytes to the log i0405 1729199,3
enable newtorkcni isolator in mesoscontainerizer as the default network isolator currently there are no default network isolators for mesoscontainerizer with the development of the networkcni isolator we have an interface to run mesos on multitude of ip networks given that its based on an open standard the cni spec which is gathering a lot of traction from vendors calico weave coreos and already works on some default networks bridge ipvlan macvlan it makes sense to make it as the default network isolator,1
commit message hook lints the diff in verbose mode in verbose mode ie git commit verbose the commit message includes the diff of the commit at the bottom delimited by the following lines code   8   do not touch the line above  everything below will be removed code we should break once we encounter such a line,2
expose taskstatus source  reason in masters state output it would be helpful if the taskstatus lists provided by the masters state endpoint included the source and reason associated with the status message the json modeling function for taskstatus should be extended to include these fields,1
update existing documentation to include references to gpus as a first class resource specifically the documentation in the following files should be udated noformat docsattributesresourcesmd docsmonitoringmd noformat,1
update the default json representation of a resource to include gpus the default json representation of a resource currently lists a value of 0 if no value is set on a first class scalar resource ie cpus mem disk we should add gpus in here as well,1
remove dashboardjs from the webui this file is no longer in use anywhere,1
fix nvidia gpu test build for namespace change of masterdetector an update to master the day after all of the nvidia gpu stuff landed has a build error in the nvidia gpu tests the namespace that masterdetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now,1
provisionerdockerlocalstoretestlocalstoretestwithtar is flaky found this on asf ci while testing 0281rc2 code  run  provisionerdockerlocalstoretestlocalstoretestwithtar e0406 182930870481 520 shellhpp93 command hadoop version 21 failed this is the output sh 1 hadoop not found e0406 182930870576 520 fetchercpp59 failed to create uri fetcher plugin hadoop failed to create hdfs client failed to execute hadoop version 21 the command was either not found or exited with a nonzero exit status 127 i0406 182930871052 520 local_pullercpp90 creating local puller with docker registry tmp3l8zbvimages i0406 182930873325 539 metadata_managercpp159 looking for image abc i0406 182930874438 539 local_pullercpp142 untarring image abc from tmp3l8zbvimagesabctar to tmp3l8zbvstorestaging5tw8bd i0406 182930901916 547 local_pullercpp162 the repositories json file for image abc is abclatest456 i0406 182930902304 547 local_pullercpp290 extracting layer tar ball tmp3l8zbvstorestaging5tw8bd123layertar to rootfs tmp3l8zbvstorestaging5tw8bd123rootfs i0406 182930909144 547 local_pullercpp290 extracting layer tar ball tmp3l8zbvstorestaging5tw8bd456layertar to rootfs tmp3l8zbvstorestaging5tw8bd456rootfs srctestscontainerizerprovisioner_docker_testscpp183 failure imageinfofailure collect failed subprocess tar tar x f tmp3l8zbvstorestaging5tw8bd456layertar c tmp3l8zbvstorestaging5tw8bd456rootfs failed tar this does not look like a tar archive tar exiting with failure status due to previous errors  failed  provisionerdockerlocalstoretestlocalstoretestwithtar 243 ms code,2
add agent flags for http authorization flags should be added to the agent to 1 enable authorization authorizers 2 provide acls acls,2
cleanup memory leaks in libprocess finalize libprocesss finalize function currently leaks memory for a few different reasons cleaning up the socketmanager will be somewhat involved mesos3910 but the remaining memory leaks should be fairly easy to address,2
masterallocatortest1rebalancedforupdatedweights is flaky observed on the asf ci code  run  masterallocatortest1rebalancedforupdatedweights i0407 223410330394 29278 clustercpp149 creating default local authorizer i0407 223410466182 29278 leveldbcpp174 opened db in 135608207ms i0407 223410516398 29278 leveldbcpp181 compacted db in 50159558ms i0407 223410516464 29278 leveldbcpp196 created db iterator in 34959ns i0407 223410516484 29278 leveldbcpp202 seeked to beginning of db in 10195ns i0407 223410516496 29278 leveldbcpp271 iterated through 0 keys in the db in 7324ns i0407 223410516547 29278 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0407 223410517277 29298 recovercpp447 starting replica recovery i0407 223410517693 29300 recovercpp473 replica is in empty status i0407 223410520251 29310 replicacpp673 replica in empty status received a broadcasted recover request from 4775172170335855 i0407 223410520611 29311 recovercpp193 received a recover response from a replica in empty status i0407 223410521164 29299 recovercpp564 updating replica status to starting i0407 223410523435 29298 mastercpp382 master f59f9057a5c743e1b12996862e640a12 129e11060069 started on 172170335855 i0407 223410523473 29298 mastercpp384 flags at startup acls allocation_interval1secs allocatorhierarchicaldrf authenticatetrue authenticate_httptrue authenticate_slavestrue authenticatorscrammd5 authorizerslocal credentialstmp3rzy8ccredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_completed_frameworks50 max_completed_tasks_per_framework1000 max_slave_ping_timeouts5 quietfalse recovery_slave_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue slave_ping_timeout15secs slave_reregister_timeout10mins user_sorterdrf versionfalse webui_dirmesosmesos0290_instsharemesoswebui work_dirtmp3rzy8cmaster zk_session_timeout10secs i0407 223410523885 29298 mastercpp433 master only allowing authenticated frameworks to register i0407 223410523901 29298 mastercpp438 master only allowing authenticated agents to register i0407 223410523913 29298 credentialshpp37 loading credentials for authentication from tmp3rzy8ccredentials i0407 223410524298 29298 mastercpp480 using default crammd5 authenticator i0407 223410524441 29298 mastercpp551 using default basic http authenticator i0407 223410524564 29298 mastercpp589 authorization enabled i0407 223410525269 29305 hierarchicalcpp145 initialized hierarchical allocator process i0407 223410525333 29305 whitelist_watchercpp77 no whitelist given i0407 223410527331 29298 mastercpp1832 the newly elected leader is master172170335855 with id f59f9057a5c743e1b12996862e640a12 i0407 223410527441 29298 mastercpp1845 elected as the leading master i0407 223410527545 29298 mastercpp1532 recovering from registrar i0407 223410527889 29298 registrarcpp331 recovering registrar i0407 223410549734 29299 leveldbcpp304 persisting metadata 8 bytes to leveldb took 2825177ms i0407 223410549782 29299 replicacpp320 persisted replica status to starting i0407 223410550010 29299 recovercpp473 replica is in starting status i0407 223410551352 29299 replicacpp673 replica in starting status received a broadcasted recover request from 4777172170335855 i0407 223410551676 29299 recovercpp193 received a recover response from a replica in starting status i0407 223410552315 29308 recovercpp564 updating replica status to voting i0407 223410574865 29308 leveldbcpp304 persisting metadata 8 bytes to leveldb took 22413614ms i0407 223410574928 29308 replicacpp320 persisted replica status to voting i0407 223410575103 29308 recovercpp578 successfully joined the paxos group i0407 223410575346 29308 recovercpp462 recover process terminated i0407 223410575913 29308 logcpp659 attempting to start the writer i0407 223410577512 29308 replicacpp493 replica received implicit promise request from 4778172170335855 with proposal 1 i0407 223410599984 29308 leveldbcpp304 persisting metadata 8 bytes to leveldb took 22453613ms i0407 223410600026 29308 replicacpp342 persisted promised to 1 i0407 223410601773 29304 coordinatorcpp238 coordinator attempting to fill missing positions i0407 223410603757 29307 replicacpp388 replica received explicit promise request from 4779172170335855 for position 0 with proposal 2 i0407 223410634392 29307 leveldbcpp341 persisting action 8 bytes to leveldb took 30269987ms i0407 223410634829 29307 replicacpp712 persisted action at 0 i0407 223410637017 29297 replicacpp537 replica received write request for position 0 from 4780172170335855 i0407 223410637099 29297 leveldbcpp436 reading position from leveldb took 52948ns i0407 223410676170 29297 leveldbcpp341 persisting action 14 bytes to leveldb took 38917487ms i0407 223410676352 29297 replicacpp712 persisted action at 0 i0407 223410677564 29306 replicacpp691 replica received learned notice for position 0 from 00000 i0407 223410717959 29306 leveldbcpp341 persisting action 16 bytes to leveldb took 40306229ms i0407 223410718202 29306 replicacpp712 persisted action at 0 i0407 223410718399 29306 replicacpp697 replica learned nop action at position 0 i0407 223410719883 29306 logcpp675 writer started with ending position 0 i0407 223410721688 29305 leveldbcpp436 reading position from leveldb took 75934ns i0407 223410723640 29306 registrarcpp364 successfully fetched the registry 0b in 195648us i0407 223410723999 29306 registrarcpp463 applied 1 operations in 108099ns attempting to update the registry i0407 223410725077 29311 logcpp683 attempting to append 170 bytes to the log i0407 223410725328 29308 coordinatorcpp348 coordinator attempting to write append action at position 1 i0407 223410726552 29299 replicacpp537 replica received write request for position 1 from 4781172170335855 i0407 223410759747 29299 leveldbcpp341 persisting action 189 bytes to leveldb took 33089719ms i0407 223410759976 29299 replicacpp712 persisted action at 1 i0407 223410761739 29299 replicacpp691 replica received learned notice for position 1 from 00000 i0407 223410801522 29299 leveldbcpp341 persisting action 191 bytes to leveldb took 39694064ms i0407 223410801602 29299 replicacpp712 persisted action at 1 i0407 223410801638 29299 replicacpp697 replica learned append action at position 1 i0407 223410803371 29311 registrarcpp508 successfully updated the registry in 79163904ms i0407 223410803829 29311 registrarcpp394 successfully recovered registrar i0407 223410804585 29311 mastercpp1640 recovered 0 agents from the registry 131b  allowing 10mins for agents to reregister i0407 223410805269 29308 logcpp702 attempting to truncate the log to 1 i0407 223410805721 29310 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0407 223410805276 29296 hierarchicalcpp172 skipping recovery of hierarchical allocator nothing to recover i0407 223410806529 29307 replicacpp537 replica received write request for position 2 from 4782172170335855 i0407 223410843320 29307 leveldbcpp341 persisting action 16 bytes to leveldb took 3677593ms i0407 223410843531 29307 replicacpp712 persisted action at 2 i0407 223410845369 29311 replicacpp691 replica received learned notice for position 2 from 00000 i0407 223410885098 29311 leveldbcpp341 persisting action 18 bytes to leveldb took 39641102ms i0407 223410885401 29311 leveldbcpp399 deleting 1 keys from leveldb took 88701ns i0407 223410885745 29311 replicacpp712 persisted action at 2 i0407 223410885862 29311 replicacpp697 replica learned truncate action at position 2 i0407 223410900660 29278 containerizercpp155 using isolation posixcpuposixmemfilesystemposix w0407 223410901793 29278 backendcpp66 failed to create bind backend bindbackend requires root privileges i0407 223410905488 29302 slavecpp201 agent started on 111172170335855 i0407 223410905553 29302 slavecpp202 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmpmasterallocatortest_1_rebalancedforupdatedweights_9acayacredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmpmasterallocatortest_1_rebalancedforupdatedweights_9acayafetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_credentialstmpmasterallocatortest_1_rebalancedforupdatedweights_9acayahttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos0290_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2mem1024disk4096ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmpmasterallocatortest_1_rebalancedforupdatedweights_9acaya i0407 223410906365 29302 credentialshpp86 loading credential for authentication from tmpmasterallocatortest_1_rebalancedforupdatedweights_9acayacredential i0407 223410906787 29302 slavecpp339 agent using credential for testprincipal i0407 223410907202 29302 credentialshpp37 loading credentials for authentication from tmpmasterallocatortest_1_rebalancedforupdatedweights_9acayahttp_credentials i0407 223410907713 29302 slavecpp391 using default basic http authenticator i0407 223410908499 29302 resourcescpp572 parsing resources as json failed cpus2mem1024disk4096ports3100032000 trying semicolondelimited string format instead i0407 223410910189 29302 slavecpp590 agent resources cpus2 mem1024 disk4096 ports3100032000 i0407 223410910362 29302 slavecpp598 agent attributes   i0407 223410910465 29302 slavecpp603 agent hostname 129e11060069 i0407 223410913280 29303 statecpp57 recovering state from tmpmasterallocatortest_1_rebalancedforupdatedweights_9acayameta i0407 223410914621 29303 status_update_managercpp200 recovering status update manager i0407 223410915226 29303 containerizercpp416 recovering containerizer i0407 223410917246 29301 provisionercpp245 provisioner recovery complete i0407 223410917733 29301 slavecpp4784 finished recovery i0407 223410918226 29301 slavecpp4956 querying resource estimator for oversubscribable resources i0407 223410918529 29301 slavecpp4970 received oversubscribable resources from the resource estimator i0407 223410918908 29304 slavecpp939 new master detected at master172170335855 i0407 223410918988 29304 slavecpp1002 authenticating with master master172170335855 i0407 223410919098 29301 status_update_managercpp174 pausing sending status updates i0407 223410919309 29304 slavecpp1007 using default crammd5 authenticatee i0407 223410919535 29304 slavecpp975 detecting new master i0407 223410919747 29308 authenticateecpp121 creating new client sasl connection i0407 223410920413 29308 mastercpp5695 authenticating slave111172170335855 i0407 223410920650 29308 authenticatorcpp413 starting authentication session for crammd5_authenticatee278172170335855 i0407 223410921020 29308 authenticatorcpp98 creating new server sasl connection i0407 223410921308 29308 authenticateecpp212 received sasl authentication mechanisms crammd5 i0407 223410921424 29308 authenticateecpp238 attempting to authenticate with mechanism crammd5 i0407 223410921596 29308 authenticatorcpp203 received sasl authentication start i0407 223410921752 29308 authenticatorcpp325 authentication requires more steps i0407 223410921957 29307 authenticateecpp258 received sasl authentication step i0407 223410922178 29308 authenticatorcpp231 received sasl authentication step i0407 223410922214 29308 auxpropcpp107 request to lookup properties for user testprincipal realm 129e11060069 server fqdn 129e11060069 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0407 223410922229 29308 auxpropcpp179 looking up auxiliary property userpassword i0407 223410922281 29308 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 i0407 223410922309 29308 auxpropcpp107 request to lookup properties for user testprincipal realm 129e11060069 server fqdn 129e11060069 sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0407 223410922322 29308 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0407 223410922332 29308 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0407 223410922353 29308 authenticatorcpp317 authentication success i0407 223410922436 29307 authenticateecpp298 authentication success i0407 223410922587 29308 mastercpp5725 successfully authenticated principal testprincipal at slave111172170335855 i0407 223410922668 29299 authenticatorcpp431 authentication session cleanup for crammd5_authenticatee278172170335855 i0407 223410923256 29307 slavecpp1072 successfully authenticated with master master172170335855 i0407 223410923429 29307 slavecpp1468 will retry registration in 3220345ms if necessary i0407 223410923707 29302 mastercpp4406 registering agent at slave111172170335855 129e11060069 with id f59f9057a5c743e1b12996862e640a12s0 i0407 223410924239 29309 registrarcpp463 applied 1 operations in 105794ns attempting to update the registry i0407 223410925787 29309 logcpp683 attempting to append 339 bytes to the log i0407 223410926028 29309 coordinatorcpp348 coordinator attempting to write append action at position 3 i0407 223410927139 29309 replicacpp537 replica received write request for position 3 from 4797172170335855 i0407 223410929083 29305 slavecpp1468 will retry registration in 39293556ms if necessary i0407 223410929363 29305 mastercpp4394 ignoring register agent message from slave111172170335855 129e11060069 as admission is already in progress i0407 223410968843 29309 leveldbcpp341 persisting action 358 bytes to leveldb took 4168025ms i0407 223410969005 29309 replicacpp712 persisted action at 3 i0407 223410969741 29309 slavecpp1468 will retry registration in 54852242ms if necessary i0407 223410970118 29309 mastercpp4394 ignoring register agent message from slave111172170335855 129e11060069 as admission is already in progress i0407 223410970852 29306 replicacpp691 replica received learned notice for position 3 from 00000 i0407 223411010634 29306 leveldbcpp341 persisting action 360 bytes to leveldb took 39680272ms i0407 223411010840 29306 replicacpp712 persisted action at 3 i0407 223411011014 29306 replicacpp697 replica learned append action at position 3 i0407 223411014020 29306 registrarcpp508 successfully updated the registry in 89684224ms i0407 223411014181 29296 logcpp702 attempting to truncate the log to 3 i0407 223411014606 29296 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0407 223411015836 29298 replicacpp537 replica received write request for position 4 from 4798172170335855 i0407 223411016973 29296 mastercpp4474 registered agent f59f9057a5c743e1b12996862e640a12s0 at slave111172170335855 129e11060069 with cpus2 mem1024 disk4096 ports3100032000 i0407 223411017518 29304 hierarchicalcpp476 added agent f59f9057a5c743e1b12996862e640a12s0 129e11060069 with cpus2 mem1024 disk4096 ports3100032000 allocated  i0407 223411017763 29311 slavecpp1116 registered with master master172170335855 given agent id f59f9057a5c743e1b12996862e640a12s0 i0407 223411018362 29311 fetchercpp81 clearing fetcher cache i0407 223411018870 29311 slavecpp1139 checkpointing slaveinfo to tmpmasterallocatortest_1_rebalancedforupdatedweights_9acayametaslavesf59f9057a5c743e1b12996862e640a12s0slaveinfo i0407 223411018890 29307 status_update_managercpp181 resuming sending status updates i0407 223411019182 29304 hierarchicalcpp1491 no resources available to allocate i0407 223411019304 29304 hierarchicalcpp1165 performed allocation for agent f59f9057a5c743e1b12996862e640a12s0 in 1077349ms i0407 223411019493 29311 slavecpp1176 forwarding total oversubscribed resources i0407 223411019726 29311 slavecpp3675 received ping from slaveobserver112172170335855 i0407 223411019878 29299 mastercpp4818 received update of agent f59f9057a5c743e1b12996862e640a12s0 at slave111172170335855 129e11060069 with total oversubscribed resources i0407 223411020845 29305 hierarchicalcpp534 agent f59f9057a5c743e1b12996862e640a12s0 129e11060069 updated with oversubscribed resources total cpus2 mem1024 disk4096 ports3100032000 allocated  i0407 223411021005 29305 hierarchicalcpp1491 no resources available to allocate i0407 223411021065 29305 hierarchicalcpp1165 performed allocation for agent f59f9057a5c743e1b12996862e640a12s0 in 173907ns i0407 223411022289 29278 containerizercpp155 using isolation posixcpuposixmemfilesystemposix w0407 223411023422 29278 backendcpp66 failed to create bind backend bindbackend requires root privileges i0407 223411026309 29309 slavecpp201 agent started on 112172170335855 i0407 223411026410 29309 slavecpp202 flags at startup appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 cgroups_cpu_e,1
add authentication to agents monitorstatistics endpoint operators may want to enforce that only authenticated users and subsequently only specific authorized users be able to view perexecutor resource usage statistics since this endpoint is handled by the resourcemonitorprocess i would expect the work necessary to be similar to what was done for files or registry endpoint authn,2
sandboxes contents should be protected from unauthorized users mesos4956 introduced authentication support for the sandboxes however authentication can only go as far as to tell whether an user is known to mesos or not an extra additional step is necessary to verify whether the known user is allowed to executed the requested operation on the sandbox browse read download debug,8
consolidate authorization actions for quota we should have just a single authz action update_quota_with_role it was a mistake in retrospect to introduce multiple actions actions that are not symmetrical are registerteardown and dynamic reservations the way they are implemented in this way is because entities that do one action differ from entities that do the other for example register framework is issued by a framework teardown by an operator what is a good way to identify a framework a role it runs in which may be different each launch and makes no sense in multirole frameworks setup or better a sort of a group id which is its principal for dynamic reservations and persistent volumes they can be both issued by frameworks and operators hence similar reasoning applies now quota is associated with a role and set only by operators do we need to care about principals that set it not that much,5
run mesos builds on powerpc platform in asf ci this is the last step to declare official support for powerpc this is currently blocked on asf infra adding powerpc based jenkins machines to the asf ci,1
update webui for gpu metrics after adding the gpu metrics and updating the resources json to include gpu information the webui should be updated accordingly,1
add test to verify error when requesting fractional gpus fractional gpu requests should immediately cause a task_failed without ever launching the task,1
make networkcni enabled as the default network isolator for mesoscontainerizer currently there are no default network isolators for mesoscontainerizer with the development of the networkcni isolator we have an interface to run mesos on multitude of ip networks given that its based on an open standard the cni spec which is gathering a lot of traction from vendors calico weave coreos and already works on some default networks bridge ipvlan macvlan it makes sense to make it as the default network isolator,1
commit message hook behaves incorrectly when a message includes a  if there is a  in a commit message there often is when we have bulleted lists due to the current use of echo line the line gets expanded with a  in it which becomes a matcher in bash and therefore subsequently gets expanded into the list of filesdirectories in the current directory in order to avoid this mess we need to wrap such variables in quotes like so echo line,2
add authorization to agents monitorstatistics endpoint operators may want to enforce that only specific authorized users be able to view perexecutor resource usage statistics for 029 mvp we can make this coarsegrained and assume that only the operator or a operatorprivileged monitoring service will be accessing the endpoint for a future release we can consider finegrained authz that filters statistics like we plan to do for tasks,5
add tests for networkcni isolator we need to add tests to verify the functionality of networkcni isolator,5
benchmark overhead of authorization based filtering when adding authorization based filtering as outlined in mesos4931 we need to be careful especially for performance critical endpoints such as state we should ensure via a benchmark that performance does not degreade below an acceptable state,3
introduce new authorizer actions for authorized based filtering of endpoints for authorization based endpoint filtering we need to introduce the authorizer actions outlined via mesos4932,3
adapt json creation for authorization based endpoint filtering for authorization based endpoint filtering we need to adapt the json endpoint creation as discussed in mesos4931,5
expose statestatehpp to public headers we want the modules to be able to use replicated log along with the apis to communicate with zookeeper this change would require us to expose at least the following headers statestoragehpp and any additional files that statehpp depends on eg zookeeperauthenticationhpp,3
registry puller cannot fetch blobs correctly from some private repos when the registry puller is pulling a private repository from some private registry eg quayio errors may occur when fetching blobs at which point fetching the manifest of the repo is finished correctly the error message is unexpected http response 400 bad request when trying to download the blob this may arise from the logic of fetching blobs or incorrect format of uri when requesting blobs,3
allow masteragent to take multiple modules manifest files when loading multiple modules into masteragent one has to merge all module metadata library name module name parameters etc into a single json file which is then passed on to the modules flag this quickly becomes cumbersome especially if the modules are coming from different vendorsdevelopers an alternate would be to allow multiple invocations of modules flag that can then be passed on to the module manager that way each flag corresponds to just one module library and modules from that library another approach is to create a new flag eg modulesdir that contains a path to a directory that would contain multiple json files one can think of it as an analogous to systemd units the operator that drops a new file into this directory and the file would automatically be picked up by the masteragent module manager further the naming scheme can also be inherited to prefix the filename with an nn_ to signify oad order,3
update the balloonframework to run on test clusters there are a couple of problems with the balloon framework that prevent it from being deployed easily on an actual cluster  the framework accepts 100 of memory in an offer this means the expected behavior finish or oom is dependent on the offer size  the framework assumes the balloonexecutor binary is available on each agent this is generally only true in the build environment or in singleagent test environments  the framework does not specify cpus with the executor this is required by many isolators  the executors task_finished logic path was untested and is flaky  the framework has no metrics  the framework only launches a single task and then exits with this behavior we cant have useful metrics,3
add logic to validate for nonfractional gpu requests in the master we should not put this logic directly into the resourcesvalidate function the primary reason is that the existing resourcesvalidate function doesnt consider the semantics of any particular resource when performing its validation it only makes sure that the fields in the resource protobuf message are correctly formed since a fractional gpus resources is actually wellformed and only semantically incorrect we should push this validation logic up into the master moreover the existing logic to construct a resources object from a repeatedptrfieldresource silently drops any resources that dont pass resourcesvalidate this means that if we were to push the nonfractional gpus validation into resourcesvalidate the gpus resources would just be silently dropped rather than causing a task_error in the master this is obviously not the desired behaviour,2
enhance the error message for duration flag enhance the error message for httpsgithubcomapachemesosblob4dfa91fc21f80204f5125b2e2f35c489f8fb41d83rdpartylibprocess3rdpartystoutincludestoutdurationhppl70 to list all of the supported duration unit,1
scheduler driver does not detect disconnection with master and reregister the existing implementation of the scheduler driver does not reregister with the master under some network partition cases when a scheduler registers with the master 1 master links to the framework 2 framework links to the master it is possible for either of these links to break without the master changing currently the scheduler driver will only reregister if the master changes if both links break or if just link 1 breaks the master views the framework as inactive and disconnected this means the framework will not receive any more events such as offers from the master until it reregisters there is currently no way for the scheduler to detect a oneway link breakage if link 2 breaks it makes almost no difference to the scheduler the scheduler usually uses the link to send messages to the master but libprocess will create another socket if the persistent one is not available to fix link breakages for 12 and 2 the scheduler driver should implement a exited event handler for the masters pid and trigger a master redetection upon a disconnection this in turn should make the driver reregister with the master the scheduler library already does this httpsgithubcomapachemesosblobmastersrcschedulerschedulercppl395 see the related issue mesos5181 for link 1 breakage,3
master should reject calls from the scheduler driver if the scheduler is not connected when a scheduler registers the master will create a link from master to scheduler if this link breaks the master will consider the scheduler inactive and mark it as disconnected this causes a couple problems 1 master does not send offers to inactive schedulers but these schedulers might consider themselves registered in a oneway network partition scenario 2 any calls from the inactive scheduler is still accepted which leaves the scheduler in a starved but semifunctional state see the related issue for more context mesos5180 there should be an additional guard for registered but inactive schedulers here httpsgithubcomapachemesosblob94f4f4ebb7d491ec6da1473b619600332981dd8esrcmastermastercppl1977 the http api already does this httpsgithubcomapachemesosblob94f4f4ebb7d491ec6da1473b619600332981dd8esrcmasterhttpcppl459 since the scheduler driver cannot return a 403 it may be necessary to return a eventerror and force the scheduler to abort,1
the mesosexecute prints confusing message when launching tasks code rootmesos002srcmesosm2mesosbuild srcmesosexecute master19216856125050 nametest docker_imageubuntu1404 commandls root i0413 072803833521 2295 schedulercpp175 version 0290 subscribed with id 3a1af11ecf664ce2826d48b3329779990001 submitted task test to agent 3a1af11ecf664ce2826d48b332977999s0 received status update task_running for task test source source_executor reason reason_command_executor_failed  received status update task_finished for task test message command exited with status 0 source source_executor reason reason_command_executor_failed  rootmesos002srcmesosm2mesosbuild code,1
allow any principal in reservationinfo when http authentication is off mesos currently provides no way for operators to pass their principal to http endpoints when http authentication is off since we enforce that reservationinfoprincipal be equal to the operator principal in requests to reserve this means that when http authentication is disabled the reservationinfoprincipal field cannot be set to address this in the shortterm we should allow reservationinfoprincipal to hold any value when http authentication is disabled,1
operator endpoints should accept a principal without http authentication mesos currently provides no way for operators to include their principal with http endpoint requests when http authentication is disabled to remedy this we should add optional principal fields to the relevant protobuf messages when http authentication is enabled we can allow the user to leave this field empty and populate it with the principal from their http auth header,3
populate frameworkinfoprincipal for authenticated frameworks if a framework authenticates and then does not provide a principal in its frameworkinfo we currently allow this and leave frameworkinfoprincipal unset instead we should populate frameworkinfoprincipal for them automatically in that case to ensure that the two principals are equal,2
update the documentation for reserve and createvolumes there are a couple issues related to the principal field in diskinfo and reservationinfo see linked jiras that should be better documented we need to help users understand the purpose of these fields and how they interact with the principal provided in the http authentication header see linked tickets for background,1
document docker volume driver isolator should include the followings 1 what features driver options are supported in docker volume driver isolator 2 how to use docker volume driver isolator related agent flags introduction and usage isolator dependency clarification eg filesystemlinux related driver daemon preprocess volumes prespecified by users and volume cleanup,5
add documentation for nvidia gpu support httpsreviewsapacheorgr46220,5
create a benchmark for scale testing http frameworks it would be good to add a benchmark for scale testing the http frameworks wrt driver based frameworks the benchmark can be as simple as trying to launch n tasks parameterized with the oldnew api we can then focus on fixing performance issues that we find as a result of this exercise,3
implement http docker executor that uses the executor library similar to what we did with the http command executor in mesos3558 we should have a http docker executor that can speak the v1 executor api,5
add tests for capability api add basic tests for the capability api,3
add capability information to containerinfo protobuf message to enable support for capability as first class framework entity we need to add capabilities related information to the containerinfo protobuf,1
the windows version of osaccess has differing behavior than the posix version the posix version of osaccess looks like this code inline trybool accessconst stdstring path int how  if accesspathc_str how  0  if errno  eacces  return false  else  return errnoerror   return true  code compare this to the windows version of osaccess which looks like this following code inline trybool accessconst stdstring filename int how  if _accessfilenamec_str how  0  return errnoerroraccess could not access path   filename    return true  code as we can see the case where errno is set to eacces is handled differently between the 2 functions we can actually consolidate the 2 functions by simply using the posix version the challenge is that on posix we should use access and _access on windows note however that this problem is already solved as we have an implementation of access for windows in 3rdpartylibprocess3rdpartystoutincludestoutwindowshpp which simply defers to _access thus i propose to simply consolidate the 2 implementations,2
check failure in appcprovisionerintegrationtestroot_simplelinuximagetest observed on the mesosphere internal ci noformat 225628w step 1010 f0420 225628056788 629 containerizercpp1634 check failed containers_containscontainerid noformat complete test log will be attached as a file,2
persistent volume dockercontainerizer support assumes proper mount propagation setup on the host we recently added persistent volume support in dockercontainerizer mesos3413 to understand the problem we first need to understand how persistent volumes are supported in dockercontainerizer to support persistent volumes in dockercontainerizer we bind mount persistent volumes under a containers sandbox container_path has to be relative for persistent volumes when the docker container is launched since we always add a volume v for the sandbox the persistent volumes will be bind mounted into the container as well since docker does a rbind the assumption that the above works is that the docker daemon should see those persistent volume mounts that mesos mounts on the host mount table its not a problem if docker daemon itself is using the host mount namespace however on systemd enabled systems docker daemon is running in a separate mount namespace and all mounts in that mount namespace will be marked as slave mounts due to this patchhttpsgithubcomdockerdockercommiteb76cb2301fc883941bc4ca2d9ebc3a486ab8e0a so what that means is that in order for it to work the parent mount of agents work_dir should be a shared mount when docker daemon starts this is typically true on centos7 coreos as all mounts are shared mounts by default however this causes an issue with the filesystemlinux isolator to understand why first i need to show you a typical problem when dealing with shared mounts let me explain that using the following commands on a centos7 machine noformat rootcoredev run cat procselfmountinfo 24 60 019  run rwnosuidnodev shared22  tmpfs tmpfs rwseclabelmode755 rootcoredev run mkdir runnetns rootcoredev run mount bind runnetns runnetns rootcoredev run cat procselfmountinfo 24 60 019  run rwnosuidnodev shared22  tmpfs tmpfs rwseclabelmode755 121 24 019 netns runnetns rwnosuidnodev shared22  tmpfs tmpfs rwseclabelmode755 rootcoredev run ip netns add test rootcoredev run cat procselfmountinfo 24 60 019  run rwnosuidnodev shared22  tmpfs tmpfs rwseclabelmode755 121 24 019 netns runnetns rwnosuidnodev shared22  tmpfs tmpfs rwseclabelmode755 162 121 03  runnetnstest rwnosuidnodevnoexecrelatime shared5  proc proc rw 163 24 03  runnetnstest rwnosuidnodevnoexecrelatime shared5  proc proc rw noformat as you can see above therere two entries runnetnstest in the mount table unexpected this will confuse some systems sometimes the reason is because when we create a self bind mount runnetns  runnetns the mount will be put into the same shared mount peer group shared22 as its parent run then when you create another mount underneath that runnetnstest that mount operation will be propagated to all mounts in the same peer group shared22 resulting an unexpected additional mount being created the reason we need to do a self bind mount in mesos is that sometimes we need to make sure some mounts are shared so that it does not get copied when a new mount namespace is created however on some systems mounts are private by default eg ubuntu 1404 in those cases since we cannot change the system mounts we have to do a self bind mount so that we can set mount propagation to shared for instance in filesytemlinux isolator we do a self bind mount on agents work_dir to avoid the self bind mount pitfall mentioned above in filesystemlinux isolator after we created the mount we do a makeslave  makeshared so that the mount is its own shared mount peer group in that way any mounts underneath it will not be propagated back however that operation will break the assumption that the persistent volume dockercontainerizer support makes as a result were seeing problem with persistent volumes in dockercontainerizer when filesystemlinux isolator is turned on,3
command executor may escalate after the task is reaped in command executor escalated may be scheduled before the task has been killed ie reaped but called after in this case escalated should be a noop,1
remove systemstatsjson endpoint the systemstatsjson endpoint was deprecated by mesos2058 this endpoint can now be removed,1
update cmake files to reflect reorganized 3rdparty,2
move 3rdpartylibprocess3rdparty to 3rdparty,5
isolator cleanup should not be invoked if they are not prepared yet if the mesos containerizer destroys a container in provisioning state isolator cleanup is still called which is incorrect because there is no isolator prepared yet in this case there no need to clean up any isolator call provisioner destroy directly,2
add uri parsing functionlibrary the urifetcher theoretically supports all uris per rfc3986httptoolsietforghtmlrfc3986 to do this we need a speccompliant parser from string to uri uriparserhttpuriparsersourceforgenet appears to fit the bill,2
add gpus to container resource consumption metrics currently the usage callback in the nvidia gpu isolator is unimplemented noformat srcslavecontainerizermesosisolatorscgroupsdevicesgpusnvidiacpp noformat it should use functionality from nvml to gather the current gpu usage and add it to a resourcestatistics object it is still an open question as to exactly what information we want to expose here power memory consumption current load etc whatever we decide on should be standard across different gpu types different gpu vendors etc,3
add support for percontainerizer resource enumeration currently the top level containerizer includes a static function for enumerating the resources available on a given agent ideally this functionality should be the responsibility of individual containerizers and specifically the responsibility of each isolator used to control access to those resources adding support for this will involve making the containerizerresources function virtual instead of static and then implementing it on a percontainerizer basis we should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources,3
add autodiscovery for gpu resources right now the only way to enumerate the available gpus on an agent is to use the nvidia_gpu_devices flag and explicitly list them out instead we should leverage nvml to autodiscover the gpus that are available and only use this flag as a way to explicitly list out the gpus you want to make available in order to restrict access to some of them,3
turn the nvidia gpu isolator into a module the nvidia gpu isolator has an external dependence on libnvidiamlso as it currently stands this forces all binaries that link with libmesosso to also link with libnvidiamlso including master agents on machines without gpus scheduler exectors etc by turning the nvidia gpu isolator into a module it will be loaded at runtime only when an agent has explicitly including the the nvidia gpu isolator in its isolation flag,5
refactor the mesosfetcher binary to use the urifetcher as a backend this is an intermediate step for combining the mesosfetcher binary and urifetcher the download method should be replaced with urifetcherfetch httpsgithubcomapachemesosblob653eca74f1080f5f55cd5092423506163e65d402srclauncherfetchercppl179 combining the two will  attach the urifetcher to the existing fetcher caching logic  remove some code duplication for downloading uris,3
extend the urifetcherplugin interface to include a fetchsize in order to replace the mesosfetcher binary with the urifetcher each plugin must be able to determineestimate the size of a download this is used by the fetcher cache when it creates cache entries and such the logic for each of the four fetcherplugins can be taken and refactored from the existing fetcher httpsgithubcomapachemesosblob653eca74f1080f5f55cd5092423506163e65d402srcslavecontainerizerfetchercppl267,2
combine the internalslavefetcher class and mesosfetcher binary after mesos5259 the mesosfetcher will no longer need to be a separate binary and can be safely folded back into the agent process it was a separate binary because libcurl has synchronousblocking calls this will likely mean  a change to the fetch continuation chain httpsgithubcomapachemesosblob653eca74f1080f5f55cd5092423506163e65d402srcslavecontainerizerfetchercppl315  this protobuf can be deprecated or just removed httpsgithubcomapachemesosblob653eca74f1080f5f55cd5092423506163e65d402includemesosfetcherfetcherproto,3
pivot_root is not available on arm when compile on arm it will through error the current code logic in srclinuxfscpp is code ifdef __nr_pivot_root int ret  syscall__nr_pivot_root newrootc_str putoldc_str elif __x86_64__  a workaround for systems that have an old glib but have a new  kernel the magic number 155 is the syscall number for  pivot_root on the x86_64 architecture see  archx86syscallssyscall_64tbl int ret  syscall155 newrootc_str putoldc_str elif __powerpc__  __ppc__  __powerpc64__  __ppc64__  a workaround for powerpc the magic number 203 is the syscall  number for pivot_root on the powerpc architecture see  httpsw3challscomsyscallsarchpowerpc_64 int ret  syscall203 newrootc_str putoldc_str else error pivot_root is not available endif code possible sollution is to add unistdh header,1
update mesosexecute to support docker volume isolator the mesosexecute needs to be updated to support docker volume isolator,3
add test cases for docker volume driver,5
add alias support for flags currently there is no support for a flag to have an alias such support would be useful to renamedeprecate a flag for example for mesos4386 we could let the flag have authenticate name and a authenticate_frameworks alias the alias can be marked as deprecated need to add support for this as well this support will also be useful for slaveagent flag rename see mesos3781 for details,5
support docker image labels docker image labels should be supported in unified containerizer which can be used for applying custom metadata image labels are necessary for mesos features to support docker in unified containerizer eg for mesos gpu device isolator,3
need support for authorization information via help we should add information about authentication to the help message and thereby endpoint documentation similarly as mesos4934 has done for authentication,3
add capabilities support for unified containerizer add capabilities support for unified containerizer requirements 1 use the mesos capabilities api 2 frameworks be able to add capability requests for containers 3 agents be able to add maximum allowed capabilities for all containers launched design document httpsdocsgooglecomdocumentd1yitift8tqla2vq3upqr7kriq_pqfkocosysqjrogceditheadinghrgfwelqrskmd,5
need to add remove semantics to the copy backend some dockerfiles run the rm command to remove files from the base image using the run directive in the dockerfile an example can be found here httpsgithubcomngineerednginxphpfpmgit in the final rootfs the removed files should not be present presence of these files in the final image can make the container misbehave for example the nginxphpfpm docker image that is referenced tries to remove the default nginx config and replaces it with its own config to point to a different html root if the default nginx config is still present after the building the image nginx will start pointing to a different html root than the one set in the dockerfile currently the copy backend cannot handle removal of files from intermediate layers this can cause issues with docker images built using a dockerfile similar to the one listed here hence we need to add remove semantics to the copy backend,5
add authorization to libprocess http endpoints now that the libprocesslevel http endpoints have had authentication added to them in mesos4902 we can add authorization to them as well as a first step we can implement a coarsegrained approach in which a principal is granted or denied access to a given endpoint we will likely need to register an authorizer with libprocess,5
status updates after a health check are incomplete or invalid with command health checks enabled via marathon mesosdns will resolve the task correctly until the task is reported as healthy at that point mesosdns stops resolving the task correctly digging through srcdockerexecutorcpp i found that in the taskhealthupdated function is attempting to copy the taskid to the new status instance with codestatusmutable_task_idcopyfromtaskidcode but other instances of status updates have a similar line codestatusmutable_task_idcopyfromtaskidgetcode my assumption is that this difference is causing the status update after a health check to not have a proper taskid which in turn is causing an incorrect statejson output ill try to get a patch together soon update none of the above assumption are correct something else is causing the issue,1
split resource and inverse offer protobufs for v1 api the protobufs for the v1 api regarding inverse offers initially reused the existing offer  rescind  accept  decline messages for regular offers we should split these out the be more explicit and provide the ability to augment the messages with particulars to either resource or inverse offers,5
add authorization to the masters flags endpoint coarse http endpoint authorization using the get_endpoint_with_path acl rule needs to be added to the flags endpoint of the master,3
add synchronous validation for all types of calls currently we do a best effort validation for all calls sent to the master from the scheduler by invoking validationschedulercallvalidatecall principal this is a generic validation helper for all calls however for more fine grained validation for a particular call we invoke the validation as part of the call handle itself code optionerror validationerror  rolesvalidateframeworkinforole code this in turn makes all validations asynchronous ie the framework gets them as eventerror events later it would be good if such validations can be handled while processing the call message itself synchronously,5
consider adding an executor shimadapter for the newold api currently all the business logic for http based command executordriver based command executor lives in 2 different files as more features are addedbugs are discovered in the executor itself they need to be fixed in two places it would be nice to have some kind of a shimadapter that abstracts away the underlying library details from the executor hence the executor can toggle between whether it wants to use the driver or the new api via an environment variable,5
add capabilities support for mesos execute cli add support for user and capabilities to execute cli this will help in testing the capabilities feature for unified containerizer,3
metricssnapshot endpoint help disappeared on agent after httpsgithubcomapachemesoscommit066fc4bd0df6690a5e1a929d3836e307c1e22586 the help for the metricssnapshot endpoint on the agent doesnt appear anymore master endpoint help is unchanged,1
sandbox mounts should not be in the host mount namespace currently if a container uses container image well do a bind mount of its sandbox sandbox  rootfsmntmesossandbox in the host mount namespace however doing the mounts in the host mount table is not ideal that complicates both the cleanup path and the recovery path instead we can do the sandbox bind mount in the containers mount namespace so that cleanup and recovery will be greatly simplified we can setup mount propagation properly so that persistent volumes mounted at sandboxxxx can be propagated into the container here is a simple proof of concept console 1 noformat vagrantvagrantubuntutrusty64tmpmesos ll  total 12 drwxrwxrx 3 vagrant vagrant 4096 apr 25 1605  drwxrwxrx 6 vagrant vagrant 4096 apr 25 2317  drwxrwxrx 5 vagrant vagrant 4096 apr 25 2317 slave vagrantvagrantubuntutrusty64tmpmesos ll slave total 20 drwxrwxrx 5 vagrant vagrant 4096 apr 25 2317  drwxrwxrx 3 vagrant vagrant 4096 apr 25 1605  drwxrwxrx 6 vagrant vagrant 4096 apr 26 2106 directory drwxrxrx 12 vagrant vagrant 4096 apr 25 2320 rootfs drwxrwxrx 2 vagrant vagrant 4096 apr 25 1609 volume vagrantvagrantubuntutrusty64tmpmesos sudo mount bind slave slave vagrantvagrantubuntutrusty64tmpmesos sudo mount makeshared slave vagrantvagrantubuntutrusty64tmpmesos cat procselfmountinfo 50 22 81 homevagranttmpmesosslave homevagranttmpmesosslave rwrelatime shared1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered noformat console 2 noformat vagrantvagrantubuntutrusty64tmpmesos cd slave vagrantvagrantubuntutrusty64tmpmesosslave sudo unshare m binbash rootvagrantubuntutrusty64tmpmesosslave sudo mount makerslave  rootvagrantubuntutrusty64tmpmesosslave cat procselfmountinfo 124 63 81 homevagranttmpmesosslave homevagranttmpmesosslave rwrelatime master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered rootvagrantubuntutrusty64tmpmesosslave mount rbind directory rootfsmntmesossandbox rootvagrantubuntutrusty64tmpmesosslave mount rbind rootfs rootfs rootvagrantubuntutrusty64tmpmesosslave mount t proc proc rootfsproc rootvagrantubuntutrusty64tmpmesosslave pivot_root rootfs rootfstmprootfs rootvagrantubuntutrusty64tmpmesosslave cd  rootvagrantubuntutrusty64 cat procselfmountinfo 126 61 81 homevagranttmpmesosslaverootfs  rwrelatime master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered 127 126 81 homevagranttmpmesosslavedirectory mntmesossandbox rwrelatime master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered 128 126 03  proc rwrelatime  proc proc rw noformat console 1 noformat agrantvagrantubuntutrusty64tmpmesos cd slave vagrantvagrantubuntutrusty64tmpmesosslave sudo mount bind volume directoryv1 vagrantvagrantubuntutrusty64tmpmesosslave cat procselfmountinfo 50 22 81 homevagranttmpmesosslave homevagranttmpmesosslave rwrelatime shared1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered 129 50 81 homevagranttmpmesosslavevolume homevagranttmpmesosslavedirectoryv1 rwrelatime shared1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered noformat console 2 noformat rootvagrantubuntutrusty64 cat procselfmountinfo 126 61 81 homevagranttmpmesosslaverootfs  rwrelatime master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered 127 126 81 homevagranttmpmesosslavedirectory mntmesossandbox rwrelatime master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered 128 126 03  proc rwrelatime  proc proc rw 132 127 81 homevagranttmpmesosslavevolume mntmesossandboxv1 rwrelatime shared4 master1  ext4 devdiskbyuuidbaf292e50bb64e588a715b912e0f09b6 rwdataordered noformat,5
enable networkcni isolator to allow modifications and deletion of cni config currently the networkcni isolator can only load the cni configs at startup this makes the cni networks immutable from an operational standpoint this can make deployments painful for operators to make cni more flexible the networkcni isolator should be able to load configs at run time the proposal is to add an endpoint to the networkcni isolator to which when the operator sends a put request the networkcni isolator will reload cni configs,5
env mesos_sandbox is not set properly for command tasks that changes rootfs this is in the context of mesos containerizer aka unified containerizer i did a simple test noformat sudo sbinmesosmaster work_dirtmpmesosmaster sudo glog_v1 sbinmesosslave master1002155050 isolationdockerruntimefilesystemlinux work_dirtmpmesosslave image_providersdocker executor_environment_variables sudo binmesosexecute master1002155050 nametest docker_imagealpine commandenv mesos_executor_idtest shlvl1 mesos_checkpoint0 mesos_executor_shutdown_grace_period5secs libprocess_port0 mesos_agent_endpoint1002155051 mesos_sandboxtmpmesosslaveslaves2d7e44bb32824193bdc4eeab9e0943c2s0frameworks1a1cad182d8743dd97b61dbf2d2290610000executorstestrunsbb8dd72cfb4c426abe1851b0621339f6 mesos_native_java_libraryhomevagrantdistmesosliblibmesos0290so mesos_framework_id1a1cad182d8743dd97b61dbf2d2290610000 mesos_slave_id2d7e44bb32824193bdc4eeab9e0943c2s0 mesos_native_libraryhomevagrantdistmesosliblibmesos0290so mesos_directorytmpmesosslaveslaves2d7e44bb32824193bdc4eeab9e0943c2s0frameworks1a1cad182d8743dd97b61dbf2d2290610000executorstestrunsbb8dd72cfb4c426abe1851b0621339f6 pwdmntmesossandbox mesos_slave_pidslave11002155051 noformat mesos_sandbox above should be mntmesossandbox,2
failed to set quota and update weight according to document code rootmesos002test curl d jsonmessagebody x post http19216856125050quota failed to parse set quota request json jsonmessagebody syntax error at line 1 near jsonmessagebodyrootmesos002test cat jsonmessagebody  role role1 guarantee  name cpus type scalar scalar  value 1    name mem type scalar scalar  value 128    rootmesos002test curl d weightjson x put http19216856125050weights failed to parse update weights request json weightjson syntax error at line 1 near weightjs rootmesos002test cat weightjson   role role1 weight 20   role role2 weight 35   code the right command should be adding  before the quota json file jsonmessagebody,1
authenticate the agents containers endpoint the containers endpoint was recently added to the agent authentication should be enabled on this endpoint,2
authorize the agents containers endpoint after the agents containers endpoint is authenticated we should enabled authorization as well,2
make osclose always catch structured exceptions on windows,2
add authorization to get weights we already authorize which http users can update weights for particular roles but even knowing of the existence of these roles let alone their weights may be sensitive information we should add authz around get operations on weights easy option get_endpoint_with_path weights  pro no new verb  con all or nothing complex option get_weights_with_role  pro filters contents based on roles the user is authorized to see  con more authorize calls one per role in each weights request,3
add authorization to get quota we already authorize which http users can setremove quota for particular roles but even knowing of the existence of these roles let alone their quotas may be sensitive information we should add authz around get operations on quota,3
add master flag to enable finegrained filtering of http endpoints as the finegrained filtering of endpoints can the rather expensive we should create a master flag to enabledisable this feature,1
add user to task protobuf message the localauthorizer is supposed to use the os user under which tasks are running for authorization as the master keeps track of running and completed processes we need access to this information in task in order to authorize such tasks,1
create tests for testing finegrained http endpoint filtering,3
behavior of custom http authenticators with disabled http authentication is inconsistent between master and agent when setting a custom authenticator with http_authenticators and also specifying authenticate_httpfalse currently agents refuse to start with code a custom http authenticator was specified with the http_authenticators flag but http authentication was not enabled via authenticate_http code masters on the other hand accept this setting having differing behavior between master and agents is confusing and we should decide on whether we want to accept these settings or not and make the implementations consistent,3
design doc for task_lost_pending the task_lost task status describes two different situations a the task was not launched because of an error eg insufficient available resources or b the master lost contact with a running task eg due to a network partition the master will kill the task when it can eg when the network partition heals but in the meantime the task may still be running this has two problems 1 using the same task status for two fairly different situations is confusing 2 in the partitionedbutstillrunning case frameworks have no easy way to determine when a task has truly terminated to address these problems we propose introducing a new task status task_lost_pending if a framework opts into this behavior using a new capability task_lost would mean the task is definitely not running whereas task_lost_pending would mean the task may or may not be running weve lost contact with the agent but the master will try to shut it down when possible,5
enhance the log message when launching mesos containerizer log the launch flag which includes the executor command prelaunch commands and other information when launching the mesos containerizer,2
enhance the log message when launching docker containerizer log the launch flag which includes the executor command and other information when launching the docker containerizer,2
add asynchronous hook for validating docker containerizer tasks it is possible to plug in custom validation logic for the mesoscontainerizer via an isolator module but the same is not true of the dockercontainerizer basic logic can be plugged into the dockercontainerizer via hooks but this has some notable differences compared to isolators  hooks are synchronous  modifications to tasks via hooks have lower priority compared to the task itself ie if both the taskinfo and slaveexecutorenvironmentdecorator define the same environment variable the taskinfo wins  hooks have no effect if they fail short of segfaulting ie the slaveprelaunchdockerhook has a return type of trynothing httpsgithubcomapachemesosblob628ccd23501078b04fb21eee85060a6226a80ef8includemesoshookhppl90 but the effect of returning an error is a log message httpsgithubcomapachemesosblob628ccd23501078b04fb21eee85060a6226a80ef8srchookmanagercppl227l230 we should add a hook to the dockercontainerizer to narrow this gap this new hook would  be called at roughly the same place as slaveprelaunchdockerhook httpsgithubcomapachemesosblob628ccd23501078b04fb21eee85060a6226a80ef8srcslavecontainerizerdockercppl1022  return a future and require splitting up dockercontainerizerlaunch  prevent a task from launching if it returns a failure,5
use connection abstraction to compare stale connections in scheduler library previously we had a bug in the connection abstraction in libprocess that hindered the ability to pass it onto defer callbacks since it could sometimes lead to deadlock mesos4658 now that it is resolved we might consider not using uuid objects for stale connection checks but directly using the connection abstraction in the scheduler library,3
add windows support for stopwatch,2
the scheduler library should have a delay before initiating a connection with master currently the scheduler library srcschedulerschedulercpp does have an artificially induced delay when trying to initially establish a connection with the master in the event of a master failover or zk disconnect a large number of frameworks can get disconnected and then thereby overwhelm the master with tcp syn requests on a large cluster with many agents the master is already overwhelmed with handling connection requests from the agents this compounds the issue further on the master,3
set death signal for dvdcli subprocess in docker volume isolator if the slave crashes we should kill the dvdcli subprocess otherwise if the dvdcli subprocess gets stuck itll not be cleaned up,2
add authentication to example frameworks some example frameworks do not have the ability to authenticate with the master adding authentication to the example frameworks that dont already have it implemented would allow us to use these frameworks for testing in authenticatedauthorized scenarios,2
introduce a timeout for docker volume driver mountunmount operation dvdcli might hang indefinitely we should introduce timeout for both mountunmount operation so that launchcleanup are not blocked forever,2
add deprecation support for flags mesos5271 adds support for a flag name to have an alias this ticket captures the work need to add deprecation support the idea is for the caller to explicitly specify deprecation via flagsbaseadd and get a list of deprecation warnings when doing flagsbaseload,5
add random to os namespace the function random is not available in windows after this improvement the calls to osrandom will result in calls to random on posix and rand on windows,1
remove zookeepers ntddi_version define zookeeper client library defines ntddi_version to 0x0400 in winconfigh while this api level is suficient to compile the client library mesos have to use a newer api set after this improvement the code will compile with the latest ntddi_version,2
add support for console ctrl handling in slavecpp extract supporting code to handle posix signals in a separate header and add support for ctrl handler when running on windows,3
implement stoutoswindowskillhpp implement equivalent functionality on windows,5
terminating a framework during master failover leads to orphaned tasks repro steps 1 setup code binmesosmastersh work_dirtmpmaster binmesosslavesh work_dirtmpslave masterlocalhost5050 srcmesosexecute checkpoint commandsleep 1000 masterlocalhost5050 nametest code 2 kill all three from 1 in the order they were started 3 restart the master and agent do not restart the framework result  the agent will reconnect to an orphaned task  the web ui will report no memory usage  curl localhost5050metricssnapshot will say mastermem_used 128 cause when a framework registers with the master it provides a failover_timeout in case the framework disconnects if the framework disconnects and does not reconnect within this failover_timeout the master will kill all tasks belonging to the framework however the master does not persist this failover_timeout across master failover the master will forget about a framework if 1 the master dies before failover_timeout passes 2 the framework dies while the master is dead when the master comes back up the agent will reregister the agent will report the orphaned tasks because the master failed over it does not know these tasks are orphans ie it thinks the frameworks might reregister proposed solution the master should save the frameworkid and failover_timeout in the registry upon recovery the master should resume the failover_timeout timers,3
killing a queued task can cause the corresponding command executor to never terminate we observed this in our testing environment sequence of events 1 a command task is queued since the executor has not registered yet 2 the framework issues a killtask 3 since executor is in registering state agent calls statusupdatetask_killed upid 4 statusupdate now will call containerizerstatus before calling executorterminatetaskstatustask_id status which will remove the queued task introduced in this patch httpsreviewsapacheorgr43258 5 since the above is async its possible that the task is still in queued task when we trying to see if we need to kill unregistered executor in killtask code  todojieyu here we kill the executor if it no longer has  any task to run and has not yet registered this is a  workaround for those single task executors that do not have a  proper self terminating logic when they havent received the  task within a timeout if executorqueuedtasksempty  checkexecutorlaunchedtasksempty   unregistered executor   executorid   has launched tasks logwarning  killing the unregistered executor   executor   because it has no tasks executorstate  executorterminating containerizerdestroyexecutorcontainerid  code 6 consequently the executor will never be terminated by mesos attaching the relevant agent log noformat may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613640527 1342 slavecpp1361 got assigned task mesosvol6ccd993c192011e6a7229648cb19afd6 for framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613641034 1342 slavecpp1480 launching task mesosvol6ccd993c192011e6a7229648cb19afd6 for framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613641440 1342 pathscpp528 trying to chown varlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497a to user root may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613644664 1342 slavecpp5389 launching executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 with resources cpus01 mem32 in work directory varlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613645195 1342 slavecpp1698 queuing task mesosvol6ccd993c192011e6a7229648cb19afd6 for executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613645491 1338 containerizercpp671 starting container 24762d432134475eb724caa72110497a for executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613647897 1345 cpusharecpp389 updated cpushares to 1126 cpus 11 for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613648619 1345 cpusharecpp411 updated cpucfs_period_us to 100ms and cpucfs_quota_us to 110ms cpus 11 for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613650180 1341 memcpp602 started listening for oom events for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613650718 1341 memcpp722 started listening on low memory pressure events for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613651147 1341 memcpp722 started listening on medium memory pressure events for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613651599 1341 memcpp722 started listening on critical memory pressure events for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613652015 1341 memcpp353 updated memorysoft_limit_in_bytes to 160mb for container 24762d432134475eb724caa72110497a may 13 153613 ip100274uswest2computeinternal mesosslave1304 i0513 153613652719 1341 memcpp388 updated memorylimit_in_bytes to 160mb for container 24762d432134475eb724caa72110497a may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625508930 1342 slavecpp1891 asked to kill task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625509063 1342 slavecpp3048 handling status update task_killed uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 from 00000 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625509702 1340 diskcpp169 updating the disk resources for container 24762d432134475eb724caa72110497a to cpus01 mem32 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625510298 1343 memcpp353 updated memorysoft_limit_in_bytes to 32mb for container 24762d432134475eb724caa72110497a may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625510349 1341 cpusharecpp389 updated cpushares to 102 cpus 01 for container 24762d432134475eb724caa72110497a may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625511102 1343 memcpp388 updated memorylimit_in_bytes to 32mb for container 24762d432134475eb724caa72110497a may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625511495 1341 cpusharecpp411 updated cpucfs_period_us to 100ms and cpucfs_quota_us to 10ms cpus 01 for container 24762d432134475eb724caa72110497a may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625511715 1341 status_update_managercpp320 received status update task_killed uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625512032 1341 status_update_managercpp824 checkpointing update for status update task_killed uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625513849 1343 slavecpp3446 forwarding the update task_killed uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 to master1005795050 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625528929 1344 status_update_managercpp392 received status update acknowledgement uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153625 ip100274uswest2computeinternal mesosslave1304 i0513 153625529002 1344 status_update_managercpp824 checkpointing ack for status update task_killed uuid f9d159556c9a4a7398c397c0128510ba for task mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628199105 1345 isolatorcpp469 mounting docker volume mount point varlibrexrayvolumesjdeftest125data to varlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497adata for container 24762d432134475eb724caa72110497a may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628207062 1338 containerizercpp1184 checkpointing executors forked pid 5810 to varlibmesosslavemetaslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497apidsforkedpid may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628832330 1338 slavecpp2689 got registration for executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 from executor110027446154 may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628833149 1345 diskcpp169 updating the disk resources for container 24762d432134475eb724caa72110497a to cpus01 mem32 may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628833804 1342 memcpp353 updated memorysoft_limit_in_bytes to 32mb for container 24762d432134475eb724caa72110497a may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628833871 1340 cpusharecpp389 updated cpushares to 102 cpus 01 for container 24762d432134475eb724caa72110497a may 13 153628 ip100274uswest2computeinternal mesosslave1304 i0513 153628835160 1340 cpusharecpp411 updated cpucfs_period_us to 100ms and cpucfs_quota_us to 10ms cpus 01 for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 5804 mesoslogrotatelogger helpfalse log_filenamevarlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497astdout logrotate_optionsrotate 9 logrotate_pathlogrotate max_size2mb  may 13 165830 ip100274uswest2computeinternal mesosslave30985 5809 mesoslogrotatelogger helpfalse log_filenamevarlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497astderr logrotate_optionsrotate 9 logrotate_pathlogrotate max_size2mb  may 13 165830 ip100274uswest2computeinternal mesosslave30985 5804 mesoslogrotatelogger helpfalse log_filenamevarlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497astdout logrotate_optionsrotate 9 logrotate_pathlogrotate max_size2mb  may 13 165830 ip100274uswest2computeinternal mesosslave30985 5809 mesoslogrotatelogger helpfalse log_filenamevarlibmesosslaveslavesa3ad8418cb774705b3534b514ceca52cs0frameworksa3ad8418cb774705b3534b514ceca52c0000executorsmesosvol6ccd993c192011e6a7229648cb19afd6runs24762d432134475eb724caa72110497astderr logrotate_optionsrotate 9 logrotate_pathlogrotate max_size2mb  may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830374567 30993 slavecpp5498 recovering executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830420411 30990 status_update_managercpp208 recovering executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830513164 30994 containerizercpp467 recovering container 24762d432134475eb724caa72110497a for executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830533478 30988 memcpp602 started listening for oom events for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830534553 30988 memcpp722 started listening on low memory pressure events for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830535269 30988 memcpp722 started listening on medium memory pressure events for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830536198 30988 memcpp722 started listening on critical memory pressure events for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830579385 30988 dockercpp859 skipping recovery of executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 because it was not launched from docker containerizer may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830587158 30989 slavecpp4527 sending reconnect request to executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 at executor110027446154 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830588287 30990 slavecpp2838 reregistering executor mesosvol6ccd993c192011e6a7229648cb19afd6 of framework a3ad8418cb774705b3534b514ceca52c0000 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830589736 30988 diskcpp169 updating the disk resources for container 24762d432134475eb724caa72110497a to cpus01 mem32 may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830590117 30990 cpusharecpp389 updated cpushares to 102 cpus 01 for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830591284 30990 cpusharecpp411 updated cpucfs_period_us to 100ms and cpucfs_quota_us to 10ms cpus 01 for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830595403 30992 memcpp353 updated memorysoft_limit_in_bytes to 32mb for container 24762d432134475eb724caa72110497a may 13 165830 ip100274uswest2computeinternal mesosslave30985 i0513 165830596102 30992 memcpp388 updated memorylimit_in_bytes to 32mb for container 24762d432134475eb724caa72110497a noformat,3
implement osfsync,1
implement ossethostname,1
add handle overloads for functions that take a file descriptor,3
mesoscontainerizerlaunch flags execute arbitrary commands via shell for example the docker volume isolators containerpath is appended without sanitation to a command thats executed in this manner as such its possible to inject arbitrary shell commands to be executed by mesos httpsgithubcomapachemesosblob17260204c833c643adf3d8f36ad8a1a606ece809srcslavecontainerizermesoslaunchcppl206 perhaps instead of strings these commands couldshould be sent as string arrays that could be passed as argv arguments wo shell interpretation,3
docker containerizer should prefix relative volumecontainer_path values with the path to the sandbox docker containerizer currently requires absolute paths for values of volumecontainer_path this is inconsistent with the mesos containerizer which requires relative container_path it makes for a confusing api both at the mesos level as well as at the marathon level ideally the docker containerizer would allow a framework to specify a relative path for volumecontainer_path and in such cases automatically convert it to an absolute path by prepending the sandbox directory to it cc jieyu,3
v1 executor protos not included in maven jar according to mesos4793 the executor v1 http api was released in mesos 0280 however the corresponding protos are not included in the maven jar for version 0280 or 0281 script to verify code wget httpsrepomavenapacheorgmaven2orgapachemesosmesos0281mesos0281jar  unzip lf mesos0281jar  grep v1executor  wc l code,1
add support for controlling resource limits in mesos containerizer currently we dont have ability to control system resource limits add support for   frameworks to specify resource limits  operators to override default resource limits,5
design doc for adding resource limits support for mesos containerizer this will be the design doc for mesos5391,3
slaveagent rename phase 1 update terms in the website the following files need to be updated sitesourceindexhtmlmd,1
rewrite osread to be friendlier to reading binary files the existing read implementation is based on calling getline to read in chunks of data from a file this is fine for textbased files but is a little strange for binary files,3
add utility for parsing ldsocache on linux the etcldsocache file on linux contains a mapping of dynamic library names to their fully resolved paths for use by ld when linking we should write a utility that knows how to parse this file so we can find the paths to these libraries as well this is especially important for collecting libraries into a common location for supporting nvidia gpus in mesos,5
add preliminary support for parsing elf files in stout the upcoming nvidia gpu support for docker containers in mesos relies on consolidating all nvidia shared libraries into a common location for injecting a volume into a container as part of this we need some preliminary parsing capabilities for elf file to infer things about each shared library we are consolidating,5
add ability to inject a volume of nvidia librariesbinaries into a dockerimage container in mesos containerizer in order to support nvidia gpus with docker containers in mesos we need to be able to consolidate all nvidia libraries into a common volume and inject that volume into the container this tracks the support in the mesos containerizer the docker containerizer support will be tracked separately more info on why this is necessary here httpsgithubcomnvidianvidiadocker,5
introduce objectapprover interface to authorizer as outlined here httpsdocsgooglecomdocumentd1fus79p8uj5pibycrblkjsbkotmeo8ezauinxxwia3qa we plan to add the option of retrieving a filterobject from the authorizer with the goal of allowing for efficient authorization of a large number of potentially large objects,5
allow task to be authorized as we need to be able to authorize tasks eg for deciding whether to include them in the state endpoint when applying authorization based filtering we need to expose it to the authorizer secondly we also need to include some additional information user and env variables in order to provide the authorizer with meaning information,3
make fields in authorizationrequest protobuf optional currently authorizationrequest protobuf declares subject and object as required fields however in the codebase we not always set them which renders the message in the uninitialized state for example  httpsgithubcomapachemesosblob0bfd6999ebb55ddd45e2c8566db17ab49bc1ffecsrccommonhttpcppl603  httpsgithubcomapachemesosblob0bfd6999ebb55ddd45e2c8566db17ab49bc1ffecsrcmasterhttpcppl2057 i believe that the reason why we dont see issues related to this is because we never send authz requests over the wire ie never serializedeserialize them however they are still invalid protobuf messages moreover some external authorizers may serialize these messages we can either ensure all required fields are set or make both subject and object fields optional this will also require updating local authorizer which should properly handle the situation when these fields are absent we may also want to notify authors of external authorizers to update their code accordingly it looks like no deprecation is necessary mainly because we alreadyerroneouslytreat these fields as optional,3
validate acls on creating an instance of local authorizer some combinations of acls are not allowed for example specifying both setquota and updatequota we should capture such issues and error out early this ticket aims to add as many validations as possible to a dedicated validate routine instead of having them implicitly in the codebase,3
delete the observe http endpoint the observe endpoint was introduced a long time ago for supporting functionality that was never implemented we should just kill this endpoint and associated code to avoid tech debt,2
networkcni isolator should skip the bind mounting of the cni network information root directory if possible currently in the create method networkcni isolator for the cni network information root directory ie varrunmesosisolatorsnetworkcni we do a self bind mount and make sure it is a shared mount of its own peer group however we should not do a self bind mount if the mount containing the cni network information root directory is already a shared mount in its own share peer group just like what we did for filesystemlinux isolator in mesos5239  httpsissuesapacheorgjirabrowsemesos5239,3
document all known client libraries for the schedulerexecutor api previously during various community syncs we had decided that we would only be supporting the c schedulerexecutor library in the mesos code base going forward we should however still document the client libraries available in various languages to drive adoptionhave a recommended list for users to look up this can be similar to the already existing frameworks doc httpmesosapacheorgdocumentationlatestframeworks other projects also seem to have been following a similar practice httpsdocsdockercomenginereferenceapiremote_api_client_libraries httpsgithubcomkuberneteskubernetesblobmasterdocsdevelclientlibrariesmd,2
implement osexists for processes osexists returns true if the process identified by the parameter is still running or was running and we are able to get information about it such us the exit code in windows after obtaining a handle to the process it is possible perform those operations,1
consider using intervalset for port range resource math followup jira for comments raised in mesos3051 see comments there we should consider utilizing intervalsethttpsgithubcomapachemesosbloba0b798d2fac39445ce0545cfaf05a682cd393abe3rdpartystoutincludestoutintervalhpp in port range resource mathhttpsgithubcomapachemesosbloba0b798d2fac39445ce0545cfaf05a682cd393abesrccommonvaluescppl143,3
relax version compatibility requirement for some modules some module interfaces such as authenticatee have not changed for a while and so we should be able to relax the version compatibility checks this needs to be done on a casebycase basis i am also hoping this change will also provide a framework for updating the version requirement for other modules as we go towards a stable module api cc adammesos tillt,5
add default implementations to all isolator virtual functions currently all of the virtual functions in mesosslaveisolator are pure virtual expect status for many isolators however it doesnt make sense to implement all of these virtual functions each isolator has to provide its own default implementation of these functions even if they arent really relying on them this adds unnecessary extra code to many isolators that dont need them moreover the mesosisolatorprocess has the same problem for each of its virtual functions we should provide defaults for these instead of making each and every isolator implement even in cases when it doesnt make sense,1
gpu resource broke framework data table in webui in agent_frameworkhtml and masterstaticagenthtml we add gpus used  allocated in table header but we didnt add the corresponding column to the table body as well on the other hand we didnt provide statistics for gpus on monitor endpoints to provide those data in webui it requires we implement gpus statistics in monitor endpoints firstly,1
appc appc_simple_discovery_uri_prefix is lost in configurationmd appc appc_simple_discovery_uri_prefix is lost in configurationmd,1
allow libprocessstout to build without first doing make in 3rdparty after the 3rdparty reorg libprocessstout are enable to build their dependencies and so one has to do make in 3rdpart before building libprocessstout,2
make the sasl dependency optional right now there is a hard dependency on sasl which probably wont work well on windows at least in the near future for our use cases in the future it would be nice to have a pluggable authentication layer,2
agent modules should be initialized before all components except firewall on mesos agents anonymous modules should not have any dependencies by design on any other mesos components this implies that anonymous modules should be initialized before all other mesos components other than firewall the dependency on firewall is primarily to enforce any policies to secure endpoints that might be owned by the anonymous module,1
cni should not store subnet of address in networkinfo when the cni isolator executes the cni plugin that cni plugin will return an ip address and subnet 1921680132 mesos should strip the subnet before storing the address in the tasknetworkinfoipaddress reason being  most current mesos components are not expecting a subnet in the tasks networkinfoipaddress and instead expect just the ip address this can cause errors in those components such as mesosdns failing to return a networkinfo address and instead defaulting to the next configured ipsource and marathon generating invalid links to tasks as it includes 32 in the link,2
master anonymous modules should initialized before any other components anonymous modules on the master are by design supposed to be independent of any mesos components however there might be a dependency in the reverse direction for eg anonymous modules might want to influence the behavior of mesos components say by generating configuration that might be consumed later by the components the anonymous modules on the master therefore need to be initialized before other mesos components,1
update run_task_with_user to use additional metadata currently the authorizationaction run_task_with_user will pass the user as its objectvalue string but some authorizers may want to make authorization decisions based on additional task attributes like role resources labels container type etc we should create a new action run_task that passes frameworkinfo and taskinfo in its object and the localauthorizers runtaskwithuser acl can be implemented using the user found in taskinfoframeworkinfo we may need to leave the old _with_user action around but its arguable whether we should call the authorizer once for run_task and once for run_task_with_user or only use the new action and deprecate the old one,5
remove hardcoded principals in persistentvolumeendpointstestslavesendpointfullresources in the test persistentvolumeendpointstestslavesendpointfullresources the value testprincipal is hardcoded into the json strings expected in http responses it would be more durable to use default_credentialprincipal instead,1
confirm errors in authorized persistent volume tests the tests persistentvolumetestbadacldropcreateanddestroy and persistentvolumetestbadaclnoprincipal check for a failed destroy operation by confirming that the persistent volume is still contained in an offer received after the attempted operation we should also explicitly check that the operation did not succeed due to failed authorization,1
enable option to handle string literals gracefully in flagsbaseadd mesos5064 begins making use of template function parameters like t2 for the default flag value rather than optiont2 this is because in some places in the code base we pass string literals for this argument if an option type is used the compiler infers a char x type for t2 which breaks optiongetorelse which attempts to return that same type since returning arrays is disallowed to fix this we could employ stddecay which would convert a return type of char x into const char,2
reenable stylecheck for stout after the 3rdparty reorg the mesosstyle checker stopped checking stout,1
maven build is too verbose for batch builds during a noninteractive without terminal mesos build maven generates several thousands of log lines when downloading artifacts this often makes several webbased log viewers unresponsive further these several thousand line long progress indicator logs dont provide any meaningful information either from a users point of view just knowing that the artifact download succeededfailed is often enough we should be using batchmode flag to disable these additionals log lines,1
http v1 subscribed scheduler event always has nil http_interval_seconds im writing a controller in go to monitor heartbeats id like to use the interval as communicated by the master which should be specified in the subscribed event but its not code 20160603 183404 typesubscribed subscribedevent_subscribedframeworkidmesosframeworkidvalueffdb6d6e01674fa298f92c3f8157fc250004heartbeatintervalsecondsnil offersnil rescindnil updatenil messagenil failurenil errornil code code  dpkg l grep e mesos ii mesos 02802016ubuntu1404 amd64 cluster resource manager with efficient resource isolation code i am seeing heartbeat events just not seeing the interval specified in the subscribed event,1
document aufs provisioner backend we should update containerimagemd with the newly supported backend,2
remove nvidia gpu isolators linktime dependence on libnvidiaml the current nvidia gpu isolator has a dependence on libnvidiaml and as such pulls a hard dependence on this library into libmesos the consequence of this is that any process that relies on libmesos has to have libnvidiaml available as well even on machines where no gpus are available since this library is not easily installable through standard package managers having such a hard dependence can be burdensome this ticket proposes to pull in libnvidiaml as a runtime dependence instead of a linktime dependence as such only machines that actually have gpus installed and would like to rely on this library need to have it installed,2
move the nvidia gpu isolator from cgroupsdevicesgpunvidia to gpunvidia currently the nvidia gpu isolator lives in srcslavecontainerizersmesosisolatorscgroupsdevicesgpunvidia however in the future this isolator will do more than simply isolate gpus using the cgroups devices subsystem eg volume management for injecting machine specific nvidia libraries into a container for this reason we should preemptively move this isolator up to srcslavecontainerizersmesosisolatorsgpunvidia as part of this we should update the string we pass to the isolator agent flag to reflect this,2
bundle nvml headers for nvidia gpu support currently we rely on a script to install the nvidia gdk as a build dependence for building mesos with nvidia gpu support a previous ticket removed the mesos build dependence on libnvidiaml which comes as part of the gdk this ticket proposes bundling the nvml headers with mesos in order to completely remove the build dependence on the gdk with this change it will be much simpler to configure and build with nvidia gpu support all that will be required is noformat configure enablenvidiagpusupport make j noformat,1
change majorminor device types for nvidia gpus to unsigned int currently the gpu struct specifies the type of its major and minor fields as dev_t which is actually a concatenation of both the major and minor device numbers accessible through the major and minor macros these macros return an unsigned int when handed a dev_t so it makes sense for these fields to be of that type instead,1
always provide access to nvidia control devices within containers if gpu isolation is enabled currently access to devnvidiactl and devnvidiauvm is only granted to  revoked from a container as gpus are added and removed from them on some level this makes sense because most jobs dont need access to these devices unless they are also using a gpu however there are cases when access to these files is appropriate even when not making use of a gpu running nvidiasmi to control the global state of the underlying nvidia driver for example we should add devnvidiactl and devnvidiauvm to the default whitelist of devices to include in every container when the gpunvidia isolator is enabled this will allow a container to run standard nvidia driver tools such as nvidiasmi without failing with abnormal errors when no gpus have been granted to it as such these tools will now report that no gpus are installed instead of failing abnormally,3
fix method of populating device entries for devnvidiauvm etc currently the majorminor numbers of devnvidiactl and devnvidiauvm are hardcoded this causes problems for devnvidiauvm because its major number is part of the experimental device range on linux because this range is experimental there is no guarantee which device number will be assigned to it on a given machine we should use osstatrdev to extract the majorminor numbers programatically,2
add nvidiagpuallocator component for crosscontainerizer gpu allocation we need some way of allocating gpus from a centralized location to allow both the mesos containerizer and the docker containerizer to pull from central pool we propose to build a nvidiagpuallocator for this purpose this component should also be overloaded to do resource enumeration of gpus based on the agent flags this keeps all code for enumerating gpus and the resources they represent in a single centralized location,5
update containerizerresources to use the nvidiagpuallocator with the introduction of the shared nvidiagpuallocator component containerizerresources should be updated to use it,2
integrate the nvidiagpuallocator into the nvidiagpuisolator,3
need to remove references to messagesmessageshpp from state api in order to expose the state api for using replicated log in mesos modules it is necessary that the state api does not reference headers that are not exposed as part of the mesos installation currently includemesosstateprotobufhpp references srcmessagesmessageshpp making the state api unusable in a module we need to move the protobuf serializedeserialize functions out of messageshpp and move them to stoutprotobufhpp this will help us remove references to messageshpp from the state api,2
add class to share nvidiaspecific components between containerizers once we have an nvidiagpuallocator component we need some way to share it across multiple containerizers moreover we anticipate needing other nvidia components to share across multiple containerizers as well eg an nvidiavolumemanager component as such we should add a wrapper class around these components to make it easily passable to each containerizer without having to continually add a bunch of parameters to the containerizer interface,2
rearrange nvidia gpu files to cleanup semantics for header inclusion currently components outside of srcslavecontainerizersmesosisolatorsgpu have to protect their includes for certain nvidia header files with the enable_nvidia_gpu_support flag other headers strictly could not be wrapped in this flag we need to clean up this header madness by creating a common nvidiahpp header that takes care of all the dependencies all componenents outside of srcslavecontainerizersmesosisolatorsgpu should only need to include this one header instead of managing everything themselves,1
document common use cases of authorization our authorization documentation covers the existing functionality but it doesnt provide a practical howto guide to help users accomplish common authorized use cases for example a user recently reported that to gain full use of the web ui after upgrading to mesos 10 six new acl rules needed to be added get_endpoints view_frameworks view_tasks view_executors access_sandboxes and access_mesos_logs rather than expecting users to figure this out on their own we should document the acls needed to accomplish a common goal like this similarly authorizing a stateful framework to accomplish the actions it would usually be expected to perform would involve setting rules for register_frameworks run_tasks shutdown_frameworks reserve_resources unreserve_resources create_volumes and destroy_volumes,1
improve changelog and upgradesmd currently we have a lot of data duplication between the changelog and upgradesmd we should try to improve this and potentially make the changlog a markdown file as well for inspiration see the hadoop changelog httpsgithubcomapachehadoopblob2e1d0ff4e901b8313c8d71869735b94ed8bc40a0hadoopcommonprojecthadoopcommonsrcsitemarkdownrelease120changes120md,3
masters may drop the first message they send between masters after a network partition we observed the following situation in a cluster of five masters  time  master 1  master 2  master 3  master 4  master 5   0  follower  follower  follower  follower  leader   1  follower  follower  follower  follower  partitioned from cluster by downing this vms network   2  elected leader by zk  voting  voting  voting  suicides due to lost leadership   3  performs consensus  replies to leader  replies to leader  replies to leader  still down   4  performs writing  acks to leader  acks to leader  acks to leader  still down   5  leader  follower  follower  follower  still down   6  leader  follower  follower  follower  comes back up   7  leader  follower  follower  follower  follower   8  partitioned in the same way as master 5  follower  follower  follower  follower   9  suicides due to lost leadership  elected leader by zk  follower  follower  follower   10  still down  performs consensus  replies to leader  replies to leader  doesnt get the message   11  still down  performs writing  acks to leader  acks to leader  acks to leader   12  still down  leader  follower  follower  follower  master 2 sends a series of messages to the recentlyrestarted master 5 the first message is dropped but subsequent messages are not dropped this appears to be due to a stale link between the masters before leader election the replicated log actors create a network watcher which adds links to masters that join the zk group httpsgithubcomapachemesosblob7a23d0da817be4e8f68d96f524cecf802431033csrclognetworkhppl157l159 this link does not appear to break master 2  5 when master 5 goes down perhaps due to how the network partition was induced in the hypervisor layer rather than in the vm itself when master 2 tries to send an promiserequest to master 5 we do not observe the expected log messagehttpsgithubcomapachemesosblob7a23d0da817be4e8f68d96f524cecf802431033csrclogreplicacppl493l494 instead we see a log line in master 2 code processcpp2040 failed to shutdown socket with fd 27 transport endpoint is not connected code the broken link is removed by the libprocess socket_manager and the following writerequest from master 2 to master 5 succeeds via a new socket,5
modules using replicated log state api require zookeeper headers the state api uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during mesos installation,1
support static address allocation in cni currently a framework cant specify a static ip address for the container when using the networkcni isolator the ipaddress field in the networkinfo protobuf was designed for this specific purpose but since the cni spec does not specify a means to allocate an ip address to the container the networkcni isolator cannot honor this field even when it is filled in by the framework creating this ticket to act as a place holder to track this limitation as and when the cni spec allows us to specify a static ip address for the container we can resolve this ticket,1
support static ip address allocation with dockercontainerizer docker run supports the ip option to allocate a specific ipv4 address to the container also the networkinfo protobuf has an ipaddress field that all frameworks to specify an ip address for the container the docker executor should therefore invoke the docker run command with the ip option whenever the ipaddress field of the networkinfo is set allowing frameworks to try and assign a static ip address for their services,1
implement authnauthz for the networkcni isolator currently any framework can launch containers on any cni network irrespective of its role and principal we need perform authnauthz in the networkcni isolator or master to make sure that only rolesprincipals specified by the operator can launch containers on a given network,3
guarantee ordering between isolators some isolators depend on other isolators however we currently do not have a generic method of expressing these dependencies we special case the filesystem isolators to make sure that dependencies on them are satisfied but no other dependencies can be expressed instead we should use a vector to represent the pairing of isolator name to isolator creator function this way the relative dependencies between each isolator will be implicit in the ordering of the vector currently a hashmap is used to hold this pairing but this is inadequate because hashmaps are inherently unordered the new implementation using a vector will ensure everything is processed in the order it is listed,3
create a cgroupsdevices isolator currently all the logic for the cgroupsdevices isolator is bundled into the nvidia gpu isolator we should abstract it out into its own component and remove the redundant logic from the nvidia gpu isolator assuming the guaranteed ordering between isolators from mesos5581 we can be sure that the dependency order between the cgroupsdevices and gpunvidia isolators is met,2
improve authorization documentation when setting permissive flag a common problem for a users starting to use acls is that once they set permisse  false and not add acls allowing common operations eg register_framework their mesos cluster dont behave as expected,1
improve error handling when parsing acls during parsing of the authorizer errors are ignored this can lead to undetected security issues consider the following acl with an typo usr instead of user code view_frameworks   principals  type any  usr  type none    code when the master is started with these flags it will interprete the acl int he following way which gives any principal access to any framework noformat view_frameworks  principals  type any   noformat,5
pass networkinfo to cni plugins mesos has adopted the container network interface as a simple means of networking mesos tasks launched by the unified containerizer the cni specification covers a minimum feature set granting the flexibility to add customized networking functionality in the form of agreements made between the orchestrator and cni plugin this proposal is to pass networkinfolabels to the cni plugin by injecting it into the cni network configuration json during plugin invocation design doc on this change httpsdocsgooglecomdocumentd1rxrucccjqpppsqxqrztbhfvnnw6cgq2otieyamwl284edituspsharing reviewboard httpsreviewsapacheorgr48527,3
document mesos health check feature we dont talk about this feature at all,5
improve documentation for using persistent volumes when using persistent volumes at a arangodb we ran into a few pitfalls we should document them in order for others to avoid those issues,2
put initial scaffolding in place for implementing subscribe call on v1 master api as discussed on mesos5498 this ticket is for tracking work to put the initial scaffolding in place for streaming task status update events to a client that has subscribed to the apiv1 operator api endpoint other eventssupport for snapshots would be done as part of mesos5498,5
added a metric indicating if replicated log for the registrar has recovered or not this gives operator insight about the state of the replicated log for registrar the operator needs to know when it is safe to move on to another master in the upgrade orchestration pipeline,3
agent segfaults after request to filesbrowse we observed a number of agent segfaults today on an internal testing cluster here is a log excerpt code jun 16 171228 ip1010087 mesosslave24818 i0616 171228522925 24830 status_update_managercpp392 received status update acknowledgement uuid e79ab0f42fa24df29b5989b97a482167 for task datadogmonitor804b138b33e511e6ac16566ccbdde23e of framework 6d4248cd28324152b5d0defbf36f67590000 jun 16 171228 ip1010087 mesosslave24818 i0616 171228523006 24830 status_update_managercpp824 checkpointing ack for status update task_running uuid e79ab0f42fa24df29b5989b97a482167 for task datadogmonitor804b138b33e511e6ac16566ccbdde23e of framework 6d4248cd28324152b5d0defbf36f67590000 jun 16 171229 ip1010087 mesosslave24818 i0616 171229147181 24824 httpcpp192 http get for slave1state from 101008733356 jun 16 171229 ip1010087 mesosslave24818  aborted at 1466097149 unix time try date d 1466097149 if you are using gnu date  jun 16 171229 ip1010087 mesosslave24818 pc  0x7ff4d68b12a3 unknown jun 16 171229 ip1010087 mesosslave24818  sigsegv 0x0 received by pid 24818 tid 0x7ff4d31ab700 from pid 0 stack trace  jun 16 171229 ip1010087 mesosslave24818  0x7ff4d6431100 unknown jun 16 171229 ip1010087 mesosslave24818  0x7ff4d68b12a3 unknown jun 16 171229 ip1010087 mesosslave24818  0x7ff4d7eced33 processdispatch jun 16 171229 ip1010087 mesosslave24818  0x7ff4d7e7aad7 _znst17_function_handlerifn7process6futureibeerk6optionisseezn5mesos8internal5slave9framework15recoverexecutorerknsa_5state13executorstateeeuls6_e_e9_m_invokeerkst9_any_datas6_ jun 16 171229 ip1010087 mesosslave24818  0x7ff4d7bd1752 mesosinternalfilesprocessauthorize jun 16 171229 ip1010087 mesosslave24818  0x7ff4d7bd1bea mesosinternalfilesprocessbrowse jun 16 171229 ip1010087 mesosslave24818  0x7ff4d7bd6e43 std_function_handler_m_invoke jun 16 171229 ip1010087 mesosslave24818  0x7ff4d85478cb _zzzn7process11processbase5visiterkns_9httpeventeenkulrkns_6futurei6optionins_4http14authentication20authenticationresulteeeee0_clesc_enkulrkns4_ibeee1_clesg_ jun 16 171229 ip1010087 mesosslave24818  0x7ff4d8551341 processprocessmanagerresume jun 16 171229 ip1010087 mesosslave24818  0x7ff4d8551647 _znst6thread5_implist12_bind_simpleifzn7process14processmanager12init_threadseveut_veee6_m_runev jun 16 171229 ip1010087 mesosslave24818  0x7ff4d6909220 unknown jun 16 171229 ip1010087 mesosslave24818  0x7ff4d6429dc5 start_thread jun 16 171229 ip1010087 mesosslave24818  0x7ff4d615728d __clone jun 16 171229 ip1010087 systemd1 dcosmesosslaveservice main process exited codekilled status11segv jun 16 171229 ip1010087 systemd1 unit dcosmesosslaveservice entered failed state jun 16 171229 ip1010087 systemd1 dcosmesosslaveservice failed jun 16 171234 ip1010087 systemd1 dcosmesosslaveservice holdoff time over scheduling restart code in every case the stack trace indicates one of the files endpoints i observed this a number of times coming from browse and twice from read the agent was built from the 100rc1 branch with two cherrypicks applied thishttpsreviewsapacheorgr48563 and thishttpsreviewsapacheorgr48566 which were done to repair a different segfault issuehttpsissuesapacheorgjirabrowsemesos5587 on the master and agent thanks go to bmahler for digging into this a bit and discovering a possible cause herehttpsgithubcomapachemesosblobmastersrcslaveslavecppl5737l5745 where use of defer may be necessary to keep execution in the correct context,3
change build to always enable nvidia gpu support for linux see summary,2
add framework capability for gpu_resources due to the scarce resource problem described in mesos5377 we plan to introduce a gpu_resources framework capability this capability will allow the mesos allocator to make better decisions about which frameworks should receive resources from gpu capable machines in essence the allocator will only allocate resources from gpu capable machines to frameworks that have this capability this is necessary to prevent nongpu workloads from filling up the gpu machines and preventing gpu workloads to run,3
check all omissions of defer for safety when registering callbacks with then onany etc we sometimes omit defer in cases where its deemed safe for example when the callback uses no process state and thus could be executed in an arbitrary context because of recent bugs due to the unsafe omission of defer we should do a sweep of the codebase for all such occurrences and evaluate their safety we should also consider using defer consistently in all such cases as our documentationhttpsgithubcomapachemesostreemaster3rdpartylibprocessdefer recommends,5
add documentation about metadata for cni plugins we need to document the behavior implemented in mesos5592,2
build networkcni isolator with libnl support currently the networkcni isolator does not have the ability to collect network statistics for containers launched on a cni network we need to give the networkcni isolator the ability to query interfaces route tables and statistics in the containers network namespace to achieve this the networkcni isolator will need to talk netlink for enabling netlink api we need the networkcni isolator to be built with libnl support,3
expose a statistics endpoint on the networkcni isolator we need a statistics endpoint in the networkcni isolator to expose metrics relating to a containers network traffic on receiving a request for a given container the networkcni isolator could use netlink system calls to query the kernel for interface and routing statistics for a given containers network namespace,5
build an example framework to consume gpus this framework should show how to build a gpu capable framework that can accept offers with gpus and launch tasks that use them,3
unreserve operation causes master to crash reserve operation may cause a master failure noformat i0619 050202298602 11194 httpcpp312 http get for masterslaves from 172170449617 with useragentpythonrequests291 i0619 050202305542 11193 httpcpp312 http post for masterdestroyvolumes from 172170449618 with useragentpythonrequests291 i0619 050202306731 11191 mastercpp6560 sending checkpointed resources memkafkatestrole kafkatestprincipal resource_id 7408cc53183c48c2a07f7087806219f3256 cpuskafkatestrole kafkatestprincipal resource_id d7888099db8f40189109f70fb1174f5315 memkafkatestrole kafkatestprincipal resource_id b5dd90fc2c1241999fc4cf9f918e332b2304 portskafkatestrole kafkatestprincipal resource_id a0ee4e01803f4b71950d483caeb01a5793059305 1159611596 cpuskafkatestrole kafkatestprincipal resource_id 8cd72abb70894220bb9046b70c9953ab05 diskkafkatestrole kafkatestprincipal resource_id ed06ec6e2d154d0ebbc495a942e5859611204 to slave a80ff9dde04643abb76328365b136f6bs0 at slave1100055051 10005 i0619 050202311069 11189 httpcpp312 http post for masterdestroyvolumes from 172170449619 with useragentpythonrequests291 i0619 050202312191 11187 mastercpp6560 sending checkpointed resources cpuskafkatestrole kafkatestprincipal resource_id f1ff48060c244d60ad2bb06462ee408115 memkafkatestrole kafkatestprincipal resource_id cb8dc92d64f0400785201f63625b98c02304 portskafkatestrole kafkatestprincipal resource_id 225b4172be77453aa94f8845edc3f09a96929692 1182411824 cpuskafkatestrole kafkatestprincipal resource_id 942e102aca63480d98539a39e2695ec905 memkafkatestrole kafkatestprincipal resource_id cad57f8c27f5484ca3fbe80da74f0813256 diskkafkatestrole kafkatestprincipal resource_id e6563e09e2844aaf8d5372056695de4111204 to slave 489aa72fae074383a56f6fe9346ace37s7 at slave1100075051 10007 i0619 050202316118 11189 httpcpp312 http get for masterslaves from 172170449620 with useragentpythonrequests291 i0619 050202321527 11189 httpcpp312 http post for masterunreserve from 172170449621 with useragentpythonrequests291 i0619 050202323523 11193 mastercpp6560 sending checkpointed resources to slave a80ff9dde04643abb76328365b136f6bs0 at slave1100055051 10005 i0619 050202327658 11191 httpcpp312 http post for masterunreserve from 172170449622 with useragentpythonrequests291 f0619 050202329208 11190 sortercpp284 check failed total_scalarquantitiescontainsoldslavequantity noformat possible reasons  recent improvements in allocator b4d746f  bug in bookkeeping during the previous unreserve  network partition that happened after reserve and before unreserve,5
executors should not inherit environment variables from the agent currently executors are inheriting environment variables form the slave in mesos containerizer this is problematic because of two reasons 1 when we use docker images such as mongo in unified containerizer duplicated environment variables inherited from the slave lead to initialization failures because lang andor lc_ environment variables are not set correctly 2 when we are looking at the environment variables from the executor tasks there are pages of environment variables listed which is redundant and dangerous depending on the reasons above we propose that no longer allow executors to inherit environment variables from the slave instead users should specify all environment variables they need by setting the slave flag executor_environment_variables as a json format,3
design doc for task_unreachable see mesos4049,5
containerizertestroot_cgroups_balloonframework fails because executor environment isnt inherited a recent change forbits the executor to inherit environment variables from the agents environment as a regression this break containerizertestroot_cgroups_balloonframework,2
use snake casing for flag names consistently historically we have always used snake casing for the flag variables eg docker_config etc however there are some instances in our cpp code where we define the flag name in the cpp file in camel case eg modulesdir but still have the flag name as modules_dir when taking arguments from the user it would be good to audit all such occurrences and consistently uses snake casing in our cpphpp files everywhere,1
remove hard dependence on libelf for linux we recently added a hard dependency for libelf on linux this was in preparation for some upcoming nvidia gpu support for injecting volumes into containers since this dependence is not actually necessary for the upcoming release we should remove it for now and rethink the best way to add it back in later possibly as a runtime dependence instead of a linktime one,1
invalid resources sent to reserve are silently dropped if an invalid resource is passed to the masters reserve endpoint it will be silently dropped and not cause an error this can lead for example to a reserve request containing a single invalid resource receiving a 200 ok response despite the fact that no resources were reserved as a result of the request this is due to the fact that the  operator for resources silently drops invalid resources and this operator is used when parsing the resources in the http request this could be addressed by validating the resource objects one at a time as they are parsed,1
deprecate camel case proto field in isolator containerconfig currently there are extra executorinfo and taskinfo in isolator contaienrconfig because a deprecation cycle is needed to deprecate camel cased proto field names this jira is used for tracking this issue which should address the todo in isolatorproto,2
cniisolatortestroot_internet_curl_launchcommandtask fails on centos 7 noformat 224154  step 1010  run  cniisolatortestroot_internet_curl_launchcommandtask 224154w step 1010 i0619 224154348641 30896 clustercpp155 creating default local authorizer 224154w step 1010 i0619 224154353384 30896 leveldbcpp174 opened db in 4634552ms 224154w step 1010 i0619 224154354763 30896 leveldbcpp181 compacted db in 1360201ms 224154w step 1010 i0619 224154354784 30896 leveldbcpp196 created db iterator in 3421ns 224154w step 1010 i0619 224154354790 30896 leveldbcpp202 seeked to beginning of db in 633ns 224154w step 1010 i0619 224154354797 30896 leveldbcpp271 iterated through 0 keys in the db in 401ns 224154w step 1010 i0619 224154354811 30896 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 224154w step 1010 i0619 224154354990 30913 recovercpp451 starting replica recovery 224154w step 1010 i0619 224154355123 30915 recovercpp477 replica is in empty status 224154w step 1010 i0619 224154355391 30915 replicacpp673 replica in empty status received a broadcasted recover request from 1869517230210540724 224154w step 1010 i0619 224154355479 30912 recovercpp197 received a recover response from a replica in empty status 224154w step 1010 i0619 224154355581 30914 recovercpp568 updating replica status to starting 224154w step 1010 i0619 224154356091 30910 mastercpp382 master 27c796db6f984d6196c0f583f22787ff ip172302105mesosphereio started on 17230210540724 224154w step 1010 i0619 224154356104 30910 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmpkhgyrqcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpkhgyrqmaster zk_session_timeout10secs 224154w step 1010 i0619 224154356237 30910 mastercpp434 master only allowing authenticated frameworks to register 224154w step 1010 i0619 224154356245 30910 mastercpp448 master only allowing authenticated agents to register 224154w step 1010 i0619 224154356247 30910 mastercpp461 master only allowing authenticated http frameworks to register 224154w step 1010 i0619 224154356251 30910 credentialshpp37 loading credentials for authentication from tmpkhgyrqcredentials 224154w step 1010 i0619 224154356351 30910 mastercpp506 using default crammd5 authenticator 224154w step 1010 i0619 224154356389 30910 mastercpp578 using default basic http authenticator 224154w step 1010 i0619 224154356439 30910 mastercpp658 using default basic http framework authenticator 224154w step 1010 i0619 224154356467 30910 mastercpp705 authorization enabled 224154w step 1010 i0619 224154356531 30913 whitelist_watchercpp77 no whitelist given 224154w step 1010 i0619 224154356549 30912 hierarchicalcpp142 initialized hierarchical allocator process 224154w step 1010 i0619 224154356868 30916 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1232816ms 224154w step 1010 i0619 224154356884 30916 replicacpp320 persisted replica status to starting 224154w step 1010 i0619 224154356945 30916 recovercpp477 replica is in starting status 224154w step 1010 i0619 224154357100 30917 mastercpp1969 the newly elected leader is master17230210540724 with id 27c796db6f984d6196c0f583f22787ff 224154w step 1010 i0619 224154357115 30917 mastercpp1982 elected as the leading master 224154w step 1010 i0619 224154357122 30917 mastercpp1669 recovering from registrar 224154w step 1010 i0619 224154357213 30910 registrarcpp332 recovering registrar 224154w step 1010 i0619 224154357429 30913 replicacpp673 replica in starting status received a broadcasted recover request from 1869817230210540724 224154w step 1010 i0619 224154357549 30914 recovercpp197 received a recover response from a replica in starting status 224154w step 1010 i0619 224154357728 30913 recovercpp568 updating replica status to voting 224154w step 1010 i0619 224154358937 30913 leveldbcpp304 persisting metadata 8 bytes to leveldb took 114792ms 224154w step 1010 i0619 224154358952 30913 replicacpp320 persisted replica status to voting 224154w step 1010 i0619 224154358986 30913 recovercpp582 successfully joined the paxos group 224154w step 1010 i0619 224154359041 30913 recovercpp466 recover process terminated 224154w step 1010 i0619 224154359180 30916 logcpp553 attempting to start the writer 224154w step 1010 i0619 224154359578 30917 replicacpp493 replica received implicit promise request from 1869917230210540724 with proposal 1 224154w step 1010 i0619 224154360752 30917 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1157449ms 224154w step 1010 i0619 224154360767 30917 replicacpp342 persisted promised to 1 224154w step 1010 i0619 224154360982 30914 coordinatorcpp238 coordinator attempting to fill missing positions 224154w step 1010 i0619 224154361426 30910 replicacpp388 replica received explicit promise request from 1870017230210540724 for position 0 with proposal 2 224154w step 1010 i0619 224154362571 30910 leveldbcpp341 persisting action 8 bytes to leveldb took 1124969ms 224154w step 1010 i0619 224154362587 30910 replicacpp712 persisted action at 0 224154w step 1010 i0619 224154362999 30911 replicacpp537 replica received write request for position 0 from 1870117230210540724 224154w step 1010 i0619 224154363030 30911 leveldbcpp436 reading position from leveldb took 14967ns 224154w step 1010 i0619 224154364264 30911 leveldbcpp341 persisting action 14 bytes to leveldb took 1214497ms 224154w step 1010 i0619 224154364279 30911 replicacpp712 persisted action at 0 224154w step 1010 i0619 224154364470 30910 replicacpp691 replica received learned notice for position 0 from 00000 224154w step 1010 i0619 224154365622 30910 leveldbcpp341 persisting action 16 bytes to leveldb took 1131398ms 224154w step 1010 i0619 224154365636 30910 replicacpp712 persisted action at 0 224154w step 1010 i0619 224154365643 30910 replicacpp697 replica learned nop action at position 0 224154w step 1010 i0619 224154365769 30915 logcpp569 writer started with ending position 0 224154w step 1010 i0619 224154366080 30913 leveldbcpp436 reading position from leveldb took 8794ns 224154w step 1010 i0619 224154366284 30915 registrarcpp365 successfully fetched the registry 0b in 9053952ms 224154w step 1010 i0619 224154366315 30915 registrarcpp464 applied 1 operations in 3436ns attempting to update the registry 224154w step 1010 i0619 224154366487 30911 logcpp577 attempting to append 209 bytes to the log 224154w step 1010 i0619 224154366539 30917 coordinatorcpp348 coordinator attempting to write append action at position 1 224154w step 1010 i0619 224154366839 30917 replicacpp537 replica received write request for position 1 from 1870217230210540724 224154w step 1010 i0619 224154367966 30917 leveldbcpp341 persisting action 228 bytes to leveldb took 1106053ms 224154w step 1010 i0619 224154367982 30917 replicacpp712 persisted action at 1 224154w step 1010 i0619 224154368201 30915 replicacpp691 replica received learned notice for position 1 from 00000 224154w step 1010 i0619 224154371786 30915 leveldbcpp341 persisting action 230 bytes to leveldb took 3566076ms 224154w step 1010 i0619 224154371803 30915 replicacpp712 persisted action at 1 224154w step 1010 i0619 224154371809 30915 replicacpp697 replica learned append action at position 1 224154w step 1010 i0619 224154372032 30910 registrarcpp509 successfully updated the registry in 5693952ms 224154w step 1010 i0619 224154372097 30910 registrarcpp395 successfully recovered registrar 224154w step 1010 i0619 224154372107 30911 logcpp596 attempting to truncate the log to 1 224154w step 1010 i0619 224154372151 30910 coordinatorcpp348 coordinator attempting to write truncate action at position 2 224154w step 1010 i0619 224154372218 30911 mastercpp1777 recovered 0 agents from the registry 170b  allowing 10mins for agents to reregister 224154w step 1010 i0619 224154372242 30915 hierarchicalcpp169 skipping recovery of hierarchical allocator nothing to recover 224154w step 1010 i0619 224154372467 30914 replicacpp537 replica received write request for position 2 from 1870317230210540724 224154w step 1010 i0619 224154373693 30914 leveldbcpp341 persisting action 16 bytes to leveldb took 1207676ms 224154w step 1010 i0619 224154373708 30914 replicacpp712 persisted action at 2 224154w step 1010 i0619 224154373920 30913 replicacpp691 replica received learned notice for position 2 from 00000 224154w step 1010 i0619 224154375115 30913 leveldbcpp341 persisting action 18 bytes to leveldb took 117978ms 224154w step 1010 i0619 224154375145 30913 leveldbcpp399 deleting 1 keys from leveldb took 14216ns 224154w step 1010 i0619 224154375154 30913 replicacpp712 persisted action at 2 224154w step 1010 i0619 224154375159 30913 replicacpp697 replica learned truncate action at position 2 224154w step 1010 i0619 224154383839 30896 containerizercpp201 using isolation dockerruntimefilesystemlinuxnetworkcni 224154w step 1010 i0619 224154388789 30896 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 224154w step 1010 e0619 224154393234 30896 shellhpp106 command hadoop version 21 failed this is the output 224154w step 1010 sh hadoop command not found 224154w step 1010 i0619 224154393265 30896 fetchercpp62 skipping uri fetcher plugin hadoop as it could not be created failed to create hdfs client failed to execute hadoop version 21 the command was either not found or exited with a nonzero exit status 127 224154w step 1010 i0619 224154393316 30896 registry_pullercpp111 creating registry puller with docker registry httpsregistry1dockerio 224154w step 1010 i0619 224154395668 30896 clustercpp432 creating default local authorizer 224154w step 1010 i0619 224154396100 30914 slavecpp203 agent started on 46917230210540724 224154w step 1010 i0619 224154396116 30914 slavecpp204 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xicredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpkhgyrqstore docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xifetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xihttp_credentials image_providersdocker image_provisioner_backendcopy initialize_driver_loggingtrue isolationdockerruntimefilesystemlinuxnetworkcni launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo network_cni_config_dirtmpkhgyrqconfigs network_cni_plugins_dirtmpkhgyrqplugins oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xi 224154w step 1010 i0619 224154396380 30914 credentialshpp86 loading credential for authentication from mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xicredential 224154w step 1010 i0619 224154396495 30914 slavecpp341 agent using credential for testprincipal 224154w step 1010 i0619 224154396509 30914 credentialshpp37 loading credentials for authentication from mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6xihttp_credentials 224154w step 1010 i0619 224154396586 30914 slavecpp393 using default basic http authenticator 224154w step 1010 i0619 224154396698 30914 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 224154w step 1010 trying semicolondelimited string format instead 224154w step 1010 i0619 224154396780 30896 schedcpp224 version 100 224154w step 1010 i0619 224154396991 30914 slavecpp592 agent resources cpus2 mem1024 disk1024 ports3100032000 224154w step 1010 i0619 224154397020 30914 slavecpp600 agent attributes   224154w step 1010 i0619 224154397029 30914 slavecpp605 agent hostname ip172302105mesosphereio 224154w step 1010 i0619 224154397040 30916 schedcpp328 new master detected at master17230210540724 224154w step 1010 i0619 224154397068 30916 schedcpp394 authenticating with master master17230210540724 224154w step 1010 i0619 224154397078 30916 schedcpp401 using default crammd5 authenticatee 224154w step 1010 i0619 224154397188 30916 authenticateecpp121 creating new client sasl connection 224154w step 1010 i0619 224154397467 30914 statecpp57 recovering state from mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_gcx6ximeta 224154w step 1010 i0619 224154397476 30912 mastercpp5943 authenticating scheduleraf10d6a31ebc4377b44d8c0dfbffcb8e17230210540724 224154w step 1010 i0619 224154397544 30913 authenticatorcpp414 starting authentication session for crammd5_authenticatee95317230210540724 224154w step 1010 i0619 224154397614 30915 status_update_managercpp200 recovering status update manager 224154w step 1010 i0619 224154397668 30912 authenticatorcpp98 creating new server sasl connection 224154w step 1010 i0619 224154397709 30915 containerizercpp514 recovering containerizer 224154w step 1010 i0619 224154397869 30912 authenticateecpp213 received sasl authentication mechanisms crammd5 224154w step 1010 i0619 224154397886 30912 authenticateecpp239 attempting to authenticate with mechanism crammd5 224154w step 1010 i0619 224154397927 30912 authenticatorcpp204 received sasl authentication start 224154w step 1010 i0619 224154397964 30912 authenticatorcpp326 authentication requires more steps 224154w step 1010 i0619 224154398000 30912 authenticateecpp259 received sasl authentication step 224154w step 1010 i0619 224154398052 30912 authenticatorcpp232 received sasl authentication step 224154w step 1010 i0619 224154398066 30912 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302105mesosphereio server fqdn ip172302105mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false 224154w step 1010 i0619 224154398073 30912 auxpropcpp179 looking up auxiliary property userpassword 224154w step 1010 i0619 224154398087 30912 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 224154w step 1010 i0619 224154398098 30912 auxpropcpp107 request to lookup properties for user testprincipal realm ip172302105mesosphereio server fqdn ip172302105mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true 224154w step 1010 i0619 224154398103 30912 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 224154w step 1010 i0619 224154398108 30912 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true 224154w step 1010 i0619 224154398116 30912 authenticatorcpp318 authentication success 224154w step 1010 i0619 224154398162 30914 authenticateecpp299 authentication success 224154w step 1010 i0619 224154398181 30913 authenticatorcpp432 authentication session cleanup for crammd5_authenticatee95317230210540724 224154w step 1010 i0619 224154398200 30912 mastercpp5973 successfully authenticated principal testprincipal at scheduleraf10d6a31ebc4377b44d8c0dfbffcb8e17230210540724 224154w step 1010 i0619 224154398270 30914 schedcpp484 successfully authenticated with master master17230210540724 224154w step 1010 i0619 224154398280 30914 schedcpp800 sending subscribe call to m,2
add cgroup namespace to linux ns helper since linux kernel 46 cgroup namespace is added we need to support the handle for the cgroup namespace of the process this also relates to two test failures on ubuntu 16 noformat 224126  step 1010  run  nstestroot_setns 224126  step 1010 srctestscontainerizerns_testscpp75 failure 224126  step 1010 nstype unknown namespace cgroup 224126  step 1010  failed  nstestroot_setns 1 ms noformat noformat 224126  step 1010  run  nstestroot_getns 224126  step 1010 srctestscontainerizerns_testscpp160 failure 224126  step 1010 nstype unknown namespace cgroup 224126  step 1010  failed  nstestroot_getns 0 ms noformat,3
cni isolator should not return failure if etchostname does not exist on host etchostname may not necessarily exist on every system eg centos 6 currently cni isolator just return a failure if it does not exist on host because the isolator need to mount it into the container this is fine for etchost and etcresolvconf but we should make an exception for etchostname because hostname may still be accessible even if etchostname doesnt exist this issue relates to 3 failure tests noformat 224521  step 1010  run  cniisolatortestroot_internet_curl_launchcommandtask 224521w step 1010 i0619 224521647611 24647 clustercpp155 creating default local authorizer 224521w step 1010 i0619 224521655230 24647 leveldbcpp174 opened db in 7510408ms 224521w step 1010 i0619 224521657680 24647 leveldbcpp181 compacted db in 2427309ms 224521w step 1010 i0619 224521657702 24647 leveldbcpp196 created db iterator in 6209ns 224521w step 1010 i0619 224521657709 24647 leveldbcpp202 seeked to beginning of db in 692ns 224521w step 1010 i0619 224521657713 24647 leveldbcpp271 iterated through 0 keys in the db in 431ns 224521w step 1010 i0619 224521657727 24647 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 224521w step 1010 i0619 224521657888 24662 recovercpp451 starting replica recovery 224521w step 1010 i0619 224521658051 24668 recovercpp477 replica is in empty status 224521w step 1010 i0619 224521658495 24664 replicacpp673 replica in empty status received a broadcasted recover request from 1840117230224742024 224521w step 1010 i0619 224521658583 24662 recovercpp197 received a recover response from a replica in empty status 224521w step 1010 i0619 224521658687 24664 recovercpp568 updating replica status to starting 224521w step 1010 i0619 224521659111 24664 mastercpp382 master 9a4a353b91c543b98c3719245c37758c ip172302247mesosphereio started on 17230224742024 224521w step 1010 i0619 224521659126 24664 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmpl8346zcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpl8346zmaster zk_session_timeout10secs 224521w step 1010 i0619 224521659267 24664 mastercpp434 master only allowing authenticated frameworks to register 224521w step 1010 i0619 224521659276 24664 mastercpp448 master only allowing authenticated agents to register 224521w step 1010 i0619 224521659278 24664 mastercpp461 master only allowing authenticated http frameworks to register 224521w step 1010 i0619 224521659282 24664 credentialshpp37 loading credentials for authentication from tmpl8346zcredentials 224521w step 1010 i0619 224521659375 24664 mastercpp506 using default crammd5 authenticator 224521w step 1010 i0619 224521659415 24664 mastercpp578 using default basic http authenticator 224521w step 1010 i0619 224521659495 24664 mastercpp658 using default basic http framework authenticator 224521w step 1010 i0619 224521659569 24664 mastercpp705 authorization enabled 224521w step 1010 i0619 224521659684 24666 hierarchicalcpp142 initialized hierarchical allocator process 224521w step 1010 i0619 224521659696 24665 whitelist_watchercpp77 no whitelist given 224521w step 1010 i0619 224521660269 24666 mastercpp1969 the newly elected leader is master17230224742024 with id 9a4a353b91c543b98c3719245c37758c 224521w step 1010 i0619 224521660281 24666 mastercpp1982 elected as the leading master 224521w step 1010 i0619 224521660290 24666 mastercpp1669 recovering from registrar 224521w step 1010 i0619 224521660342 24662 registrarcpp332 recovering registrar 224521w step 1010 i0619 224521661232 24669 leveldbcpp304 persisting metadata 8 bytes to leveldb took 248585ms 224521w step 1010 i0619 224521661254 24669 replicacpp320 persisted replica status to starting 224521w step 1010 i0619 224521661326 24669 recovercpp477 replica is in starting status 224521w step 1010 i0619 224521661667 24668 replicacpp673 replica in starting status received a broadcasted recover request from 1840417230224742024 224521w step 1010 i0619 224521661758 24665 recovercpp197 received a recover response from a replica in starting status 224521w step 1010 i0619 224521661893 24664 recovercpp568 updating replica status to voting 224521w step 1010 i0619 224521663851 24664 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1915617ms 224521w step 1010 i0619 224521663866 24664 replicacpp320 persisted replica status to voting 224521w step 1010 i0619 224521663899 24664 recovercpp582 successfully joined the paxos group 224521w step 1010 i0619 224521663944 24664 recovercpp466 recover process terminated 224521w step 1010 i0619 224521664088 24668 logcpp553 attempting to start the writer 224521w step 1010 i0619 224521664556 24668 replicacpp493 replica received implicit promise request from 1840517230224742024 with proposal 1 224521w step 1010 i0619 224521666551 24668 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1971938ms 224521w step 1010 i0619 224521666566 24668 replicacpp342 persisted promised to 1 224521w step 1010 i0619 224521666767 24667 coordinatorcpp238 coordinator attempting to fill missing positions 224521w step 1010 i0619 224521667230 24668 replicacpp388 replica received explicit promise request from 1840617230224742024 for position 0 with proposal 2 224521w step 1010 i0619 224521669271 24668 leveldbcpp341 persisting action 8 bytes to leveldb took 202399ms 224521w step 1010 i0619 224521669287 24668 replicacpp712 persisted action at 0 224521w step 1010 i0619 224521669656 24669 replicacpp537 replica received write request for position 0 from 1840717230224742024 224521w step 1010 i0619 224521669680 24669 leveldbcpp436 reading position from leveldb took 10808ns 224521w step 1010 i0619 224521671674 24669 leveldbcpp341 persisting action 14 bytes to leveldb took 1977316ms 224521w step 1010 i0619 224521671689 24669 replicacpp712 persisted action at 0 224521w step 1010 i0619 224521671907 24665 replicacpp691 replica received learned notice for position 0 from 00000 224521w step 1010 i0619 224521673920 24665 leveldbcpp341 persisting action 16 bytes to leveldb took 1991274ms 224521w step 1010 i0619 224521673935 24665 replicacpp712 persisted action at 0 224521w step 1010 i0619 224521673941 24665 replicacpp697 replica learned nop action at position 0 224521w step 1010 i0619 224521674190 24665 logcpp569 writer started with ending position 0 224521w step 1010 i0619 224521674489 24663 leveldbcpp436 reading position from leveldb took 9059ns 224521w step 1010 i0619 224521674718 24663 registrarcpp365 successfully fetched the registry 0b in 14355968ms 224521w step 1010 i0619 224521674747 24663 registrarcpp464 applied 1 operations in 3070ns attempting to update the registry 224521w step 1010 i0619 224521674935 24665 logcpp577 attempting to append 209 bytes to the log 224521w step 1010 i0619 224521674978 24665 coordinatorcpp348 coordinator attempting to write append action at position 1 224521w step 1010 i0619 224521675242 24666 replicacpp537 replica received write request for position 1 from 1840817230224742024 224521w step 1010 i0619 224521677088 24666 leveldbcpp341 persisting action 228 bytes to leveldb took 1823904ms 224521w step 1010 i0619 224521677103 24666 replicacpp712 persisted action at 1 224521w step 1010 i0619 224521677299 24667 replicacpp691 replica received learned notice for position 1 from 00000 224521w step 1010 i0619 224521679270 24667 leveldbcpp341 persisting action 230 bytes to leveldb took 1952303ms 224521w step 1010 i0619 224521679286 24667 replicacpp712 persisted action at 1 224521w step 1010 i0619 224521679291 24667 replicacpp697 replica learned append action at position 1 224521w step 1010 i0619 224521679481 24663 registrarcpp509 successfully updated the registry in 4715264ms 224521w step 1010 i0619 224521679503 24666 logcpp596 attempting to truncate the log to 1 224521w step 1010 i0619 224521679560 24667 coordinatorcpp348 coordinator attempting to write truncate action at position 2 224521w step 1010 i0619 224521679581 24663 registrarcpp395 successfully recovered registrar 224521w step 1010 i0619 224521679745 24664 mastercpp1777 recovered 0 agents from the registry 170b  allowing 10mins for agents to reregister 224521w step 1010 i0619 224521679774 24662 hierarchicalcpp169 skipping recovery of hierarchical allocator nothing to recover 224521w step 1010 i0619 224521679986 24662 replicacpp537 replica received write request for position 2 from 1840917230224742024 224521w step 1010 i0619 224521681895 24662 leveldbcpp341 persisting action 16 bytes to leveldb took 1891877ms 224521w step 1010 i0619 224521681910 24662 replicacpp712 persisted action at 2 224521w step 1010 i0619 224521682160 24666 replicacpp691 replica received learned notice for position 2 from 00000 224521w step 1010 i0619 224521684331 24666 leveldbcpp341 persisting action 18 bytes to leveldb took 2153217ms 224521w step 1010 i0619 224521684375 24666 leveldbcpp399 deleting 1 keys from leveldb took 26973ns 224521w step 1010 i0619 224521684383 24666 replicacpp712 persisted action at 2 224521w step 1010 i0619 224521684389 24666 replicacpp697 replica learned truncate action at position 2 224521w step 1010 i0619 224521691529 24647 containerizercpp201 using isolation dockerruntimefilesystemlinuxnetworkcni 224521w step 1010 i0619 224521694491 24647 linux_launchercpp101 using cgroupfreezer as the freezer hierarchy for the linux launcher 224521w step 1010 e0619 224521699741 24647 shellhpp106 command hadoop version 21 failed this is the output 224521w step 1010 sh hadoop command not found 224521w step 1010 i0619 224521699769 24647 fetchercpp62 skipping uri fetcher plugin hadoop as it could not be created failed to create hdfs client failed to execute hadoop version 21 the command was either not found or exited with a nonzero exit status 127 224521w step 1010 i0619 224521699823 24647 registry_pullercpp111 creating registry puller with docker registry httpsregistry1dockerio 224521w step 1010 i0619 224521700865 24647 linuxcpp146 bind mounting mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpg and making it a shared mount 224521w step 1010 i0619 224521707801 24647 cnicpp286 bind mounting varrunmesosisolatorsnetworkcni and making it a shared mount 224521w step 1010 i0619 224521714337 24647 clustercpp432 creating default local authorizer 224521w step 1010 i0619 224521714825 24668 slavecpp203 agent started on 46817230224742024 224521w step 1010 i0619 224521714839 24668 slavecpp204 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpgcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpl8346zstore docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpgfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpghttp_credentials image_providersdocker image_provisioner_backendcopy initialize_driver_loggingtrue isolationdockerruntimefilesystemlinuxnetworkcni launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo network_cni_config_dirtmpl8346zconfigs network_cni_plugins_dirtmpl8346zplugins oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirmntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpg 224521w step 1010 i0619 224521715116 24668 credentialshpp86 loading credential for authentication from mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpgcredential 224521w step 1010 i0619 224521715195 24668 slavecpp341 agent using credential for testprincipal 224521w step 1010 i0619 224521715214 24668 credentialshpp37 loading credentials for authentication from mntteamcitytempbuildtmpcniisolatortest_root_internet_curl_launchcommandtask_cvawpghttp_credentials 224521w step 1010 i0619 224521715296 24668 slavecpp393 using default basic http authenticator 224521w step 1010 i0619 224521715400 24668 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 noformat noformat 224538  step 1010  run  cniisolatortestroot_verifycheckpointedinfo 224538w step 1010 i0619 224538459836 24647 clustercpp155 creating default local authorizer 224538w step 1010 i0619 224538470319 24647 leveldbcpp174 opened db in 1034226ms 224538w step 1010 i0619 224538472771 24647 leveldbcpp181 compacted db in 2403554ms 224538w step 1010 i0619 224538472795 24647 leveldbcpp196 created db iterator in 4446ns 224538w step 1010 i0619 224538472801 24647 leveldbcpp202 seeked to beginning of db in 810ns 224538w step 1010 i0619 224538472806 24647 leveldbcpp271 iterated through 0 keys in the db in 393ns 224538w step 1010 i0619 224538472822 24647 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 224538w step 1010 i0619 224538473093 24665 recovercpp451 starting replica recovery 224538w step 1010 i0619 224538473260 24663 recovercpp477 replica is in empty status 224538w step 1010 i0619 224538473647 24663 replicacpp673 replica in empty status received a broadcasted recover request from 1846417230224742024 224538w step 1010 i0619 224538473752 24665 recovercpp197 received a recover response from a replica in empty status 224538w step 1010 i0619 224538473896 24667 recovercpp568 updating replica status to starting 224538w step 1010 i0619 224538474319 24663 mastercpp382 master 64f1f7ace8104fb1b5496e29fc62622b ip172302247mesosphereio started on 17230224742024 224538w step 1010 i0619 224538474329 24663 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmpqjwqsycredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpqjwqsymaster zk_session_timeout10secs 224538w step 1010 i0619 224538474452 24663 mastercpp434 master only allowing authenticated frameworks to register 224538w step 1010 i0619 224538474457 24663 mastercpp448 master only allowing authenticated agents to register 224538w step 1010 i0619 224538474459 24663 mastercpp461 master only allowing authenticated http frameworks to register 224538w step 1010 i0619 224538474463 24663 credentialshpp37 loading credentials for authentication from tmpqjwqsycredentials 224538w step 1010 i0619 224538474551 24663 mastercpp506 using default crammd5 authenticator 224538w step 1010 i0619 224538474598 24663 mastercpp578 using default basic http authenticator 224538w step 1010 i0619 224538474643 24663 mastercpp658 using default basic http framework authenticator 224538w step 1010 i0619 224538474674 2466,3
memorypressuremesostestcgroups_root_slaverecovery is flaky noformat 033629  step 1010  run  memorypressuremesostestcgroups_root_slaverecovery 033629w step 1010 i0618 033629461802 2797 clustercpp155 creating default local authorizer 033629w step 1010 i0618 033629469468 2797 leveldbcpp174 opened db in 7527163ms 033629w step 1010 i0618 033629470188 2797 leveldbcpp181 compacted db in 699544ns 033629w step 1010 i0618 033629470206 2797 leveldbcpp196 created db iterator in 4293ns 033629w step 1010 i0618 033629470211 2797 leveldbcpp202 seeked to beginning of db in 535ns 033629w step 1010 i0618 033629470216 2797 leveldbcpp271 iterated through 0 keys in the db in 321ns 033629w step 1010 i0618 033629470230 2797 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 033629w step 1010 i0618 033629470510 2815 recovercpp451 starting replica recovery 033629w step 1010 i0618 033629470592 2817 recovercpp477 replica is in empty status 033629w step 1010 i0618 033629471029 2813 replicacpp673 replica in empty status received a broadcasted recover request from 198001723022937328 033629w step 1010 i0618 033629471139 2816 recovercpp197 received a recover response from a replica in empty status 033629w step 1010 i0618 033629471271 2818 recovercpp568 updating replica status to starting 033629w step 1010 i0618 033629471606 2811 mastercpp382 master 6d44b7c1ac0b440997dfa53fa2e39d09 ip17230229mesosphereio started on 1723022937328 033629w step 1010 i0618 033629471619 2811 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmpbaxwq5credentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpbaxwq5master zk_session_timeout10secs 033629w step 1010 i0618 033629471745 2811 mastercpp434 master only allowing authenticated frameworks to register 033629w step 1010 i0618 033629471753 2811 mastercpp448 master only allowing authenticated agents to register 033629w step 1010 i0618 033629471757 2811 mastercpp461 master only allowing authenticated http frameworks to register 033629w step 1010 i0618 033629471761 2811 credentialshpp37 loading credentials for authentication from tmpbaxwq5credentials 033629w step 1010 i0618 033629471829 2811 mastercpp506 using default crammd5 authenticator 033629w step 1010 i0618 033629471868 2811 mastercpp578 using default basic http authenticator 033629w step 1010 i0618 033629471941 2811 mastercpp658 using default basic http framework authenticator 033629w step 1010 i0618 033629471977 2811 mastercpp705 authorization enabled 033629w step 1010 i0618 033629472034 2817 hierarchicalcpp142 initialized hierarchical allocator process 033629w step 1010 i0618 033629472038 2814 whitelist_watchercpp77 no whitelist given 033629w step 1010 i0618 033629472506 2811 mastercpp1969 the newly elected leader is master1723022937328 with id 6d44b7c1ac0b440997dfa53fa2e39d09 033629w step 1010 i0618 033629472522 2811 mastercpp1982 elected as the leading master 033629w step 1010 i0618 033629472527 2811 mastercpp1669 recovering from registrar 033629w step 1010 i0618 033629472573 2812 registrarcpp332 recovering registrar 033629w step 1010 i0618 033629473511 2816 leveldbcpp304 persisting metadata 8 bytes to leveldb took 2195002ms 033629w step 1010 i0618 033629473527 2816 replicacpp320 persisted replica status to starting 033629w step 1010 i0618 033629473578 2816 recovercpp477 replica is in starting status 033629w step 1010 i0618 033629473877 2815 replicacpp673 replica in starting status received a broadcasted recover request from 198031723022937328 033629w step 1010 i0618 033629473989 2814 recovercpp197 received a recover response from a replica in starting status 033629w step 1010 i0618 033629474126 2817 recovercpp568 updating replica status to voting 033629w step 1010 i0618 033629474735 2811 leveldbcpp304 persisting metadata 8 bytes to leveldb took 547332ns 033629w step 1010 i0618 033629474748 2811 replicacpp320 persisted replica status to voting 033629w step 1010 i0618 033629474783 2811 recovercpp582 successfully joined the paxos group 033629w step 1010 i0618 033629474829 2811 recovercpp466 recover process terminated 033629w step 1010 i0618 033629474969 2818 logcpp553 attempting to start the writer 033629w step 1010 i0618 033629475361 2811 replicacpp493 replica received implicit promise request from 198041723022937328 with proposal 1 033629w step 1010 i0618 033629475944 2811 leveldbcpp304 persisting metadata 8 bytes to leveldb took 559444ns 033629w step 1010 i0618 033629475956 2811 replicacpp342 persisted promised to 1 033629w step 1010 i0618 033629476215 2815 coordinatorcpp238 coordinator attempting to fill missing positions 033629w step 1010 i0618 033629476660 2816 replicacpp388 replica received explicit promise request from 198051723022937328 for position 0 with proposal 2 033629w step 1010 i0618 033629477262 2816 leveldbcpp341 persisting action 8 bytes to leveldb took 584333ns 033629w step 1010 i0618 033629477273 2816 replicacpp712 persisted action at 0 033629w step 1010 i0618 033629477699 2815 replicacpp537 replica received write request for position 0 from 198061723022937328 033629w step 1010 i0618 033629477726 2815 leveldbcpp436 reading position from leveldb took 8842ns 033629w step 1010 i0618 033629478277 2815 leveldbcpp341 persisting action 14 bytes to leveldb took 537361ns 033629w step 1010 i0618 033629478291 2815 replicacpp712 persisted action at 0 033629w step 1010 i0618 033629478569 2811 replicacpp691 replica received learned notice for position 0 from 00000 033629w step 1010 i0618 033629479132 2811 leveldbcpp341 persisting action 16 bytes to leveldb took 545208ns 033629w step 1010 i0618 033629479146 2811 replicacpp712 persisted action at 0 033629w step 1010 i0618 033629479152 2811 replicacpp697 replica learned nop action at position 0 033629w step 1010 i0618 033629479317 2814 logcpp569 writer started with ending position 0 033629w step 1010 i0618 033629479568 2811 leveldbcpp436 reading position from leveldb took 8325ns 033629w step 1010 i0618 033629479786 2814 registrarcpp365 successfully fetched the registry 0b in 7192064ms 033629w step 1010 i0618 033629479822 2814 registrarcpp464 applied 1 operations in 3018ns attempting to update the registry 033629w step 1010 i0618 033629479995 2818 logcpp577 attempting to append 205 bytes to the log 033629w step 1010 i0618 033629480044 2818 coordinatorcpp348 coordinator attempting to write append action at position 1 033629w step 1010 i0618 033629480309 2811 replicacpp537 replica received write request for position 1 from 198071723022937328 033629w step 1010 i0618 033629480928 2811 leveldbcpp341 persisting action 224 bytes to leveldb took 596433ns 033629w step 1010 i0618 033629480942 2811 replicacpp712 persisted action at 1 033629w step 1010 i0618 033629481148 2815 replicacpp691 replica received learned notice for position 1 from 00000 033629w step 1010 i0618 033629481710 2815 leveldbcpp341 persisting action 226 bytes to leveldb took 545656ns 033629w step 1010 i0618 033629481722 2815 replicacpp712 persisted action at 1 033629w step 1010 i0618 033629481727 2815 replicacpp697 replica learned append action at position 1 033629w step 1010 i0618 033629481958 2816 registrarcpp509 successfully updated the registry in 2119168ms 033629w step 1010 i0618 033629482014 2816 registrarcpp395 successfully recovered registrar 033629w step 1010 i0618 033629482045 2817 logcpp596 attempting to truncate the log to 1 033629w step 1010 i0618 033629482117 2817 coordinatorcpp348 coordinator attempting to write truncate action at position 2 033629w step 1010 i0618 033629482166 2816 mastercpp1777 recovered 0 agents from the registry 166b  allowing 10mins for agents to reregister 033629w step 1010 i0618 033629482177 2817 hierarchicalcpp169 skipping recovery of hierarchical allocator nothing to recover 033629w step 1010 i0618 033629482404 2817 replicacpp537 replica received write request for position 2 from 198081723022937328 033629w step 1010 i0618 033629482975 2817 leveldbcpp341 persisting action 16 bytes to leveldb took 552763ns 033629w step 1010 i0618 033629482986 2817 replicacpp712 persisted action at 2 033629w step 1010 i0618 033629483301 2813 replicacpp691 replica received learned notice for position 2 from 00000 033629w step 1010 i0618 033629483870 2813 leveldbcpp341 persisting action 18 bytes to leveldb took 547529ns 033629w step 1010 i0618 033629483896 2813 leveldbcpp399 deleting 1 keys from leveldb took 12161ns 033629w step 1010 i0618 033629483904 2813 replicacpp712 persisted action at 2 033629w step 1010 i0618 033629483911 2813 replicacpp697 replica learned truncate action at position 2 033629w step 1010 i0618 033629492995 2797 containerizercpp201 using isolation cgroupsmemfilesystemposixnetworkcni 033629w step 1010 i0618 033629496548 2797 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 033629w step 1010 i0618 033629503572 2797 clustercpp432 creating default local authorizer 033629w step 1010 i0618 033629503936 2817 slavecpp203 agent started on 4881723022937328 033629w step 1010 i0618 033629503952 2817 slavecpp204 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos_test_ecfecccd67144ec7b5eba3071b772617 container_disk_watch_interval15secs containerizersmesos credentialmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlhttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationcgroupsmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwl 033629w step 1010 i0618 033629504148 2817 credentialshpp86 loading credential for authentication from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlcredential 033629w step 1010 i0618 033629504189 2817 slavecpp341 agent using credential for testprincipal 033629w step 1010 i0618 033629504199 2817 credentialshpp37 loading credentials for authentication from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlhttp_credentials 033629w step 1010 i0618 033629504245 2817 slavecpp393 using default basic http authenticator 033629w step 1010 i0618 033629504410 2797 schedcpp224 version 100 033629w step 1010 i0618 033629504416 2817 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 033629w step 1010 trying semicolondelimited string format instead 033629w step 1010 i0618 033629504580 2818 schedcpp328 new master detected at master1723022937328 033629w step 1010 i0618 033629504613 2818 schedcpp394 authenticating with master master1723022937328 033629w step 1010 i0618 033629504622 2818 schedcpp401 using default crammd5 authenticatee 033629w step 1010 i0618 033629504649 2817 slavecpp592 agent resources cpus2 mem1024 disk1024 ports3100032000 033629w step 1010 i0618 033629504673 2817 slavecpp600 agent attributes   033629w step 1010 i0618 033629504678 2817 slavecpp605 agent hostname ip17230229mesosphereio 033629w step 1010 i0618 033629504703 2816 authenticateecpp121 creating new client sasl connection 033629w step 1010 i0618 033629504830 2818 mastercpp5943 authenticating scheduler3e992438052b45f0af6a8510911457391723022937328 033629w step 1010 i0618 033629504887 2816 authenticatorcpp414 starting authentication session for crammd5_authenticatee9911723022937328 033629w step 1010 i0618 033629504982 2811 authenticatorcpp98 creating new server sasl connection 033629w step 1010 i0618 033629505004 2816 statecpp57 recovering state from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_slaverecovery_mbzwwlmeta 033629w step 1010 i0618 033629505105 2813 authenticateecpp213 received sasl authentication mechanisms crammd5 033629w step 1010 i0618 033629505131 2813 authenticateecpp239 attempting to authenticate with mechanism crammd5 033629w step 1010 i0618 033629505138 2818 status_update_managercpp200 recovering status update manager 033629w step 1010 i0618 033629505167 2813 authenticatorcpp204 received sasl authentication start 033629w step 1010 i0618 033629505200 2813 authenticatorcpp326 authentication requires more steps 033629w step 1010 i0618 033629505200 2814 containerizercpp514 recovering containerizer 033629w step 1010 i0618 033629505241 2813 authenticateecpp259 received sasl authentication step 033629w step 1010 i0618 033629505300 2812 authenticatorcpp232 received sasl authentication step 033629w step 1010 i0618 033629505317 2812 auxpropcpp107 request to lookup properties for user testprincipal realm ip17230229mesosphereio server fqdn ip17230229mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false 033629w step 1010 i0618 033629505323 2812 auxpropcpp179 looking up auxiliary property userpassword 033629w step 1010 i0618 033629505331 2812 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 033629w step 1010 i0618 033629505337 2812 auxpropcpp107 request to lookup properties for user testprincipal realm ip17230229mesosphereio server fqdn ip17230229mesosphereio sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true 033629w step 1010 i0618 033629505342 2812 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 033629w step 1010 i0618 033629505347 2812 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true 033629w step 1010 i0618 033629505355 2812 authenticatorcpp318 authentication success 033629w step 1010 i0618 033629505399 2813 authenticateecpp299 authentication success 033629w step 1010 i0618 033629505421 2811 authenticatorcpp432 authentication session cleanup for crammd5_authenticatee9911723022937328 033629w step 1010 i0618 033629505436 2812 mastercpp5973 successfully authenticated principal testprincipal at scheduler3e992438052b45f0af6a8510911457391723022937328 033629w step 1010 i0618 033629505534 2816 schedcpp484 successfully authenticated with master master1723022937328 033629w step 1010 i0618 033629505553 2816 schedcpp800 sending subscribe call to master1723022937328 033629w step 1010 i0618 033629505591 2816 schedcpp833 will retry registration in 11319315ms if necessary 033629w step 1010 i0618 033629505672 2815 mastercpp2539 received subscribe call for framework default at scheduler3e992438052b45f0af6a8510911457391723022937328 033629w step 1010 i0618 033629505702 2815 mastercpp2008 authorizing framework principal testprincipal to receive offers for role  033629w step 1010 i0618 033629505854 2818 mastercpp2615 subscribing framework default with checkpointing enabled and capabilities   033629w step 1010 i0618 033629506031 2818 schedcpp723 framework registered with 6d44b7c1ac0b440997dfa53fa2e39d090000 033629w,2
memorypressuremesostestcgroups_root_statistics is flaky noformat 004829  step 1010  run  memorypressuremesostestcgroups_root_statistics 004829w step 1010 10 records in 004829w step 1010 10 records out 004829w step 1010 1048576 bytes 10 mb copied 0000517638 s 20 gbs 004830w step 1010 i0617 004830000998 25413 clustercpp155 creating default local authorizer 004830w step 1010 i0617 004830020459 25413 leveldbcpp174 opened db in 19338463ms 004830w step 1010 i0617 004830022897 25413 leveldbcpp181 compacted db in 2416906ms 004830w step 1010 i0617 004830022919 25413 leveldbcpp196 created db iterator in 4037ns 004830w step 1010 i0617 004830022927 25413 leveldbcpp202 seeked to beginning of db in 769ns 004830w step 1010 i0617 004830022932 25413 leveldbcpp271 iterated through 0 keys in the db in 390ns 004830w step 1010 i0617 004830022944 25413 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned 004830w step 1010 i0617 004830023272 25432 recovercpp451 starting replica recovery 004830w step 1010 i0617 004830023425 25434 recovercpp477 replica is in empty status 004830w step 1010 i0617 004830023748 25434 replicacpp673 replica in empty status received a broadcasted recover request from 193611723025653790 004830w step 1010 i0617 004830023849 25429 recovercpp197 received a recover response from a replica in empty status 004830w step 1010 i0617 004830024019 25435 recovercpp568 updating replica status to starting 004830w step 1010 i0617 004830024338 25432 mastercpp382 master 0e92ffa44f264cea84d39c67612de1bd ip17230256mesosphereio started on 1723025653790 004830w step 1010 i0617 004830024348 25432 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmpjbjy5pcredentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirtmpjbjy5pmaster zk_session_timeout10secs 004830w step 1010 i0617 004830024502 25432 mastercpp434 master only allowing authenticated frameworks to register 004830w step 1010 i0617 004830024508 25432 mastercpp448 master only allowing authenticated agents to register 004830w step 1010 i0617 004830024513 25432 mastercpp461 master only allowing authenticated http frameworks to register 004830w step 1010 i0617 004830024516 25432 credentialshpp37 loading credentials for authentication from tmpjbjy5pcredentials 004830w step 1010 i0617 004830024603 25432 mastercpp506 using default crammd5 authenticator 004830w step 1010 i0617 004830024644 25432 mastercpp578 using default basic http authenticator 004830w step 1010 i0617 004830024701 25432 mastercpp658 using default basic http framework authenticator 004830w step 1010 i0617 004830024770 25432 mastercpp705 authorization enabled 004830w step 1010 i0617 004830024883 25435 whitelist_watchercpp77 no whitelist given 004830w step 1010 i0617 004830024885 25434 hierarchicalcpp142 initialized hierarchical allocator process 004830w step 1010 i0617 004830025539 25433 mastercpp1969 the newly elected leader is master1723025653790 with id 0e92ffa44f264cea84d39c67612de1bd 004830w step 1010 i0617 004830025555 25433 mastercpp1982 elected as the leading master 004830w step 1010 i0617 004830025560 25433 mastercpp1669 recovering from registrar 004830w step 1010 i0617 004830025611 25432 registrarcpp332 recovering registrar 004830w step 1010 i0617 004830026397 25431 leveldbcpp304 persisting metadata 8 bytes to leveldb took 2288187ms 004830w step 1010 i0617 004830026438 25431 replicacpp320 persisted replica status to starting 004830w step 1010 i0617 004830026486 25431 recovercpp477 replica is in starting status 004830w step 1010 i0617 004830026793 25432 replicacpp673 replica in starting status received a broadcasted recover request from 193641723025653790 004830w step 1010 i0617 004830026897 25429 recovercpp197 received a recover response from a replica in starting status 004830w step 1010 i0617 004830027031 25428 recovercpp568 updating replica status to voting 004830w step 1010 i0617 004830028960 25432 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1874668ms 004830w step 1010 i0617 004830028975 25432 replicacpp320 persisted replica status to voting 004830w step 1010 i0617 004830029007 25432 recovercpp582 successfully joined the paxos group 004830w step 1010 i0617 004830029047 25432 recovercpp466 recover process terminated 004830w step 1010 i0617 004830029209 25430 logcpp553 attempting to start the writer 004830w step 1010 i0617 004830029614 25429 replicacpp493 replica received implicit promise request from 193651723025653790 with proposal 1 004830w step 1010 i0617 004830031486 25429 leveldbcpp304 persisting metadata 8 bytes to leveldb took 1850474ms 004830w step 1010 i0617 004830031502 25429 replicacpp342 persisted promised to 1 004830w step 1010 i0617 004830031726 25431 coordinatorcpp238 coordinator attempting to fill missing positions 004830w step 1010 i0617 004830032245 25428 replicacpp388 replica received explicit promise request from 193661723025653790 for position 0 with proposal 2 004830w step 1010 i0617 004830034101 25428 leveldbcpp341 persisting action 8 bytes to leveldb took 1831441ms 004830w step 1010 i0617 004830034117 25428 replicacpp712 persisted action at 0 004830w step 1010 i0617 004830034561 25433 replicacpp537 replica received write request for position 0 from 193671723025653790 004830w step 1010 i0617 004830034589 25433 leveldbcpp436 reading position from leveldb took 10586ns 004830w step 1010 i0617 004830036419 25433 leveldbcpp341 persisting action 14 bytes to leveldb took 1817267ms 004830w step 1010 i0617 004830036434 25433 replicacpp712 persisted action at 0 004830w step 1010 i0617 004830036679 25429 replicacpp691 replica received learned notice for position 0 from 00000 004830w step 1010 i0617 004830038661 25429 leveldbcpp341 persisting action 16 bytes to leveldb took 196521ms 004830w step 1010 i0617 004830038677 25429 replicacpp712 persisted action at 0 004830w step 1010 i0617 004830038682 25429 replicacpp697 replica learned nop action at position 0 004830w step 1010 i0617 004830038839 25435 logcpp569 writer started with ending position 0 004830w step 1010 i0617 004830039198 25433 leveldbcpp436 reading position from leveldb took 10572ns 004830w step 1010 i0617 004830039412 25433 registrarcpp365 successfully fetched the registry 0b in 13778944ms 004830w step 1010 i0617 004830039448 25433 registrarcpp464 applied 1 operations in 4778ns attempting to update the registry 004830w step 1010 i0617 004830039643 25428 logcpp577 attempting to append 205 bytes to the log 004830w step 1010 i0617 004830039696 25432 coordinatorcpp348 coordinator attempting to write append action at position 1 004830w step 1010 i0617 004830039945 25430 replicacpp537 replica received write request for position 1 from 193681723025653790 004830w step 1010 i0617 004830041738 25430 leveldbcpp341 persisting action 224 bytes to leveldb took 1771112ms 004830w step 1010 i0617 004830041754 25430 replicacpp712 persisted action at 1 004830w step 1010 i0617 004830041977 25432 replicacpp691 replica received learned notice for position 1 from 00000 004830w step 1010 i0617 004830043805 25432 leveldbcpp341 persisting action 226 bytes to leveldb took 1810425ms 004830w step 1010 i0617 004830043820 25432 replicacpp712 persisted action at 1 004830w step 1010 i0617 004830043825 25432 replicacpp697 replica learned append action at position 1 004830w step 1010 i0617 004830044040 25430 registrarcpp509 successfully updated the registry in 4556032ms 004830w step 1010 i0617 004830044100 25430 registrarcpp395 successfully recovered registrar 004830w step 1010 i0617 004830044124 25428 logcpp596 attempting to truncate the log to 1 004830w step 1010 i0617 004830044215 25431 coordinatorcpp348 coordinator attempting to write truncate action at position 2 004830w step 1010 i0617 004830044244 25430 mastercpp1777 recovered 0 agents from the registry 166b  allowing 10mins for agents to reregister 004830w step 1010 i0617 004830044317 25433 hierarchicalcpp169 skipping recovery of hierarchical allocator nothing to recover 004830w step 1010 i0617 004830044497 25433 replicacpp537 replica received write request for position 2 from 193691723025653790 004830w step 1010 i0617 004830046368 25433 leveldbcpp341 persisting action 16 bytes to leveldb took 1851883ms 004830w step 1010 i0617 004830046383 25433 replicacpp712 persisted action at 2 004830w step 1010 i0617 004830046583 25430 replicacpp691 replica received learned notice for position 2 from 00000 004830w step 1010 i0617 004830048426 25430 leveldbcpp341 persisting action 18 bytes to leveldb took 1821628ms 004830w step 1010 i0617 004830048455 25430 leveldbcpp399 deleting 1 keys from leveldb took 14283ns 004830w step 1010 i0617 004830048463 25430 replicacpp712 persisted action at 2 004830w step 1010 i0617 004830048468 25430 replicacpp697 replica learned truncate action at position 2 004830w step 1010 i0617 004830055145 25413 containerizercpp203 using isolation cgroupsmemfilesystemposixnetworkcni 004830w step 1010 i0617 004830058349 25413 linux_launchercpp101 using cgroupfreezer as the freezer hierarchy for the linux launcher 004830w step 1010 i0617 004830069301 25413 clustercpp432 creating default local authorizer 004830w step 1010 i0617 004830069707 25431 slavecpp203 agent started on 4851723025653790 004830w step 1010 i0617 004830069718 25431 slavecpp204 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchycgroup cgroups_limit_swapfalse cgroups_rootmesos_test_d7ff4961cb6d4d51bb2110129a5c5572 container_disk_watch_interval15secs containerizersmesos credentialmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0pcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0pfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0phttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationcgroupsmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirmntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0p 004830w step 1010 i0617 004830069916 25431 credentialshpp86 loading credential for authentication from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0pcredential 004830w step 1010 i0617 004830069967 25431 slavecpp341 agent using credential for testprincipal 004830w step 1010 i0617 004830069984 25431 credentialshpp37 loading credentials for authentication from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0phttp_credentials 004830w step 1010 i0617 004830070050 25431 slavecpp393 using default basic http authenticator 004830w step 1010 i0617 004830070127 25431 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 004830w step 1010 trying semicolondelimited string format instead 004830w step 1010 i0617 004830070282 25431 slavecpp592 agent resources cpus2 mem1024 disk1024 ports3100032000 004830w step 1010 i0617 004830070309 25431 slavecpp600 agent attributes   004830w step 1010 i0617 004830070314 25431 slavecpp605 agent hostname ip17230256mesosphereio 004830w step 1010 i0617 004830070484 25413 schedcpp224 version 100 004830w step 1010 i0617 004830070667 25433 schedcpp328 new master detected at master1723025653790 004830w step 1010 i0617 004830070711 25429 statecpp57 recovering state from mntteamcitytempbuildtmpmemorypressuremesostest_cgroups_root_statistics_af5x0pmeta 004830w step 1010 i0617 004830070749 25433 schedcpp394 authenticating with master master1723025653790 004830w step 1010 i0617 004830070758 25433 schedcpp401 using default crammd5 authenticatee 004830w step 1010 i0617 004830070793 25430 status_update_managercpp200 recovering status update manager 004830w step 1010 i0617 004830070904 25432 authenticateecpp121 creating new client sasl connection 004830w step 1010 i0617 004830070914 25430 containerizercpp518 recovering containerizer 004830w step 1010 i0617 004830071049 25432 mastercpp5943 authenticating scheduler21f8a98862884ec19d6ab66ae746896a1723025653790 004830w step 1010 i0617 004830071105 25428 authenticatorcpp414 starting authentication session for crammd5_authenticatee9841723025653790 004830w step 1010 i0617 004830071164 25434 authenticatorcpp98 creating new server sasl connection 004830w step 1010 i0617 004830071241 25434 authenticateecpp213 received sasl authentication mechanisms crammd5 004830w step 1010 i0617 004830071254 25434 authenticateecpp239 attempting to authenticate with mechanism crammd5 004830w step 1010 i0617 004830071292 25434 authenticatorcpp204 received sasl authentication start 004830w step 1010 i0617 004830071336 25434 authenticatorcpp326 authentication requires more steps 004830w step 1010 i0617 004830071374 25434 authenticateecpp259 received sasl authentication step 004830w step 1010 i0617 004830071553 25434 authenticatorcpp232 received sasl authentication step 004830w step 1010 i0617 004830071574 25434 auxpropcpp107 request to lookup properties for user testprincipal realm ip17230256 server fqdn ip17230256 sasl_auxprop_override false sasl_auxprop_authzid false 004830w step 1010 i0617 004830071586 25434 auxpropcpp179 looking up auxiliary property userpassword 004830w step 1010 i0617 004830071594 25434 auxpropcpp179 looking up auxiliary property cmusaslsecretcrammd5 004830w step 1010 i0617 004830071604 25434 auxpropcpp107 request to lookup properties for user testprincipal realm ip17230256 server fqdn ip17230256 sasl_auxprop_override false sasl_auxprop_authzid true 004830w step 1010 i0617 004830071615 25434 auxpropcpp129 skipping auxiliary property userpassword since sasl_auxprop_authzid  true 004830w step 1010 i0617 004830071619 25434 auxpropcpp129 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true 004830w step 1010 i0617 004830071630 25434 authenticatorcpp318 authentication success 004830w step 1010 i0617 004830071684 25428 authenticatorcpp432 authentication session cleanup for crammd5_authenticatee9841723025653790 004830w step 1010 i0617 004830071687 25431 authenticateecpp299 authentication success 004830w step 1010 i0617 004830071704 25434 mastercpp5973 successfully authenticated principal testprincipal at scheduler21f8a98862884ec19d6ab66ae746896a1723025653790 004830w step 1010 i0617 004830071826 25431 schedcpp484 successfully authenticated with master master1723025653790 004830w step 1010 i0617 004830071841 25431 schedcpp800 sending subscribe call to master1723025653790 004830w step 1010 i0617 004830071954 25431 schedcpp833 will retry registration in 731385085ms if necessary 004830w step 1010 i0617 004830071996 25434 mastercpp2539 received subscribe call for framework default at scheduler21f8a98862884ec19d6ab66ae746896a1723025653790 004830w step 1010 i0617 004830072013 25434 mastercpp2008 authorizing framework principal testprincipal to receive offers for role  004830w step 1010 i0617 004830072180 25430 mastercpp2615 subscribing framework default with checkpointing disabled and capabilities   004830w step 1010 i0617 004830072305 25429 hierarchicalcpp264 added framework 0e92ffa44f264cea84d39c67612de1bd000,2
port mapping isolator may cause segfault if it bind mount root does not exist a check is needed for port mapping isolator for its bind mount root otherwise nonexisted portmapping bind mount root may cause segmentation fault for some cases here is the test log noformat 005742  step 1010  11 tests from portmappingisolatortest 005742  step 1010  run  portmappingisolatortestroot_nc_containertocontainertcp 005742w step 1010 i0604 005742723029 24841 port_mapping_testscpp229 using eth0 as the public interface 005742w step 1010 i0604 005742723348 24841 port_mapping_testscpp237 using lo as the loopback interface 005742w step 1010 i0604 005742735090 24841 resourcescpp572 parsing resources as json failed cpus2mem1024disk1024ephemeral_ports3000130999ports3100032000 005742w step 1010 trying semicolondelimited string format instead 005742w step 1010 i0604 005742736006 24841 port_mappingcpp1557 using eth0 as the public interface 005742w step 1010 i0604 005742736331 24841 port_mappingcpp1582 using lo as the loopback interface 005742w step 1010 i0604 005742737501 24841 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh3  1024 005742w step 1010 i0604 005742737545 24841 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh1  128 005742w step 1010 i0604 005742737578 24841 port_mappingcpp1869 procsysnetipv4tcp_wmem  4096 16384 4194304 005742w step 1010 i0604 005742737608 24841 port_mappingcpp1869 procsysnetipv4tcp_synack_retries  5 005742w step 1010 i0604 005742737637 24841 port_mappingcpp1869 procsysnetcorermem_max  212992 005742w step 1010 i0604 005742737666 24841 port_mappingcpp1869 procsysnetcoresomaxconn  128 005742w step 1010 i0604 005742737694 24841 port_mappingcpp1869 procsysnetcorewmem_max  212992 005742w step 1010 i0604 005742737720 24841 port_mappingcpp1869 procsysnetipv4tcp_rmem  4096 87380 6291456 005742w step 1010 i0604 005742737746 24841 port_mappingcpp1869 procsysnetipv4tcp_keepalive_time  7200 005742w step 1010 i0604 005742737772 24841 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh2  512 005742w step 1010 i0604 005742737798 24841 port_mappingcpp1869 procsysnetcorenetdev_max_backlog  1000 005742w step 1010 i0604 005742737828 24841 port_mappingcpp1869 procsysnetipv4tcp_keepalive_intvl  75 005742w step 1010 i0604 005742737854 24841 port_mappingcpp1869 procsysnetipv4tcp_keepalive_probes  9 005742w step 1010 i0604 005742737879 24841 port_mappingcpp1869 procsysnetipv4tcp_max_syn_backlog  512 005742w step 1010 i0604 005742737905 24841 port_mappingcpp1869 procsysnetipv4tcp_retries2  15 005742w step 1010 f0604 005742737968 24841 port_mapping_testscpp448 check_someisolator failed to get realpath for bind mount root varrunnetns not found 005742w step 1010  check failure stack trace  005742w step 1010  0x7f8bd52583d2 googlelogmessagefail 005742w step 1010  0x7f8bd525832b googlelogmessagesendtolog 005742w step 1010  0x7f8bd5257d21 googlelogmessageflush 005742w step 1010  0x7f8bd525ab92 googlelogmessagefatallogmessagefatal 005742w step 1010  0xa62171 _checkfatal_checkfatal 005742w step 1010  0x1931b17 mesosinternaltestsportmappingisolatortest_root_nc_containertocontainertcp_testtestbody 005742w step 1010  0x19e17b6 testinginternalhandlesehexceptionsinmethodifsupported 005742w step 1010  0x19dc864 testinginternalhandleexceptionsinmethodifsupported 005742w step 1010  0x19bd2ae testingtestrun 005742w step 1010  0x19bda66 testingtestinforun 005742w step 1010  0x19be0b7 testingtestcaserun 005742w step 1010  0x19c4bf5 testinginternalunittestimplrunalltests 005742w step 1010  0x19e247d testinginternalhandlesehexceptionsinmethodifsupported 005742w step 1010  0x19dd3a4 testinginternalhandleexceptionsinmethodifsupported 005742w step 1010  0x19c38d1 testingunittestrun 005742w step 1010  0xfd28cb run_all_tests 005742w step 1010  0xfd24b1 main 005742w step 1010  0x7f8bceb89580 __libc_start_main 005742w step 1010  0xa607c9 _start 005743w step 1010 mntteamcitytempagenttmpcustom_script659125926639545396 line 3 24841 aborted core dumped glog_v1 binmesostestssh verbose gtest_filtergtest_filter 005743w step 1010 process exited with code 134 noformat,3
port mapping isolator may fail in isolate method port mapping isolator may return failure in isolate method if a symlink to the network namespace handle using that containerid already existed we should overwrite the symlink if it exist this affects a couple test failures noformat portmappingisolatortestroot_toomanycontainers portmappingisolatortestroot_containerarpexternal portmappingisolatortestroot_containercmpinternal portmappingisolatortestroot_nc_hosttocontainertcp noformat here is an example failure test log noformat 002837  step 1010  run  portmappingisolatortestroot_toomanycontainers 002837w step 1010 i0606 002837046444 24846 port_mapping_testscpp229 using eth0 as the public interface 002837w step 1010 i0606 002837046728 24846 port_mapping_testscpp237 using lo as the loopback interface 002837w step 1010 i0606 002837058758 24846 resourcescpp572 parsing resources as json failed cpus2mem1024disk1024ephemeral_ports3000130999ports3100032000 002837w step 1010 trying semicolondelimited string format instead 002837w step 1010 i0606 002837059711 24846 port_mappingcpp1557 using eth0 as the public interface 002837w step 1010 i0606 002837059998 24846 port_mappingcpp1582 using lo as the loopback interface 002837w step 1010 i0606 002837061126 24846 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh3  1024 002837w step 1010 i0606 002837061172 24846 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh1  128 002837w step 1010 i0606 002837061206 24846 port_mappingcpp1869 procsysnetipv4tcp_wmem  4096 16384 4194304 002837w step 1010 i0606 002837061256 24846 port_mappingcpp1869 procsysnetipv4tcp_synack_retries  5 002837w step 1010 i0606 002837061297 24846 port_mappingcpp1869 procsysnetcorermem_max  212992 002837w step 1010 i0606 002837061331 24846 port_mappingcpp1869 procsysnetcoresomaxconn  128 002837w step 1010 i0606 002837061360 24846 port_mappingcpp1869 procsysnetcorewmem_max  212992 002837w step 1010 i0606 002837061390 24846 port_mappingcpp1869 procsysnetipv4tcp_rmem  4096 87380 6291456 002837w step 1010 i0606 002837061419 24846 port_mappingcpp1869 procsysnetipv4tcp_keepalive_time  7200 002837w step 1010 i0606 002837061450 24846 port_mappingcpp1869 procsysnetipv4neighdefaultgc_thresh2  512 002837w step 1010 i0606 002837061480 24846 port_mappingcpp1869 procsysnetcorenetdev_max_backlog  1000 002837w step 1010 i0606 002837061511 24846 port_mappingcpp1869 procsysnetipv4tcp_keepalive_intvl  75 002837w step 1010 i0606 002837061540 24846 port_mappingcpp1869 procsysnetipv4tcp_keepalive_probes  9 002837w step 1010 i0606 002837061569 24846 port_mappingcpp1869 procsysnetipv4tcp_max_syn_backlog  512 002837w step 1010 i0606 002837061599 24846 port_mappingcpp1869 procsysnetipv4tcp_retries2  15 002837w step 1010 i0606 002837069964 24846 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 002837w step 1010 i0606 002837070144 24846 resourcescpp572 parsing resources as json failed ports3100031499 002837w step 1010 trying semicolondelimited string format instead 002837w step 1010 i0606 002837070677 24867 port_mappingcpp2512 using nonephemeral ports 3100031500 and ephemeral ports 3020830720 for container container1 of executor  002837w step 1010 i0606 002837071688 24846 linux_launchercpp281 cloning child process with flags  clone_newns  clone_newnet 002837w step 1010 i0606 002837084079 24863 port_mappingcpp2576 bind mounted proc11997nsnet to runnetns11997 for container container1 002837  step 1010 srctestscontainerizerport_mapping_testscpp1438 failure 002837  step 1010 isolatorgetisolatecontainerid1 pidgetfailure failed to symlink the network namespace handle varrunmesosnetnscontainer1  runnetns11997 file exists 002837  step 1010  failed  portmappingisolatortestroot_toomanycontainers 57 ms noformat,3
provide doc examples for dynamic reservationpersistent volumes users have found it difficult to make use of the dynamic reservation and persistent volume features the api governing use of these features is a bit complicated and this leads to users having trouble forming correct requests for reservations volume creation etc providing multiple examples of reserveunreservecreatedestroy requests would make it much easier for users to get started,3
example frameworks should allow setting failover timeout the example frameworks do not currently set a framework failover timeout when they register with the master this means that when these frameworks are used in prolonged testing scenarios small network outages can lead to flapping frameworks we should either set the failover timeout to a reasonable value in the example frameworks or add commandline flags that allow the timeout to be set,2
master captures this when creating authorization callback when exposing its log file the master currently installs an authorization callback for the log file which captures the masters this pointer such captures have previously caused bugs mesos5629 and this one should be fixed as well the callback should be dispatched to the master process and it should be dispatched via the self pid,1
the filesdownload endpoints authorization can be compromised if a forward slash is appended to the path of a file a user wishes to download via filesdownload the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions this is because we store the authorization callbacks for these paths in a map which is keyed by the path name so a request to masterlog fails to find the callback which is installed for masterlog when the master fails to find the callback it assumes authorization is not required for that path and authorizes the action consider the following excerpt code gmanngmacsrcmesosbuild http get http1270015050filesdownloadpathmasterlog a foobar http11 403 forbidden contentlength 0 date wed 22 jun 2016 212853 gmt gmanngmacsrcmesosbuild http get http1270015050filesdownloadpathmasterlog a foobar http11 200 ok contentdisposition attachment filenamemesosmastergmacgmannloginfo2016062214284365615 contentlength 14432 contenttype applicationoctetstream date wed 22 jun 2016 212856 gmt log file created at 20160622 142843 running on machine gmac log line format iwefmmdd hhmmssuuuuuu threadid fileline msg i0622 142843476925 2080764672 loggingcpp194 info level logging started i0622 142843477522 2080764672 maincpp367 using hierarchicaldrf allocator i0622 142843480650 2080764672 leveldbcpp174 opened db in 2961us i0622 142843481046 2080764672 leveldbcpp181 compacted db in 372us i0622 142843481078 2080764672 leveldbcpp196 created db iterator in 13us i0622 142843481096 2080764672 leveldbcpp202 seeked to beginning of db in 9us i0622 142843481111 2080764672 leveldbcpp271 iterated through 0 keys in the db in 8us i0622 142843481165 2080764672 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0622 142843481967 219914240 recovercpp451 starting replica recovery i0622 142843482193 219914240 recovercpp477 replica is in empty status i0622 142843482589 2080764672 maincpp488 creating default local authorizer i0622 142843482719 2080764672 maincpp545 starting mesos master i0622 142843483085 218841088 replicacpp673 replica in empty status received a broadcasted recover request from 41270015050 i0622 142843487284 218304512 recovercpp197 received a recover response from a replica in empty status i0622 142843487694 219914240 recovercpp568 updating replica status to starting code we could consider disallowing paths which end in trailing slashes,2
ssl downgrade support will leak sockets in close_wait status repro steps 1 start a master code binmesosmastersh work_dirtmpmaster code 2 start an agent with ssl and downgrade enabled code  taken from httpmesosapacheorgdocumentationlatestssl openssl genrsa des3 f4 passout passsome_password out keypem 4096 openssl req new x509 passin passsome_password days 365 key keypem out certpem ssl_key_filekeypem ssl_cert_filecertpem ssl_enabledtrue ssl_support_downgradetrue sudo e binmesosagentsh masterlocalhost5050 work_dirtmpagent code 3 start a framework that launches lots of executors one after another code sudo srcballoonframework masterlocalhost5050 task_memory64mb task_memory_usage_limit256mb long_running code 4 check fds repeatedly code sudo lsof i  grep mesos  grep close_wait  wc l code the number of sockets in close_wait will increase linearly with the number of launched executors,5
support file volume in mesos containerizer currently in mesos containerizer the host_path volume to be bind mounted from a host path specified in containerinfo can only be a directory we should also support the volume type as a file,3
quota sorter not updated for resource changes at agent consider this sequence of events 1 slave connects with 128mb of disk 2 master offers resources at slave to framework 3 framework creates a dynamic reservation for 1mb and a persistent volume of the same size on the slaves resources  this invokes masterapply which invokes allocatorupdateallocation which invokes sorterupdate on the framework sorter and role sorter if the frameworks role has a configured quota it also invokes update on the quota role sorter  in this case the frameworks role has no quota so the quota role sorter is not updated  drfsorterupdate updates the total resources at a given slave among updating other state new total resources will be 127mb of unreserved disk and 1mb of reserved disk with a volume note that the quota role sorter still thinks the slave has 128mb of unreserved disk 4 the slave is removed from the cluster hierarchicalallocatorprocessremoveslave invokes code rolesorterremoveslaveid slavesslaveidtotal quotarolesorterremoveslaveid slavesslaveidtotalnonrevocable code slavesslaveidtotalnonrevocable is 127mb of unreserved disk and 1mb of reserved disk with a volume when we remove this from the quota role sorter were left with total resources on the reserved slave of 1mb of unreserved disk since that is the result of subtracting 127mb unreserved 1mb reservedvolume from 128mb unreserved the implications of this cant be good at minimum were leaking resources for removed slaves in the quota role sorter were also introducing an inconsistency between total_resourcesslaveid and total_scalarquantities since the latter has already strippedout volumereservation information,5
create new documentation for mesos networking with introduction of cni and dockers support docker userdefined networks there are quite a few options within mesos for ippercontainer solutions for container networking we therefore need to rewrite networking documentation for mesos highlighting all the networking support that mesos provides for orchestrating containers on ip networks,1
finegrained authorization on frameworks even if acls were defined for the actions view_frameworks view_executors and view_tasks the data these actions were supposed to protect could still leaked through the masters frameworks endpoint since it didnt enable any authorization mechanism,3
zk credential is exposed in flags and state mesos allows zk credentials to be embedded in the zk url but exposes these credentials in the flags and state endpoint even though state is authorized it only filters out frameworkstasks so the toplevel flags are shown to any authenticated user zk zkdcos_mesos_mastermy_secret_password1270012181mesos we need to find some way to hide this data or even add a firstclass view_flags acl that applies to any endpoint that exposes flags,5
get_endpoint_with_path authz doesnt make sense for flags the master or agent flags are exposed in state as well as flags so any user who wants to disablecontrol access to the flags likely intends to control access to flags no matter what endpoint exposes them as such flags is a poor candidate for get_endpoint_with_path authz since we care more about protecting the flag data than the specific endpoint path we should remove the get_endpoint authz from master and agent flags until we can come up with a better solution perhaps a firstclass view_flags acl,2
localauthorizer should error if passed a get_endpoint acl with an unhandled path since get_endpoint_with_path doesnt yet work with any arbitrary path we should a validate acls and error if get_endpoint_with_path has a path object that doesnt match an endpoint that uses this authz strategy b document exactly which endpoints support get_endpoint_with_path,3
add authz to filesdebug the filesdebug endpoint exposes the attached masteragent log paths and every attached sandbox path which includes the frameworkid and executorid even if sandboxes are protected we still dont want to expose this information to unauthorized users,3
authorization for roles the roles endpoint exposes the list of all roles and their weights as well as the list of all frameworkids registered with each role this is a superset of the information exposed on get weights which we already protect we should protect the data in roles the same way  should we reuse view_framework with role from state  should we add a new view_role and adapt get_weights to use it,3
the loggingtoggle endpoint accepts requests with any http method any of a get post put or delete to masterloggingtogglelevelinfoduration5mins will set the log level and return 200 to be consistent with restlike syntax delete get and even post are wrong and should return a methodnotallowed once this endpoint no longer accepts get it is no longer appropriate to use the get_endpoint acl here instead we could create a new put_endpoint_with_path acl which hopefully ignores query params or add a firstclass toggle_logging acl,3
update authorization strings in endpoint help the endpoint help macros support authentication and authorization sections we added authorization help for some of the newer endpoints but not the previously authenticated endpoints authorization endpoints needing help string updates masterhttpcreate_volumes_help masterhttpdestroy_volumes_help masterhttpreserve_help masterhttpstate_help masterhttpstatesummary_help masterhttpteardown_help masterhttptasks_help masterhttpunreserve_help slavehttpstate_help,2
document exactly what is handled by get_endpoints_with_path acl users may expect that the get_endpoint_with_path acl can be used with any mesos endpoint but that is not yet the case we should clearly document the list of applicable endpoints in authorizationmd and probably even upgradesmd,1
add a __sockets__ diagnostic endpoint to libprocess libprocess exposes a endpoint __processes__ which displays some info on the existing actors and messages queued up on each it would be nice to inspect the state of libprocesss socketmanager too this could be an endpoint like __sockets__ that exposes information like  inbound fds type and source  outbound fds type and source  temporary and persistent sockets  linkers and linkees  outgoing messages and their associated socket,3
document docker private registry with authentication support in unified containerizer add documentation for docker private registry with authentication support in unified containerizer this is the basic support for docker private registry,3
cant autodiscovery gpu resources without enablenvidiagpusupport and nvidia_gpu_devices flags prerequisite in mesos5257 by default with no nvidia_gpu_devices flag or gpus resources flag the new autodiscovery will simply enumerate all of the gpus on the system and in mesos5630 removes this flagenablenvidiagpusupport and enables this support for all builds on linux so i configure without any flag and start agent without resources or nvidia_gpu_devices  but can not discovery gpu resources and i also start agent with resources and nvidia_gpu_devices  it also does not work im sure the nvidia gpus on my machines are ok because with enablenvidiagpusupport when configure and with resources nvidia_gpu_devices when starting agents it works well,2
sslenabled libprocess will leak incoming links to forks encountered two different buggy behaviors that can be tracked down to the same underlying problem repro 1 noncrashy 1 start a master doesnt matter if ssl is enabled or not 2 start an agent with ssl enabled downgrade support has the same problem the masteragent link to one another 3 run a sleep task keep this alive if you inspect fds at this point youll notice the task has inherited the link fd master  agent 4 restart the agent due to 3 the masters link stays open 5 check masters logs for the agents reregistration message 6 check the agents logs for reregistration the message will not appear the master is actually using the old link which is not connected to the agent  repro 2 crashy 1 start a master doesnt matter if ssl is enabled or not 2 start an agent with ssl enabled downgrade support has the same problem 3 run 100 sleep task one after the other keep them all alive each task links back to the agent due to an fd leak each task will inherit the incoming links from all other actors 4 at some point the agent will run out of fds and kernel panic  it appears that the ssl socket accept call is missing osnonblock and oscloexec calls httpsgithubcomapachemesosblob4b91d936f50885b6a66277e26ea3c32fe942cf1a3rdpartylibprocesssrclibevent_ssl_socketcppl794l806 for reference heres poll sockets accept httpsgithubcomapachemesosblob4b91d936f50885b6a66277e26ea3c32fe942cf1a3rdpartylibprocesssrcpoll_socketcppl53l75,2
command executor health check does not work when the task specifies container image since we launch the task after pivot_root we no longer has the access to the mesoshealthcheck binary the solution is to refactor health check to be a library libprocess so that it does not depend on the underlying filesystem one note here is that we should strive to keep both the command executor and the task in the same mount namespace so that mesos cli tooling does not need to find the mount namespace for the task it just need to find the corresponding pid for the executor,5
consider allowing the libprocess caller an option to not set cloexec on libprocess sockets both implementations of libprocesss socket interface will set the cloexec option on all new sockets incoming or outgoing this assumption is pervasive across mesos but since libprocess aims to be a generalpurpose library the caller should be able to not cloexec sockets when desired see todos added here httpsreviewsapacheorgr49281,3
consider adding relink functionality to libprocess currently we dont have the relink functionality in libprocess ie a way to create a new persistent connection between actors even if a connection already exists this can benefit us in a couple of ways  the application may have more information on the state of a connection than libprocess does as libprocess only checks if the connection is alive or not for example a linkee may accept a connection then fork pass the connection to a child and subsequently exit as the connection is still active libprocess may not detect the exit  sometimes the exitedevent might be delayed or might be dropped due to the remote instance being unavailable eg partition network intermediaries not sending rsts etc,3
when start an agent with resources the gpu resource can be fractional so far the gpu resource is not fractional only integer values are allowed but when starting agents with resourcesgpu12 it can also work without any warning or error and in the webui the gpu resource is 12,1
potential segfault in link and send when linking to a remote process there is a race in the socketmanager between a remote link and disconnection of the underlying socket we potentially segfault here httpsgithubcomapachemesosblob215e79f571a989e998488077d713c28c7528926e3rdpartylibprocesssrcprocesscppl1512 socket dereferences the shared pointer underpinning the socket object however the code above this line actually has ownership of the pointer httpsgithubcomapachemesosblob215e79f571a989e998488077d713c28c7528926e3rdpartylibprocesssrcprocesscppl1494l1499 if the socket dies during the link the ignore_recv_data may delete the socket underneath link httpsgithubcomapachemesosblob215e79f571a989e998488077d713c28c7528926e3rdpartylibprocesssrcprocesscppl1399l1411  the same race exists for send this race was discovered while running a new test in repetition httpsreviewsapacheorgr49175 on osx i hit the race consistently every 500800 repetitions code 3rdpartylibprocesslibprocesstests gtest_filterprocessremotelinktestremotelink gtest_break_on_failure gtest_repeat1000 code,2
command executor should use mesoscontainerizer launch to launch user task currently command executor and mesoscontainerizer launch share a lot of the logic command executor should in fact just use mesoscontainerizer launch to launch the user task potentially mesoscontainerizer launch can be also used by custom executor to launch user tasks,8
commandinfouser not honored in docker containerizer repro by creating a framework that starts a task with commandinfouser set and observe that the dockerized executor is still running as the default eg root cc kaysoky,3
nvml headers are not installed as part of 3rdparty install with enableinstallmoduledependencies review httpsreviewsapacheorgr49480,2
processremotelinktestremoteusestalelink and remotestalelinkrelink are flaky processremotelinktestremoteusestalelink and processremotelinktestremotestalelinkrelink are failing occasionally with the error code  run  processremotelinktestremotestalelinkrelink warning logging before initgooglelogging is written to stderr i0630 074234661110 18888 processcpp1066 libprocess is initialized on 172170256294 with 16 worker threads e0630 074234666393 18765 processcpp2104 failed to shutdown socket with fd 7 transport endpoint is not connected mesos3rdpartylibprocesssrctestsprocess_testscpp1059 failure value of exitedpidispending actual false expected true  failed  processremotelinktestremotestalelinkrelink 56 ms code there appears to be a race between establishing a socket connection and the test calling shutdown on the socket under some circumstances the shutdown may actually result in failing the future in socketmanagerlink_connect error and thereby trigger socketmanagerclose,1
improve the logic of orphan tasks right now a task is called orphaned if an agent reregisters with it but the corresponding framework information is not known to the master this happens immediately after a master failover it would great if the master knows the information about the framework even after a failover irrespective of whether a framework reregisters so that we dont have orphan tasks getting rid of orphan tasks will make the task authorization story easy see mesos5757,5
add systemgetdriverversion to nvml abstraction this command returns a string representing the version of the underlying nvidia drivers installed on a host it will be used by the upcoming nvidiavolume component,2
missing license information for bundled nvml headers see summary,1
add elfio as bundled dependency to mesos elfio is a headeronly replacement for parsing elf binaries previously we were using libelf which introduced both a new buildtime dependency as well as a runtime dependence even though we only really needed this library when operating on machines that have gpus by using this headeronly library and bundling it with mesos we can remove this external dependence altogether,2
reimplement the stout elf abstraction in terms of elfio with the introduction of the new bundled elfio library we need to reimplement our stout elf abstraction in terms of it as part of this we need to update the tests that use it ie ldcache_testcpp,2
add get_abi_version to elf abstraction in stout this function allows us to inspect the noteabitag section of an elf binary to determine the abi version of the executable  library this is needed for checking soe of the logic in building up an nvidiavolume for injection into a container,2
allow docker v1 imagemanifests to be parsed from the output of docker inspect the dockerspecv1imagemanifest protobuf implements the official v1 image manifest specification found at httpsgithubcomdockerdockerblobmasterimagespecv1md the field names in this spec are all written in snake_case as are the field names of the json representing the image manifest when reading it from disk for example after performing a docker save as such the protobuf for imagemanifest also provides these fields in snake_case unfortunately the docker inspect command also provides a method of retrieving the json for an image manifest with one major caveat  it represents all of its top level keys in camelcase to allow both representations to be parsed in the same way we should intercept the incoming json from either source disk or docker inspect and convert it to a canonical snake_case representation,3
renamed commands to pre_exec_commands in containerlaunchinfo currently the commands in isolatorproto containerlaunchinfo is somehow confusing it is a preexecuted command can be any script or shell command before launch we should renamed commands to pre_exec_commands in containerlaunchinfo and add comments,2
add ability to set framework capabilities in mesosexecute for now we want to add this so that we can run mesosexecute against agents that offer gpu resources in the future as we add more framework capabilities this functionality will become more generally useful,2
consider adding a java scheduler shimadapter for the newold api currently for existing java based frameworks moving to try out the new api can be cumbersome this change intends to introduce a shimadapter interface that makes this easier by allowing to toggle between the oldnew api drivernew scheduler library implementation via an environment variable this would allow framework developers to transition their older frameworks to the new api rather seamlessly this would look similar to the work done for the executor shim for c commanddocker executor,8
add mesos tests to cmake make check provide cmakeliststxt and configuration files to build mesos tests using cmake,8
add ability to inject nvidia devices into a container,3
slaveauthorizertest0viewflags is flaky noformat 152447  step 1010  run  slaveauthorizertest0viewflags 152447w step 1010 i0707 152447025609 25322 containerizercpp196 using isolation posixcpuposixmemfilesystemposixnetworkcni 152447w step 1010 i0707 152447030421 25322 linux_launchercpp101 using sysfscgroupfreezer as the freezer hierarchy for the linux launcher 152447w step 1010 i0707 152447032060 25339 slavecpp205 agent started on 335172302743076 152447w step 1010 i0707 152447032078 25339 slavecpp206 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authentication_backoff_factor1secs authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialmntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5ccredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirmntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5cfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helptrue hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsmntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5chttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmntteamcitywork4240ba9ddd0997c3buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirmntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5c xfs_project_range500010000 152447w step 1010 i0707 152447032306 25339 credentialshpp86 loading credential for authentication from mntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5ccredential 152447w step 1010 i0707 152447032424 25339 slavecpp343 agent using credential for testprincipal 152447w step 1010 i0707 152447032441 25339 credentialshpp37 loading credentials for authentication from mntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5chttp_credentials 152447w step 1010 i0707 152447032528 25339 slavecpp395 using default basic http authenticator 152447w step 1010 i0707 152447032754 25339 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 152447w step 1010 trying semicolondelimited string format instead 152447w step 1010 i0707 152447032838 25339 resourcescpp572 parsing resources as json failed cpus2gpus0mem1024disk1024ports3100032000 152447w step 1010 trying semicolondelimited string format instead 152447w step 1010 i0707 152447032968 25339 slavecpp594 agent resources cpus2 mem1024 disk1024 ports3100032000 152447w step 1010 i0707 152447032994 25339 slavecpp602 agent attributes   152447w step 1010 i0707 152447032999 25339 slavecpp607 agent hostname ip1723027ec2internalmesosphereio 152447w step 1010 i0707 152447033291 25339 processcpp3322 handling http event for process slave335 with path slave335flags 152447w step 1010 i0707 152447033329 25343 statecpp57 recovering state from mntteamcitytempbuildtmpslaveauthorizertest_0_viewflags_osjb5cmeta 152447w step 1010 i0707 152447033576 25342 status_update_managercpp200 recovering status update manager 152447  step 1010 srctestsslave_authorization_testscpp316 failure 152447w step 1010 i0707 152447033604 25340 httpcpp269 http get for slave335flags from 172302733866 152447  step 1010 value of responsegetstatus 152447  step 1010 actual 503 service unavailable 152447w step 1010 i0707 152447033687 25340 containerizercpp522 recovering containerizer 152447  step 1010 expected okstatus 152447  step 1010 which is 200 ok 152447w step 1010 i0707 152447034953 25340 processcpp3322 handling http event for process slave335 with path slave335state 152447  step 1010 agent has not finished recovery 152447  step 1010 srctestsslave_authorization_testscpp320 failure 152447w step 1010 i0707 152447035152 25343 httpcpp269 http get for slave335state from 172302733868 152447  step 1010 parse syntax error at line 1 near agent has not finished recovery 152447w step 1010 i0707 152447035768 25341 slavecpp841 agent terminating 152447w step 1010 i0707 152447036150 25337 provisionercpp253 provisioner recovery complete 152447  step 1010  failed  slaveauthorizertest0viewflags where typeparam  mesosinternallocalauthorizer 14 ms noformat,2
cni isolator should prepare network related etc files for containers using host mode but specify container images currently the cni isolator will just ignore those containers that want to join the host network ie not specifying networkinfo however if the container specifies a container image we need to make sure that it has access to host etc files we should perform the bind mount for the container this is also what docker does when a container is running in host mode,5
masterapitestsubscribe is flaky this test seems to be flaky although on mac os x and centos 7 the error a bit different on mac os x noformat run  contenttypemasterapitestsubscribe0 i0708 114248474665 1927435008 clustercpp155 creating default local authorizer i0708 114248480677 1927435008 leveldbcpp174 opened db in 5727us i0708 114248481494 1927435008 leveldbcpp181 compacted db in 722us i0708 114248481541 1927435008 leveldbcpp196 created db iterator in 19us i0708 114248481572 1927435008 leveldbcpp202 seeked to beginning of db in 9us i0708 114248481587 1927435008 leveldbcpp271 iterated through 0 keys in the db in 7us i0708 114248481617 1927435008 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0708 114248482030 350982144 recovercpp451 starting replica recovery i0708 114248482203 350982144 recovercpp477 replica is in empty status i0708 114248484107 348299264 replicacpp673 replica in empty status received a broadcasted recover request from 378012700150325 i0708 114248484318 350982144 recovercpp197 received a recover response from a replica in empty status i0708 114248484750 348835840 mastercpp382 master e055d60c05ff487e82dad0a43e52605c localhost started on 12700150325 i0708 114248484850 349908992 recovercpp568 updating replica status to starting i0708 114248484788 348835840 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval1secs allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialsprivatetmpsn2kf4credentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue root_submissionstrue user_sorterdrf versionfalse webui_dirusrlocalsharemesoswebui work_dirprivatetmpsn2kf4master zk_session_timeout10secs w0708 114248485263 348835840 mastercpp387  master bound to loopback interface cannot communicate with remote schedulers or agents you might want to set ip flag to a routable ip address  i0708 114248485291 348835840 mastercpp434 master only allowing authenticated frameworks to register i0708 114248485314 348835840 mastercpp448 master only allowing authenticated agents to register i0708 114248485335 348835840 mastercpp461 master only allowing authenticated http frameworks to register i0708 114248485347 348835840 credentialshpp37 loading credentials for authentication from privatetmpsn2kf4credentials i0708 114248485373 349372416 leveldbcpp304 persisting metadata 8 bytes to leveldb took 397us i0708 114248485414 349372416 replicacpp320 persisted replica status to starting i0708 114248485608 350982144 recovercpp477 replica is in starting status i0708 114248485749 348835840 mastercpp506 using default crammd5 authenticator i0708 114248485852 348835840 mastercpp578 using default basic http authenticator i0708 114248486018 348835840 mastercpp658 using default basic http framework authenticator i0708 114248486140 348835840 mastercpp705 authorization enabled i0708 114248486486 350982144 replicacpp673 replica in starting status received a broadcasted recover request from 378312700150325 i0708 114248486758 352055296 recovercpp197 received a recover response from a replica in starting status i0708 114248487176 350982144 recovercpp568 updating replica status to voting i0708 114248487576 352055296 leveldbcpp304 persisting metadata 8 bytes to leveldb took 300us i0708 114248487658 352055296 replicacpp320 persisted replica status to voting i0708 114248487736 350982144 recovercpp582 successfully joined the paxos group i0708 114248487951 350982144 recovercpp466 recover process terminated i0708 114248489441 348835840 mastercpp1973 the newly elected leader is master12700150325 with id e055d60c05ff487e82dad0a43e52605c i0708 114248489518 348835840 mastercpp1986 elected as the leading master i0708 114248489545 348835840 mastercpp1673 recovering from registrar i0708 114248489637 350982144 registrarcpp332 recovering registrar i0708 114248490120 351518720 logcpp553 attempting to start the writer i0708 114248491161 350445568 replicacpp493 replica received implicit promise request from 378412700150325 with proposal 1 i0708 114248491461 350445568 leveldbcpp304 persisting metadata 8 bytes to leveldb took 252us i0708 114248491528 350445568 replicacpp342 persisted promised to 1 i0708 114248492337 348299264 coordinatorcpp238 coordinator attempting to fill missing positions i0708 114248493482 349372416 replicacpp388 replica received explicit promise request from 378512700150325 for position 0 with proposal 2 i0708 114248493854 349372416 leveldbcpp341 persisting action 8 bytes to leveldb took 283us i0708 114248493904 349372416 replicacpp712 persisted action at 0 i0708 114248495302 348299264 replicacpp537 replica received write request for position 0 from 378612700150325 i0708 114248495455 348299264 leveldbcpp436 reading position from leveldb took 45us i0708 114248495761 348299264 leveldbcpp341 persisting action 14 bytes to leveldb took 261us i0708 114248495803 348299264 replicacpp712 persisted action at 0 i0708 114248496484 350445568 replicacpp691 replica received learned notice for position 0 from 00000 i0708 114248496795 350445568 leveldbcpp341 persisting action 16 bytes to leveldb took 255us i0708 114248496857 350445568 replicacpp712 persisted action at 0 i0708 114248496896 350445568 replicacpp697 replica learned nop action at position 0 i0708 114248497445 350982144 logcpp569 writer started with ending position 0 i0708 114248498523 350982144 leveldbcpp436 reading position from leveldb took 80us i0708 114248499307 349908992 registrarcpp365 successfully fetched the registry 0b in 963712ms i0708 114248499464 349908992 registrarcpp464 applied 1 operations in 36us attempting to update the registry i0708 114248499953 351518720 logcpp577 attempting to append 159 bytes to the log i0708 114248500088 350982144 coordinatorcpp348 coordinator attempting to write append action at position 1 i0708 114248500880 348299264 replicacpp537 replica received write request for position 1 from 378712700150325 i0708 114248501186 348299264 leveldbcpp341 persisting action 178 bytes to leveldb took 259us i0708 114248501231 348299264 replicacpp712 persisted action at 1 i0708 114248501786 351518720 replicacpp691 replica received learned notice for position 1 from 00000 i0708 114248502118 351518720 leveldbcpp341 persisting action 180 bytes to leveldb took 311us i0708 114248502260 351518720 replicacpp712 persisted action at 1 i0708 114248502305 351518720 replicacpp697 replica learned append action at position 1 i0708 114248503475 349908992 registrarcpp509 successfully updated the registry in 3944192ms i0708 114248503909 349908992 registrarcpp395 successfully recovered registrar i0708 114248504003 350982144 logcpp596 attempting to truncate the log to 1 i0708 114248504250 349372416 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0708 114248504546 350445568 mastercpp1781 recovered 0 agents from the registry 121b  allowing 10mins for agents to reregister i0708 114248506022 352055296 replicacpp537 replica received write request for position 2 from 378812700150325 i0708 114248506479 352055296 leveldbcpp341 persisting action 16 bytes to leveldb took 320us i0708 114248506513 352055296 replicacpp712 persisted action at 2 i0708 114248506978 351518720 replicacpp691 replica received learned notice for position 2 from 00000 i0708 114248507155 351518720 leveldbcpp341 persisting action 18 bytes to leveldb took 169us i0708 114248507237 351518720 leveldbcpp399 deleting 1 keys from leveldb took 37us i0708 114248507264 351518720 replicacpp712 persisted action at 2 i0708 114248507285 351518720 replicacpp697 replica learned truncate action at position 2 i0708 114248521363 1927435008 clustercpp432 creating default local authorizer i0708 114248522498 350982144 slavecpp205 agent started on 11912700150325 i0708 114248522538 350982144 slavecpp206 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authentication_backoff_factor1secs authorizerlocal container_disk_watch_interval15secs containerizersmesos credentialvarfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirvarfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialsvarfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxhttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_diruserszhitaoubersynczhitaomesos1devubercomhomeubermesosbuildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcescpus2gpus0mem1024disk1024ports3100032000 sandbox_directorymntmesossandbox stricttrue switch_usertrue versionfalse work_dirvarfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndx w0708 114248522903 350982144 slavecpp209  agent bound to loopback interface cannot communicate with remote masters you might want to set ip flag to a routable ip address  i0708 114248522922 350982144 credentialshpp86 loading credential for authentication from varfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxcredential w0708 114248522965 1927435008 schedulercpp157  scheduler driver bound to loopback interface cannot communicate with remote masters you might want to set libprocess_ip environment variable to use a routable ip address  i0708 114248522992 1927435008 schedulercpp172 version 100 i0708 114248523066 350982144 slavecpp343 agent using credential for testprincipal i0708 114248523092 350982144 credentialshpp37 loading credentials for authentication from varfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxhttp_credentials i0708 114248523334 350982144 slavecpp395 using default basic http authenticator i0708 114248523973 352055296 schedulercpp461 new master detected at master12700150325 i0708 114248524050 350982144 slavecpp594 agent resources cpus2 mem1024 disk1024 ports3100032000 i0708 114248524196 350982144 slavecpp602 agent attributes   i0708 114248524224 350982144 slavecpp607 agent hostname localhost i0708 114248525522 350445568 statecpp57 recovering state from varfoldersnytcvyblqj43s2gdh2_895v9nw0000gptcontenttype_masterapitest_subscribe_0_vapndxmeta i0708 114248525853 350445568 status_update_managercpp200 recovering status update manager i0708 114248526165 350445568 slavecpp4856 finished recovery i0708 114248527223 349372416 status_update_managercpp174 pausing sending status updates i0708 114248527231 352055296 slavecpp969 new master detected at master12700150325 i0708 114248527276 352055296 slavecpp1028 authenticating with master master12700150325 i0708 114248527328 352055296 slavecpp1039 using default crammd5 authenticatee i0708 114248527561 352055296 slavecpp1001 detecting new master i0708 114248527582 348299264 authenticateecpp121 creating new client sasl connection i0708 114248528666 349908992 mastercpp6006 authenticating slave11912700150325 i0708 114248528880 352055296 authenticatorcpp98 creating new server sasl connection i0708 114248529089 350445568 httpcpp381 http post for masterapiv1scheduler from 12700150918 i0708 114248529233 350445568 mastercpp2272 received subscription request for http framework default i0708 114248529261 350445568 mastercpp2012 authorizing framework principal testprincipal to receive offers for role  i0708 114248529323 352055296 authenticateecpp213 received sasl authentication mechanisms crammd5 i0708 114248529357 352055296 authenticateecpp239 attempting to authenticate with mechanism crammd5 i0708 114248529417 352055296 authenticatorcpp204 received sasl authentication start i0708 114248529503 352055296 authenticatorcpp326 authentication requires more steps i0708 114248529561 352055296 mastercpp2370 subscribing framework default with checkpointing disabled and capabilities   i0708 114248529721 349908992 authenticateecpp259 received sasl authentication step i0708 114248530005 348835840 authenticatorcpp232 received sasl authentication step i0708 114248530241 348835840 authenticatorcpp318 authentication success i0708 114248530254 350445568 hierarchicalcpp271 added framework e055d60c05ff487e82dad0a43e52605c0000 i0708 114248530900 349908992 authenticateecpp299 authentication success i0708 114248531186 350982144 mastercpp6036 successfully authenticated principal testprincipal at slave11912700150325 i0708 114248531657 348299264 slavecpp1123 successfully authenticated with master master12700150325 i0708 114248531935 349372416 mastercpp4676 registering agent at slave11912700150325 localhost with id e055d60c05ff487e82dad0a43e52605cs0 i0708 114248532304 349908992 registrarcpp464 applied 1 operations in 55us attempting to update the registry i0708 114248532908 348835840 logcpp577 attempting to append 326 bytes to the log i0708 114248533015 352055296 coordinatorcpp348 coordinator attempting to write append action at position 3 i0708 114248533641 349372416 replicacpp537 replica received write request for position 3 from 379812700150325 i0708 114248533867 349372416 leveldbcpp341 persisting action 345 bytes to leveldb took 186us i0708 114248533917 349372416 replicacpp712 persisted action at 3 i0708 114248537066 349908992 replicacpp691 replica received learned notice for position 3 from 00000 i0708 114248538169 349908992 leveldbcpp341 persisting action 347 bytes to leveldb took 914us i0708 114248538226 349908992 replicacpp712 persisted action at 3 i0708 114248538255 349908992 replicacpp697 replica learned append action at position 3 i0708 114248539247 352055296 registrarcpp509 successfully updated the registry in 6895104ms i0708 114248539302 348299264 logcpp596 attempting to truncate the log to 3 i0708 114248539393 348299264 coordinatorcpp348 coordinator attempting to write truncate action at position 4 i0708 114248539798 348835840 mastercpp4745 registered agent e055d60c05ff487e82dad0a43e52605cs0 at slave11912700150325 localhost with cpus2 mem1024 disk1024 ports3100032000 i0708 114248539881 348299264 hierarchicalcpp478 added agent e055d60c05ff487e82dad0a43e52605cs0 localhost with cpus2 mem1024 disk1024 ports3100032000 allocated  i0708 114248539901 349908992 slavecpp1169 registered with master master12700150325 given agent id e055d60c05ff487e82dad0a43e52605cs0 i0708 114248540287 350445568 status_update_managercpp181 resuming sending status updates i0708 114248540501 351518720 replicacpp537 replica received write request for position 4 from 379912700150325 i0708 114248540583 352055296 mastercpp5835 sending 1 offers to framework e055d60c05ff487e82dad0a43e52605c0000 default i0708 114248540798 351518720 leveldbcpp341 persisting action 16 bytes to leveldb took 247us i0708 114248540868 351518720 replicacpp712 persisted action at 4 i0708 114248540895 349908992 slavecpp1229 forwarding total oversubscribed resources i0708 114248541035 352055296 mastercpp5128 received update of agent e055d60c05ff487e82dad0a43e52605cs0 at slave11912700150325 localhost with total oversubscribed resources i0708 114248541291 349908992 hierarchicalcpp542 agent e055d60c05ff487e82dad0a43e52605cs0 localhost updated with oversubscribed resources total cpus2 mem1024 disk1024 ports3100032000 allocated cpus2 mem1024 disk1024 ports3100032000 i0708 114248541630 350982144 replicacpp691 replica received learned notice for position 4 from 00000 i0708 114248541911 350982144 leveldbcpp341 persisting action 18 bytes to leveldb took 189us i0708 114248541965 350982144 leveldbcpp399 deleting 2 keys from leveldb took 28us i0708 114248541987 350982144 replicacpp712 persisted action at 4 i0708 114248542006 350982144 replicacpp697 replica learned truncate action at position 4 i0708 114248544836 352055296 httpcpp381 http post for masterapiv1 from 12700150920 i0708 114248544884 352055296 httpcpp484 processing call subscribe i0708 114248545382 352055296 mastercpp7599 added subscriber a85e7341ac154f1890211a2efa326442 to the list of active subscribers i0708 114248550048 348835840 httpcpp381 http post for masterapiv1scheduler from 12700150919 i0708 114248550339 348835840 mastercpp3468 processing accept call for offers  e055d60c05ff487e82dad0a43e52605co0  on agent e055d60c05ff487e82dad0a43e52605cs0 at slave11912700150325 localhost for framework e055d60c05ff487e82dad0a43e52605c0000 default i0708 114248550390 348835840 mas,3
add a build script for the windows ci the asf ci for mesos runs a script that lives inside the mesos codebase httpsgithubcomapachemesosblob1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1supportdocker_buildsh asf infrastructure have set up a machine that we can use for building mesos on windows considering the environment we will need a separate script to build here,3
include disk source information in stringification some frameworks like kafka_mesos ignore the source field when trying to reserve an offered mount or path persistent volume the resulting error message is bewildering codenone task uses more resources cpus4 mem4096 ports3100031000 diskkafka kafkakafka_0data960679 than available cpus32 mem256819 ports3100032000 diskkafka kafkakafka_0data960679 disk240169 code the stringification of disk resources should include source information,3
support mounting image volume in mesos containerizer mesos containerizer should be able to support mounting image volume type specifically both image rootfs and default manifest should be reachable inside containers mount namespace,5
modularize network in replicated_log currently replicated_log relies on zookeeper for coordinator election this is done through network abstraction _zookeepernetwork_ we need to modularize this part in order to enable replicated_log when using master contenderdetector modules,8
clean up flagsbaseadd in the definition for flagsbase we currently have 20 overloads for the flagsbaseadd function this makes both the flagsbase class definition and the flagscpp files in mesos difficult to read we should clean up flagsbaseadd so that it does not require so many overloads,3
persistentvolumeendpointstestoffercreatethenendpointremove test is flaky observed on asf ci httpsbuildsapacheorgjobmesosbuildtoolautotoolscompilergccconfigurationverboseenvironmentglog_v120mesos_verbose1osubuntu3a1404label_expdocker7c7chadoopubuntuus1ubuntu62497changes code  run  persistentvolumeendpointstestoffercreatethenendpointremove i0713 184355968503 28220 clustercpp155 creating default local authorizer i0713 184356082345 28220 leveldbcpp174 opened db in 113403661ms i0713 184356131445 28220 leveldbcpp181 compacted db in 49034774ms i0713 184356131533 28220 leveldbcpp196 created db iterator in 28012ns i0713 184356131552 28220 leveldbcpp202 seeked to beginning of db in 3046ns i0713 184356131564 28220 leveldbcpp271 iterated through 0 keys in the db in 255ns i0713 184356131614 28220 replicacpp779 replica recovered with log positions 0  0 with 1 holes and 0 unlearned i0713 184356134064 28246 recovercpp451 starting replica recovery i0713 184356134627 28246 recovercpp477 replica is in empty status i0713 184356136396 28252 replicacpp673 replica in empty status received a broadcasted recover request from 9553172170835418 i0713 184356136759 28252 recovercpp197 received a recover response from a replica in empty status i0713 184356137676 28246 recovercpp568 updating replica status to starting i0713 184356148720 28242 mastercpp382 master 2258d072b0c94c40874c6cf933ee345a 500c3e866abe started on 172170835418 i0713 184356148759 28242 mastercpp384 flags at startup acls agent_ping_timeout15secs agent_reregister_timeout10mins allocation_interval50ms allocatorhierarchicaldrf authenticate_agentstrue authenticate_frameworkstrue authenticate_httptrue authenticate_http_frameworkstrue authenticatorscrammd5 authorizerslocal credentialstmplrwrl4credentials framework_sorterdrf helpfalse hostname_lookuptrue http_authenticatorsbasic http_framework_authenticatorsbasic initialize_driver_loggingtrue log_auto_initializetrue logbufsecs0 logging_levelinfo max_agent_ping_timeouts5 max_completed_frameworks50 max_completed_tasks_per_framework1000 quietfalse recovery_agent_removal_limit100 registryreplicated_log registry_fetch_timeout1mins registry_store_timeout100secs registry_stricttrue rolesrole1 root_submissionstrue user_sorterdrf versionfalse webui_dirmesosmesos110_instsharemesoswebui work_dirtmplrwrl4master zk_session_timeout10secs i0713 184356149247 28242 mastercpp434 master only allowing authenticated frameworks to register i0713 184356149265 28242 mastercpp448 master only allowing authenticated agents to register i0713 184356149273 28242 mastercpp461 master only allowing authenticated http frameworks to register i0713 184356149283 28242 credentialshpp37 loading credentials for authentication from tmplrwrl4credentials i0713 184356149780 28242 mastercpp506 using default crammd5 authenticator i0713 184356149940 28242 mastercpp578 using default basic http authenticator i0713 184356150091 28242 mastercpp658 using default basic http framework authenticator i0713 184356150209 28242 mastercpp705 authorization enabled w0713 184356150233 28242 mastercpp768 the roles flag is deprecated this flag will be removed in the future see the mesos 027 upgrade notes for more information i0713 184356150760 28240 hierarchicalcpp151 initialized hierarchical allocator process i0713 184356151018 28249 whitelist_watchercpp77 no whitelist given i0713 184356155668 28242 mastercpp1973 the newly elected leader is master172170835418 with id 2258d072b0c94c40874c6cf933ee345a i0713 184356155781 28242 mastercpp1986 elected as the leading master i0713 184356155848 28242 mastercpp1673 recovering from registrar i0713 184356156065 28254 registrarcpp332 recovering registrar i0713 184356201568 28245 hierarchicalcpp1537 no allocations performed i0713 184356201666 28245 hierarchicalcpp1172 performed allocation for 0 agents in 167962ns i0713 184356218626 28246 leveldbcpp304 persisting metadata 8 bytes to leveldb took 80746657ms i0713 184356218705 28246 replicacpp320 persisted replica status to starting i0713 184356219219 28246 recovercpp477 replica is in starting status i0713 184356221391 28246 replicacpp673 replica in starting status received a broadcasted recover request from 9556172170835418 i0713 184356221869 28253 recovercpp197 received a recover response from a replica in starting status i0713 184356222760 28249 recovercpp568 updating replica status to voting i0713 184356252303 28254 hierarchicalcpp1537 no allocations performed i0713 184356252404 28254 hierarchicalcpp1172 performed allocation for 0 agents in 167038ns i0713 184356270256 28249 leveldbcpp304 persisting metadata 8 bytes to leveldb took 47316392ms i0713 184356270387 28249 replicacpp320 persisted replica status to voting i0713 184356270700 28250 recovercpp582 successfully joined the paxos group i0713 184356271121 28250 recovercpp466 recover process terminated i0713 184356271503 28248 logcpp553 attempting to start the writer i0713 184356273140 28240 replicacpp493 replica received implicit promise request from 9557172170835418 with proposal 1 i0713 184356303086 28254 hierarchicalcpp1537 no allocations performed i0713 184356303175 28254 hierarchicalcpp1172 performed allocation for 0 agents in 155905ns i0713 184356312978 28240 leveldbcpp304 persisting metadata 8 bytes to leveldb took 39718643ms i0713 184356313405 28240 replicacpp342 persisted promised to 1 i0713 184356314775 28245 coordinatorcpp238 coordinator attempting to fill missing positions i0713 184356316547 28250 replicacpp388 replica received explicit promise request from 9558172170835418 for position 0 with proposal 2 i0713 184356354794 28239 hierarchicalcpp1537 no allocations performed i0713 184356354898 28239 hierarchicalcpp1172 performed allocation for 0 agents in 178033ns i0713 184356363484 28250 leveldbcpp341 persisting action 8 bytes to leveldb took 46846904ms i0713 184356363585 28250 replicacpp712 persisted action at 0 i0713 184356365622 28250 replicacpp537 replica received write request for position 0 from 9559172170835418 i0713 184356365727 28250 leveldbcpp436 reading position from leveldb took 45172ns i0713 184356406314 28252 hierarchicalcpp1537 no allocations performed i0713 184356406421 28252 hierarchicalcpp1172 performed allocation for 0 agents in 177001ns i0713 184356421867 28250 leveldbcpp341 persisting action 14 bytes to leveldb took 5606514ms i0713 184356421968 28250 replicacpp712 persisted action at 0 i0713 184356423286 28254 replicacpp691 replica received learned notice for position 0 from 00000 i0713 184356458665 28250 hierarchicalcpp1537 no allocations performed i0713 184356458799 28250 hierarchicalcpp1172 performed allocation for 0 agents in 250863ns i0713 184356470486 28254 leveldbcpp341 persisting action 16 bytes to leveldb took 4713918ms i0713 184356470552 28254 replicacpp712 persisted action at 0 i0713 184356470584 28254 replicacpp697 replica learned nop action at position 0 i0713 184356471782 28247 logcpp569 writer started with ending position 0 i0713 184356475908 28253 leveldbcpp436 reading position from leveldb took 79764ns i0713 184356479058 28247 registrarcpp365 successfully fetched the registry 0b in 322939904ms i0713 184356479388 28247 registrarcpp464 applied 1 operations in 50643ns attempting to update the registry i0713 184356483093 28247 logcpp577 attempting to append 168 bytes to the log i0713 184356483269 28249 coordinatorcpp348 coordinator attempting to write append action at position 1 i0713 184356484673 28245 replicacpp537 replica received write request for position 1 from 9560172170835418 i0713 184356509866 28239 hierarchicalcpp1537 no allocations performed i0713 184356509959 28239 hierarchicalcpp1172 performed allocation for 0 agents in 157789ns i0713 184356512147 28245 leveldbcpp341 persisting action 187 bytes to leveldb took 27358967ms i0713 184356512193 28245 replicacpp712 persisted action at 1 i0713 184356513278 28250 replicacpp691 replica received learned notice for position 1 from 00000 i0713 184356537894 28250 leveldbcpp341 persisting action 189 bytes to leveldb took 24568093ms i0713 184356537973 28250 replicacpp712 persisted action at 1 i0713 184356538008 28250 replicacpp697 replica learned append action at position 1 i0713 184356539737 28252 registrarcpp509 successfully updated the registry in 6026496ms i0713 184356539949 28252 registrarcpp395 successfully recovered registrar i0713 184356540544 28252 mastercpp1781 recovered 0 agents from the registry 129b  allowing 10mins for agents to reregister i0713 184356540832 28250 hierarchicalcpp178 skipping recovery of hierarchical allocator nothing to recover i0713 184356541285 28251 logcpp596 attempting to truncate the log to 1 i0713 184356541637 28248 coordinatorcpp348 coordinator attempting to write truncate action at position 2 i0713 184356542763 28240 replicacpp537 replica received write request for position 2 from 9561172170835418 i0713 184356571691 28240 leveldbcpp341 persisting action 16 bytes to leveldb took 28798341ms i0713 184356571889 28240 replicacpp712 persisted action at 2 i0713 184356573218 28240 replicacpp691 replica received learned notice for position 2 from 00000 i0713 184356620200 28240 leveldbcpp341 persisting action 18 bytes to leveldb took 46927607ms i0713 184356620338 28240 leveldbcpp399 deleting 1 keys from leveldb took 59898ns i0713 184356620512 28240 replicacpp712 persisted action at 2 i0713 184356620630 28240 replicacpp697 replica learned truncate action at position 2 i0713 184356624091 28249 hierarchicalcpp1537 no allocations performed i0713 184356624169 28249 hierarchicalcpp1172 performed allocation for 0 agents in 140818ns i0713 184356628180 28220 containerizercpp196 using isolation posixcpuposixmemfilesystemposixnetworkcni w0713 184356629341 28220 backendcpp75 failed to create aufs backend aufsbackend requires root privileges but is running as user mesos w0713 184356629616 28220 backendcpp75 failed to create bind backend bindbackend requires root privileges i0713 184356631988 28220 clustercpp432 creating default local authorizer i0713 184356635001 28243 slavecpp205 agent started on 251172170835418 i0713 184356635308 28220 resourcescpp572 parsing resources as json failed disk512 trying semicolondelimited string format instead i0713 184356635026 28243 slavecpp206 flags at startup acls appc_simple_discovery_uri_prefixhttp appc_store_dirtmpmesosstoreappc authenticate_httptrue authenticateecrammd5 authentication_backoff_factor1secs authorizerlocal cgroups_cpu_enable_pids_and_tids_countfalse cgroups_enable_cfsfalse cgroups_hierarchysysfscgroup cgroups_limit_swapfalse cgroups_rootmesos container_disk_watch_interval15secs containerizersmesos credentialtmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqcredential default_role disk_watch_interval1mins dockerdocker docker_kill_orphanstrue docker_registryhttpsregistry1dockerio docker_remove_delay6hrs docker_socketvarrundockersock docker_stop_timeout0ns docker_store_dirtmpmesosstoredocker docker_volume_checkpoint_dirvarrunmesosisolatorsdockervolume enforce_container_disk_quotafalse executor_registration_timeout1mins executor_shutdown_grace_period5secs fetcher_cache_dirtmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqfetch fetcher_cache_size2gb frameworks_home gc_delay1weeks gc_disk_headroom01 hadoop_home helpfalse hostname_lookuptrue http_authenticatorsbasic http_command_executorfalse http_credentialstmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqhttp_credentials image_provisioner_backendcopy initialize_driver_loggingtrue isolationposixcpuposixmem launcher_dirmesosmesos110_buildsrc logbufsecs0 logging_levelinfo oversubscribed_resources_interval15secs perf_duration10secs perf_interval1mins qos_correction_interval_min0ns quietfalse recoverreconnect recovery_timeout15mins registration_backoff_factor10ms resourcesdisk1024 revocable_cpu_low_prioritytrue sandbox_directorymntmesossandbox stricttrue switch_usertrue systemd_enable_supporttrue systemd_runtime_directoryrunsystemdsystem versionfalse work_dirtmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxq i0713 184356635709 28243 credentialshpp86 loading credential for authentication from tmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqcredential i0713 184356635892 28243 slavecpp343 agent using credential for testprincipal i0713 184356635924 28243 credentialshpp37 loading credentials for authentication from tmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqhttp_credentials i0713 184356636272 28243 slavecpp395 using default basic http authenticator i0713 184356636615 28243 resourcescpp572 parsing resources as json failed disk1024 trying semicolondelimited string format instead i0713 184356636878 28243 resourcescpp572 parsing resources as json failed disk1024 trying semicolondelimited string format instead i0713 184356637318 28243 slavecpp594 agent resources disk1024 cpus16 mem47270 ports3100032000 i0713 184356637859 28243 slavecpp602 agent attributes   i0713 184356638073 28220 schedcpp226 version 110 i0713 184356638074 28243 slavecpp607 agent hostname 500c3e866abe i0713 184356640148 28253 schedcpp330 new master detected at master172170835418 i0713 184356640650 28253 schedcpp396 authenticating with master master172170835418 i0713 184356640738 28253 schedcpp403 using default crammd5 authenticatee i0713 184356640801 28239 statecpp57 recovering state from tmppersistentvolumeendpointstest_offercreatethenendpointremove_gqstxqmeta i0713 184356640976 28243 authenticateecpp121 creating new client sasl connection i0713 184356641319 28253 status_update_managercpp200 recovering status update manager i0713 184356641477 28243 mastercpp6006 authenticating scheduler398078e06dae4c028197af69d9eb230a172170835418 i0713 184356641636 28239 authenticatorcpp414 starting authentication session for crammd5_authenticatee554172170835418 i0713 184356641542 28240 containerizercpp522 recovering containerizer i0713 184356642201 28239 authenticatorcpp98 creating new server sasl connection i0713 184356642602 28252 authenticateecpp213 received sasl authentication mechanisms crammd5 i0713 184356642634 28252 authenticateecpp239 attempting to authenticate with mechanism crammd5 i0713 184356642714 28239 authenticatorcpp204 received sasl authentication start i0713 184356642792 28239 authenticatorcpp326 authentication requires more steps i0713 184356642882 28239 authenticateecpp259 received sasl authentication step i0713 184356642978 28239 authenticatorcpp232 received sasl authentication step i0713 184356643009 28239 auxpropcpp109 request to lookup properties for user testprincipal realm 500c3e866abe server fqdn 500c3e866abe sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid false i0713 184356643026 28239 auxpropcpp181 looking up auxiliary property userpassword i0713 184356643064 28239 auxpropcpp181 looking up auxiliary property cmusaslsecretcrammd5 i0713 184356643091 28239 auxpropcpp109 request to lookup properties for user testprincipal realm 500c3e866abe server fqdn 500c3e866abe sasl_auxprop_verify_against_hash false sasl_auxprop_override false sasl_auxprop_authzid true i0713 184356643107 28239 auxpropcpp131 skipping auxiliary property userpassword since sasl_auxprop_authzid  true i0713 184356643117 28239 auxpropcpp131 skipping auxiliary property cmusaslsecretcrammd5 since sasl_auxprop_authzid  true i0713 184356643136 28239 authenticatorcpp318 authentication success i0713 184356643290 28239 authenticateecpp299 authentication success i0713 184356643379 28239 mastercpp6036 successfully authenticated principal testprincipal at scheduler398078e06dae4c028197af69d9eb230a172170835418 i0713 184356643501 28244 authenticatorcpp432 authentication session cleanup for crammd5_authenticatee554172170835418 i0713 184356643987 28244 schedcpp502 successfully authenticated with master master172170835418 i0713 184356644011 28244 schedcpp820 sending subscribe call to master172170835418 i0713 184356644103 28244 schedcpp853 will retry registration in 809142674ms if necessary i0713 184356644287 28244 mastercpp2550 received subscribe call for framework default at scheduler398078e06dae4c028197af69d9eb230a172170835418 i0713 184356644346 28244 mastercpp2012 authorizing framework principal testprincipal to receive offers for role role1 i0713 184356644675 28249 provisionercpp253 provisioner recovery complete i0713 184356645089 28245 mastercpp2626 subscribing framework default with checkpointing disabled and capabilities   i0713 184356645783 28249 hierarchicalcpp271 added framework 2258d072b0c94c40874c6cf933ee345a0000 i0713 184356645916 28249 hierarchicalcpp1537 no allocations performed i0713 184356646000 28249 hierarchicalcpp1632 no inverse offers to send out i0713 184356646083 28248 schedcpp743 framework registered with 2258d072b0c94c40874c6cf933ee345a0000 i0713 184356646116 28249 hierarchicalcpp1172 performed allocation for 0 agents in 249850ns i0713 184356646163 28248 schedcpp757 schedulerregistered took 21831ns i0713 184356646317 28246 slavecpp4856 finished recovery i0713 184356663516 28246 slavecpp5028 querying resource estimator for oversubscribable resources i0713 184356664029 28254 status_update_managercpp174 pausing sending status updates i0713 184356664043 28246 slavecpp969 new master detected at master172170835418 i0713 184356664567 28246 slavecpp1028 authenticating with master master172170835418 i0713 184356665148 28246 slavecpp1039 using default crammd5 authenticatee i0713 184356665555 28246 slavecpp1001 detecting new master i0713 184356665590 28244 authenticateecpp121 creating new client sasl connection i0713 184356665889 28246 slavecpp5042 received oversubscribable resources from the resource estimator i0713 184356666071 28253 mastercpp6006 authenticating slave251172170835418 i0713 184356666316 28244 authenticatorcpp414 starting authentication session for crammd5_authenticat,1
the fetcher can access any local file as root the mesos fetcher currently runs as root and does a blind cpchown of any file uri into the tasks sandbox to be owned by the task user even if frameworks are restricted from running tasks as root it seems they can still access rootprotected files in this way we should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as one option would be to run the fetcher as the same user that the task will run as,3
docker health checks are malformed when wrapping the health check command into docker exec docker executor erroneously forms the health check command itself here is an excerpt from an executor log noformat launching health check process docker exec mesos2070f452212045ada8d2a339d234da41s0c27d1b78d4aa424b91fa1e91576db9b5 sh c  true  optmesospherepackagesmesos59d45b30116143cb8d9995ca26f9dec5e93dc710libexecmesosmesoshealthcheck executor110014140651 health_check_jsoncommandshelltruevaluedocker exec mesos2070f452212045ada8d2a339d234da41s0c27d1b78d4aa424b91fa1e91576db9b5 sh c  true consecutive_failures1delay_seconds00grace_period_seconds100interval_seconds50timeout_seconds100 task_idtesthcdb69c60b4a7511e6b9b0c254ada9b06d noformat,1
add a test that runs the mesoslocal binary the balloon framework test runs the mesos master and agent binaries but we dont seem to have any tests which run the mesoslocal binary at the moment such a test should be added or one of the existing example framework tests could be modified to accomplish this,2
cmake build needs to generate protobufs before building libmesos the existing cmake lists place protobufs at the same level as other mesos sources httpsgithubcomapachemesosblobc4cecf9c279c5206faaf996fef0b1810b490b329srccmakeliststxtl415 this is incorrect as protobuf changes need to be regenerated before we can build against them note in the autotools build this is done by compiling protobufs into libmesos which then builds libmesos_no_3rdparty httpsgithubcomapachemesosblobc4cecf9c279c5206faaf996fef0b1810b490b329srcmakefileaml1304l1305,2
create a disk not full example framework we need example frameworks for verifying the correct behavior of posixdisk isolator when the disk quota enforcement is in place one framework for verifying that disk quota enforcement is working and that container gets terminated when it goes beyond disk quota and another one for verifying that container does not get killed if it stays within its disk quota bounds,3
logrotate containerlogger module does not rotate logs when run as root with switch_user the logrotate containerlogger module runs as the agents user in most cases this is root when logrotate is run as root there is an additional check the configuration files must pass because a root logrotate needs to be secured against nonroot modifications to the configuration httpsgithubcomlogrotatelogrotateblobfe80cb51a2571ca35b1a7c8ba0695db5a68feabaconfigcl807l815 log rotation will fail under the following scenario 1 the agent is run with switch_user default true 2 a task is launched with a nonroot user specified 3 the logrotate module spawns a few companion processes as root and this creates the stdout stderr stdoutlogrotateconf and stderrlogrotateconf files as root this step races with the next step 4 the mesos containerizer and fetcher will chown the tasks sandbox to the nonroot user including the files just created 5 when logrotate is run it will skip any nonroot configuration files this means the files are not rotated  fix the logrotate modules companion processes should call setuid and setgid,1
enabling ssl causes fetcher fail to fetch from https sites this is because curl which fetcher relies on also relies on some of the environment variables used by libprocess ssl support for instance ssl_cert_file if the operator sets ssl_cert_file env var for mesos agent the fetcher will inherit this env var and cause curl to fail noformat centosip10100205  ssl_cert_filerundcospkitlscertsmesosslavecrt curl httpsregistry1dockerio443v2libraryalpinemanifestslatest curl 60 ssl certificate problem unable to get local issuer certificate more details here httpscurlhaxxsedocssslcertshtml curl performs ssl certificate verification by default using a bundle of certificate authority ca public keys ca certs if the default bundle file isnt adequate you can specify an alternate file using the cacert option if this https server uses a certificate signed by a ca represented in the bundle the certificate verification probably failed due to a problem with the certificate it might be expired or the name might not match the domain name in the url if youd like to turn off curls verification of the certificate use the k or insecure option centosip10100205  curl httpsregistry1dockerio443v2libraryalpinemanifestslatest errorscodeunauthorizedmessageauthentication requireddetailtyperepositorynamelibraryalpineactionpull noformat to solve this problem we deprecated the existing ssl_ env variables and used libprocess_ssl_ instead to be backward compatible we still accept ssl_ env variables for the time being,3
document mesos_sandbox executor env variable and we should document the difference with mesos_directory,2
only send shutdownframeworkmessage to agents associated with framework slavecpp2079 asked to shut down framework framework by mastermaster slavecpp2094 cannot shut down unknown framework framework for high frameworkchurn clusters this saturates agent logs with these messages when a framework terminates a shutdownframeworkmessage is sent to every registered slave in a for loop this patch proposes sending this message to agents with executors associated with the framework also proposed is moving the logline to vlog1,1
strictregistrartestupdatequota0 is flaky observed on asf ci httpsbuildsapacheorgjobmesosbuildtoolautotoolscompilerclangconfigurationverbose20enablelibevent20enablesslenvironmentglog_v120mesos_verbose1osubuntu1404label_expdocker7c7chadoopubuntuus1ubuntu62539consolefull log file is attached note that this might have been uncovered due to the recent removal of ossleep from clocksettle,3
cgroupsnet_cls isolator causing agent recovery issues we run with cgroupsnet_cls in our isolator list and when we restart any agent process in a cluster running an experimental custom isolator as well the agents are unable to recover from checkpoint because net_cls reports that unknown orphan containers have duplicate net_cls handles while this is a problem that needs to be solved probably by fixing our custom isolator its also a problem that the net_cls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery can this be fixed,1
future_dispatch may react on irrelevant dispatch future_dispatchhttpsgithubcomapachemesosblobe8ebbe5fe4189ef7ab046da2276a6abee41deeb23rdpartylibprocessincludeprocessgmockhppl50 uses dispatchmatcherhttpsgithubcomapachemesosblobe8ebbe5fe4189ef7ab046da2276a6abee41deeb23rdpartylibprocessincludeprocessgmockhppl350 to figure out whether a processed dispatchevent is the same the user is waiting for however comparing stdtype_info of function pointers is not enough different class methods with same signatures will be matched here is the test that proves this noformat class dispatchprocess  public processdispatchprocess  public mock_method0func0 void mock_method1func1 boolbool mock_method1func1_same_but_different boolbool mock_method1func2 futureboolbool mock_method1func3 intint mock_method2func4 futureboolbool int  noformat noformat testprocesstest dispatchmatch  dispatchprocess process piddispatchprocess pid  spawnprocess futurenothing future  future_dispatch pid dispatchprocessfunc1_same_but_different expect_callprocess func1_ willoncereturnarg0 dispatchpid dispatchprocessfunc1 true await_readyfuture terminatepid waitpid  noformat the test passes noformat  run  processtestdispatchmatch  ok  processtestdispatchmatch 1 ms noformat this change was introduced in httpsreviewsapacheorgr28052,5
enhance dispatchevent to include demangled method name currently dispatcheventhttpsgithubcomapachemesosblobe8ebbe5fe4189ef7ab046da2276a6abee41deeb23rdpartylibprocessincludeprocesseventhppl148 does not include any userfriendly information about the actual method being dispatched this can be helpful in order to simplify triaging and debugging eg using __processes__ endpoint now we print the event type onlyhttpsgithubcomapachemesosblobe8ebbe5fe4189ef7ab046da2276a6abee41deeb23rdpartylibprocesssrcprocesscppl3198l3203,5
help endpoint does not set contenttype to html this change added a default contenttype to all responses httpsgithubcomapachemesoscommitb2c5d91addbae609af3791f128c53fb3a26c7d53 unfortunately this changed the help endpoint from no contenttype to textplain for a browser to render this page correctly we need an html content type,1
make the command executor unversioned currently the command executor in srclauncherexecutorcpp is in the v1 namespace as referenced in the versioning design doc we had agreed to keep the mesos internal code in the unversioned namespace and use evolvedevolve helpers for requestsresponses following this pattern we should bring the command executor in the mesosinternal namespace,2
examplestestdiskfullframework fails on arch this test fails consistently on recent arch linux running in a vm,1
stout ostestuser test can fail on some systems libc call getgrouplist doesnt return the gid list in a sorted manner in my case its returning 471 100  whereas id g return a sorted list 100 471 in my case causing the validation inside the loop to fail we should sort both lists before comparing the values,2
ubuntu 1404 lts gpu isolator run directory is noexec in ubuntu 1404 lts the mount for run directory is noexec it affect the varrunmesosisolatorsgpunvidia_35263bin directory which mesos gpu isolators depended on billbillzvarrun mount  grep noexec proc on proc type proc rwnoexecnosuidnodev sysfs on sys type sysfs rwnoexecnosuidnodev devpts on devpts type devpts rwnoexecnosuidgid5mode0620 tmpfs on run type tmpfs rwnoexecnosuidsize10mode0755 the varrun is link to run billbillzvar ll total 52 drwxrxrx 13 root root 4096 may 5 2000  drwxrxrx 27 root root 4096 jul 14 1729  lrwxrwxrwx 1 root root 9 may 5 1950 lock  runlock drwxrwxrx 19 root syslog 4096 jul 28 0800 log drwxrxrx 2 root root 4096 aug 4 2015 opt lrwxrwxrwx 1 root root 4 may 5 1950 run  run current the work around is mount without noexec sudo mount o remountexec run,3
fetcher may print logging error when run as unprivileged user now that the fetcher performs its fetching as the frameworkstasks user when one is specified it prints an error message when its user does not have permissions to create the default glog logging file code i0728 172939337363 25464 loggingcpp194 info level logging started i0728 172939337628 25464 fetchercpp498 fetcher info cache_directorytmpmesosfetchslaves57c21e0d487d46688da6005f13401598s0centositemsactionbypass_cacheuricachefalseexecutablefalseextracttruevaluefilenonrootdirnonroottestsandbox_directoryvarlibmesosslaveslaves57c21e0d487d46688da6005f13401598s0frameworks57c21e0d487d46688da6005f134015980001executorsnonrootsuccessdc9e820d54e811e6b08270b3d5120001runs3cae229f8c6d439e811678bf06ac3731usercentos i0728 172939339738 25464 fetchercpp409 fetching uri filenonrootdirnonroottest i0728 172939339758 25464 fetchercpp250 fetching directly into the sandbox directory i0728 172939339777 25464 fetchercpp187 fetching uri filenonrootdirnonroottest i0728 172939339792 25464 fetchercpp167 copying resource with commandcp nonrootdirnonroottest varlibmesosslaveslaves57c21e0d487d46688da6005f13401598s0frameworks57c21e0d487d46688da6005f134015980001executorsnonrootsuccessdc9e820d54e811e6b08270b3d5120001runs3cae229f8c6d439e811678bf06ac3731nonroottest could not create logging file permission denied could not create a loggingfile 2016072817293925464w0728 172939342435 25464 fetchercpp289 copying instead of extracting resource from uri with extract flag because it does not seem to be an archive filenonrootdirnonroottest i0728 172939342511 25464 fetchercpp547 fetched filenonrootdirnonroottest to varlibmesosslaveslaves57c21e0d487d46688da6005f13401598s0frameworks57c21e0d487d46688da6005f134015980001executorsnonrootsuccessdc9e820d54e811e6b08270b3d5120001runs3cae229f8c6d439e811678bf06ac3731nonroottest  optmesospherepackagesmesos6c64154890d6c22595d7d047193773cda8de6a7clibexecmesosmesoscontainerizer mount helpfalse operationmakerslave path i0728 172939603394 25433 execcpp161 version 100 i0728 172939699053 25490 execcpp236 executor registered on agent 57c21e0d487d46688da6005f13401598s0 code it seems that the fetcher binary is unable to create the default logging file due to a permissions issue however when i set the relevant glog_logtostderrtrue flag which should prevent the creation of this default file it had no effect note that the fetchers logging output was piped to stdoutstderr as expected and the task ran and completed successfully so these errors do not seem to affect the execution of the task,2
agents version flag doesnt work with the removal of the agents default work_dir the version flag no longer works instead the agent complains about the lack of a work_dir and prints the usage instructions,1
enable the upgrade test script to run multiple mastersagents the script designed to test upgrades between different mesos versions supporttestupgradepy should be improved to test upgrades with multiple masters and agents,3
add upgrade testing to the asf ci we should add execution of the supporttestupgradepy script to the asf ci runs this will require having a build of a previous mesos version to run against latest master perhaps we could cache builds of the last stable release somewhere which could be fetched and executed against ci builds,5
incremental http parsing of urls leads to decoder error when requests arrive to the decoder in pieces eg mes followed by a separate chunk of osapacheorg the http parser is not able to handle this case if the split is within the url component this causes the decoder to error out and can lead to connection invalidation the scheduler driver is susceptible to this,3
remove o_sync from statusupdatemanager logs currently the statusupdatemanager uses o_sync to flush status updates to disk we dont need to use o_sync because we only read this file if the host did not crash oswrite success implies the kernel will have flushed our data to the page cache this is sufficient for the recovery scenarios we use this data for,1
nvidiavolumecreate should check for root before creating volume without root we cannot create the nvidia volume in varrunmesos or mount a tmpfs in cases where we need to override the noexec on the current file system,2
docker executor does not use healthchecker library httpsgithubcomapachemesoscommit1556d9a3a02de4e8a90b5b64d268754f95b12d77 refactored health checks into a library command executor uses the library instead of the mesoshealthcheck binary docker executor should do the same for consistency,3
the mesoshealthcheck binary is not used anymore mesos5727 and mesos5954 refactored the health check code into the healthchecker library hence the mesoshealthcheck binary became unused while the command and docker executors could just use the library to avoid the subprocess complexity we may want to consider keeping a binary version that ships with the installation because the intention of the binary was to allow other executors to reuse our implementation on the other side this binary is ill suited to this since it uses libprocess message passing so if we do not have code that requires the binary it seems ok to remove it for now custom executors may use the healthchecker library directly it is not much more complex than using the binary,3
all nonroot tests fail on gpu machine a recent addition to ensure that nvidiavolumecreate ran as root broke all nonroot tests on gpu machines the reason is that we unconditionally create this volume so long as we detect nvmlisavailable which will fail now that we are only allowed to create this volume if we have root permissions we should fix this by adding the proper conditions to determine when  if we should create this volume based on some combination of containerizer and isolation flags,2
